{"data":{"createNewExamAttempt":{"attempt":{"id":"579d905a-2128-44d6-8213-1dcb2d8e46e3"},"exam":{"id":"daa64d8d-8b11-4de5-b471-0e4fb5b97c0b","title":"AWS Certified Solutions Architect – Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"21051f27-9979-4271-a125-129741b7a180","domain":"SecureSolutions","question":"A bucket is configured to use S3 Object Lock in order to meet regulatory requirements and as extra layer of protection against object changes or deletion. Which of the following conditions are true with regards to Amazon S3 Object lock?","explanation":"Amazon S3 Object Lock can be used to prevent an object in the bucket from being overwritten or deleted for a fixed time or indefinitely. It also helps to meet regulatory requirements that require WORM (Write Once Read Many) storage. The Object Locks can be enabled only for new buckets. This enables versioning automatically. The object lock once enabled cannot be disabled. Before you can lock any objects, you have to configure a bucket to use Amazon S3 Object Lock. To do this, you specify when you create the bucket that you want to enable Amazon S3 Object Lock. After you configure a bucket for Amazon S3 Object Lock, you can lock objects in that bucket using retention periods, legal holds, or both.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html","title":"AWS S3 Object Locking"}],"answers":[{"id":"83c8bbb88fd5f45f6ad31d054db35a00","text":"When a bucket is created with Amazon S3 Object Lock enabled, Amazon S3 automatically enables versioning for the bucket","correct":true},{"id":"fb645358786418dc7ef8f85e0c269ff5","text":"Amazon S3 Object Lock works in non-versioned buckets","correct":false},{"id":"f17d34f44f0579dd4acc3d40e0f7b9e7","text":"Once a bucket is created with Amazon S3 Object Lock enabled, Object Lock cannot be disabled or versioning for the bucket cannot be suspended","correct":true},{"id":"332720f0bfcec480380cbebc39917fda","text":"Once a bucket is created with Amazon S3 Object Lock enabled, Object Lock can be disabled at anytime","correct":false},{"id":"a744d40b96b4f9bbf64ef8dc793d1178","text":"Amazon S3 Object Lock can be enabled at individual objects within the buckets without enabling the Object Lock at bucket level","correct":false},{"id":"8db3a24f560fbef1fa711a047cbe3f4b","text":"Amazon S3 Object Lock can only be enabled for new buckets. To turn on Amazon S3 Object Lock for an existing bucket, contact AWS Support","correct":true}]},{"id":"04a5e1e1-9fc6-4371-a53c-cecc6fad3b2a","domain":"ResilientDesign","question":"Elasticity is a fundamental property of the cloud. Which of the following best describes elasticity?","explanation":"In cloud computing, elasticity is defined as 'the degree to which a system is able to adapt to workload changes by provisioning and de-provisioning resources in an autonomic manner, such that at each point in time the available resources match the current demand as closely as possible'.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html","title":"Scalable Computing Capacity"}],"answers":[{"id":"2d60e660232553b4fb11100329fdb97e","text":"The power to scale resources both up and down with changes in demand.","correct":true},{"id":"9b1cc2503b70776f795a2155b7f2e380","text":"The ability to manually deploy instances quickly in response to events.","correct":false},{"id":"80b90762201e3ac35c7921416cc86c81","text":"The power to increase the number of resources at your hands at the click of a mouse.","correct":false},{"id":"71babaf2f5ed118cdbb0f5ff115354ce","text":"The ability to deploy managed services into your environment.","correct":false}]},{"id":"b1f70ce6-0e60-408d-b911-395f6238f5e7","domain":"CostOptimized","question":"Your legal team has just identified a significant confidentiality breach in your web site and you have instructions to take all content down immediately. which of the following statements are correct.","explanation":"While the first 1000 invalidation paths per month are free, additional invalidation paths are charged for per request.  There is a limit of 3000 concurrent individual invalidation, however you can stage them or combine then with wildcard path invalidations.  It takes time for the invalidation instruction to circulate and pull down content from all edge locations.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html","title":"CloudFront Invalidation"},{"url":"https://aws.amazon.com/blogs/aws/simplified-multiple-object-invalidation-for-amazon-cloudfront/","title":"Multiple Object Invalidation for CloudFront"}],"answers":[{"id":"74d845824030304a665a29b3d252dcb6","text":"Versioning object names can be used in place of invalidation if you set it up ahead of time.","correct":true},{"id":"27cf368fc73e24fe8c1fa2823bd75179","text":"Invalidation are effective immediately on request.","correct":false},{"id":"ad95f0498b65e02aed5aa16a59b6c2a6","text":"You cannot invalidate more that 3000 files in CloudFront at a time.","correct":false},{"id":"3e5e72b6232c6205e16547a888294e05","text":"Only under certain circumstances will CloudFront invalidations be charged to your account.","correct":true},{"id":"e21d19a175e9c402d4ce86ca522c4514","text":"Invalidation requests can be cancelled if you issue the cancellation instruction in time.","correct":false}]},{"id":"536131e1-bbd8-4ef5-b765-4bd089487b28","domain":"Performant","question":"Your on-premise servers are running low on disk storage space, but your company is not yet ready for a complete move to the public cloud. You've been tasked with finding an interim storage solution that also offers backup and archiving capabilities. Which AWS service would you recommend to meet this immediate need?","explanation":"Storage Gateway is a storage solution that provides on-premise capacity while taking advantage of some of the benefits of Cloud Storage.","links":[{"url":"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#storage-gateway-cached-concepts","title":"Gateway-Cached Volumes Architecture"}],"answers":[{"id":"759533f205f8af35f7da47dc76331eee","text":"Storage Gateway with Gateway-Stored Volumes.","correct":false},{"id":"04567b32a7bb8490dda99a0fe4c3323a","text":"Storage Gateway with Gateway-Cached Volumes","correct":true},{"id":"cdd0d04de2b79c78e792aec6263d2d3d","text":"DirectConnect","correct":false},{"id":"c0429b6a658dd488f7262d983c7e02bb","text":"Snowball","correct":false}]},{"id":"1b1cfffe-896f-420c-b64e-8eee23af9a3e","domain":"ResilientDesign","question":"You work at a large financial institution.  You have many files that need to be stored for 7 years or more for regulatory purposes. These files need to be stored at the lowest cost possible. It is acceptable to wait for files to become available. Which of the following S3 Storage Tiers is best suited for this request?","explanation":"S3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that won’t be regularly accessed. It is designed for customers — particularly those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors — that retain data sets for 7-10 years or longer to meet regulatory compliance requirements. S3 Glacier Deep Archive can also be used for backup and disaster recovery use cases, and is a cost-effective and easy-to-manage alternative to magnetic tape systems, whether they are on-premises libraries or off-premises services. S3 Glacier Deep Archive complements Amazon S3 Glacier, which is ideal for more active archives where data is regularly retrieved and needed in minutes. All objects stored in S3 Glacier Deep Archive are replicated and stored across at least three geographically-dispersed Availability Zones, protected by 99.999999999% of durability, and can be restored within 12 hours.","links":[{"url":"https://aws.amazon.com/s3/storage-classes/#____","title":"Amazon S3 Storage Classes - Glacier Deep Archive"}],"answers":[{"id":"bd3a39ea89511889f7971688dceeac97","text":"S3 Standard","correct":false},{"id":"e1e33699d68fc57a2e226244acd0ff86","text":"S3 Glacier","correct":false},{"id":"a4172aee8a692bd73f2781afe65fda72","text":"S3 Infrequently Accessed","correct":false},{"id":"a5f6e1bef7eaef71d9ea6446f8c21a2e","text":"S3 Glacier Deep Archive","correct":true}]},{"id":"eb7c5d7d-09d6-4dee-bf55-d97a53af72f2","domain":"SecureSolutions","question":"Your organisation is about to deploy a new website into their AWS environment that will publish news articles created by your content team, which will reside on the URL “www.cloud-news.com”. This website makes use of two EC2 austoscaling groups to serve content - one is for publicly accessible content and one for members-only content. An in-house developed authentication mechanism redirects users to “members.cloud-news.com” to access the members-only content. Which load balancer configuration is most appropriate for this architecture?","explanation":"The load balancer needs to be able to look at the hostname of the request and redirect it to the appropriate EC2 Autoscaling group - this requires the ability to do host-based routing, which is a feature of ALBs and is not available in NLBs. As we are routing to a different hostname, the path is irrelevant - only host-based will work, and the condition for this on an ALB is \"host-header\"","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/ ","title":"Load Balancing Features"}],"answers":[{"id":"9748d8cbee76d8e5ee49130319e6dabc","text":"Use an Application Load Balancer, with a target group for each Autoscaling group.  Configure 2 listeners - one for each content type. Use the “path-pattern” condition on each listener rule to redirect users to the appropriate target group.","correct":false},{"id":"c6e704e1b985b6cd50ffe0d7750681ff","text":"Use a Network Load Balancer in a public subnet, with a target group for each Autoscaling group.  Configure 2 listeners - one for each content type. Use the \"host-header\" condition on each listener rule to redirect users to the appropriate target group.","correct":false},{"id":"1d8d6c68ab28d0288ad4747f3e53f1e4","text":"Use a Network Load Balancer in a public subnet, with a target group for each Autoscaling group.  Configure 2 listeners - one for each content type. Use the \"path-pattern\" condition on each listener rule to redirect users to the appropriate target group.","correct":false},{"id":"512616e676f1cc76a267567948528f9e","text":"Use an Application Load Balancer, with a target group for each Autoscaling group.  Configure 2 listeners - one for each content type. Use the \"host-header\" condition on each listener rule to redirect users to the appropriate target group.","correct":true}]},{"id":"e4b337d8-7452-42a3-a097-309fe4f98437","domain":"ResilientDesign","question":"You have a web application that serves different data to users based on their location. To do this, you are using Geolocation Routing to redirect users to the appropriate load balancer depending on their location around the globe. This seems to work well for most, however you have been getting reports of some users not being able to resolve your DNS address. What is the most likely cause of this?","explanation":"If a user is coming in from a region for which you have not created a record for, or if AWS is not able to identify the region the user is coming in from by their IP, AWS will return whatever appears in the \"Default\" record. If no \"Default\" record is created, the user will receive a \"no answer\" response from AWS, and not be able to resolve the address to your application. There is no such thing as the \"NoRegion\" record type. Although anonymity software may obfuscate the user's IP, the anonymity provider's IP will still be exposed and used to geo-route the user. This may mean they are served content not appropriate for their region, but DNS resolution will still work as intended. There is no need to configure user systems to use AWS's DNS servers, as the user's DNS servers will forward the request to AWS DNS servers automatically as needed.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy"}],"answers":[{"id":"f79e47f5b912f02fdfe5a75fb12b1733","text":"You have not created a \"Default\" record","correct":true},{"id":"f18264809fd609800ffb6fea7a3e7dce","text":"They are running anonymity software that is hiding their IP address","correct":false},{"id":"0b9287eab67b8becd6729727a278f862","text":"You have not created a \"NoRegion\" record","correct":false},{"id":"59ee85167fdecc9091750343ac40ea18","text":"They need to configure their system to use AWS's DNS servers for DNS resolution","correct":false}]},{"id":"f1715a54-ef4a-4912-9093-e8e36698b0c9","domain":"CostOptimized","question":"Your company needs to run several monthly workloads that will each take several hours to complete. Although critical, these workloads can be stopped and restarted without adversely affecting the outcome of the job. Which pricing model would you use to deliver the most economical solution?","explanation":"Spot instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html","title":"About Spot Instances"}],"answers":[{"id":"026db7f2265667575c05772f32778b8a","text":"Spot Instances","correct":true},{"id":"29068f6044e3fedf44165e646a2d2bb1","text":"Reserved Instances","correct":false},{"id":"de53d38fe38e0fce729f15c292a59891","text":"Free-Tier Instances","correct":false},{"id":"c658c72ec41cc513ad91a3f3e6d2c060","text":"On-demand Instances","correct":false}]},{"id":"05ab8679-89d2-4db7-83f2-6cd0a315ae13","domain":"CostOptimized","question":"Amazon Web Services offers 4 different levels of support. Which of the following are valid support levels?","explanation":"The correct answers are Enterprise, Business, Developer. The 4th level is Basic.  Remember that Free Tier is a billing rebate not an account type or support level.","links":[{"url":"https://aws.amazon.com/premiumsupport/compare-plans/","title":"AWS Support Plans"}],"answers":[{"id":"a0f27638260642afb474516cc918407e","text":"Enterprise","correct":true},{"id":"d6e6cb19e3b9c02f89d6cd54cfa7c613","text":"Business","correct":true},{"id":"672caf27f5363dc833bda5099775e891","text":"Developer","correct":true},{"id":"5ef2f1c1df8b6b0fed2186a0abe18f50","text":"Free Tier","correct":false},{"id":"7effe80425095de4d5b996a01e4f00a3","text":"Corporate","correct":false}]},{"id":"6360570c-c801-4b78-a0ba-7860817cb309","domain":"ResilientDesign","question":"You have a busy media website that runs on a fleet of EC2 instances behind an application load balancer. You have a number of different target groups for different purposes. One of these target groups is a fleet of EC2 instances which contains the images for your website. When ever a user visits www.yoursite.com/images/ you need your application load balancer to direct the request to the images target group. How do you configure this rule on your application load balancer?","explanation":"One of the major benefits of teh ALB is that it supports 'path-based' routing which allows you to direct the traffic based on the content of the URL path.  In this case /images/ can be directed to a specific target group.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-update-rules.html","title":"ALB Listener Rules"},{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html","title":"What Is an Application Load Balancer?"}],"answers":[{"id":"86e943f32a460169646a1a9a54de8934","text":"Using Sticky Sessions.","correct":false},{"id":"002f909cd633d40a7e860e8793272fe4","text":"Using Cross Zone Load Balancing.","correct":false},{"id":"5a3ddbcafc8cd528ecdb152c181daba6","text":"Using Path Patterns.","correct":true},{"id":"4d15809e9acf1ee9617c2e118157323e","text":"Using Query String Parameters.","correct":false}]},{"id":"2387590d-58f0-42dc-8a72-2c2c9581ce6c","domain":"Performant","question":"A floral company's online ordering website has experienced regular seasonal volumes for the past few years. They've been able to predict demand and size their Amazon Aurora PostgreSQL primary instance appropriately. Recently however, the marketing department has been running campaigns that have created out-of-season spikes in volume. Marketing is unable to predict when these spikes might occur. What should they do to scale their Aurora instances?","explanation":"Aurora Serverless is an on-demand, auto-scaling configuration that will scale capacity up or down based on an application's needs. Aurora Auto Scaling is also a good solution, but it requires that the original cluster have at least one Aurora Replica. In our case, there is only a primary instance. Resizing for the highest anticipated volume will result in paying for excess capacity outside of peak times, and implementing elasticity in a script ignores Aurora's managed service capabilities.","links":[{"url":"https://aws.amazon.com/rds/aurora/serverless/","title":"Amazon Aurora Serverless"}],"answers":[{"id":"105fdf6f9a9095f2e3f47979bc4b566c","text":"Resize your Aurora instance for the highest anticipated volume","correct":false},{"id":"a00427752c554276f4d87c57aca5e455","text":"Create a script to monitor Amazon CloudWatch metrics and resize the instance as volume ramps up","correct":false},{"id":"3cf376bdff842161a4edbe7bfd76ef87","text":"Restore a snapshot of the Aurora cluster to an Aurora Serverless DB cluster","correct":true},{"id":"45fe8af0b244d4ff0ab30fd4213428d2","text":"Turn on Aurora Auto Scaling to elastically provision and decommission Aurora Replicas as needed","correct":false}]},{"id":"dbb146aa-ae1d-411e-82be-4777bd07c916","domain":"Performant","question":"A financial market dashboard needs to update asset values almost instantaneously for customers across the United States. Updates will be written to the primary application instance which resides in the AWS us-east-1 region. Which database architecture will provide the best performance for consumers of the dashboard's information?","explanation":"With Aurora MySQL you can configure cross-region Aurora Replicas using logical replication to up to five secondary AWS regions. Aurora PostgreSQL currently does not support cross-region replicas. Aurora Replica physical replication can only replicate to one secondary region. Using Aurora over RDS provides multiple read replicas in the deployment region and other benefits automatically without having to configure them.","links":[{"url":"https://aws.amazon.com/rds/aurora/?nc=sn&loc=0","title":"Amazon Aurora"},{"url":"https://aws.amazon.com/rds/aurora/faqs/?nc=sn&loc=6","title":"Amazon Aurora FAQs - High Availability and Replication"}],"answers":[{"id":"f95722da1c54e4a7b38d775dec9d3952","text":"Implement Amazon Aurora MySQL with Aurora Replicas using cross-region physical replication. Create the replicas in the AWS us-east-2 and us-west-2 regions.","correct":false},{"id":"91857d4bed60ff0818ab1d95e0314b9c","text":"Deploy Amazon Aurora MySQL with Aurora Replicas using cross-region logical replication. Create the replicas in the AWS us-east-2 and us-west-2 regions.","correct":true},{"id":"31441ec91d70f043876be523dca440b3","text":"Use Amazon RDS PostgreSQL with read replicas. Create the replicas in the AWS us-east-1, us-east-2, and us-west-2 regions.","correct":false},{"id":"a3f216bc2a3134a725e71894673ef3cd","text":"Deploy Amazon Aurora PostgreSQL with Aurora Replicas using cross-region logical replication. Create the replicas in the AWS us-east-2 and us-west-1 regions.","correct":false}]},{"id":"e8d01592-5b1f-418e-84e9-fcc23e15ae0d","domain":"Performant","question":"You need to store files as objects in Amazon S3. Which AWS service provides that ability?","explanation":"You will need to go with Storage Gateway, which gives you the option of creating a file gateway so that S3 can support files. EFS provides file storage, but without the compatibility with S3. True to its naming, EBS is a block-level storage service. And RDS is for creating relational databases, not file gateways.","links":[{"url":"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html","title":"What Is AWS Storage Gateway?"}],"answers":[{"id":"5ba49687f2f5ade4cd1c756a7e9024e3","text":"AWS Storage Gateway","correct":true},{"id":"d2a6652ddeb631da029d1f2806e11fdc","text":"Amazon Elastic File System (EFS)","correct":false},{"id":"3e8f51149454b27e82ddd26c897a4167","text":"Amazon Relational Database Service (RDS)","correct":false},{"id":"9155453f43b8a6472df0b8ffa5b5a028","text":"Amazon Elastic Block Storage (EBS)","correct":false}]},{"id":"8ce0cff9-4e62-41cb-8edd-a45f0b2a2bd3","domain":"Performant","question":"You have a application that is running in an EC2 instance that performs some heavy processing on sales data stored in S3. This sales data is first loaded into memory and numerous operations are performed on it before it is written back to S3. During the processing phase, a large amount of temporary data is created which is not needed once processing completes. This data needs to be stored on as low-latency storage as possible - which of the below storage types should you use?","explanation":"Although all 4 options would work, Instance Store has the lowest latency as it is located on the same physical infrastructure as the EC2 instance. As data permanency is not required, Instance Store is the best choice.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html","title":"AWS Instance Store"}],"answers":[{"id":"248c1b0bcb74394beb4a08030c4a6847","text":"EBS","correct":false},{"id":"41c96096fbbf551daa42cd6455c15603","text":"Instance Store","correct":true},{"id":"4867860f253fdfd13af73b9154d1199b","text":"S3 Intelligent Tiering","correct":false},{"id":"43fd7af2adc3101adebb61366bf16df2","text":"Provisioned IOPS SSD","correct":false}]},{"id":"27f1f472-2ea9-43e0-b75d-6be01c620049","domain":"CostOptimized","question":"You have a website that allows users in third world countries to store their important documents safely and securely online. Internet connectivity in these countries is unreliable, so you implement multipart uploads to improve the success rate of uploading files. Although this approach works well, you notice that when an object is not uploaded successfully, incomplete parts of that object are still being stored in S3 and you are still being charged for those objects. What S3 feature can you implement to delete incomplete multipart uploads?","explanation":"You can create a lifecycle policy that expires incomplete multipart uploads, allowing you to save on costs by limiting the time non-completed multipart uploads are stored.","links":[{"url":"https://aws.amazon.com/blogs/aws/s3-lifecycle-management-update-support-for-multipart-uploads-and-delete-markers/","title":"S3 Lifecycle Management - Incomplete Multipart Uploads"}],"answers":[{"id":"86d93427d74df0b30713341818e3d556","text":"S2 Reduced Redundancy Storage","correct":false},{"id":"5ff7e884d027004938c218aafa63c215","text":"S3 Lifecycle Policies","correct":true},{"id":"ee966c2ebd84a4f7add1d9ceabe082c9","text":"Have S3 trigger DataPipeling Auto-delete.","correct":false},{"id":"bc5e8477b1dde2747a8cb281160b01f7","text":"Have CloudWatch trigger a Lambda function that deletes the S3 data.","correct":false}]},{"id":"ce84785e-7010-4243-99f8-3a1ab7d43e31","domain":"ResilientDesign","question":"You work for a cosmetic company which has their production website on AWS. The site itself is in a two-tier configuration with web servers in the front end and database servers at the back end. The site uses using Elastic Load Balancing and Auto Scaling. The databases maintain consistency by replicating changes to each other as and when they occur. This requires the databases to have extremely low latency. Your website needs to be highly redundant and must be designed so that if one availability zone goes offline and Auto Scaling cannot launch new instances in the remaining Availability Zones, the site will not go offline. How can the current architecture be enhanced to ensure this?","explanation":"Deploy your site in three different AZs within the same region. Configure the Auto Scaling minimum to handle 50 percent of the peak load per zone. So, if one AZ goes down, the two remaining AZs can each accommodate 50% of your traffic, giving you 100% coverage.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/latest/userguide/as-register-lbs-with-asg.html","title":"How To Set Up a Scaled and Load-Balanced Application"}],"answers":[{"id":"1dd3aa163155f9eafe380ab0c55794fe","text":"Deploy your website in 2 different regions. Configure Route53 with Weighted Routing. Assign a weight of 25% to region 1 and a weight of 75% to region 2.","correct":false},{"id":"89990e4ab47ac0e1aa67ae395fa51530","text":"Deploy your site in three different AZs within the same region. Configure the Auto Scaling minimum to handle 50 percent of the peak load per zone.","correct":true},{"id":"311b1d3a4668bfd8843e4d5dac1e52d5","text":"Deploy your site in three different AZs within the same region. Configure the Auto Scaling minimum to handle 33 percent of the peak load per zone.","correct":false},{"id":"0dfa307c14f360c0a222400c5eecda8d","text":"Deploy your website in 2 different regions. Configure Route53 with a failover routing policy, and set up health checks on the primary site.","correct":false}]},{"id":"c9b8d0e7-d31d-4641-a11c-28681f078732","domain":"SecureSolutions","question":"A new team member hasn't used AWS before and is interested in learning more about its global infrastructure and related concepts. Select all valid and correct statements.","explanation":"No VPC peering is required for S3 cross-region replication. Since July 2017, Amazon Virtual Private Cloud allows customers to create a new default VPC directly from the console or by using the CLI.  Osaka is a special Region that has only one AZ available for consumer use.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/07/create-a-new-default-vpc-using-aws-console-or-cli/","title":"Create a New Default VPC using AWS Console or CLI"},{"url":"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html","title":"What is VPC Peering?"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html","title":"Cross-Region Replication"},{"url":"https://aws.amazon.com/about-aws/global-infrastructure/","title":"AWS Global Infrastructure"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-regions-availability-zones","title":"Region and Availability Zone Concepts"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html","title":"Regions and Availability Zones"},{"url":"https://aws.amazon.com/vpc/faqs/","title":"Amazon VPC FAQs"}],"answers":[{"id":"f8e30527ccced5256b677cdda7901c9c","text":"After creating a VPC, you can add one or more subnets in each Availability Zone. Each subnet must reside entirely within one Availability Zone and cannot span zones.","correct":true},{"id":"5d9924102f5bb35d22225ddaead7bb7b","text":"An Availability Zone (AZ) is an isolated location within a region. While AWS operates some with only one EC2 Availability Zone, other regions have between two and six AZs.","correct":true},{"id":"f0007b62472d187cfae568a97cfdfd27","text":"It is not possible to create a new default VPC directly from the console if the default VPC in a region has been deleted. Instead, you need to contact AWS support and raise a ticket for that.","correct":false},{"id":"fd6d1a12cbfa90d38b2d701360224d21","text":"For cross-region replication to work, you must set up a peering connection between the two relevant VPCs in each region","correct":false}]},{"id":"adbe2585-a484-493a-bec9-ed54d0ff4672","domain":"SecureSolutions","question":"You have a website that contains public content and member-only content. These are being served from 2 different auto-scaling EC2 instance groups, one for members and one for non-members. Once a member logs in, they are re-directed to a \"members.myawesomesite.com\" URL. You would like to put a single load balancer in front of both groups to direct users as appropriate - how would you design this?","explanation":"Both the network load balancer and classic load balancer do not support layer-7, and therefore cannot help route based on the hostname. Although having 2 load balancers and using R53 would work, the scenario asks for 1 if possible - and this is possible using an application load balancer as these operate at Level 7","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/","title":"Elastic Load Balancing features"}],"answers":[{"id":"bae789245cb0d3f5b010dd39d2af14bb","text":"Use a classic load balancer with URL based routing configured","correct":false},{"id":"18e6dd30c5604d6a1b97f7f9fd9b21f3","text":"Use an application load balancer with host-based routing configured","correct":true},{"id":"8ab4d2b861c5ab884c9ef4cbdebe221a","text":"It is not possible to have multiple auto-scaling groups attached to one load balancer, so 2 load balancers will we required, then use R53 to point to the appropriate load balancer based on URL.","correct":false},{"id":"f3551559eb91b2d8b762d7c8fd38a725","text":"Use a network load balancer with domain-based routing enabled","correct":false}]},{"id":"c487c002-443f-4d86-bdde-a915adb7924d","domain":"ResilientDesign","question":"You have chosen to use S3 - OneZone-IA with your cloud application. Which limitations have you considered in doing so?","explanation":"In exchange for a significant cost savings, 1Zone-IA has the same Durability as S3, but a lower Availability SLA.","links":[{"url":"https://aws.amazon.com/s3/storage-classes/?nc=sn&loc=3","title":"S3 Storage Classes"}],"answers":[{"id":"4068f35933f215b2818a03234567736a","text":"1Zone-IA requires supplementary Access Control Lists.","correct":false},{"id":"3ffe3458e13ccc426e49a1893b78a3f3","text":"1Zone-IA has a 3 - 5 hour data recovery windows.","correct":false},{"id":"e0a1c69e07da2a370da0dedf28084491","text":"1Zone-IA offers only 99.50% durability. Therefore you have to design your application to re-create any objects that may be lost.","correct":false},{"id":"bcddbe2d89ec46d45172296d890040c3","text":"1Zone-IA is available only in the US-STANDARD region.","correct":false},{"id":"a86f1cf2d99a9643587b837f464b5ef8","text":"1Zone-IA offers only 99.50% availability. Therefore you have to design your application to re-create any objects that may be temporally unavailable.","correct":true}]},{"id":"b21cf2a7-0cf1-47d4-a0c2-60403bb9cf37","domain":"CostOptimized","question":"To stay within the AWS Free Tier using Amazon EC2 for the first 12 months of having an AWS account, which of the following instance types should you use?","explanation":"One of the EC2 requirements for staying within the AWS Free Tier is using EC2 micro instances only. That makes t2.micro and t3.micro the correct responses.","links":[{"url":"https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc","title":"AWS Free Tier"}],"answers":[{"id":"1d4f2c610dbeb44e7ba09fed19564c76","text":"t2.micro","correct":true},{"id":"affa6cb0576af5aa6e603780fe7b203c","text":"t3a.small","correct":false},{"id":"7d3869f3c790e32d408d21d331095b0b","text":"t3.micro","correct":true},{"id":"ab61127647912c159c3fc08e9a102efc","text":"t2.small","correct":false}]},{"id":"772793a3-5d9f-43bd-bb76-179e8dbc8253","domain":"ResilientDesign","question":"An enterprise is looking to implement Amazon SQS as a messaging service to integrate multiple application components which are hosted in AWS. Which of the following are true about Amazon SQS?","explanation":"Amazon SQS stores all messages within a region and can store messages across AZs within the region. Data transfer between Amazon SQS and Amazon EC2 or AWS Lambda within a single region is free.","links":[{"url":"https://aws.amazon.com/sqs/","title":"Application Integration Service"}],"answers":[{"id":"0648fd50f7cdc581d31b8dfe410ce275","text":"Amazon SQS stores all messages and message queues within single AWS region within an Availability Zone.","correct":false},{"id":"d9860cb4ca904902096df550eb91902e","text":"Data transfer cost between Amazon SQS and Amazon EC2 or AWS Lambda within a single region incur a standard data transfer rate.","correct":false},{"id":"93aceaf2019b6438fdc9de7c79d18dee","text":"Amazon SQS stores all messages and message queues across several highly-available AWS regions with multiple redundant Availability Zones.","correct":false},{"id":"6e7cab4c58ce36e1f4e1acc709c9996e","text":"Data transfer cost between Amazon SQS and Amazon EC2 or AWS Lambda within a single region is free.","correct":true},{"id":"fb65f20dafe54ebea9013ceb9e10b8d2","text":"Amazon SQS stores all messages and message queues within a single highly-available AWS region with multiple redundant Availability Zones.","correct":true}]},{"id":"3072d04e-e5b2-42a6-97d7-dbb17155071d","domain":"SecureSolutions","question":"You have an EC2 Instance with an EIP allocated sitting in a Public subnet in your VPC. This instance is serving web content, and you want to make sure that users on the Internet can only access it via ports 80 and 443. Which of the below options lets you achieve this?","explanation":"DENY Rules cannot be created for security groups - so all options where this is mentioned can be ignored. With inbound traffic, NACLs are evaluated first - so an NACL with a default deny rule will block all incoming traffic before it reaches the instance - so this is not the correct option. This leaves creating an allow rule for the instance's security group as the correct answer.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html","title":"VPC Security"}],"answers":[{"id":"f57136f22cc8a2fb5ed83f93962dd750","text":"Create an NACL with a default allow rule on incoming traffic. Create a security group with a DENY rule for all ports except 80 & 443 and attach it to the instance.","correct":false},{"id":"a910e411655dbf1d19c15b2690076861","text":"Create and NACL with a default deny rule on incoming traffic. Create a security group with an ALLOW rule for ports 80 & 443 and attach it to the instance.","correct":false},{"id":"68d0d074ef501849c8908fe33b77b37f","text":"Create a security group with an ALLOW rule for ports 80 & 443 and attach it to the instance","correct":true},{"id":"3802db4fb97fa84757d5b21b38464fc4","text":"Create a security group with an ALLOW rule for ports 80 & 443, and a DENY Rule for all other ports. Attach it to the instance","correct":false}]},{"id":"5deb4341-6227-4888-a90f-7d60c7a1d98b","domain":"SecureSolutions","question":"From the command line, which of the following should you run to get the public hostname of an EC2 instance?","explanation":"You would use the command: curl http://169.254.169.254/latest/meta-data/public-hostname","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html","title":"Instance Metadata and User Data"}],"answers":[{"id":"6c35b09a9009a5dac8ac16ad9b47b8f4","text":"curl http://254.169.254.169/latest/meta-data/public-hostname","correct":false},{"id":"1d247c911fbeac310a9432b2783691a1","text":"curl http://169.254.169.254/latest/meta-data/public-hostname","correct":true},{"id":"c956ddf5572e1ce76e0c9822361df9d6","text":"curl http://169.254.169.254/latest/user-data/public-hostname","correct":false},{"id":"c3d29bd94a60071732654a962c3f9893","text":"curl http://254.169.254.169/latest/user-data/public-hostname","correct":false}]},{"id":"181cab5b-23af-4947-8821-51ed16f55a7d","domain":"ResilientDesign","question":"A large jewelry distributor has installed their new inventory application in a development environment on AWS. After completing their testing, they're ready to deploy the application into its production environment. They've been using VPN connections for the development phase, but they need to upgrade to a higher resiliency network connection scheme to communicate back and forth from other on-premises business applications that are mirrored across two data centers. Testing results indicate that some transactions may require more than a 1.25 Gbps connection to ensure a quality customer experience. Which network architecture will provide the appropriate resiliency for this inventory application?","explanation":"For critical production workloads like an inventory application that require high resiliency, it is recommended to have one connection at multiple locations. Such a topology ensures resilience to connectivity failure due to a fiber cut or a device failure as well as a complete location failure. Use of a Direct Connect Gateway will provide access to any AWS Region from any Direct Connect location. Installing separate connections terminating on separate devices in more than one location gives another layer of resiliency, but that configuration, along with its added costs, is not necessary for this use case. Creating separate connections to only a single Direct Connect location from a single data center does not mitigate the risk of full facility outages. AWS doesn't recommend using a VPN as a backup for connections that require speeds greater than 1 Gbps.","links":[{"url":"https://aws.amazon.com/directconnect/","title":"AWS Direct Connect"},{"url":"https://aws.amazon.com/directconnect/resiliency-recommendation/","title":"AWS Direct Connect Resiliency Recommendations"}],"answers":[{"id":"8a967bd089e29673bac3c13fd00105ca","text":"Implement an AWS Direct Connect connection from one data center to an AWS Direct Connect location. Establish a VPN connection from the other data center as a backup.","correct":false},{"id":"508aa0084ed6faa994e17273231899f1","text":"Create an AWS Direct Connect connection from one data center to an AWS Direct Connect location. Install another AWS Direct Connect connection from the other data center to a different AWS Direct Connect location.","correct":true},{"id":"a4c6c0827f35637ae33dfd03b1b3e0f6","text":"Configure two AWS Direct Connect connections to an AWS Direct Connect location from two different network devices in one data center.","correct":false},{"id":"6dd9941a3ca38128754b0fb66f8642cb","text":"Install two AWS Direct Connect connections to an AWS Direct Connect location from two different network devices in one data center. Create another two AWS Direct Connect connections to a different AWS Direct Connect location from two different network devices in the other data center.","correct":false}]},{"id":"9cfdc945-a7e0-479c-9bcc-973f9ebfd7dc","domain":"CostOptimized","question":"You want to set up 2 CloudWatch alarms in addition to the 6 you already have to monitor your cloud environment. It has been 13 months since you created your AWS account, and you want to avoid being charged for creating the alarms. What should you do?","explanation":"Upon signing up for an AWS account, you will get a range of service usage that will never cost you anything. Such offers include 10 alarms with CloudWatch. That’s why creating the alarms is the correct answer. Contacting support is wrong because it’s not necessary to request a service increase limit. Avoiding creating new alarms is also wrong because there’s no term limits on the CloudWatch alarm offer; it’s always free. Creating more alarms in this case is still free of charge because the free offer is limited to 10 CloudWatch alarms.","links":[{"url":"https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsm.page-all-free-tier=1","title":"AWS Free Tier"}],"answers":[{"id":"3e9a7ae3a0c82f4aaf031d7400a3774f","text":"Do not create the alarms; you will be charged, since you get a maximum of 10 alarms with CloudWatch for the first 12 months after your account sign-up.","correct":false},{"id":"222fcbee1f68dae67b4406597659a622","text":"Contact AWS Support for a service increase limit.","correct":false},{"id":"a336fb6001d622e126c7d02da7ab218f","text":"Go ahead and create the alarms; CloudWatch alarms are always free of charge, regardless of number.","correct":false},{"id":"d8991b206aa9d731ab999eddedf26d6e","text":"Go ahead and create the alarms; you can have up to 10 CloudWatch alarms without being charged.","correct":true}]},{"id":"9da56843-5a00-40d2-91e6-9043e665cd1d","domain":"ResilientDesign","question":"What type of replication is supported by Multi-AZ RDS instances?","explanation":"Multi-AZ deployments utilize synchronous replication, making database writes concurrently on both the primary and standby so that the standby will be up-to-date in the event a failover occurs.","links":[{"url":"https://aws.amazon.com/rds/details/multi-az/","title":"RDS Multi-AZ Synchronous Replication"}],"answers":[{"id":"2dba9be51ba243660abd8717959eb4b3","text":"Sequential replication","correct":false},{"id":"b56265ceb6311e42003e668438136a59","text":"Synchronous replication","correct":true},{"id":"4350a58e8d877c13ae12d0a4aa1c3f2f","text":"Continuous replication","correct":false},{"id":"18807bab4162a8219f67943263205f38","text":"Asynchronous replication","correct":false}]},{"id":"3252d84d-08d9-4bde-a8e7-d716502d1855","domain":"Performant","question":"You have a very heavily-trafficked WordPress blog that has approximately 95% read traffic and 5% write traffic. You notice that the blog is getting slower and slower. You discover that the bottleneck is in your RDS instance. Which of the following answers can improve your WordPress blog's performance?","explanation":"You should use a combination of Read Replicas and ElastiCache to help offload the traffic.","links":[{"url":"https://aws.amazon.com/elasticache/","title":"About ElastiCache"}],"answers":[{"id":"fdc556bb3ab9b5da3b290c181aaefb3c","text":"Create a number of read replicas and update the connection string on your EC2 instances so that traffic is evenly shared amongst these new RDS instances.","correct":true},{"id":"1c551a09129057627b3b75fb70e6f527","text":"Export the database to DynamoDB which has push button scalability.","correct":false},{"id":"e94a05a7348f87c7b9c4f7036d632a9c","text":"Use ElastiCache to cache the most commonly read posts of your WordPress blog.","correct":true},{"id":"1af32ee0c62b109e45b92828dbc33f2d","text":"Create a secondary Multi-AZ database and run the queries off the secondary Multi-AZ database.","correct":false}]},{"id":"cdb5b6d9-37bb-41e5-bd69-985dabcc7bd0","domain":"SecureSolutions","question":"You are hosting a web application that runs on a number of Web Servers in public subnets and Database Servers in private subnets. A NAT Instance is being used for connectivity to the internet for the Private Subnets. The NAT Instance is now becoming a bottleneck, and you are looking to replace it with NAT Gateway. Which of the following would ensure high availability for the NAT Gateway?","explanation":"If you have resources in multiple Availability Zones and they share one NAT gateway, in the event that the NAT gateway’s Availability Zone is down, resources in the other Availability Zones lose internet access. To create an Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html","title":"NAT Gateways"}],"answers":[{"id":"b0b35801e71ae33d446780112d8438dc","text":"Deploy a NAT Gateway in 2 Availability Zones","correct":true},{"id":"18c2392152bcb34e4f0933dfcff0de2b","text":"Deploy a NAT Gateway in 2 Regions","correct":false},{"id":"8956081d09cf7a5203cbb2fe0a9cbd9e","text":"Deploy a NAT Gateway along with the NAT Instance","correct":false},{"id":"5ae78abee9f8fe1afe2ab443a4480098","text":"Disable source/destination check on the NAT Instances","correct":false}]},{"id":"6f99dc29-9e5a-4d5c-9152-c4e9d5e2325c","domain":"SecureSolutions","question":"When making use of EC2 instances on Dedicated Hosting, which of the following modes are you able to transition between by stopping the instance and starting it again?","explanation":"The tenancy of an instance can only be change between variants of 'dedicated' tenancy hosting. It cannot be changed from or to default tenancy hosting.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html","title":"About Dedicated Instances and Tenancy"}],"answers":[{"id":"aa095ba3b4f590c56ebe3dd7632cf1d4","text":"Host & Default","correct":false},{"id":"d31d964b101ff444e700016740c91a5e","text":"Default & Dedicated","correct":false},{"id":"d15424a6445328926f6ca57c29e143ca","text":"Host & Dedicated","correct":true},{"id":"04459d2ad5ea321a35a184d5db44d7ec","text":"Dedicated & Host","correct":true}]},{"id":"04438f2e-6164-444a-97e6-2927410fe048","domain":"Performant","question":"You are running a production database using MySQL on RDS. From time to time, management asks you to run highly complex SQL queries with multiple table joins against the database. These queries often overwhelm your database, and the production environment is beginning to be affected. Which of the following would you recommend as a means of reducing the load on the database?","explanation":"You cannot run queries off a multi-AZ secondary copy database. You should use a read replica instead. ","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.Create","title":"Creating a Read Replica"}],"answers":[{"id":"4e27b92377dbee3d162ff925d504ed0d","text":"Migrate the database to DynamoDB which will scale automatically in order to deal with the load.","correct":false},{"id":"50bb0b55997bb31f998633d055ae5877","text":"Use Route53 health checks to determine the current load on the database and if there is a minimum load , configure the health check to run the SQL queries.","correct":false},{"id":"1af32ee0c62b109e45b92828dbc33f2d","text":"Create a secondary Multi-AZ database and run the queries off the secondary Multi-AZ database.","correct":false},{"id":"d893386d6e1344432c82b5b5863162d6","text":"Create a read replica of the database and run your reports against the read replica, rather than against the production database.","correct":true}]},{"id":"1ea8aa1e-9454-49b0-b4e2-379eadf674e0","domain":"CostOptimized","question":"You are an employee at a communications firm that is in the process of migrating its data to Amazon S3. The data will be stored in buckets and is sent to customers to do as they see fit. However, certain data is frequently changed when customers request revisions, while the rest of the data is rarely changed. You must be able to immediately access certain data while minimizing costs. Which S3 storage class should you choose?","explanation":"While S3 Glacier is a low-cost storage class, it is for data archiving and thus not ideal for frequent access or changes to data. And S3 One Zone-Infrequent Access is also low-cost, but it does not address the frequently changed data. Although S3 Standard is a suitable choice, since it addresses frequent access, it is not the least expensive choice for the less frequently accessed data. If it was hard to determine which data is frequently changed and which isn’t, S3 Standard might have been the most cost-effective choice. But in this case, S3 Intelligent Tiering is. Intelligent Tiering stores data in two access tiers: one tier is optimized for frequently accessed data while the other is a lower-cost tier for infrequent access.","links":[{"url":"https://aws.amazon.com/s3/storage-classes/","title":"S3 Storage Classes"}],"answers":[{"id":"e1e33699d68fc57a2e226244acd0ff86","text":"S3 Glacier","correct":false},{"id":"bd3a39ea89511889f7971688dceeac97","text":"S3 Standard","correct":false},{"id":"5605213ade8877d8d601580dbd0a8aa2","text":"S3 One Zone-Infrequent Access","correct":false},{"id":"4867860f253fdfd13af73b9154d1199b","text":"S3 Intelligent Tiering","correct":true}]},{"id":"2466b024-1f81-11ea-978f-2e728ce88125","domain":"ResilientDesign","question":"You work as a website administrator at a real estate developer. The company’s website uses S3 to store pictures of the single-family homes it builds. The company recently released a brand-new elevation for one of its most popular models, which is called 'Greenberry C.' So far, there’s only one picture of the 'Greenberry C', so you want to ensure that it is not accidentally deleted by enabling the object lock feature. Which of the following actions will accomplish that?","explanation":"Amazon S3 object lock prevents an object from being deleted or overwritten. Object lock is enabled at the bucket level; when creating the bucket, you can select the feature to lock objects in it. However, once the bucket has been created, you cannot enable object lock, you will have to contact customer support to do so. Right-click is not a valid option - you must select the object then go to Properties, Object lock.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/object-lock.html","title":"How Do I Lock an Amazon S3 Bucket?"}],"answers":[{"id":"2c6b543047434bd3ad31060693bc8e5b","text":"Enable object lock at the object level.","correct":false},{"id":"49cd0706698c83f66fe5b9deb203a420","text":"Contact customer support.","correct":true},{"id":"84bd7d36c453cdfc1f04955deb54c376","text":"Enable object lock at the bucket level.","correct":true},{"id":"a92ccdd56e18b47fbb5d18c2c342ca6f","text":"Right-click the picture and choose the object lock option.","correct":false}]},{"id":"1ad4b101-8078-4f3c-86bf-2f865ee09a54","domain":"CostOptimized","question":"A government agency has a regulatory mandate that all archived data must be preserved exclusively in a non-rewriteable and non-erasable format. What solution satisfies this requirement in the most cost-effective way?","explanation":"Amazon S3 versioning does not protect object versions from being deleted and is therefore an incorrect solution. Amazon S3 bucket policies do not satisfy the requirement and is not the correct solution. Amazon S3 bucket policies can be changed, thus removing protection on the objects. Implementing Amazon S3 Object Locks is incorrect because it is not the most cost-effective solution. The question specifically asks about archived data. Storing archived data in Amazon S3 Glacier is more cost-effective than Amazon S3. Amazon S3 Glacier is the most cost-effective storage solution for archive data. Amazon S3 Glacier Vault Lock can be used to implement a 'Write-Once-Read-Many' archive storage solution.","links":[{"url":"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html","title":"Amazon S3 Glacier Vault Lock"},{"url":"https://aws.amazon.com/blogs/aws/glacier-vault-lock/","title":"Create Write-Once-Read-Many Archive Storage with Amazon Glacier"}],"answers":[{"id":"c367faed034f6f722c499e129d8ca5dd","text":"Implement Amazon S3 Bucket Policy with deny statements for object delete operations.","correct":false},{"id":"6a730079c6c38844cd80386edd5634f3","text":"Enable Amazon S3 Versioning.","correct":false},{"id":"554acd3f3712f630497b0180dbb5124c","text":"Implement Amazon S3 Glacier Vault Lock.","correct":true},{"id":"1430bbbd409858a9a820458c0f311d39","text":"Enable Amazon S3 Object Lock.","correct":false}]},{"id":"57656407-2a67-4ea5-81df-4b7b319a9450","domain":"Performant","question":"You want to upload a huge PDF document that will take up to 10 GB into an S3 bucket. However, you get an error message that says that your proposed upload exceeds the maximum allowed object size. What solution to this problem does AWS recommend? ","explanation":"If the PDF document was 5 GB or less, you could have simply uploaded it with a single PUT operation. In this case, though, you will have to upload it in parts with the Multipart Upload API. This solution is ideal for any object with a size of up to 5 TB.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html","title":"Amazon S3 - Uploading Objects"}],"answers":[{"id":"3df7f898acb2aad7ce50a73f5332fa5d","text":"Contact your company’s in-house Tech Support to resolve the issue.","correct":false},{"id":"c2fea743c25a16765101875568e6bff0","text":"Try uploading the document again; the error message is a mere glitch.","correct":false},{"id":"9790be20f3dfa9ae057e64412d53890a","text":"Split the document into parts and use the Multipart Upload API.","correct":true},{"id":"eb52b711b029fe92eaa3076758d8c782","text":"Create a ticket in your AWS account to request an increase in maximum object size.","correct":false}]},{"id":"c4f7d0f0-4b57-41b4-a5b1-109004f29af1","domain":"Performant","question":"A CRM application runs in your data center and you have disaster recovery instances running in the AWS cloud. The on-premises CRM application requires low-latency access to all of the storage, as the entire dataset is accessed frequently. Which architecture will provide the highest performance efficiency for your business?","explanation":"Storage Gateway Volume Gateway in the stored volume configuration keeps the entire dataset on-premises and asynchronously backs up point-in-time snapshots to Amazon S3. This provides low-latency access to the entire dataset on-premises. The snapshots can be recovered to EC2 instances. Storage Gateway Volume Gateway in cached mode keeps frequently accessed data on-premises, so it would perform many S3 reads to access the entire dataset. Using an on-premises backup solution or a Storage Gateway file gateway would require the EC2 instances to use different storage access techniques than the on-premises environment.","links":[{"url":"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html","title":"What Is AWS Storage Gateway"}],"answers":[{"id":"6c5b3c0e5b4db806f92ab9d4ab500667","text":"Implement a Storage Gateway Volume Gateway in stored mode and perform snapshots","correct":true},{"id":"27f55f1ffd3c7df753670476a47be8d9","text":"Use your on-premises backup solution to move the data to S3 each night for use by the EC2 instances in a disaster scenario","correct":false},{"id":"86480a8866121094f21ba24ad4a8c13c","text":"Implement a Storage Gateway Volume Gateway in cached mode and perform snapshots","correct":false},{"id":"2913578801d8d764ddbef31fe86af4b2","text":"Deploy a Storage Gateway file gateway as a network file share","correct":false}]},{"id":"c5cc330a-0f8f-11ea-8d71-362b9e155667","domain":"ResilientDesign","question":"You recently got hired by a sole proprietor specializing in baking and selling oatcakes within the state of Maryland, which is in the East Coast of the United States. The sole proprietor is ready to launch a website to expand her business online and sell on a national scale. She wants assurance that the website is always available to customers throughout the United States. Using Amazon Route 53 and EC2, which of the following is the best course of action?","explanation":"Ideally, you should architect AWS usage to take advantage of multiple Regions and Availability Zones. Based on the client’s demands, you need an active-passive failover configuration within the United States — not between the United States and Singapore, for example.  So, setting up a failover routing policy for the website with both EC2 instances in North American Regions and Availability Zones is the correct option. With Maryland falling within the US East Region, the secondary resources can be deployed in the US West Region for coast-to-coast national coverage. A simple routing policy won’t work, since it distributes web traffic randomly. And while geolocation routing can address the client’s national reach plans, it will not address the website’s resiliency.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive","title":"Active-Passive Failover"}],"answers":[{"id":"5aa9a31751ffd108c0b6c3dd7bf7a4c2","text":"Set up a geolocation routing policy in Route 53 for the website that directs traffic to the EC2 instance in the us-east 1 Availability Zone as the area where the company is based. It will failover to the EC2 instance in the us-west-1 Availability Zone as the secondary resource when necessary.","correct":false},{"id":"67671c325b8941bda2a15e76be801e2d","text":"Set up a failover routing policy in Route 53 for the website that has an EC2 instance in the us-east-1 Availability Zone as the primary resource and another EC2 instance in the us-west-1 Availability Zone as the secondary resource.","correct":true},{"id":"80fe4ea40ff6dc32a5c8e993ca1f435c","text":"Set up a geolocation routing policy in Route 53 for the website that directs traffic to the EC2 instance in the us-east 1 Availability Zone as the area where the company is based. It will failover to the EC2 instance in the ap-southeast-1 Availability Zone as the secondary resource when necessary.","correct":false},{"id":"00bd2e799600e5dc9ac2ed23e17b593a","text":"Set up a failover routing policy in Route 53 for the website that has an EC2 instance in the us-east-1 Availability Zone as the primary resource and another EC2 instance in the eu-west-2 Availability Zone as the secondary resource.","correct":false},{"id":"eb41eb5a739a5015ab0b9122db69f20a","text":"Set up a simple routing policy in Route 53 for the website that switches between the EC2 instance launched in the us-east 1 Availability Zone and a second EC2 instance launched in the us-west-1 Availability Zone.","correct":false}]},{"id":"d0946223-5972-4d84-b34d-e2529e3fc7e1","domain":"Performant","question":"You have a set of read only data on an EBS st1 volume which needs to be referenced by all the EC2 instances in an autoscaling groups.  Which of these are valid options ?","explanation":"Not all types of EBS volumes can be mounted to more than one instance at a time.  However Snapshots can be used to create copies. The better option would probably be EFS.  Storing the data set on S3 would also work, however the CLI script offered is not valid as the source and destination are reversed.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html#EBSFeatures","title":"EBS Volumes: Data Availability"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html","title":"AWS S3 CLI cp"},{"url":"https://aws.amazon.com/blogs/aws/new-multi-attach-for-provisioned-iops-io1-amazon-ebs-volumes/","title":"Multi-Attach for Amazon EBS Volumes"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html","title":"Restore and mount a snapshot"}],"answers":[{"id":"07edfbbebc085c363f396930d13af41c","text":"Mount the EBS volume to all the EC2 instances using 'Multi-Attach for EBS'.","correct":false},{"id":"0f813c39a0b5e5a3c85eb5f9c6fa147f","text":"Create the a Snapshot of the Master copy at regular intervals.  Then restore and mount the latest snapshot to the EC2 instances as they are brought on-line by autoscaling.","correct":true},{"id":"2868667ad6244fef7d80dcf63a2ecf56","text":"Create an EFS volume and migrate the data to the EFS instance.  Then mount the EFS volume to the EC2 instances as they are brought on-line by autoscaling.","correct":true},{"id":"db01b924625278a0a9fe4b1627595467","text":"Create a copy of the data set in an S3 bucket from time to time. Then use the script 'aws S3 cp <LocalPath> <S3Uri> --recursive' to copy the files onto the EC2 instances as they are brought on-line by autoscaling.","correct":false}]},{"id":"737e3562-19f3-11ea-978f-2e728ce88125","domain":"Performant","question":"Which of the following AWS services enables on-premises applications to use AWS Cloud storage?","explanation":"Although all four responses are similar in that they are AWS storage services, it is Storage Gateway that enables on-premises applications to use cloud-based storage. EFS is for simple, scalable file storage, EBS serves as a virtual disk for virtual servers launched with EC2, and S3 is for object-based storage.","links":[{"url":"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html","title":"What Is AWS Storage Gateway?"}],"answers":[{"id":"df346128c45efa43efc29607224fc716","text":"Amazon Simple Storage Service (S3)","correct":false},{"id":"d2a6652ddeb631da029d1f2806e11fdc","text":"Amazon Elastic File System (EFS)","correct":false},{"id":"9155453f43b8a6472df0b8ffa5b5a028","text":"Amazon Elastic Block Storage (EBS)","correct":false},{"id":"5ba49687f2f5ade4cd1c756a7e9024e3","text":"AWS Storage Gateway","correct":true}]},{"id":"2ad5b0b6-36ed-43f0-9464-2c438e5d3e76","domain":"ResilientDesign","question":"Over the past month the production environment made up of Classic Load Balances and an autoscaling web farm has failed to scale up resulting in massive disruption during the early morning peak load. Your engineering team do not want to be alerted about every change but agree that they should receive relevant SNS alerts for customer impacting problems. Which of the following are appropriate autoscaling SNS alerts to send?","explanation":"AWS are completely transparent about he fact that systems will fail and you need to design for failures. there are four standard SNS alerts of which the LAUNCH_ERROR is the is the most important for being aware of impending customer impacting problems. The offered ELB Errors are CloudWatch metrics not built in SNS notifications.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/latest/userguide/AutoScalingGroup.html","title":"Auto Scaling Groups"},{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ASGettingNotifications.html","title":"Autoscaling SNS notifications."},{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html","title":"CloudWatch Metrics for Your Classic Load Balancer"}],"answers":[{"id":"d04f697bc72c89bc10704e46aa085b2a","text":"autoscaling:ELB_SPILL_OVER_COUNT_ERROR","correct":false},{"id":"965b81ae140ff5103117c17a9f69e13b","text":"autoscaling:EC2_INSTANCE_LAUNCH_ERROR","correct":true},{"id":"abc6e1c7ee610d21427d2f9354cf3a94","text":"autoscaling:EC2_INSTANCE_TERMINATE","correct":false},{"id":"aac32c743fd98355a24d9989737e40b5","text":"autoscaling:EC2_INSTANCE_LAUNCH","correct":false},{"id":"99c3fbb9dd68cd0e89c066e4489c56d6","text":"autoscaling:ELB_SURGE_QUEUE_LENGTH_ERROR","correct":false},{"id":"89a683aa5c68153ea037ac7daf704a13","text":"autoscaling:EC2_INSTANCE_TERMINATE_ERROR","correct":false}]},{"id":"2403cf92-cd58-4ade-8a82-52be2b2a6b5b","domain":"SecureSolutions","question":"Your Security team is concerned about a recent spate of large-scale DDoS attacks on other providers in your industry. You have a number of internet-exposed services in your business, and any potential outage has significant financial impact. The security team wants to be informed of any attack as it happens, and would like some assistance from AWS to help mitigate an attack should one happen. You currently have WAF deployed, and an Enterprise support agreement in place, but which of the below extra steps would you recommend","explanation":"AWS Shield Standard does not include notification of any attacks detected, therefore can be eliminated straight away. Although WAF can be used during a DDoS attack to help mitigate the attack with custom block rules, there are no in-built DDoS protections with WAF as these are provided by Shield. AWS has a dedicated DDoS Response Team (DRT) to assist during any DDoS attacks - however in order to access them, you need to be on an Enterprise or Business support agreement, and relevant to this scenario, have purchased Shield Advanced. This combined with the alerting of attacks that is available with Shield Advanced make purchasing Shield Advanced the most appropriate choice. ","links":[{"url":"https://aws.amazon.com/shield/getting-started/","title":"Getting Started with AWS Shield"},{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-overview.html#ddos-drt","title":"How AWS Shield Works"}],"answers":[{"id":"191789dacb98c2ed977f4a62437a00af","text":"Enable the inbuilt AWS WAF DDoS protections, use SNS to notify when an attack is detected. During an attack lodge a support request using your Enterprise support agreement for assistance","correct":false},{"id":"b41f4d516374b164d0a2c054a39dfe3f","text":"Enable rate limiting on your load balancers, and during an attack lodge a support request using your Enterprise support agreement for assistance","correct":false},{"id":"b8ede9969b12d6122fcef9029ff00b52","text":"By default all AWS services are automatically protected against DDoS attacks by AWS Shield - nothing extra needs to be done. During an attack lodge a support request using your Enterprise support agreement for assistance","correct":false},{"id":"6afd0c59a36c013760fbb187d8bbb415","text":"Purchase AWS Shield Advanced, and during an attack lodge a support request asking for assistance from AWS","correct":true}]},{"id":"62c40f57-a8f9-4280-849b-34440bbcbef9","domain":"SecureSolutions","question":"Which of the following is an invalid VPC peering configuration?","explanation":"Edge-to-edge routing is not allowed through a VPN connection.","links":[{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/invalid-peering-configurations.html","title":"Invalid VPC Peering Connection Configurations"}],"answers":[{"id":"ebd4315b15afc456004798c874243bf9","text":"You have a VPC peering connection between VPCs A and B. They are in the same AWS account, and they do not have overlapping CIDR blocks.","correct":false},{"id":"808626de978678cbd8fba9cdf7ede572","text":"You have peered three VPCs in a full-mesh configuration. The VPCs are in the same AWS account and do not overlapping CIDR blocks.","correct":false},{"id":"2fa2fe643e096fd3d938fa5cd569209f","text":"You have a VPC peering connection between VPC A and VPC B. VPC A also has a VPN connection to a corporate network. You use VPC A to extend the peering relationship to exist between VPC B and the corporate network so that traffic from the corporate network can directly access VPC B by using the VPN connection to VPC A.","correct":true},{"id":"63722d8924a7b714eb8a8000961ebf13","text":"VPC A has peering connections to VPCs B and C. All three VPCs are in the same AWS account, and there are no overlapping CIDR blocks.","correct":false}]},{"id":"e05cb926-bcab-4cab-8a18-08f38bf70dfa","domain":"SecureSolutions","question":"You have an application that stores data in S3, and you need to design an integrated solution providing encryption at rest. You want Amazon to handle key management and protection using multiple layers of security. Which S3 encryption option should you use?","explanation":"SSE-S3 uses managed keys and one of the strongest block ciphers available, AES-256, to secure your data at rest.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"SSE - S3 Encryption"}],"answers":[{"id":"472035b16201f5505380c16ec3eb8518","text":"SSE-C","correct":false},{"id":"e17194408dc439e1658a544965724d44","text":"SSE-KMS","correct":false},{"id":"bac271f02854883c6bc665637d0a5de6","text":"Amazon S3 Encryption Client","correct":false},{"id":"1562bb9d2d9567740605dcb1ccab5c80","text":"SSE-S3","correct":true}]},{"id":"8db02025-839b-4cba-9396-3bf3d30f5c41","domain":"ResilientDesign","question":"You are a system administrator and you need to take a consistent snapshot of your EC2 instance. Your application holds large amounts of data in cache that is not written to disk automatically. What would be the best approach to taking an application consistent snapshot?","explanation":"As you need an application consistent snapshot, your best option would be to shutdown the EC2 instance and detach the EBS volume, then take the snapshot.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html","title":"Creating an EBS Snapshot"}],"answers":[{"id":"8f8e16b428381fb3d36f848931c82fb2","text":"Take a snapshot in real time using the EC2 API.","correct":false},{"id":"f9074c78a685a4482c46191309041432","text":"Take a snapshot using the AWS CLI.","correct":false},{"id":"0ca1045c8304935b9a8a2966f9558b13","text":"Shut down the EC2 instance and detach the EBS volume, then take the snapshot.","correct":true},{"id":"35151eb8d8b2db302878d62b44030835","text":"In the AWS console, take a snapshot and ensure that the 'application consistent' check box is ticked.","correct":false}]},{"id":"25300a43-201c-49eb-b139-8d9716469803","domain":"Performant","question":"Which of the following are examples of scaling an IT architecture horizontally to support a web application?","explanation":"To scale horizontally is to increase the number of AWS resources in your cloud environment. So, adding instances and read-replicas are correct. By comparison, resizing an EC2 instance type is an example of scaling vertically, not horizontally, since you would be increasing the specifications of a single AWS resource as needed. Terminating an unused instance is not a valid answer because it is not an example of scaling; rather, it is an example of elasticity or cost-optimizing your environment.","links":[{"url":"https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf","title":"Architecting for the Cloud"}],"answers":[{"id":"eb4cfd0fba202e21e1f495bf4b215f7d","text":"Adding an RDS database read-replica","correct":true},{"id":"ac078f0c4ec332bce4f049e85fdce843","text":"Resizing an EC2 instance type for four times the amount of CPU and memory power","correct":false},{"id":"d2ed8876ad5d876850ebcde76e28bd9f","text":"Adding two more EC2 instances","correct":true},{"id":"0409e3c213672b1502a0caa251e12494","text":"Terminating an EC2 instance that’s barely used.","correct":false}]},{"id":"7719a68f-21aa-425f-8c29-73591c37f264","domain":"Performant","question":"A mobile gaming company would like to provide real-time, customized flash offers as a result of events that occur in their new multi-player adventure game. The mobile user interface was developed with the AWS Mobile SDK, and some device actions generate REST API calls. Events are also detected by the game's backend server code. All offers presented and accepted need to be retained to determine take-rate metrics for future campaigns. Which architecture will provide the most scalable and cost effective solution?","explanation":"Using Kinesis and SQS to decouple the architecture provides the most scalable solution. S3 provides the most cost effective persistent data store for later take-rate analysis. Answer one does not provide persistent storage for offers presented, which will be used for generating take-rate metrics. Answer two is a tightly coupled architecture which could be improved with asynchronous communication between the EC2 instances and the mobile backend. Answer three will incur higher costs due to it's use of RDS, which is not needed in this case since AWS analytic services provide schema-on-read access to simply structured S3 data.","links":[{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/serverless-analytics-for-mobile-gaming.pdf?did=wp_card&trk=wp_card","title":"Serverless Real-Time Analytics for Mobile Gaming"}],"answers":[{"id":"d48a48df84b7761da880dd9edeca8aa5","text":"Have mobile device and backend server code write events to Amazon Kinesis Data Streams. Configure a Lambda function as the consumer of the stream. The Lambda function compares events with a campaigns table in DynamoDB to determine matches for offers. Matched offers are written to S3, and to an Amazon SQS queue to be picked up by the backend server for forwarding to the mobile device","correct":true},{"id":"893d664f68918ed796784a3176db90ae","text":"Have the mobile device and backend server code write events to an Amazon SQS queue. Create EC2 instances in an Auto Scaling Group that read from the SQS queue and match events with campaigns in an S3 Parquet file. Have the EC2 instances send matched offers to the backend server via Amazon SQS for forwarding to the mobile device.","correct":false},{"id":"30d3ccf0bea954b3a8245bf84c8fb6e2","text":"Have the mobile device and backend server code write events to Amazon DynamoDB. Create a Lambda function that reads from DynamoDB and matches events with campaigns in another DynamoDB table. Have the EC2 instances send matched offers to the backend server via WebSocket for forwarding to the mobile device.","correct":false},{"id":"e5c36bf51ecc4b4fdf5f7cf329e8c814","text":"Have mobile device and backend server code write events to Amazon Kinesis Data Streams. Configure a Lambda function as the consumer of the stream. The Lambda function compares events with a campaigns table in an S3 Parquet file to determine matches for offers. Matched offers are written to Amazon RDS, and to an Amazon SQS queue to be picked up by the backend server for forwarding to the mobile device","correct":false}]},{"id":"70c6a808-0d5d-40d3-9b62-aa2fd031a543","domain":"CostOptimized","question":"You have three AWS payer accounts consolidated under an AWS Organization . Which of the below statements is TRUE for purposes of volume discounts?","explanation":"If you have multiple accounts, your charges will decrease because AWS combines usage from all accounts in the organization to qualify you for volume pricing discounts.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html","title":"Consolidated Billing for Organizations"}],"answers":[{"id":"a6cd99e8b1bbcfc82e184acc9f28eede","text":"Usage across the three accounts will be aggregated in determining the volume discount your Organization is entitled to only if Consolidated Billing is enabled in each account","correct":false},{"id":"98e718508cd32b9ffb27d8648e9129d0","text":"Usage across the three accounts will be aggregated in determining the volume discount your Organization is entitled to","correct":true},{"id":"c9c2416d95c8112070b7a5b032629fdf","text":"Usage in each account will be evaluated individually to determine the volume discount it is individually entitled to","correct":false},{"id":"4fb1af36b069dca73268eedbfab53e7b","text":"Usage across the three accounts will be aggregated in determining the volume discount your Organization is entitled to only if Consolidated Billing is enabled at the Organisation level","correct":false}]},{"id":"5c6ec542-13ec-47b0-90b9-747eba40a8dd","domain":"CostOptimized","question":"What main functions can Route 53 perform? Select the best answer from the following options.","explanation":"Route53 is Amazons DNS web service that delivers the domain registration, DNS routing and health checking function in any combination.","links":null,"answers":[{"id":"20cda55464d105018afc584dcdbc80fe","text":"Domain registration, DNS routing, and health checking in any combination","correct":true},{"id":"24f6183de033dcaee97747be7c92a9f3","text":"Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service that is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications. It can be used together with CloudWatch, a service which allows you to monitor and manage applications. While Route 53 is not a domain reseller, it allows customers to bring their own domain names with them.","correct":false},{"id":"207068b486e311e8fedb355a6c2edcff","text":"Domain registration and DNS routing","correct":false},{"id":"fd7a725638e536df09c6ea5a5d0f2b46","text":"DNS routing and health checking for domains hosted on AWS","correct":false}]},{"id":"73e59bc1-e406-4646-ad2e-d3765a06eb8d","domain":"ResilientDesign","question":"You recently set up a website for customers to access over the Internet, but upon navigating to the URL, you keep getting a 'Connection timed out' error message. Which of the following answers will solve the problem?","explanation":"If you get a 'Connection timed out' error message when navigating to your website, you have to check the security group rules. You need rules that allow inbound and outbound traffic from the website’s address on the proper ports. In this case, since customers need to connect from the Internet, you will have to set inbound and outbound rules in the security group for an HTTP connection, which is through port 80. The Network Access Control List (NACLs) must also allow traffic to come in on port-80 and return back out on the ephemeral web ports. Enabling keepalives is for resolving the 'Server unexpectedly closed network connection' error, not 'Connection timed out'. Domain health checks is not a valid answer, since there’s no option to 'refresh' health checks.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide//TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectionTimeout","title":"Troubleshooting Connecting to Your Instance"}],"answers":[{"id":"f33e790919aaac8f859da3bf62875772","text":"The inbound and outbound rules in the security group are edited to include an HTTP connection.","correct":true},{"id":"cafdf1a54332e17122cd6a3a8c9512ed","text":"The website domain’s health check is refreshed to generate a Healthy status.","correct":false},{"id":"767754962f6fa24f143c1e223735ba12","text":"Keepalives is enabled.","correct":false},{"id":"1e05bb57b03ee35bd2b53b676eb09c48","text":"The inbound and outbound rules in the network access control list (network ACL) are edited to support a HTTP connection.","correct":true}]},{"id":"fd83d649-3eea-46c9-8062-e4b4eb388a43","domain":"Performant","question":"You are auditing your company's RDS estate, and you discover a database that is in a single Availability Zone which is a violation of company policy. You decide to convert this to a multi-AZ deployment. Which of the following things will happen?","explanation":"For the RDS MySQL, MariaDB, PostgreSQL and Oracle database engines, when you elect to convert your RDS instance from Single-AZ to Multi-AZ, the following happens: A snapshot of your primary instance is taken, A new standby instance is created in a different Availability Zone, from the snapshot, synchronous replication is configured between primary and standby instances.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html#Concepts.MultiAZ.Migrating","title":"Modifying a DB Instance to be a Multi-AZ Deployment"}],"answers":[{"id":"c9a788a92b58e73582c15e80b4d7dee2","text":"A snapshot of your primary instance is taken","correct":true},{"id":"193d090a761f5aa3057dc7b001d51fc9","text":"A new standby instance is created in a different Availability Zone, from the snapshot","correct":true},{"id":"ff1b29c6f313c53b2104c91355b0287d","text":"Asynchronous replication is configured between primary and standby instances","correct":false},{"id":"9d354995cee95598703a43813f67e277","text":"Synchronous replication is configured between primary and standby instances","correct":true}]},{"id":"47c3aa9c-215f-11ea-978f-2e728ce88125","domain":"ResilientDesign","question":"Even though your company is migrating to Amazon EC2, it wants to continue using its Oracle 12c software license to comply with the contract requirements of one of its clients. Which of the following pricing models accommodates this goal?","explanation":"Consisting of an actual physical EC2 server, the Dedicated Hosts plan will allow the company to use its eligible software license from Oracle on Amazon EC2. The other choices will not work because they don’t include a physical server to address corporate compliance requirements.","links":[{"url":"https://aws.amazon.com/ec2/dedicated-hosts/","title":"Amazon EC2 Dedicated Hosts"}],"answers":[{"id":"244492cdce9dce0555c52ebe1182e0a3","text":"On Demand","correct":false},{"id":"6c9d6b8aea6f3d16847bdebe05878a2d","text":"Spot","correct":false},{"id":"942d4e37dd5607ab68e54755540d4a47","text":"Reserved","correct":false},{"id":"acbe4e201e8e2ac67830983f9c8d7970","text":"Dedicated Hosts","correct":true}]},{"id":"74da1953-0d65-427a-bbba-a17f2cf81ddd","domain":"SecureSolutions","question":"You need to implement a new web application that allows users to store family photos online in such a way that only invited guests will be able to view the images. Which type of S3 encryption should you choose to maintain full end-to-end control of the encryption/decryption of objects and assure that only encrypted objects are transmitted over the Internet to Amazon S3.","explanation":"","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html","title":"Using a Client-Side Master Key"}],"answers":[{"id":"5e146922b28cce8eae49ecb39eef972d","text":"Provide a client-side master key to the Amazon S3 Encryption Client","correct":true},{"id":"472035b16201f5505380c16ec3eb8518","text":"SSE-C","correct":false},{"id":"e17194408dc439e1658a544965724d44","text":"SSE-KMS","correct":false},{"id":"1562bb9d2d9567740605dcb1ccab5c80","text":"SSE-S3","correct":false}]},{"id":"e9205ab6-d7ce-4708-b92d-e6814f79c6d4","domain":"ResilientDesign","question":"The dashboard application for multiple company contact centers requires fast update response times for a large number of concurrent users. Call center metric data is stored in an Oracle version 11 database. Which architecture will provide high-availability and the low response times needed for this mission-critical data?","explanation":"Since the dashboard updates are needed across multiple contact centers, leveraging read-only replicated databases will provide fast response times. Amazon RDS doesn’t support read replicas for Oracle version 11, so hosting the database on EC2 and replicating the data with Oracle Data Guard is the only viable solution. AWS Database Replication Service is not an offered service, and using EBS snapshots won't provide real-time replication.","links":[{"url":"https://docs.aws.amazon.com/quickstart/latest/oracle-database/overview.html","title":"Oracle Database on AWS"}],"answers":[{"id":"0129ed97e5c2ec017bdc05d836f10049","text":"Oracle hosted on Amazon EC2 in multiple Availability Zones with Oracle Data Guard replication","correct":true},{"id":"c1e824f46134bc6696eba635f30df032","text":"An Amazon RDS Oracle instance with Multi-AZ and Read Replicas","correct":false},{"id":"5ee29b50026caa8c20c9e469f93b5a2a","text":"Oracle hosted on Amazon EC2 in Multiple Availability Zones with EBS snapshots","correct":false},{"id":"de5ad68debdabd478d7d4f66542b9ca8","text":"An Amazon RDS Oracle Instance with AWS Database Replication Service","correct":false}]},{"id":"18f5f9ac-72ae-44e6-a607-875acd4b7508","domain":"CostOptimized","question":"Your company is moving their entire 20 TB data warehouse to the cloud. With your current bandwidth, it would take 2 months to transfer the data. Which service would you use to quickly get your data into AWS?","explanation":"At that amount of data and those bandwidth restrictions, Snowball would be the most expedient choice.","links":[{"url":"https://aws.amazon.com/snowball/faqs/#when-to-use","title":"When to Use Snowball"}],"answers":[{"id":"a8e1dc43989241e706e31c52d23be15c","text":"S3 with Transfer Acceleration","correct":false},{"id":"25e163616bb5cc20c769ad3e8b7a0703","text":"Multipart Upload","correct":false},{"id":"cdd0d04de2b79c78e792aec6263d2d3d","text":"DirectConnect","correct":false},{"id":"c0429b6a658dd488f7262d983c7e02bb","text":"Snowball","correct":true}]},{"id":"61614ffb-1208-4742-9ff9-c8b316f13cc4","domain":"Performant","question":"When coding a routine to upload to S3, you have the option of using either single part upload or multipart upload. Identify all the possible reasons below to use Multipart upload.","explanation":"Multipart upload provides options for more robust file upload in addition to handling larger files than single part upload.","links":[{"url":"http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html","title":"Uploading Objects Using Multipart Upload"}],"answers":[{"id":"df10e1b59a623a1543c4343ace225b87","text":"Multipart upload delivers the ability to append data into an open data file.","correct":false},{"id":"fceeb753e12f0056c923fc9115be9472","text":"Multipart upload delivers quick recovery from network issues.","correct":true},{"id":"d935110c327634d57b66601e6957ed42","text":"Multipart upload delivers improved throughput.","correct":true},{"id":"b2b49e9141fe4ce7eb0932d9c62a38b8","text":"Multipart upload delivers the ability to pause and resume object uploads.","correct":true},{"id":"61f9f705b4dc688b86e0b25708cb7d88","text":"Multipart upload delivers the ability to begin an upload before you know the final object size.","correct":true},{"id":"561cf9617064d07d28982fa0a4c4a5a3","text":"Multipart upload delivers improved security in transit.","correct":false}]},{"id":"08f57874-82ba-4e5e-a750-a59d4ba08d9f","domain":"Performant","question":"Which of the following statements is TRUE.","explanation":"You are able to attach multiple EBS volumes to an EC2 instance.  Plus you can attach an EBS volume (of certain types) to multiple EC2 instances","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html#EBSFeatures","title":"EC2 and Multiple EBS Volumes"},{"url":"https://aws.amazon.com/blogs/aws/new-multi-attach-for-provisioned-iops-io1-amazon-ebs-volumes/","title":"Multi-Attach for Amazon EBS Volumes"}],"answers":[{"id":"1fac5b3ed26be82526f45ca40c282786","text":"It is possible to configure an Autoscaling Group to repair degraded EBS volumes, without the need to terminate the EC2 instances.","correct":false},{"id":"c17b4af7da4243bea4febf9426c65175","text":"You are able to attach multiple EBS volumes to an EC2 instance.","correct":true},{"id":"f2e3ed76c7a4224a6d0e521f7a926129","text":"You are able to attach multiple EC2 instances to some EBS Volume types.","correct":true},{"id":"d3554bd6eace9b51dc90488e17d4f209","text":"It is possible to use Autoscaling with EBS, rather than EC2.","correct":false}]},{"id":"c3fd126e-3f97-443d-8aa6-30c582c8f4f0","domain":"Performant","question":"Amazon RDS supports which of the following databases?","explanation":"Amazon RDS currently supports MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server, and Amazon Aurora database engines.","links":[{"url":"https://aws.amazon.com/rds/","title":"RDS: Supported Engines"}],"answers":[{"id":"4e6ac8e997ca8f5896cbc28cad3ede24","text":"Sybase","correct":false},{"id":"7f9733e208088b1ce6df3d4be1765396","text":"MariaDB","correct":true},{"id":"62a004b95946bb97541afa471dcca73a","text":"MySQL","correct":true},{"id":"c890515f1055143925ae4fb85b86ec70","text":"DB2","correct":false}]},{"id":"f3e923a2-2e1a-11ea-978f-2e728ce88125","domain":"CostOptimized","question":"You want to track the amount of money you ideally want your company to spend for EC2 data transfers every month. Which of the following actions will accomplish that?","explanation":"AWS Cost Explorer is for providing information that you can use to track and manage costs, but it doesn’t enable the creation of budgets; that’s what AWS Budgets is for. If the question was strictly addressing cost, then creating a Cost budget with AWS Budgets would have been the correct answer. However, your concern is specifically with a usage type, which is EC2 data transfers. In this case, you would need to create a Usage budget with AWS Budgets and receive alerts when your defined threshold is met.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing Your Costs with Budgets"}],"answers":[{"id":"4407cc58412e1c4727ad336bd8b8453f","text":"Create a Reservation budget with AWS Budgets.","correct":false},{"id":"41e006394cc745a90a25e57065b658c2","text":"Create a Cost budget with AWS Budgets.","correct":false},{"id":"8d35bf16b7d263f4ab864e392d023e54","text":"Enable AWS Cost Explorer","correct":false},{"id":"e7f799a3bc73b229cd75d773a9d7f547","text":"Create a Usage budget with AWS Budgets.","correct":true}]},{"id":"f1eea1a4-0b1e-4702-bb51-0937708005a3","domain":"ResilientDesign","question":"In the future, you will need to preserve, restore, and retrieve every version of every file that you have stored in AWS. Which service should you use?","explanation":"Versioning allows you to preserve, retrieve, and restore every version of every object stored in an Amazon S3 bucket.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html","title":"Using Versioning with S3"}],"answers":[{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":false},{"id":"8eeeb63c58948c56dc93f4dd229fc796","text":"S3 with Versioning enabled.","correct":true},{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":false},{"id":"e9a5105fa288ef2b71c037e42d665d91","text":"S3 - OneZone-IA","correct":false}]},{"id":"bc814aa4-03e8-4a64-aa0e-84f22d812a95","domain":"SecureSolutions","question":"Which of the following DNS record types does Route 53 not support?","explanation":"Route 53 is a scalable and highly available DNS service and it currently supports 13 different DNS record types including; AAAA, CNAME and SPF.  However, Route 53 does not support DNSSEC (other than during domain registration) and therefore any DNSSEC related records, such as DNSKEY, are also not supported.","links":[{"url":"https://aws.amazon.com/route53/faqs/","title":"Amazon Route 53 FAQs"}],"answers":[{"id":"adc4bfdb0829dae99e3699393e3fbaa4","text":"CNAME","correct":false},{"id":"548deb43a9afe4abcde34a605eb44700","text":"DNSKEY","correct":true},{"id":"b4efb35349e5d93905531be07dbacd6d","text":"SPF","correct":false},{"id":"098890dde069e9abad63f19a0d9e1f32","text":"AAAA","correct":false}]},{"id":"32d8dc4a-c501-4d42-beb1-040dd7c3a48e","domain":"SecureSolutions","question":"The marketing department at your company wants to run a Hadoop cluster on Amazon EMR to perform data mining on a 2 TB dataset. Your information security group requires that all data be encrypted in transit and at rest, both on-premises and in the cloud at all times. Encryption keys must be stored in the on-premises key management solution. You've decided to transmit the data to Amazon S3 using TLS. How will you protect the data at rest for processing by the EMR cluster?","explanation":"With S3 client-side encryption, the S3 decryption needs to take place in the EMRFS client on your cluster. In this case, since the data was encrypted with on-premises keys before being uploaded to S3, we need to retrieve those keys using the custom key provider capabilities provided by the AWS SDK for Java. Once the keys are retrieved in the EMRFS client, we can decrypt the S3 data. You can't load an AWS Managed Customer Master Key into KMS. AWS Managed CMKs are created, managed, and used on your behalf by AWS services that are integrated with KMS. KMS does provide the capability to load Customer Managed CMKs. S3 currently doesn't allow you to specify a key management system other than AWS KMS for encryption. Decrypting the data and letting S3 re-encrypt it will work with SSE-S3, but it will require significant additional processing.","links":[{"url":"https://aws.amazon.com/emr/","title":"Amazon EMR"},{"url":"https://aws.amazon.com/blogs/big-data/best-practices-for-securing-amazon-emr/","title":"Best Practices for Securing Amazon EMR"},{"url":"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-emrfs-encryption-cse.html","title":"Amazon S3 Client-Side Encryption"}],"answers":[{"id":"e5ff58ff075cc21a8c2a32987b58fa91","text":"Use the AWS SDK for Java to implement the encryption materials provider interface in the EMRFS client. Retrieve the key from the on-premises key management system to decrypt the data stored in S3","correct":true},{"id":"77239786bd9e0fb418f350764c51cac7","text":"Specify client-side encryption when you create the S3 bucket where the data will be uploaded. Configure S3 to use the on-premises key management system. Let S3 handle decryption for EMR","correct":false},{"id":"ea732a1ef76dae83c429f20c03379a83","text":"Load the on-premises encryption key into AWS Key Management Service as an AWS Managed Customer Master Key. In the EMRFS client, retrieve the key from KMS to decrypt the data stored in S3","correct":false},{"id":"ff476caa2e70b5b71fa9e82fa2132d94","text":"Specify server-side encryption when you create the S3 bucket where the data will be uploaded. Decrypt the data before uploading it into S3 and let S3 manage encryption and decryption going forward","correct":false}]},{"id":"fedb7a40-0fc4-11ea-8d71-362b9e155667","domain":"Performant","question":"A company needs to collect, store and analyze data, from various data sources, to calculate the net profit from sales of its bags, socks, shoes, and underwear brands in the United States, England, France, and South Africa during the first quarter of the year. Which of the following AWS services is most suitable for this application?","explanation":"Amazon Redshift is designed for companies like this one to pull very large and complex data sets and analyze them to make critical business decisions. Unlike Redshift, neither the Aurora RDS database engine nor DynamoDB are suitable for data warehousing, and S3 is simply for storing objects.","links":[{"url":"https://aws.amazon.com/redshift/","title":"Amazon Redshift"}],"answers":[{"id":"270fcb785810d0206945029bb05f4e97","text":"Amazon S3","correct":false},{"id":"69670a9d53817d1ec89e685997343ce2","text":"Amazon Aurora","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":false},{"id":"f7415e33f972c03abd4f3fed36748f7a","text":"Amazon Redshift","correct":true}]},{"id":"eba5470e-a2c2-4974-a3f0-4d4024790153","domain":"SecureSolutions","question":"Although your company initially had a single AWS account, it now has four additional ones. Consequently, it has become more difficult for you as a System Administrator to manage permissions for users. Which of the following options will enable you to consolidate the accounts and centrally manage their permissions?","explanation":"AWS Organizations is what you need for consolidating the accounts in an organization, which gives you control over what users and roles in the account group can do. The resulting permissions intersect what Organizations allows at the account level and the permissions that users or roles have within each account. IAM is the service used to set permissions, not centrally manage them. Although System Manager and CloudWatch in the same classification as Organizations—they are also Management & Governance tools—they have completely different functions; the former is for grouping AWS resources together, and the latter is for monitoring them.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html","title":"What is AWS Organizations?"}],"answers":[{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"113b1ad9ce6cdc3a37ad8475bc9bb2b2","text":"AWS Systems Manager","correct":false},{"id":"b1820ee1cf68e2e65f263ff7bb207626","text":"AWS Organizations","correct":true},{"id":"5fe5bde89ae88d436f1f8ba2d3a64130","text":"Identity and Access Management (IAM)","correct":false}]},{"id":"d3a69a14-8fa0-4820-80ed-04f386b437a1","domain":"ResilientDesign","question":"You are reviewing your colleagues' AWS infrastructure design to handle large distributed and replicated workloads - in this case for a Cassandra non-relational database cluster with many nodes spanning multiple AZs in the same region. It specifies the placement of instances into partitions so that these do not share underlying hardware to reduce the likelihood of correlated failures. Which of the following statements about that is incorrect?","explanation":"A cluster placement group can't span multiple Availability Zones, and a spread placement group cannot use dedicated instances or dedicated hosts. (note that you are looking for 'incorrect' answers)","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html","title":"Placement Groups"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-ec2-ntroduces-partition-placement-groups/","title":"AWS - Partition Placement Groups announcement "}],"answers":[{"id":"7f6572c063647af090c784d97c4ee2bc","text":"Because you have more than 7 running instances per Availability Zone per group, you cannot use a spread placement group.","correct":false},{"id":"bb7339e9c120b35a0f218339b10e34dc","text":"The number of instances that you can launch in a partition placement group is limited only by your account limits but also a partition placement group supports a maximum of 7 partitions per Availability Zone.","correct":false},{"id":"3c7b4f245217b06cc9768cf64221e10e","text":"You can best achieve this with a combination of a spread placement group and dedicated hosts.","correct":true},{"id":"f6d473e88ac8ee3505d59f8787d8e9f5","text":"You can best achieve this with the use of a cluster placement group.","correct":true},{"id":"2d1c087097dd168b43a0b9dfa2a961c6","text":"A partition placement group with Dedicated Instances can have a maximum of two partitions while partition placement groups for Dedicated Hosts are not supported.","correct":false}]},{"id":"86bc5815-e2a2-435d-b36a-de0291f03384","domain":"ResilientDesign","question":"A company disaster recovery policy requires that all RDS backups are retained in a secondary AWS region. What is the optimal solution to meet this requirement?","explanation":"RDS automated backups store backup data in the same region as the RDS instance. It is not possible to configure RDS automated backups to store data in a different region. The correct solution to meet the requirement is to copy RDS snapshots to the secondary region. It is not possible to copy RDS DB snapshots to an S3 bucket. Although it is possible to configure an RDS read-replica with automated backups in the secondary region, this is not an optimal solution as it involves additional costs associated with the running RDS instance.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html","title":"Working With Backups"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html","title":"Creating a DB Snapshot"}],"answers":[{"id":"0ffef8639c8e9a75fa845bb6e9e6488b","text":"Configure RDS Read-Replica instance in the secondary region. Enable RDS automated backups on the read-replica instance.","correct":false},{"id":"6cb16d4a90eb4167d69dad6d62a4afa0","text":"Create an RDS DB snapshot. Copy the RDS DB snapshot to an S3 bucket. Enable Cross-Region replication on the S3 bucket.","correct":false},{"id":"bc87f2bd894c0887220a36be79571629","text":"Create an RDS DB snapshot. Copy the RDS DB snapshot to the secondary region.","correct":true},{"id":"1575bfe0f50108d320a7b04107a11d21","text":"Configure an RDS automated backups target region to the secondary region.","correct":false}]},{"id":"b8cb8890-c16a-4609-904f-2cd52cfeba0e","domain":"CostOptimized","question":"What is the minimum billable object size for S3 - IA?","explanation":"The minimum object size is 0 bytes, however you will be billed for 128 KB.  Objects smaller that 128 can still be stored, but will be billed as if they are 128KB. ","links":[{"url":"https://aws.amazon.com/s3/storage-classes/","title":"S3 Standard - IA Object Size (table)"},{"url":"https://aws.amazon.com/s3/faqs/?nc=sn&loc=6","title":"S3 Standard - IA minimums"}],"answers":[{"id":"fdd68bff35708140c14d3cd3a3b0759d","text":"0 Bytes","correct":false},{"id":"5de6c8bb0062d1883700e0bd14152d0d","text":"128 KB","correct":true},{"id":"bf361755334066f22d019854dd2be686","text":"1 KB","correct":false},{"id":"05a402af63179f5ea4189bcb6a7e8bc5","text":"1 Byte","correct":false}]}]}}}}
