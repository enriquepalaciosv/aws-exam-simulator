{"data":{"createNewExamAttempt":{"attempt":{"id":"f3c4a7dc-630a-4a79-a0fc-7fbcb271b865"},"exam":{"id":"1852395f-3019-49f6-aa8c-6e2bef779b37","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"c1333471-d052-4710-bcdb-facadc095d70","domain":"awscsapro-domain5","question":"You are setting up a corporate newswire service for a global news company.  The service consists of a REST API deployed on EC2 instances where customers can retrieve the latest news articles in real-time that happen to contain their company name.  This allows companies to monitor all news sources for stories where they are mentioned.  Because of the worldwide reach of the new site, you want to position servers around the globe.  You want to publish one subdomain name globally (api.domain.com) and have the requesters directed to the nearest region based on latency.  In each region, you want to be able to accommodate blue-green deployments without downtime as well.  What steps do you take?","explanation":"We want to use weighted routing records for local instances so we have the ability to adjust weights and shift traffic during blue-green deployments.  Latency-based routing would take care of funneling requests to the site with the lowest latency.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html","title":"How Health Checks Work in Complex Amazon Route 53 Configurations - Amazon  Route 53"}],"answers":[{"id":"41e30b8e30cbd737ced85953d7e3e939","text":"Use CloudFormation to create a distribution of the website.  Create an alias record for the subdomain (api.domain.com) in Route 53 and assign it to the CloudFront distribution.  To ensure no lag in news retrieval, set the maximum TTL on the CloudFront distribution to 0.","correct":false},{"id":"b850f1c18972d022213271c5d673e07f","text":"First setup weighted routing records for the local instances in the region in Route 53.  Assign equal weights with all sharing the same regional subdomain name (us-east-2.api.domain.com).  Next, create latency alias records by creating multiple entries for api.domain.com--each pointing to the regional subdomains.","correct":true},{"id":"6e56101de6ed2ca97550d5025ddf559a","text":"Using Route 53, we first create the top-level api.domain.com with a geolocation policy.  We then create latency-based routing records for the instances in each region (us-east-2.api.domain.com).  Next, we configure the countries closest to each region in the geolocation policy to direct them to the regional records.","correct":false},{"id":"a60aaaf971cbd546c1ec57e08ea38274","text":"We would first create geo-spatial records for the local resources in each region (us-east-2.api.domain.com) and assign equal weights.  Next, we create latency-based routing records for the top level subdomain (api.domain.com) and direct those to the regional records as an alias.  We must also disable Health Check on the latency record to ensure the localized Health Check is used.","correct":false}]},{"id":"bdd6d6a9-48a5-45f8-bb74-873f266d85df","domain":"awscsapro-domain2","question":"A hospital would like to reduce the number of readmissions for high risk patients by implementing an interactive voice response system to provide reminders about follow up visit requirements after patients are discharged. The hospital has the capability to automatically send HL7 messages that include the patient's phone number and follow up visit information from its medical records application via Apache Camel. They've chosen to deploy the solution on AWS. They already have a VPN connection to AWS, and all aspects of the application need to be HIPAA eligible. Which architecture will provide the most resilient and cost effective solution for the automated call system?","explanation":"S3 provides a low cost repository for the HL7 messages received. Having Lambda write the object keys to SQS, and having another Lambda function retrieve and parse the messages gives the architecture asynchronous workflow. Amazon Connect provides the capability to define call flows and perform IVR functions. Each of these services is HIPAA eligible. DynamoDB is also a good option for storing message information, but will be more expensive than S3. Amazon Pinpoint can place outbound calls, but is not able to perform interactive voice response functions. Amazon Comprehend Medical doesn't create call flow sequences.","links":[{"url":"https://aws.amazon.com/compliance/hipaa-eligible-services-reference/","title":"HIPAA Eligible Services Reference"},{"url":"https://aws.amazon.com/connect/","title":"Amazon Connect"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/reduce-hospital-readmissions-ra.pdf?did=wp_card&trk=wp_card","title":"Reducing Hospital Readmissions"}],"answers":[{"id":"c9bee4c5e4463b67e2726d3bed2ceed1","text":"Configure Apache Camel to write the HL7 messages to Amazon Kineses Data Firehose, which stores the patient information in Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"a8c26f088ade23063b2bf5f202a74cce","text":"Have Apache Camel write the HL7 messages to Amazon Kineses Data Streams. Configure a Lambda function as a consumer of the stream to parse the HL7 message and write the information to Amazon DynamoDB. Trigger another Lambda function to pull the patient data from DynamoDB and send it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"8121c4e6e285161311cacf7b8031d5af","text":"Configure Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to write each HL7 message object key to Amazon Simple Queue Service FIFO. Have another Lambda function read messages in sequence from the SQS queue and use the object key to retrieve and parse the HL7 messages. Use that same Lambda function to write patient information to Amazon Connect to place the call using an established call flow.","correct":true},{"id":"9585e2e5994ba5cdc2db33c22d9230cf","text":"Set up Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Connect to place the call to the patient.","correct":false}]},{"id":"08a68d51-48ba-43b7-b0c3-c24e04bb33a8","domain":"awscsapro-domain3","question":"You have just completed the move of a Microsoft SQL Server database over to a Windows Server EC2 instance.  Rather than logging in periodically to check for patches, you want something more proactive.  Which of the following would be the most appropriate for this?","explanation":"The default predefined patch baseline for Windows servers in Patch Manager is AWS-DefaultPatchBaseline.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html","title":"Default and Custom Patch Baselines - AWS Systems Manager"}],"answers":[{"id":"4b5715f588231d7445bb512181ea13a2","text":"Make use of Server Manager and the AWS-LinuxWindowsDefaultPatchBaseline pre-defined baseline","correct":false},{"id":"42c46b2fcd3034cf79b83f0d5dc37f7d","text":"Make use of Patch Manager and the AWS-DefaultPatchBaseline pre-defined baseline","correct":true},{"id":"b40084bb5d5139b353505e5f942afc34","text":"Make use of Patch Manager to apply patches as you have defined in the Patch Groups","correct":false},{"id":"78beead90b3e8deb4bf1ee7e3544a309","text":"Make use of AWS Batch to apply patches as they appear on the RSS feed from Microsoft","correct":false},{"id":"5ee836bac8680ff00d457a8d7f90fad6","text":"Make use of Patch Manager and the AWS-WindowsDefaultPatchBaseline pre-defined baseline","correct":false}]},{"id":"d58acd29-561d-4017-a55d-bcff8eb40f90","domain":"awscsapro-domain4","question":"Last month's AWS service cost is much higher than the previous months. You check the billing information and find that the used hours of Elastic Load Balancer (ELB) increases dramatically. You manager asks you to plan and control the ELB usage. When the ELB service has been used for over 5000 hours in a month, the team should get an email notification immediately and further actions will be taken accordingly. Which of the following options is the easiest one for you to choose?","explanation":"AWS Budgets include the types of cost budget, usage budget, reservation budget and savings plan budget. The usage budget enables you to plan the usage of ELB service and receive budget alerts when the actual usage becomes more than a threshold (5000 hours in this scenario). The cost budget type is incorrect as it evaluates the cost instead of usage and you cannot receive budget alerts from either Cost Explorer or AWS Config.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing your costs with Budgets"}],"answers":[{"id":"baf09736ecf556311eb6e77579877ccd","text":"In AWS Config, monitor the usage of all ELB resources within the AWS account. Create a custom Config rule via a Lambda function that calculates the ELB usage and sends an alert message to an SNS topic when the usage is over 5000 hours.","correct":false},{"id":"15fe48e7010a90e06774d9aa9668704a","text":"Create a usage budget in AWS Budgets. Check the ELB running hours for every month and set the budgeted amount to be 5000 hours. Configure an email alert or an Amazon Simple Notification Service (SNS) notification when the actual usage is more than the threshold.","correct":true},{"id":"0f88bd3ee9fd3724922aa89baa9f656d","text":"Launch the Cost Explorer in AWS billing dashboard, filter the EC2 ELB service and configure a CloudWatch alert to track its actual monthly usage. When the monthly ELB usage grows more than 5000 hours, raise the CloudWatch alert and notify an SNS topic.","correct":false},{"id":"39f1073c8a5f51031cb743e7cf45dce2","text":"Calculate the estimated ELB cost when the total ELB usage is 5000 hours in a month. Configure a cost budget in AWS Budgets for the EC2 ELB service and set the number as the threshold. When the cost is greater than the user-defined threshold, send an email alert to the team.","correct":false}]},{"id":"d750b73a-f283-4da4-97c8-4686049e9ccf","domain":"awscsapro-domain4","question":"The marketing department at your company has been using a homegrown approach to content management for the company's customer facing website. Recent content publishing issues have resulted in a directive to move to Drupal for all content management going forward. The IT budget is already stretched, and there is no capital available to purchase new servers, so the Drupal solution will be implemented on AWS. The current on-premises content management system runs on six Linux servers, each with four CPUs and sixteen gigabytes of memory. There are also two MySQL database servers, each with four CPUs and thirty-two gigabytes of memory. Spikes in volume are common during the fourth quarter of the year with increases up to fifty percent during these months. Which architecture will provide the most cost effective solution?","explanation":"Since the company has deployed six on-premises content management servers to handle seasonal spikes in volume up to fifty percent, four servers will be needed during most of the year. So we should purchase four EC2 Reserved Instances, and have Auto Scaling add up to an additional two when needed. Since content management systems are mostly read-intensive, we can save on the sizing and cost of our Aurora nodes, and improve performance by implementing larger ElastiCache nodes. The rich feature set of Redis is not needed for this use case, so we'll go with Memcached for simplicity.","links":[{"url":"https://github.com/aws-samples/aws-refarch-drupal?did=wp_card&trk=wp_card","title":"Running Drupal on AWS"},{"url":"https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/","title":"Amazon EC2 Reserved Instances Pricing"},{"url":"https://aws.amazon.com/rds/aurora/pricing/","title":"Amazon Aurora Pricing"},{"url":"https://aws.amazon.com/elasticache/pricing/","title":"Amazon ElastiCache Pricing"}],"answers":[{"id":"d67c7bb266f4dcdc91559f6f046d4f01","text":"Purchase three EC2 m5.xlarge Reserved Instances on a 3-year term. Configure Auto Scaling with Drupal AMI launch configurations. Implement one Amazon Aurora db.r5.large instance and one Amazon ElastiCache Redis r5.2xlarge instance.","correct":false},{"id":"7adefaeb634686fb343da16d89ddd8fc","text":"Purchase four EC2 m5.xlarge Reserved Instances on a 3-year term. Configure Auto Scaling with Drupal AMI launch configurations. Implement one Amazon Aurora db.r5.large instance and one Amazon ElastiCache Memcached r5.2xlarge instance.","correct":true},{"id":"61a0c4b4a4069b1f4343231156850d8a","text":"Purchase four EC2 m5.xlarge Reserved Instances on a 3-year term. Configure Auto Scaling with Drupal AMI launch configurations. Implement one Amazon Aurora db.r5.xlarge instance and one Amazon ElastiCache Redis r5.xlarge instances.","correct":false},{"id":"ae881ce32027d7c1d77feaa29c31af97","text":"Purchase six EC2 m5.xlarge Reserved Instances on a 3-year term. Configure Auto Scaling with Drupal AMI launch configurations. Implement one Amazon Aurora db.r5.xlarge instance and one Amazon ElastiCache Memcached r5.xlarge instance.","correct":false}]},{"id":"6a0e9756-1b9e-495c-965b-a8c715843d4f","domain":"awscsapro-domain1","question":"A client has asked you to help troubleshoot a Service Control Policy.  Upon reviewing the policy, you notice that they have used multiple \"Statement\" elements for each Effect/Action/Resource object but the policy is not working. What would you suggest next?  ","explanation":"The syntax for an SCP requires only one Statement element.  You can have multiple objects within a single Statement element though. ","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/org_troubleshoot_policies.html","title":"Troubleshooting AWS Organizations Policies - AWS Organizations"}],"answers":[{"id":"3410d9f58eea1a807fdc3d0f6194c3ce","text":"Change the policy to combine the multiple Statement elements into one element with an object array.","correct":true},{"id":"c88df8df089061b0ce17c8aba3a63305","text":"Have them apply the same policy on another OU to eliminate any localized conflicts.","correct":false},{"id":"4b06a8d83f7ea11c2700b91cae578fbb","text":"Split the SCP out into multiple policies and apply in a cascading manner to higher level OUs.","correct":false},{"id":"6d5becfe4962c177196b4dce486c4e58","text":"Look elsewhere as multiple Statement elements are used when multiple conditions are specified in SCPs.","correct":false}]},{"id":"edb30172-3f76-4423-a6bb-78a3d2fdeb42","domain":"awscsapro-domain2","question":"Your team is managing hundreds of Linux and Windows EC2 instances in different environments such as development, QA, staging and production. You need a tool to help you automate the process of patching instances so that the operating systems have latest patches and meet the compliance policies. You want to manage the patching in different groups depending on the environment. For example, patches should be deployed and tested in the QA environment first before the production environment. How would you achieve this requirement through an AWS service?","explanation":"AWS Systems Manager Patch Manager is the most appropriate tool to manage the patching for large groups of EC2 or on-premises instances. For different environments, users can configure patch groups using the \"Patch Group\" tag and then establish a patch baseline for each patch group. It is better to manage instances with the \"Patch Group\" tag rather than other customized tags. AWS SSM Session Manager and AWS SSM Run Command are not suitable to deploy patches across a large number of instances. The AWS-RunRemoteScript command is also incorrect as it is used to execute scripts stored in a remote location.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-patchgroups.html","title":"Patch Manager Patch Groups"}],"answers":[{"id":"6c4763d45c6cb24622b0db9533f95e0c","text":"Create environmental tags in EC2 instances such as a tag key named \"env\". In AWS SSM Patch Manager, configure patching activities by selecting the instances using the tag. Patch on the QA environment first and perform the necessary testing.","correct":false},{"id":"b99f4271e073e1e030f3c26c383c5959","text":"In AWS Systems Manager Patch Manager, create different patch groups using the tag key \"Patch Group\" and configure a patch baseline for each patch group. Schedule the patching in a maintenance window by selecting a patch group.","correct":true},{"id":"08b9feba2a12ba67141a0ffe02938798","text":"Add patch group tags in the EC2 instances. Perform the patching using the command AWS-RunRemoteScript in AWS SSM Run Command. Patch on the QA environment first by selecting the QA patch group tag.","correct":false},{"id":"32ac8d27221ec8e699eb4e872d3f6ed0","text":"Centrally manage the instances in AWS SSM Managed Instances and divide them into different categories. Perform the patching activities from AWS SSM Session Manager in a maintenance window.","correct":false}]},{"id":"375e7161-43df-4d2f-adab-75cc6166a453","domain":"awscsapro-domain5","question":"You build a CloudFormation stack for a new project. The CloudFormation template includes an AWS::EC2::Volume resource that specifies an Amazon Elastic Block Store (Amazon EBS) volume. The EBS volume is mounted in an EC2 instance and contains some important customer data and logs. However, when the CloudFormation stack is deleted, the EBS volume is deleted as well and the data is lost. You want to create a snapshot of the volume when the resource is deleted by CloudFormation. What is the easiest method for you to take?","explanation":"The easiest method is using the DeletePolicy attribute in the CloudFormation template. The \"Snapshot\" value ensures that a snapshot is created before the CloudFormation stack is deleted. The \"Retain\" value is incorrect as it keeps the volume rather than creates a snapshot. The EBS lifecycle manager can create daily snapshot however it is not required in the question. When the CloudWatch Event rule is triggered, the EBS volume may already be deleted and no snapshot can be taken. Besides, the CloudFormation deletion cannot be suspended by a Lambda function.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html","title":"DeletionPolicy Attribute"}],"answers":[{"id":"555e7ef93e63402dd6a074edd5b8a16d","text":"Create a CloudWatch Event rule that checks the CloudFormation delete-stack event. Trigger a Lambda function that pauses the CloudFormation stack deletion, creates the EBS snapshot of the volume and resumes the stack deletion after the snapshot is created successfully.","correct":false},{"id":"061af6db30e4b65a1d544d4e64546085","text":"Modify the CloudFormation template by adding a DeletePolicy variable for the AWS::EC2::Volume resource. Specify the value of \"Retain\" to automatically create a snapshot of the EBS volume before the stack is deleted.","correct":false},{"id":"0719057e0f4aca49eb47cd94c333e599","text":"Add a DeletePolicy attribute in the CloudFormation template and specify \"Snapshot\" to have AWS CloudFormation create a snapshot for the EBS volume before deleting the resource.","correct":true},{"id":"692ea271f5ada1d555a5889be933267e","text":"Modify the CloudFormation template to create an EBS snapshot strategy in EBS lifecycle manager which creates a daily snapshot as backup and also another snapshot when the EBS volume’s CloudFormation stack is being deleted.","correct":false}]},{"id":"c9d5dfbb-8dda-4a84-bf04-49aa7f88d4db","domain":"awscsapro-domain2","question":"A healthcare provider has a recent history of failing to match patient clinical records from other providers with their own records. This has resulted in missed diagnoses and delayed treatments. To address the issue, they've begun an initiative to store patient records, both those generated in-house and those coming from other providers, in CSV format on an Amazon S3 data lake. They'll use other AWS services to aggregate and deduplicate patient information. Which HIPAA compliant solution will provide them with the highest level of accuracy to improve their level of patient care?","explanation":"An AWS Glue crawler can scan the raw patient CSV data and store the schemas in the Glue data catalog. A Glue ML Transform can be written to find patient information matches in tables defined in the Glue data catalog. If you don't want to write an ML Transform to find the matches, AWS provides a custom one called FindMatches. After training the ML Transform, you can use it as part of a scheduled Glue ETL job, which can write results back to the S3 data lake. Kinesis Data Analytics isn't capable of querying data in S3. It only provides SQL manipulation of the incoming stream. An EMR Hive job will be able to perform some basic record matching, but machine learning will provide greater matching accuracy over time. While SageMaker could provide a viable solution, the Caffe framework is most commonly used for computer vision use cases, not record matching.","links":[{"url":"https://aws.amazon.com/glue/","title":"AWS Glue"},{"url":"https://aws.amazon.com/about-aws/whats-new/2019/08/aws-glue-provides-findmatches-ml-transform-to-deduplicate/","title":"AWS Glue now provides FindMatches ML transform to deduplicate and find matching records in your dataset"},{"url":"https://aws.amazon.com/blogs/big-data/matching-patient-records-with-the-aws-lake-formation-findmatches-transform/","title":"Matching patient records with the AWS Lake Formation FindMatches transform"}],"answers":[{"id":"734eda6752df4074b7d1de2dc63055d6","text":"Create and train an Amazon SageMaker Caffe model to match patient records. Schedule an AWS Lambda function to periodically run the SageMaker model and deposit the results back into S3","correct":false},{"id":"4745b6f6f2c711702b03c99faaab04b1","text":"Implement an AWS Glue crawler to determine patient record formats. Create and train a Glue ML Transform to match patient records. Execute a Glue ETL job using the ML Transform, and store results back in the data lake. Configure the Glue scheduler to run the crawler and the ETL jobs periodically","correct":true},{"id":"50bb504039515eef7c1372c744aa4d79","text":"As new patient records are ingested to S3, trigger an AWS Lambda function to start an AWS Glue crawler to update the Glue data catalog. Have the Lambda function then run an Amazon EMR Hive job to perform patient record matching and write the results back to the data lake","correct":false},{"id":"7a5ba1c8cc4da315c99a4458891c790d","text":"Prior to depositing patient records in the S3 data lake, ingest them with Amazon Kinesis Data Streams and configure Amazon Kinesis Data Analytics as the consumer of the stream. Have Kinesis Data Analytics correlate records with those already ingested to S3 and write them to the corresponding patient folder","correct":false}]},{"id":"12556935-2b08-4f73-b7f6-6b82bd7fa1a0","domain":"awscsapro-domain1","question":"Your company has recently acquired another business unit and is in the process of integrating it into the corporate structure.  Like your company, the acquisition's IT assets are fully hosted on AWS.  They have a mix of EC2 instances, RDS instances and Lambda-based applications but these will become end-of-life as the new business unit transitions to your company's standard applications over the next year.  Fortunately, the CIDR blocks of the respective VPCs do not overlap.  If the goal is to integrate the new network into your current hub-and-spoke network architecture to provide full access to each other's resource, what can you do that will require the least amount of disruption and management?","explanation":"VPC Peering provides a way to connect VPCs so they can communicate with each other.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/peering/invalid-peering-configurations.html","title":"Unsupported VPC Peering Configurations - Amazon Virtual Private Cloud"}],"answers":[{"id":"8e438cf633d07a0d70f2d21978e083bb","text":"Configure a VPC Gateway Endpoint for EC2, RDS and Lambda services.  Configure route tables in your existing VPCs to use the endpoints to communicate with the new VPCs.","correct":false},{"id":"d8d7beaa505f84c757a93d64f70a0ae7","text":"Initiate a VPC peering request from each of your spoke VPCs to each new VPC.  Configure route tables in each spoke VPC and new VPC to route traffic to the respective VPC.","correct":false},{"id":"b4cd9cd07c83868a626224de0d92c70a","text":"Initiate a Transitive Peering request from each new VPC to your hub VPC.  Configure routes in the hub to direct traffic to and from the new VPCs.","correct":false},{"id":"6088d94d774b3a3e9be7fc71331181d1","text":"setup a VPN connection via Internet Gateway between each new and existing VPC to create a mesh network.  Update route tables to direct traffic to the appropriate VPC.","correct":false},{"id":"0d3332dc947d3085b33697562a70c9f5","text":"Initiate a VPC peering request between your hub VPC and all VPCs from the new business.  Setup routes in the hub to direct traffic to and from the new VPCs.","correct":true}]},{"id":"91e4ebb5-18ac-45d6-867e-4c2eddefc075","domain":"awscsapro-domain4","question":"Your company has come under some hard times resulting in downsizing and cuts in operating budgets.  You have been asked to create a process that will increase expense awareness and your enhance your team's ability to contain costs.  Given the reduction in staff, any sort of manual analysis would not be popular so you need to leverage the AWS platform itself for automation.  What is the best design for this objective?","explanation":"AWS Budgets is specifically designed for creating awareness and transparency in your AWS spending rate and trends.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing Your Costs with Budgets - AWS Billing and Cost Management"}],"answers":[{"id":"aadb36f869250c141ae2eeec088d3bb3","text":"Implement a Cost Allocation Tagging strategy.  Create a periodic aggregation process in AWS Glue to read costs based on the tags.  Configure Glue to send the report to SNS where it can be either emailed or texted to the team depending on their own preference.","correct":false},{"id":"be1b52e799fe44ef89f78b167401c067","text":"Export AWS bill data into Redshift.  Use Quicksight to create reports per cost center.  Provide access for users to QuickSight to monitor their usage.","correct":false},{"id":"8e9e7154ed9b1f92015c5fde5ca7a2a3","text":"Use Kinesis Streams to Ingest CloudTrail log streams.  Create a Lambda function to parse the log stream and insert a record into DynamoDB whenever a pay-per-use activity happens.  Use DynamoDB streams to alert the team when a threshold has been exceeded.","correct":false},{"id":"0e5c5be553400912c2517757d9aba6b2","text":"Provide access to Cost Explorer for your team for transparency.  Allow them to periodically review the accrued costs for the month and take the appropriate action.","correct":false},{"id":"ce7a1fa4194d00b76a1491e66b2a10ec","text":"Use AWS Budgets to create a budget.  Choose to be notified when monthly costs are forecasted to exceed your updated monthly target.","correct":true}]},{"id":"02a9611c-591c-4280-bb83-6c65c7c4921f","domain":"awscsapro-domain5","question":"A sporting goods retailer runs WordPress on Amazon EC2 Linux instances to host their customer-facing website. An ELB Application Load Balancer sits in front of the EC2 instances in Auto Scaling Groups in two different Availability Zones of a single AWS region. The load balancer serves as an origin for Amazon CloudFront. Amazon Aurora provides the database for WordPress with the master instance in one of the Availability Zones and a read replica in the other. Many custom and downloaded WordPress plugins have been installed. Much of the DevOps teams' time is spent manually updating plugins across the EC2 instances in the two Availability Zones. The website suffers from poor performance between the Thanksgiving and Christmas holidays due to a high occurrence of product catalog lookups. What should be done to increase ongoing operational efficiency and performance during high-volume periods?","explanation":"ElastiCache Memcached will provide in-memory access speeds for the catalog read transactions. A WordPress plugin is required to leverage caching. WordPress can access an EFS Mount Target for file sharing across all instances. Aurora offers a MySQL option, and WordPress requires MySQL, so the solution would have been set up that way already. CodeDeploy could update plugins on all instances, and will work well for the custom in-house code, but triggering the updates of downloaded plugins will need to be orchestrated. Aurora Auto Scaling will distribute catalog reads across multiple replicas for increased performance, but not to the extent of in-memory caching. Elastic File System is a managed service providing operational advantages over NFS file shares. ElastiCache Redis will provide the in-memory read performance desired, but changing the wp-config.php file won't provide access to it, as a plugin is needed for that. WordPress does work with S3, but a shared file system is easier to implement.","links":[{"url":"https://aws.amazon.com/getting-started/projects/build-wordpress-website/","title":"Build a WordPress Website"},{"url":"https://github.com/aws-samples/aws-refarch-wordpress?did=wp_card&trk=wp_card","title":"Hosting WordPress on AWS"}],"answers":[{"id":"17b12e57cd85610e888cda82b5a8a145","text":"Migrate the WordPress database to RDS MySQL since MySQL is WordPress's native database and WordPress is performance optimized for MySQL. Implement AWS CodeDeploy to update WordPress plugins on all EC2 instances.","correct":false},{"id":"bc643d3342a5a675e65e5baed00e88b9","text":"Deploy Amazon ElastiCache Memcached as a caching layer between the EC2 instances and the database. Install a WordPress plugin to read from Memcached. Implement Amazon Elastic File System to store the WordPress files and create mount targets in each EC2 subnet.","correct":true},{"id":"f060160b20c4408b2442010d3ea4d387","text":"Use Amazon ElastiCache Redis as a caching layer between the EC2 instances and the database. Change wp-config.php to point to the Redis caching layer, and have Redis point to Aurora. Move the WordPress files to S3 and have WordPress access them there.","correct":false},{"id":"aeb27370afc3b672eb0a1afcb28e9176","text":"Implement Aurora Auto Scaling to increase the number of replicas automatically as demand increases. Create an NFS file share to hold the WordPress files. Access the file share from the EC2 instances in both Availability Zones.","correct":false}]},{"id":"ffae5615-188b-4023-aae2-71270158730a","domain":"awscsapro-domain5","question":"Several teams are using an AWS VPC at the same time. The VPC has three subnets (subnet-1a, subnet-1b, subnet-1c) in three availability zones (eu-west-1a, eu-west-1b, eu-west-1c) respectively. As there are more and more AWS resources created, there is a shortage of available IP addresses in subnet-1a and subnet-1b. The subnet subnet-1c in availability zone eu-west-1c still has plenty of IP addresses. You use a CloudFormation template to create an Auto Scaling group (ASG) and an application load balancer for the ASG. You enable three availability zones for the load balancer and the Auto Scaling group also spans all the three subnets. The CloudFormation stack usually fails to launch because there are not enough IP addresses. High availability is not required for your project since it is a proof of concept. Which of the following methods is the easiest one to resolve your problem?","explanation":"The easiest way is to enable only the eu-west-1c availability zone for ELB and launch ASG instances in subnet-1c. Only the IP addresses from eu-west-1c are required to create the resources. For a VPC, the existing CIDR cannot be modified and you also cannot add another CIDR IP range to an existing subnet. For an application load balancer, there is no IP balancing feature and IP addresses from a subnet cannot be reserved by other subnets.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-subnets.html","title":"Availability Zones for Your Application Load Balancer"}],"answers":[{"id":"1f29e88227ce0771f9e43093bd53583a","text":"Add more IP addresses by extending the CIDR range for the VPC. Create a new subnet in each availability zone and reserve at least 128 IP addresses in a subnet. Modify the CloudFormation template to use the new subnets for the Auto Scaling group and ELB.","correct":false},{"id":"94887cfd8a06ae986d5721a940c3c52b","text":"Modify your CloudFormation template to enable only the availability zone eu-west-1c for the application load balancer and launch the Auto Scaling group in subnet-1c which belongs to eu-west-1c.","correct":true},{"id":"b8c040b89e07e5150d4a8a3899d41659","text":"Add another IPv4 CIDR to the VPC which should have at least 256 IP addresses. Add another IP CIDR block to subnet-1a and subnet-1b to increase the available IP addresses.","correct":false},{"id":"9cb12513be8578fb017ebb7d1b302f9a","text":"Enable the IP balancing feature in the application load balancer so that the IP addresses are equally distributed among subnets. When elastic load balancer is created, available IP addresses in one subnet can be reserved by other subnets.","correct":false}]},{"id":"a4d41d3b-abbe-4121-8e1d-5567b1ec7294","domain":"awscsapro-domain5","question":"You are an AWS administrator and you need to maintain multiple Amazon Linux EC2 instances. You can SSH to the instances with a .pem key file created by another colleague. However, as the colleague will leave the company shortly, the SSH key pair needs to be changed to a new one created by you. After the change, users should be able to access the instances only with the new .pem key file. The old key should not work. How would you get the key pair replaced properly?","explanation":"You do not need to launch new instances as you can simply paste the public key content in the \".ssh/authorized_keys\" file to enable the new key pair. You cannot directly change the key through AWS Management Console by clicking the \"change SSH key\" button. You are also not allowed to change the SSH key when stopping and starting instances. Users can only select an SSH key pair when they launch a new instance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#replacing-key-pair","title":"Adding or Replacing a Key Pair for Your Instance"}],"answers":[{"id":"be5d2e07f838b7752b235431feaae361","text":"The SSH key pair cannot be changed after EC2 instances are launched. Take AMIs or EBS snapshots from existing instances, terminate the instances, launch new ones with the AMIs/snapshots and select another SSH key pair.","correct":false},{"id":"b216d43c3e1e71b67af45a138184b181","text":"Create a new SSH key pair, get the public key information from it, paste it in the \".ssh/authorized_keys\" file in the Amazon Linux EC2 instances and remove the old public key.","correct":true},{"id":"4fa9ce5fdb3a2215ed8798d2e734fd42","text":"In AWS Management Console, create a new key pair and download the .pem file at a safe place. Select the Amazon EC2 instances and click \"change SSH key\" to get the key replaced. The EC2 instances are automatically restarted after the change.","correct":false},{"id":"521ec8da4f5fa8bee591740d44a8427e","text":"Create a new key pair locally or through AWS EC2 service. Take AMIs of the instances as backups. Stop the instances and choose the new key pair when restarting the instances. Notify the team to start using the private key for SSH connections.","correct":false}]},{"id":"cb61ecae-afb0-4437-970e-72ddfe908e6b","domain":"awscsapro-domain1","question":"You are an AWS architect working for a B2B Merger and Acquisitions consulting firm, which has 15 business units spread across several US cities. Each business unit has its own AWS account. For administrative ease and standardization of AWS Usage patterns, corporate headquarters have decided to use AWS Organizations to manage the individual accounts by grouping them into relevant Organization Units (OU-s).\nYou have assisted the Organization Administrator to write and attach Service Control Policies (SCP-s) to the OU-s. SCP-s have been configured as the default Deny list, and they are written to explicitly deny actions wherever required.\nData Scientists in one of the Business Units are complaining that they are unable to spin up or access Sagemaker Clusters for building, training and deploying Machine Learning models. Which of the following can be a possible cause and how can this be fixed?","explanation":"This question tests the conceptual knowledge of Service Control Policies (SCP-s) in AWS Organizations.\nThe choice that requires the SCP to be modified is incorrect because there is no need to grant explicit allows from SCP, especially when it is configured in the default mode (Deny List mode). In this mode, everything is allowed by default. We only need to specify what we want to deny.\nThe choice that requires the IAM Policy to be modified is correct because SCPs do not actually grant any permission. The permission that is missing in this case must be granted via IAM Roles and Policies at the Account level.\nThe choice mentioning Service Linked Roles is incorrect as Trust Policies on Service Linked Roles cannot be modified to let an IAM user assume that role. Service Linked Roles are for AWS Services.\nThe choice that requires re-configuration of SCP as Allow List is incorrect because configuring SCP as Allow List is usually a messy idea. In that case, all permissions will need to be explicitly granted, and it can easily defeat the purpose of streamlining management and reducing administrative overhead by using AWS Organizations. Allow Lists have very specific use cases. In addition, no change in the SCP grants or allows any permission. Permission needs to be granted using IAM Roles and Policies at the Account level.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_denylist","title":"Using SCPs as a Deny List"},{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_allowlist","title":"Using SCPs as an Allow List"},{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Documentation for Service Control Policies"}],"answers":[{"id":"1b1615b479c5aa7e1e9a3547b926aaef","text":"The Service Linked Role associated with AWS Sagemaker does not allow the data scientists to assume the Role. To fix this, add a Trust Policy to the Sagemaker Service Linked Role that lists the IAM user ids of the data scientists as Principal, with the value of Action is AssumeRole and Effect is set to Allow","correct":false},{"id":"f64b0d2d0b81fd2aa0badced0c59abfe","text":"The IAM Policy attached to the IAM Role that the data scientists are assuming in the Business Unit Account does not grant them Sagemaker access. To fix this, add the following to the IAM Policy Statement for that Role - Effect set to Allow, Action set to Everything starting with SageMaker, Resource set to All","correct":true},{"id":"cd63be86b6c62d032d40d6408a54bda9","text":"The SCP for the OU to which the Business Unit Account belongs does not explicitly allow granting Sagemaker access. To fix this, add the following to the attached policy Statement of the SCP - Effect set to Allow, Action set to Everything starting with SageMaker, Resource set to All","correct":false},{"id":"ad6186009ae4678d814923c4929282a5","text":"SCP is configured as a Deny List. To fix this, SCP must be configured as an Allow List instead of a Deny List for the OU. Then, Sagemaker access should be added explicitly","correct":false}]},{"id":"63a6def8-9b52-4d89-8248-6079ca1393e2","domain":"awscsapro-domain3","question":"You are helping a client prepare a business case for cloud migration.  One of the required parts of the business case is an estimation of AWS costs per month.  The client has about 200 VMs in their landscape under VMware vCenter.  Due to security concerns, they will not allow any external agents to be installed on their VMs for discovery.  How might you most efficiently gather information about their VMs to build a cost estimate with the least amount of effort? ","explanation":"The Application Discover Service uses agent-based or agentless collection methods.  Agentless collection is only available for those customers using VMware.  The AWS Application Discovery Agentless Connector is delivered as an Open Virtual Appliance (OVA) package that can be deployed to a VMware host. Once configured with credentials to connect to vCenter, the Discovery Connector collects VM inventory, configuration, and performance history such as CPU, memory, and disk usage and uploads it to Application Discovery Service data store.  This data can then be used to estimate monthly costs.","links":[{"url":"https://aws.amazon.com/application-discovery/faqs/?nc=sn&loc=6","title":"AWS Application Discovery Service FAQs"}],"answers":[{"id":"30cbd0a822a4905f8a795dcb7cc3d31e","text":"Use a custom script to iteratively log into each VM and pull network, hardware and performance details of the VM.  Write the data out to S3 in CSV format.  Use that data to select corresponding EC2 instance sizes and calculate estimated monthly cost.","correct":false},{"id":"0a144eb3993e48693aab4c9744b6acb2","text":"Use AWS OpsWorks to remotely pull hardware, network connection and performance of the VMs.  Export the collected data from OpsWorks in Excel format.  Use the collected data to align current VMs with similar EC2 instance types and calculate an estimated monthly cost.","correct":false},{"id":"09d153439d976dabcdd13a4a2f8a4a5f","text":"Use Application Discovery Service to gather details on the network connections, hardware and performance of the VMs.  Export this data as CSV and use it to approximate monthly AWS costs by aligning current VMs with similar EC2 instances types.","correct":true},{"id":"b279286ed54b2394c29d2fbd0061b4c4","text":"Provision an S3 bucket for data collection.  Use SCT to scan the existing VMware landscape for VM hardware, network connection and performance parameters.  Retrieve the SCT CSV data from the data collection bucket and use it to align EC2 instance types with existing VM parameters.  Use this cross-reference to calculate estimated monthly costs for AWS.","correct":false}]},{"id":"54d12a9a-149b-42a7-8491-300583d5c2b8","domain":"awscsapro-domain5","question":"A client is trying to setup a new VPC from scratch.  They are not able to reach the Amazon Linux web server instance launched in their VPC from their on-prem network using a web browser.  You have verified the internet gateway is attached and the main route table is configured to route 0.0.0.0/0 to the internet gateway properly.  The instance also is being assigned a public IP address.  Which of the following would be another potential cause of the problem?","explanation":"For an HTTP connection to be successful, you need to allow port 80 inbound and allow the ephemeral ports outbound.  Additionally, it is possible that the subnet is not associate with the route table containing the default route to the internet.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html","title":"Troubleshooting Connecting to Your Instance - Amazon Elastic Compute Cloud"}],"answers":[{"id":"32cdd854c9d71059eac396dd1249830e","text":"The subnet of the instance is not associated with the main route table.","correct":true},{"id":"25d3393c550b8dbc2179367832573598","text":"The outbound network ACL allows port 80 and 22 only.","correct":true},{"id":"c9f886542d1dabe99bc64dd39c5e1615","text":"The inbound security group allows port 80 and 22 only.","correct":false},{"id":"01b7463243eb231b840fcd4b737e044b","text":"The default route to the internet gateway is incorrect.","correct":false},{"id":"c7ca1b6a8fe855bda71123163488960b","text":"The customer has disabled the ec2-user account on the Amazon Linux instance.","correct":false},{"id":"2bde109ce87f4a4f513679f31116184d","text":"The instance does not have an elastic IP address assigned. ","correct":false},{"id":"d0f5e76f9fc753305b11c4c3a11e97ef","text":"The IAM role assigned to the LAMP instances does not have any policies assigned.","correct":false}]},{"id":"9564dd14-763c-45c7-8546-a8bbe687a55e","domain":"awscsapro-domain2","question":"You are helping a company design a new data analysis system.  The company captures data in the form of small JSON files from thousands of pollution sensors across the country every 10 minutes.  Presently, they use some BASH scripts to load data into an aging IBM DB2 database but they would like to upgrade to a more scalable cloud-based option.  A key requirement is the ability to replay the time-series data to create visualizations so they have to keep and query lots of detailed data. They intend to keep their current visualization tools which are compatible with any database that provides JDBC drivers.  Which of the following architectures IS NOT suited to help them Ingest and analyze the sensor data?","explanation":"For EMR, the task nodes do not store any HDFS data.  Task node storage is considered ephemeral so this would not be a good choice.","links":[{"url":"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html","title":"Understanding Master, Core, and Task Nodes - Amazon EMR"}],"answers":[{"id":"23ad2d64a285f603e5ca0a29c6a444e7","text":"Spin up an EMR cluster.  Ingest the data into HDFS partitions on the EMR task nodes using Flume.  Use Hive to provide JDBC access.","correct":true},{"id":"0439c9dae7a15237bd7e1bfdca41bbd2","text":"Spin up a Redshift database.  Use Kinesis Firehose to load the data directly into Redshift.","correct":false},{"id":"98678aaeaf75f088f0d57ca7b321f688","text":"Use Lambda to read the sensor files and write them to a DynamoDB table.  Use a third-party JDBC driver to provide query access.","correct":false},{"id":"c552ecc7ce7256e713857b27429e7753","text":"Load the files into S3 and use AWS Athena as the database layer.","correct":false},{"id":"9ee2b80a7b38c15f156422085185bf91","text":"Deploy Amazon Aurora for MySQL and download the MySQL JDBC drivers. Use AWS Glue to Ingest the raw sensor files into Aurora.","correct":false}]},{"id":"d8bfc54e-024a-4fbb-9daa-9a218a10b738","domain":"awscsapro-domain5","question":"Your company has an Inventory Control database running on Amazon Aurora deployed as a single Writer role. Over the years more departments have started querying the database and you have scaled up when necessary.  Now the Aurora instance cannot be scaled vertically any longer, but demand is still growing.  The traffic is 90% Read based.  Choose an option from below which would meet the needs of the company in the future.","explanation":"This question is about scaling, and if you have scaled up to the maximum level (db.r4.16xlarge) the next step is to consider scaling out.  In this case the application is Read heavy, which lends itself perfectly to adding extra Read replicas and using Read-Write splitting to help future growth.  Changing the max_connections value or using Query plan optimisation may make performance more efficient, but they are not long term solutions. Adding Multi-AZ simply adds High Availability.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html","title":"Managing Performance and Scaling for Aurora DB Clusters"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-replicas-adding.html","title":"Adding Aurora Replicas to a DB Cluster"}],"answers":[{"id":"15cd367246a8991991de371856908334","text":"Create multiple additional Readers within the Aurora cluster and alter the application to make use of Read-Write splitting","correct":true},{"id":"c6229282cfe5ed1024d22e629709b94d","text":"Increase the maximum number of connections into the database by changing the max_connections parameter","correct":false},{"id":"d1fe9035612273c2d0c6b9f4229e1d3d","text":"Convert Aurora to a Multi-AZ Deployment in three or more zones","correct":false},{"id":"ce644fa65dde55c85ae051c5081ba860","text":"Use Query plan management to allow the optimizer to choose the most efficient plan for each job and make transactions quicker","correct":false}]},{"id":"cb24982e-2c2d-43d4-872f-2dabbdb7e367","domain":"awscsapro-domain2","question":"You are helping an IT Operations group transition into AWS.  They will be created several instances based off the latest Amazon Linux 2 AMI.  They are unsure of the best way to enable secure connections for all members of the group and certainly do not want to share credentials. Which of the following methods would you recommend?","explanation":"Of the provided options, the only one that upholds AWS best practices for providing secure access to EC2 instances is to use AWS Session Manager.  ","links":[{"url":"https://aws.amazon.com/blogs/aws/new-session-manager/","title":"New – AWS Systems Manager Session Manager for Shell Access to EC2 Instances  | AWS News Blog"}],"answers":[{"id":"cce617457cd8701c595d236b7fa4ed7c","text":"Allow each administrator to create their own SSH keypair and assign them all to the SSH Key for the instance upon each launch.","correct":false},{"id":"e36f501af80c8f03e08321d565cc900e","text":"Share the single private SSH key with each administrator in the group.","correct":false},{"id":"93ecb84b9fc90ec0bc4db84968ab5ebf","text":"Allow administrators to update the SSH key of the instance in the AWS console each time they need access to a system.","correct":false},{"id":"b1ab01927f47a067694506df9d5249e3","text":"Configure IAM role access for AWS Systems Manager Session Manager.","correct":true},{"id":"c594a26a8a9141cdd5615a0761fb2438","text":"Create a bastion host and use it like a jump-box.  Paste each administrators private key into the known_hosts file on the bastion host.","correct":false}]},{"id":"338ce579-a236-4282-9670-8da3b0baf2e9","domain":"awscsapro-domain3","question":"You are helping a Retail client migrate some of their assets over to AWS.  Presently, they are in the process of moving their Enterprise Data Warehouse.  They are planning to re-host their very large Oracle data warehouse on EC2 in a high availability configuration across AZs.  They presently have several Scala scripts that process some detailed Point of Sale data that is collected each day.  The scripts perform some aggregation on the data and import the aggregate into their Oracle database.  They want to move this process to AWS as well.  Which option would be the most cost-effective way for them to do this?","explanation":"AWS Glue is a fully managed extract, translate and loading service and is compatible with Scala.  EMR could do this but represents more overhead than necessary.  Lambda is not compatible with Scala and migrating to Redshift does not bring anything in this case if the customer wants to retain their Oracle database.","links":[{"url":"https://aws.amazon.com/glue/faqs/","title":"AWS Glue Features - Amazon Web Services"}],"answers":[{"id":"4f1ff8b853c3ba363bdd2bda53538ab4","text":"Migrate from Oracle to Redshift and use Kinesis Firehose.","correct":false},{"id":"8b01d948d5ad2f4b1c8e817c2d98e7c2","text":"Migrate the processing to AWS Glue.","correct":true},{"id":"a445a1a877009cd7c31858687a818116","text":"Import your Scala scripts into AWS SCT for processing.","correct":false},{"id":"1a4de6676c8c078310e08aad71d9dce6","text":"Migrate the processing to AWS EMR.","correct":false},{"id":"04578ae8419780f9dc441d01fe11582d","text":"Create Lambda functions using your Scala scripts.","correct":false}]},{"id":"2b66bd04-756d-4e2f-a628-1e9b76a57066","domain":"awscsapro-domain2","question":"A media company is producing a live streaming video broadcast of a sporting event for a customer. The announcers will be delivering play-by-play analysis in English. The broadcast will be aired over the Internet, and will require real-time subtitles in Spanish. The company has decided to run all aspects of the production on AWS. Which architecture will provide the functionality needed to deliver the broadcast with real-time subtitles?","explanation":"AWS Elemental MediaLive is a broadcast-grade live video processing service that lets you create high-quality video streams for delivery to televisions and internet-connected devices. Storing it's output in S3 can trigger a Lambda function to extract the unsigned PCM audio from the video segments. A second Lambda function can use Amazon Transcribe to convert the audio to text, which can then be run through Amazon Translate to create the Spanish subtitles. The first Lambda function can send the Spanish subtitle files, the manifests, and the video files to AWS Elemental MediaPackage for distribution through CloudFront. AWS Elemental MediaTailor is used to insert targeted advertising into video streams, not enhance video with subtitles. Amazon Comprehend provides text sentiment analysis, not speech to text conversion.","links":[{"url":"https://aws.amazon.com/medialive/","title":"AWS Elemental MediaLive"},{"url":"https://aws.amazon.com/transcribe/","title":"AWS Transcribe"},{"url":"https://aws.amazon.com/translate/","title":"AWS Translate"},{"url":"https://aws.amazon.com/mediapackage/","title":"AWS Elemental MediaPackage"},{"url":"https://aws.amazon.com/solutions/live-streaming-with-automated-multi-language-subtitling/?did=sl_card&trk=sl_card","title":"Live Streaming with Automated Multi-Language Subtitling"}],"answers":[{"id":"2d6eb42c2e56159dbe9d0eb1c06b9681","text":"Transmit the live video to AWS Elemental MediaLive and deliver it's output to Amazon Kinesis Video Streams. Configure S3 and Amazon Comprehend as consumers of the stream. Have Comprehend write the text files to a different S3 bucket than the video files, and trigger a Lambda function on that bucket to have Amazon Translate create the Spanish transcripts. Use AWS Elemental MediaTailor to insert the subtitles into the video segments. Send the video files to AWS Elemental MediaStore. Create an Amazon CloudFront distribution with MediaStore as its origin.","correct":false},{"id":"a8d998390ff1bff69d4a734c8c8747e6","text":"Feed the live video into AWS Elemental MediaLive and deliver it's output to Amazon S3. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Invoke a second Lambda function, which uses Amazon Transcribe to convert the audio files to text and return the text to the first Lambda function. Have the first Lambda function use Amazon Translate to create the Spanish transcript. Send the subtitle files, manifests, and video files to AWS Elemental MediaPackage. Create an Amazon CloudFront distribution with MediaPackage as its origin.","correct":true},{"id":"1cc4dd5872e2deead228ed9c6651c445","text":"Deliver the live video to AWS Kinesis Data Streams and make Amazon S3 the consumer. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Invoke a second Lambda function, which uses Amazon Comprehend to convert the audio files to text and return the text to the first Lambda function. Have the first Lambda function use Amazon Translate to create the Spanish transcript. Send the subtitle files, manifests, and video files to AWS Elemental MediaPackage. Create an Amazon CloudFront distribution with MediaPackage as its origin.","correct":false},{"id":"58eec57699d2f82a27ad049e949eb09e","text":"Send the live video to AWS Elemental MediaLive and store it's output in Amazon S3. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Have the Lambda function call Amazon Transcribe to convert the audio files to text, and then use Amazon Translate to create the Spanish transcript. Use AWS Elemental MediaTailor to insert the subtitles into the video segments. Send the video files to AWS Elemental MediaStore. Create an Amazon CloudFront distribution with MediaStore as its origin.","correct":false}]},{"id":"2c204ae8-6bba-49a1-b8f6-1aa4330c3d8c","domain":"awscsapro-domain5","question":"You are helping an IT organization meet some security audit requirements imposed on them by a prospective customer.  The customer wants to ensure their vendors uphold the same security practices as they do before they can become authorized vendors.  The organization's assets consist of around 50 EC2 instances all within a single private VPC.  The VPC is only accessible via an OpenVPN connection to an OpenVPN server hosted on an EC2 instance in the VPC.  The customer's audit requirements disallow any direct exposure to the public internet.  Additionally, prospective vendors must demonstrate that they have a proactive method in place to ensure OS-level vulnerability are remediated as soon as possible.  Which of the following AWS services will fulfill this requirement?","explanation":"AWS Macie is a service that attempts to detect confidential data rather than OS vulnerabilities.  Since there is no public internet access for the VPC, services like GuardDuty and Shield have limited usefulness. They help protect against external threats versus any OS-level needs.  AWS Artifact is simply a document repository and has no monitoring functions.  Only AWS Inspector will proactively monitor instances using a database of known vulnerabilities and suggest patches.","links":[{"url":"https://aws.amazon.com/inspector/faqs/","title":"FAQs - Amazon Inspector - Amazon Web Services (AWS)"},{"url":"https://aws.amazon.com/macie/","title":"Amazon Macie | Discover, classify, and protect sensitive data | Amazon Web  Services (AWS)"}],"answers":[{"id":"45d5e166ed185c1f7516650c714423dd","text":"Enable AWS Artifact to periodically scan my instances and prepare a report for the auditors.","correct":false},{"id":"81133f0650fa1ca2fbe1b920a6c67cc9","text":"Employ Amazon Inspector to periodically assess applications for vulnerabilities or deviations from best practices.","correct":true},{"id":"7ed3972608faa6c3dfc6fda7f151889c","text":"Enable AWS GuardDuty to monitor and remediate threats to my instances.","correct":false},{"id":"6a353f53758a3fd632209b5286a01086","text":"Enable AWS Shield to protect my instances from unauthorized access.","correct":false},{"id":"178912f5fbd90ab710621756a2ba18ff","text":"Employ AWS Macie to periodically assess my instances for vulnerabilities and proactively correct gaps.","correct":false}]},{"id":"24937858-d37e-4f9b-b195-d87c42b3f1ca","domain":"awscsapro-domain2","question":"You are designing a workflow that will handle very confidential healthcare information.  You are designing a loosely coupled system comprised of different services.  One service handles a decryption activity using a CMK stored in AWS KMS.  To meet very strict audit requirements, you must demonstrate that you are following the Principle of Least Privilege dynamically--meaning that processes should only have the minimal amount of access and only precisely when they need it.  Given this requirement and AWS limitations, what method is the most efficient to secure the Decryption service?","explanation":"Grants in KMS are useful for dynamically and programmatically allowing a process the ability to use the key then revoking after the need is over.  This is more efficient than manipulating IAM roles or policies.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/grants.html","title":"Using Grants - AWS Key Management Service"}],"answers":[{"id":"45b8475be8fa894d6c7145b5536d21fe","text":"Use a grant constraint to deny access to the key except for the service account that is running the workflow processes.  Enable CloudTrail alerts if any other role attempts to access the CMK.","correct":false},{"id":"b5f054e3c6369a2c4a9256e55359f9ff","text":"The current AWS platform services are not well suited for implementing Principle of Least Privilege in a dynamic manner.  Consider a different design that makes use of a more monolithic architecture rather than services.","correct":false},{"id":"2202679b0d915deca81caf1d431477a4","text":"Create a IAM key policy that explicitly allows access to the CMK and assign that to a role.  Assign the role to the process that is executing the Decryption service.  At the end of the day, programmatically revoke that role until the start of the next day.","correct":false},{"id":"6c5638772104a82518657e9bfbc0970d","text":"In the step right before the Decryption step, programmatically apply a grant to the CMK that allows the service access to the CMK key.  In the step immediately after the decryption, explicitly revoke the grant.","correct":true},{"id":"3cc7443088dcd1d7935eaf578a49d078","text":"Create an IAM key policy that explicitly denies access to the Decryption operation of the CMK.  Assign that policy to a role that is then assigned to the process executing the Decryption service.  Use a Lambda function to programmatically remove and add the IAM policy to the role as needed by the decryption process.","correct":false}]},{"id":"ba6576ca-a3da-4742-8917-cf5852a133bc","domain":"awscsapro-domain2","question":"You are in the process of porting over a Java application to Lambda.  You find that one Java application's code exceeds the size limit Lambda allows--even when compressed.  What can you do?","explanation":"If your code is too large for Lambda, it might indicate the need to break the code down into more atomic elements to support microservice best practices.  If breaking the code down is not possible, you should consider deploying in a different way like ECS or Elastic BeanStalk.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/limits.html","title":"AWS Lambda Limits - AWS Lambda"}],"answers":[{"id":"0d95df1e6f5e72a068c3fbcbc598b16d","text":"Use AWS CodeBuild to identify unused libraries and remove them from the package. ","correct":false},{"id":"7c5f7ca2bf3fe981064a0883ba4d7158","text":"Change the Java Runtime Version in the Lambda function to one that supports BIGINT. ","correct":false},{"id":"5fd2567315fb3689d9b56ea422cba642","text":"Evaluate the structure of the program and break it into more modular components.","correct":true},{"id":"2c12b2cfb5b150801f171e15a1e6bfb7","text":"Consider containerization and deploy using Elastic Beanstalk.","correct":true},{"id":"91f37922d4b3b7e70c1f900cb95370e8","text":"Enable Extended Storage in the Lambda console to permit a larger codebase to be deployed.","correct":false},{"id":"a6bb1edf368a254d9604ca8e6419be1d","text":"Consider using API Gateway to offload some of the I/O your code requirements.","correct":false}]},{"id":"49107f33-5b31-4d7e-a2cb-95f3ce8a2d75","domain":"awscsapro-domain1","question":"Your customer has setup AWS Organizations to help manage a collection of AWS Accounts.  They are running into a problem though and need your help.  They have created accounts for each business unit and applied SCPs to those OUs. However, they notice that root accounts in in those sub-accounts can still change root access keys and disable MFA.  How do you instruct your customer?","explanation":"Service Control Policies can control many aspects but they cannot restrict root account actions of changing root access keys or disabling MFA.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"194cc2d07b5c378b62b1e090f0aea956","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the entire OU in the root account.","correct":false},{"id":"3d722952f024bcec9174a311c17dcc14","text":"You can not use SCPs to restrict root account activities of changing the root password or managing MFA settings.","correct":true},{"id":"94a2f948d3d2f9c317a6ebb1f5a24ea5","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the specific sub-accounts.","correct":false},{"id":"77df34553819fdc2e31fb79762948993","text":"You can establish a trust with the top-level account and use the \"organizations:ServicePrinciple\" condition key to restrict root access at the sub-account level.","correct":false}]},{"id":"115e1b30-23e4-4f3f-9c13-a0086f6af223","domain":"awscsapro-domain2","question":"You are working with a pharmaceutical company on designing a workflow for processing data.  Once a day, a large 2TB dataset is dropped off at a pre-defined file share where the file is processed by a Python script containing some proprietary data aggregation routines.  On average, it takes 20-30 minutes to complete the processing.  At the end, a notification has to be sent to the submitter of the dataset letting them know processing is complete.  Which of the following architectures will work in this scenario?","explanation":"While it may not be the most cost-effective, the EFS option is the only one that can work.  A processing time of 20-30 minutes rules out Lambda (at present with a processing limit of 15 minutes).  If we create an EBS volume with a full OS on it and mount as root for a new instance with the data set included, we still would not be able to dismount the root volume without shutting down the instance.  This would not let us issue an SES SDK call.  The database is also far too large for SQS.","links":[{"url":"https://aws.amazon.com/efs/features/","title":"Amazon Elastic File System (EFS) | Cloud File Storage | Product Features"}],"answers":[{"id":"5f8da9106eba72e74c1a0d6415a235af","text":"Create an S3 bucket to store the incoming dataset.  Once the dataset has been fully received, use S3 Events to launch a Lambda function with the Python script to process the data.  When finished, use an SDK call to SNS to notify when the processing is complete.  Store the processed data back on S3.","correct":false},{"id":"1831d929918e3c425b20e476c1716ccc","text":"Load inbound dataset on an EBS volume.  Stand up an EBS-optimized instance and mount the data volume as the root volume.  Once the data processing is complete, unmount the EBS volume and issue an SDK call to SES to notify of completion.  Configure SES to trigger an instance shutdown after the notification is sent.","correct":false},{"id":"dee7711bae75cac018761467694d89e3","text":"Use SQS to take in the data set.  Use a Step Function to Launch Lambda functions in a fan-out architecture for data processing and then send an SNS message to notify when the processing is complete.  Store the processed data on S3.","correct":false},{"id":"3e1542fcef251cf86c8bdcc89d83aaa7","text":"Stand up memory optimized instances and provision an EFS volume. Pre-load the data on the EFS volume.  Use a User Data script to sync the data from the EFS share to the local instance store.  Use an SDK call to SNS to notify when the processing is complete, sync the processed data back to the EFS volume and shutdown the instance. ","correct":true}]},{"id":"58fa6440-b521-4dff-b6be-b1f8818c718d","domain":"awscsapro-domain2","question":"You currently manage a website that consists of two web servers behind an Application Load Balancer.  You currently use Route 53 as a DNS service.  Going with the current trend of websites doing away with the need to enter \"www\" in front of the domain, you want to allow your users to simply enter your domain name. What is required to allow this?","explanation":"Route 53 allows you to create a record for a zone apex.  In this case, we have created an alias record for the ALB.","links":[{"url":"https://docs.aws.amazon.com/govcloud-us/latest/ug-west/setting-up-route53-zoneapex-elb.html","title":"Setting Up Amazon Route 53 Zone Apex Support with an AWS GovCloud (US-West)  Elastic Load Balancing Load Balancer - AWS GovCloud (US-West) User Guide"}],"answers":[{"id":"0dc522eb50b1c1320900ac02fcfb4dd6","text":"Create an S3 bucket as a static web host.  Create simple HTML file that redirects to the www subdomain.  Use CloudFront custom origins as a front-end for your top-level domain name.","correct":false},{"id":"f70aff68786523bc13fd44cb89feb502","text":"Create an A record for your top-level domain name using the public IP of your ALB.","correct":false},{"id":"9e87834a987d5b3556ce3e9f35ac6a82","text":"Create an A record for your top-level domain name as an alias for the ALB.","correct":true},{"id":"5c48fe59299d582ae6074d269711de20","text":"Route 53 does not currently support zone apex records.  You must use a third-party DNS provider.","correct":false},{"id":"935c6d54a7555f482da11c1ded5dfc9c","text":"Create a CNAME record for the root domain and configure it to resolve to www subdomain name.","correct":false}]},{"id":"11c28d09-1ccf-46ac-a56e-3998bff9c4e4","domain":"awscsapro-domain2","question":"You bought a domain name \"example.com\" from GoDaddy which is a domain registrar and the domain name will expire in several months. You plan to start using AWS Route 53 to manage the domain and resolve its DNS queries. The transferred domain in Route 53 should be automatically renewed every year so that the domain name will never expire and you do not need to renew it manually. Which method would you use to transfer the domain name properly?","explanation":"Users can register their new domain names in Route 53 or transfer existing domain names from other registrars such as GoDaddy to Route 53. After the transfer, the domain can automatically renew every year if the Auto Renew feature is enabled. To transfer the domain name, you do not need to wait until the domain name expires. And you cannot register the same domain name in both GoDaddy and Route 53 at the same time.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-transfer-to-route-53.html","title":"Transferring Registration for a Domain to Amazon Route 53"}],"answers":[{"id":"e27ac2c31c859905f6a51f22ab3a34bf","text":"Confirm that the domain is transferable in GoDaddy. In the Route 53 AWS Management Console, click \"Transfer Domain\" to transfer registration for the domain name from GoDaddy to Route 53. Enable the automatic renewal for this domain name.","correct":true},{"id":"d8a7c0a25d22a5534a30c668de1be1d1","text":"Register the same domain name \"example.com\" in Route 53 three months before it expires in GoDaddy. Enable the feature of Transfer Lock in Route 53 to prevent it from being transferred to another registrar. Do not renew the original domain name in GoDaddy.","correct":false},{"id":"bd0fd22629d8016f6941db326aeb4ba4","text":"Wait until the domain name expires in GoDaddy and then register the domain in AWS Route 53 by clicking the \"Register Domain\" button in AWS management console. Turn on the features of Auto Renew and Transfer Lock for the new domain.","correct":false},{"id":"32c733a4bf01f533d38704f340cb7eb5","text":"Login in the GoDaddy admin account, unlock the domain transfer and request the domain transfer to Route 53. Accept the domain transfer in Route 53 and extend the expiration date to 10 years as transferred domains cannot automatically renew.","correct":false}]},{"id":"401cbed4-e977-4303-9344-586af01a4180","domain":"awscsapro-domain2","question":"You have been contracted by a manufacturing company to create an application that uses DynamoDB to store data collected in a automotive part machining process.  Sometimes this data will be used to replay a process for a given serial number but that's always done within 7 days or so of the manufacture date.  The record consists of a MACHINE_ID (partition key) and a SERIAL_NUMBER (sort key).  Additionally, there is a CREATE_TIMESTAMP attribute that contains the creation timestamp of the record and a DATA attribute that contains a BASE64 encoded stream of machine data.  To keep the DynamoDB table as small as possible, the industrial engineers have agreed that records older than 30 days can be purged on a continual basis.  Given this, what is the best way to implement this with the least impact on provisioned throughput.","explanation":"Using DynamoDB Time to Live feature is a perfect way to purge out old data and not consume any WCU or RCU.  Other methods of deleting records would impact the provisioned capacity units.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","title":"Time To Live - Amazon DynamoDB"}],"answers":[{"id":"081f76fb4967f9d10a3799ae400ad898","text":"Update the table to add a attribute called EXPIRE  Change the application to store EXPIRE as CREATE_TIMESTAMP + 30 days.  Enable Time to Live on the DynamoDB table for the EXPIRE attribute.","correct":true},{"id":"ef62d6ec88d117a0ac0cb7c99cd1abbd","text":"Use DynamoDB Streams to trigger a Lambda function when the record ages past 30 days.  Use the DynamoDB SDK in the Lambda function to delete the record.","correct":false},{"id":"7c29d6c8effb51be4df54033ce45d01f","text":"Enabled Lifecycle Management on the DynamoDB table.  Create a rule that deletes any records where CREATE_TIMESTAMP attribute is greater than 30 days old.","correct":false},{"id":"0a945c4865c940ffaddaeade6f6bbdaf","text":"Use Step Functions to track the lifecycle of DynamoDB records.  Once 30 days has elapsed, branch to a Delete step and trigger a Lambda function to remove the record.","correct":false},{"id":"41f68b5f48ef97cc437ebfe50ae10882","text":"Use AWS Batch to execute a daily custom script which queries the DynamoDB table and deletes those records where CREATE_TIMESTAMP is older than 30 days.  ","correct":false}]},{"id":"512696b6-6160-45f7-8000-d664f78a86aa","domain":"awscsapro-domain2","question":"You have been contracted by a Security Company to build a face recognition service for its customer, Department of Corrections, in the AWS Cloud. Whenever a new inmate or personnel joins a facility, their facial image will be taken by an application running on a laptop, and stored centrally, along with metadata like their name. They will have a second application getting a live image feed from cameras installed throughout the secure areas of the facility. Whenever the second application receives an image from a camera, it needs to check against the pool of images stored centrally to check if there is a match. If a match is found, the location must be saved along with the timestamp and name in a database which can later be used to query the location of a person at or near a given time-period.\nHow will you, as the AWS Architect, design this suite of applications?","explanation":"This question tests the knowledge of the various Machine Learning technologies and services offered by AWS. Amazon Rekognition is the AWS AI service for image and video analysis. The question also offers AWS Sagemaker, AWS Personalize and AWS Comprehend as alternatives. AWS Sagemaker is used to build and train Machine Learning models. Hence, it is not relevant in this scenario, as the use case is not about training a model. Amazon Personalize is an ML service that enables developers to create individualized recommendations for customers using their applications. This use case is not related to recommendations, hence we can eliminate AWS Personalize. Amazon Comprehend is a natural language processing (NLP) service that uses ML to find relationships and insights in text. We can eliminate Comprehend as this is an image analysis scenario, as opposed to text analysis.","links":[{"url":"https://aws.amazon.com/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/","title":"Build Your Own Face Recognition Service Using Amazon Rekognition"},{"url":"https://aws.amazon.com/machine-learning/","title":"Various components of Machine Learning on AWS"}],"answers":[{"id":"3fac11da1f4ec1911a5b1c4262c16bb4","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Comprehend from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in an RDS Postgresql database. Later, when a match is required, query Amazon Comprehend with the image taken by security cameras. Comprehend will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the RDS database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to a second RDS database for later querying","correct":false},{"id":"3c168275751e0bed48552554f05bad14","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Personalize from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in a Dynamodb database. Later, when a match is required, query Amazon Personalize with the image taken by security cameras. Personalize will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the Dynamodb database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to an RDS database for later querying","correct":false},{"id":"a24bc02aa584b134a2abecb810d5b32c","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Rekognition from the Lambda function to index the face in the image as a Collection, returning a Face id. Store this face id and Name of the person in a Dynamodb database. Later, when a match is required, query the Amazon Rekognition Collection with the image taken by security cameras. Rekognition will return a set of face ids that have potentially matched. If a face id is found whose confidence value is higher than a predefined set value, query the Dynamodb database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to an RDS database for later querying","correct":true},{"id":"cc5d32350e93736f1e3ec5e359029c80","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Sagemaker Image Classification Algorithm from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in an RDS Postgresql database. Later, when a match is required, query Amazon Sagemaker with the image taken by security cameras. Sagemaker will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the RDS database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to a second RDS database for later querying","correct":false}]},{"id":"66d28221-31ce-4cf0-aca3-2b5a69535bb5","domain":"awscsapro-domain3","question":"You are consulting with a small Engineering firm that wants to move to a Bring-Your-Own-Device policy where employees are given some money to buy whatever computer they want (within certain standards).  Because of device management and security concerns, along with this policy is the need to create a virtualized desktop concept.  The only problem is that the specialized engineering applications used by the employees only run on Linux.  Considering current platform limitations, what is the best way to deliver a desktop-as-a-service for this client?","explanation":"AWS Workspaces added support for Linux desktops the middle of 2018.  BYOD scenarios work together well with a DaaS concept to provide security, manageability and cost-effectiveness.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/create-custom-bundle.html","title":"Create a Custom WorkSpaces Bundle - Amazon WorkSpaces"}],"answers":[{"id":"e8a33593afbd82697d0ab168304265ed","text":"Launch an EC2 Linux instance and install XWindows and Gnome as the GUI.  Configure VNC to allow remote login via GUI and load the required software.  Create an AMI and use that to launch subsequent desktops.","correct":false},{"id":"4018dbf7b4646b285f5ceaa7b49a5934","text":"Launch a Linux Workspace in AWS WorkSpaces and customized it with the required software.  Then, create a custom bundle from that image and use that bundle when you launch subsequent Workspaces.","correct":true},{"id":"0cbf457eccae17779144d4e64e92a43e","text":"Launch a Windows Workspace and install VirtualBox along with a minimal Linux image.  Within that Linux image, install the required software.  Create an image of the Windows Workspace and create a custom bundle from that image.  Use that bundle when launching subsequent Workspaces.","correct":false},{"id":"bf3d66705af4677c9ade8605aa6bd89a","text":"Given current limitations, running Linux GUI applications remotely on AWS is not feasible.  They should reconsider their BYOD policy decision.","correct":false},{"id":"3480305b106307937ed81bba73d294ab","text":"Package the required apps as WAM packages.  When launching new Windows Workspaces, instruct users to allow WAM to auto-install the suite of applications prior to using the Workspace.","correct":false}]},{"id":"777fd0a9-391f-4072-b147-a64a2016f5a1","domain":"awscsapro-domain2","question":"You are working with a company to design a DR strategy for the data layer of their news website.  The site serves customers globally so regional diversity is required.  The RTO is defined as 4 hours and RPO have been defined as 5 minutes. Which of the following provide the most cost-effective DR strategy for this client?","explanation":"While Multi-AZ RDS may be a best practice, the question only stipulates regional resilience.  So, we are looking for options that create regional diversity and fall within our RPO and RTO.  Those options would be cross-region bucket replication and cross-region RDS replicas.  The RPO given means that we must not loose anything more than 5 minutes of data, so any sort of backup that is less frequent than every 5 minutes is eliminated.","links":[{"url":"https://aws.amazon.com/blogs/aws/cross-region-snapshot-copy-for-amazon-rds/","title":"Cross-Region Snapshot Copy for Amazon RDS | AWS News Blog"},{"url":"https://aws.amazon.com/blogs/aws/new-whitepaper-use-aws-for-disaster-recovery/","title":"New Whitepaper: Using AWS for Disaster Recovery | AWS News Blog"}],"answers":[{"id":"7c7f5169f7bfea8c0aa5d79d8f1f1565","text":"Configure RDS Read Replicas to use cross-region replication from the primary to a backup region.","correct":true},{"id":"035a09840d025b52ad7c808976c94da2","text":"setup cross-region replication for S3 buckets.","correct":true},{"id":"aa26654aaaf6239fe7acd7dc4e952d5a","text":"Write a script to create a manual RDS snapshot and transfer it to another region.  Use AWS Batch to run the script every three hours.","correct":false},{"id":"cfe51da2c91348146ba1bb989b4c2225","text":"Configure RDS to perform daily backups then copy those to another region.","correct":false},{"id":"0890c4152307ffd34ebeb6ea7c500814","text":"Write a script to export the RDS database to S3 every hour then use cross-regional replication to stage the exports in a backup region.","correct":false},{"id":"b3810d00767bfbb6b85fe44c1c3d2dd1","text":"Configure RDS to use multi-AZ and automatically fail over in the event of a problem.","correct":false}]},{"id":"c9f44641-660b-4c42-9380-9e7f6b0a9ba4","domain":"awscsapro-domain4","question":"As the solution architect, you are assisting your customer design and develop a mobile application using API Gateway, Lambda and DynamoDB. S3 buckets are being used to serve static content. The API created using API Gateway is protected by WAF. The development team has just staged all components to the QA environment. They are using a load testing tool to generate short bursts of a high number of concurrent requests sent to the API Gateway method. During the load testing, some requests are failing with a response of 504 Endpoint Request Timed-out Exception.\nWhat is one possible reason for this error response from API Gateway endpoint?","explanation":"The SA-P exam sometimes focuses on knowledge of response codes from API Gateway and what each distinct HTTP response code could mean.\nThe key to answering this question correctly is being able to distinguish between 4XX and 5XX HTTP error response codes. Though AWS has not been entirely consistent in their error code assignment philosophy, 4XX usually happens any time throttling kicks in because the request in that case never makes to an instance of Lambda function. 5XX happens when a Lambda function is actually instantiated, but some error (like time out) happened inside the Lambda function. One sneaky way to remember this is the fact that 5XX errors are called server errors in HTTP-land, so to generate a 5XX a server process must exist (and must have failed). Of course, in this context, the HTTP server process is a Lambda function - so in scenarios where throttling prevented a Lambda function from getting spawned, the response code cannot be 5XX. This is not consistently followed by AWS API Gateway error design, though, as we can see that AUTHORIZER_CONFIGURATION_ERROR and AUTHORIZER_FAILURE are both 500, though no Lambda function is actually spawned in either case. However, the candidate must remember that throttling always results in 4XX codes. An Endpoint Request Timed-out Exception (504) suggests that the requests in question actually made its way past the API Gateway into a Lambda function instance.\nFor the scenario where request rate exceeds API Gateway limits, the request would be blocked by API Gateway itself. The response would be 429. The exact knowledge of the code 429, however, is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe scenario where 1000 Lambda functions are already running is a similar example of throttling - the 1001st Lambda function will not even be spawned. The response, again, will be 429. However, the exact knowledge of the code 429 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe WAF scenario is yet another example of the request not even crossing the protections placed at the gateway level. If WAF is activated on API Gateway, it will block requests when the rate exceeds the HTTP flood rate-based rule (provided all such requests come from a single client IP address). However, the response, again, will be in the 4XX area (specifically, 403 Forbidden) - however, the exact knowledge of the code 403 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThis leaves Lambda time-out as the only correct answer. The mention of 30 seconds or more is a diversion tactic, in case candidate believes that the relevant Lambda time-out is 5 minutes. A given Lambda function instance may have a time-out limit of 5 minutes, but when it is invoked from API Gateway, the timeout imposed by API Gateway is 29 seconds. If a Lambda function runs for longer than 29 seconds, API Gateway will stop waiting for it and return 504 Endpoint Request Timed-out Exception.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html","title":"Amazon API Gateway Limits and Important Notes"},{"url":"https://aws.amazon.com/blogs/compute/amazon-api-gateway-adds-support-for-aws-waf/","title":"Amazon API Gateway adds support for AWS WAF"}],"answers":[{"id":"8090c6b0fe9036ec84fce24b16a7dc10","text":"The load testing tool has exceeded the soft limit for request rate allowed by API Gateway","correct":false},{"id":"82f6a6eb1a4aeef5dd34f21fcd2069ef","text":"The number of requests generated by the load testing framework has exceeded the threshold for the HTTP flood rate-based rule set in the WAF settings for the stage in question","correct":false},{"id":"370770015087b4ff70656089fc9e3316","text":"The test is triggering too many Lambda functions concurrently. AWS imposes a soft limit of 1000 concurrent Lambda functions per region","correct":false},{"id":"98daaa701198f7e1c541fe8051799129","text":"The Lambda function is sometimes taking 30 seconds or more to finish executing","correct":true}]},{"id":"4eb7884e-f6fd-4803-83bf-8ab6faca4dd5","domain":"awscsapro-domain1","question":"You have been asked to give employees the simplest way of accessing the corporate intranet and other internal resources, from their iPhone or iPad.  The solution should allow access via a Web browser, authentication via SAML integration and you need to ensure that no corporate data is cached on their device. Which option would meet all of these requirements?","explanation":"Amazon WorkLink is a fully managed, cloud-based service that enables secure access to internal websites and apps from mobile devices. It provides single URL access to the applications and also links to existing SAML-based identity providers.  Amazon WorkLink does not store or cache data on user devices as the web content is rendered in AWS and sent to user devices as encrypted Scalable Vector Graphics (SVG).  WorkLink meets all of the requirements in the question and is therefore the only correct answer.","links":[{"url":"https://aws.amazon.com/worklink/faqs/","title":"Amazon WorkLink FAQs"}],"answers":[{"id":"8b3531ac066ba672af41cfd6c438fdb9","text":"Place all internal servers in a public subnet and lock down access via Security Groups to the IP address of each mobile user","correct":false},{"id":"668cfaeb2878db8e709660735f0ff009","text":"Configure Amazon WorkLink and connect to the servers using a Web Browser with the link provided","correct":true},{"id":"0d0cb2140013a20863f643412ebd4698","text":"Tunnel through a Bastion Host into your VPC and view all internal servers via a Web Browser","correct":false},{"id":"2ca6fccac916b71e240a465c8caf457e","text":"Connect into the VPC where the internal servers are located using Amazon Client VPN and view the sites using a Web Browser","correct":false}]},{"id":"6a9b273b-601e-4f90-ae40-9b87c2313945","domain":"awscsapro-domain4","question":"You are consulting for a media company that sells archival footage of old Hollywood movies.  Their customer base is largely other production companies who like to include these old clips in modern projects and pay well for the licenses.  Unfortunately, as digitization as increased, their business has decreased since companies can get clips, often illegally, from the internet.  As a result, the company has had to shift significant resources and budget to the Legal department and away from the IT department.  You had been leasing a large SAN for storage of the media but its lease is ending and you need to find a new home for the collection.  The company prides itself on fulfilling customer requests quickly and has a target of 10-15 minutes from the time a customer requests a clip until that customer is able to download that clip.  The company gets about 4-5 requests per month but licensing fees for those requests are usually tens of thousands of dollars.  You have been challenged to come up with the most cost-effective way to store the 20TB of digitized media but still meet customer needs.  Which of the following would you choose? ","explanation":"Due to the relatively infrequent requests and the SLA for retrieving the requests, Glacier and Expedited Retrieval provide the best combination of meeting the requirements and minimizing cost.","links":[{"url":"https://aws.amazon.com/s3/pricing/","title":"Cloud Storage Pricing | S3 Pricing by Region | Amazon Simple Storage Service"}],"answers":[{"id":"da98bfd22729a645118d35c438df8bfd","text":"Use Snowball to transfer the data to AWS S3.  Create a Lifecycle policy that will archive all the media clips older than 1 day to Glacier.  Upon a customer request, initiate an Expedited retrieval of the media file.  Once restored, email the customer a pre-signed URL for downloading.","correct":true},{"id":"29c0c01d440c4ac095577d1f09b935f2","text":"Use Snowball to transfer the data to S3.  Use Elastic Transcoder to compress the media files into a smaller format.  Store the media files in small groups on EFS volumes.  Upon a customer request, mount the EFS volume from a T3.micro spot instance web server and provide the customer with a download URL.","correct":false},{"id":"a0f489ddd5ea014aed1b848b3027c8eb","text":"Use DMS to import the data into S3.  Create a Lifecycle policy to move the data to One-Zone Infrequent Access state.  Upon customer request, create a pre-signed URL with an expiration of 1 day and email to the customer for downloading.","correct":false},{"id":"354579a5bfb828ebee6304cac6ea1406","text":"Use Storage Gateway to migrate the media files to S3.  Create a Lifecycle policy to move the files to Reduced Redundancy storage after 1 day. Upon a customer request, create a pre-signed URL for the resource for customer downloading.","correct":false}]},{"id":"7c5f884f-c0f9-4028-a725-50819d704324","domain":"awscsapro-domain5","question":"You deploy an application load balancer and an Auto Scaling group (ASG) in production for a new project. When instances in the ASG have a high CPU utilization, a new instance is launched. However, the new instance fails the health check from the ASG and has been terminated after some time. You check the logs in the instance and find that the startup script does not finish yet before the instance is terminated. How would you resolve the problem?","explanation":"Amazon EC2 Auto Scaling waits until the health check grace period ends before checking the health status of the instance. The grace period timer should be increased to give the instance more time to finish the startup script. Increasing the healthy threshold makes the instance more difficult to become healthy. Decreasing the timeout value also does not help as the instance may become unhealthy very quickly. Modifying the health check type from ELB to EC2 is unsuitable as the ASG cannot get the instance status from the application level. Even if the instance shows as healthy in ASG, the application may not be ready yet.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Check Grace Period"}],"answers":[{"id":"1d8964ced10da94d49d1a1efe02bbbca","text":"Modify the health check type from ELB to EC2 in the Auto Scaling group. Configure ASG to check the EC2 instance status. As long as the instance does not have a system level issue, it will not fail the health check in the ASG even when the startup script is still running.","correct":false},{"id":"b5ca84726bf6fcfd6522ab188c8224ea","text":"Increase the default healthy threshold in the health check of elastic load balancer from 5 to 10 so that the instance will become healthy more quickly once the startup script finishes in the new instance.","correct":false},{"id":"e322fd6a55a044826665dfce4ae7020b","text":"Decrease the timeout value in the ELB health check from 5 seconds to 1 second so that when the ELB performs the health check on the backend instances, the instances are able to respond in time before a timeout occurs.","correct":false},{"id":"e7ad9ea949ae87c6a6001a94f5a9bf48","text":"Increase the health check grace period in the Auto Scaling group configurations. When a new instance boots up, it is given more time to execute the startup scripts and run applications before the health check from ASG.","correct":true}]},{"id":"a21ff5b0-e658-4466-9b30-ad1292dde65d","domain":"awscsapro-domain2","question":"A composite materials company is implementing a new monitoring solution on their manufacturing floor. Wi-Fi enabled IoT devices will be registered with AWS IoT Core to read data from numerous control systems. Dashboards will be created in Amazon QuickSight to present aggregate metrics to users (average, min, max, standard deviation, variance, and percentile). Drill down capabilities will also be needed for deeper analyses of exception scenarios. Which architecture will provide the most reliable and performance efficient solution for the company's monitoring needs?","explanation":"The MQTT protocol is a publish/subscribe protocol that provides clients with independent existence from one another, enhancing the reliability of the solution. HTTP is a document-centric ,request-response protocol, requiring more processing and storage overhead for IoT devices. There is no need to use Kinesis Data Analytics in this case because QuickSight can perform all of the aggregate functions required for this use case. Answer number four won't allow for data drill down because the device messages are not written to any persistent storage service.","links":[{"url":"https://aws.amazon.com/iot-core/","title":"AWS IoT Core"},{"url":"https://aws.amazon.com/blogs/compute/visualizing-sensor-data-in-amazon-quicksight/","title":"Visualizing Sensor Data in Amazon QuickSight"},{"url":"https://aws.amazon.com/quicksight/","title":"Amazon QuickSight"},{"url":"https://aws.amazon.com/athena/","title":"Amazon Athena"}],"answers":[{"id":"35c4c115504645bedabfaccb200fee7f","text":"Install MQTT libraries on the IoT devices. Create an IoT Core rule that forwards the MQTT messages to an Amazon Kineses Data Analytics stream, which writes aggregate data to an Amazon Kinesis Data Streams stream. Have an AWS Lambda function trigger to read the aggregate data and deposit it into Amazon DynamoDB tables","correct":false},{"id":"d3578dd1f9b95b2daf716b5298605a4d","text":"Install HTTP libraries on the IoT devices. Create an IoT Core rule that forwards the HTTP messages to an Amazon Kineses Data Firehose stream, which deposits the data into S3, and writes the data to an Amazon Kinesis Data Analytics stream to aggregate the data. Have an AWS Lambda function trigger to read the aggregate data and deposit it into S3.","correct":false},{"id":"2287d42764ee3a22b9bdcb617461dbc6","text":"Install MQTT libraries on the IoT devices. Create an IoT Core rule that forwards the MQTT messages to an AWS Lambda function. Have the Lambda function write the messages to an Amazon Kinesis Data Firehose stream, which deposits them into S3","correct":true},{"id":"48b8bb005b2b6181fa6fc7161df04e9d","text":"Install HTTP libraries on the IoT devices. Create an IoT Core rule that forwards the HTTP messages to an AWS Lambda function. Have the Lambda function write the messages to S3, and to an Amazon Kinesis Data Analytics stream to aggregate the data. Have an AWS Lambda function trigger to read the aggregate data and deposit it into Amazon DynamoDb tables","correct":false}]},{"id":"951e49a8-9395-4b49-b623-1fb9e88a9639","domain":"awscsapro-domain5","question":"Your production web farm consists of a minimum of 4 instances.  The application can run on any instance type with at least 16GB of RAM but you have selected m5.xlarge in your current launch configuration.  You have defined a scaling policy such that you scale out when the average CPU across your auto scaling group reaches 70% for 5 minutes.  When this threshold is reached, your launch configuration should add more m5.xlarge instances.  You notice that auto scaling is not working as it should when your existing instances reach the scaling event threshold of 70% for 5 minutes.  Since you are deployed in a heavily used region, you suspect there are capacity issues.  Which of the following would be a reasonable way to solve this issue?","explanation":"If you do not have capacity reserved via a zonal RI or on-demand capacity reservation, it is possible that the AZ is out of available capacity for the type of instance you need.  You can reserve capacity or you can also increase the possible instance types in hopes that some other similarly equipped instance capacity is available.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html","title":"Types of Reserved Instances (Offering Classes) - Amazon Elastic Compute  Cloud"}],"answers":[{"id":"ea27906009e93ed8060db15ee28fa8ae","text":"Reduce the warm-up and cooldown period in the scaling policy to allow more time to provision resources.","correct":false},{"id":"04c8568f7761456b829145690b8d0cba","text":"Provision some regional reserved instances of m5.xlarge to ensure you have capacity when you need it. ","correct":false},{"id":"a791a52f0967a913925bc47c7ae1c0a6","text":"Version the launch configuration to include additional instances types that also have at least 16GB of RAM.","correct":false},{"id":"04211a7288d23e07315243e9b492dcb4","text":"Provision some zonal reserved instances of m5.xlarge to ensure you have capacity when you need it. ","correct":true},{"id":"989bc9eb46719d5cf9d9b69d191ee8d8","text":"Lower the CPU threshold to 60% so the scaling event triggers earlier and therefore has a better chance of getting resources.","correct":false}]},{"id":"7e090e30-00fe-4ea9-85e7-502b055a9537","domain":"awscsapro-domain2","question":"You have configured your VPC CIDR as 10.0.0.128/25.  What IP address would you expect is assigned to the DNS server?","explanation":"The DNS is a reserved address in the VPC CIDR and will be the second usable address.  In this example,  .128 is the network address, .129 is reserved for the router, .130 is reserved for the  DNS.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html","title":"VPCs and Subnets - Amazon Virtual Private Cloud"}],"answers":[{"id":"2d13502142d78cd771686cc0ba0ebdd8","text":"10.0.0.255","correct":false},{"id":"98a2cd7efa1ba950d42e42af495c4248","text":"10.0.0.2","correct":false},{"id":"cb09135ab9a635aa9719248fe6e3634d","text":"10.0.0.128","correct":false},{"id":"38fbcb4c4667561b2f5a0fca9574d615","text":"/25 is an invalid netmask for VPCs.","correct":false},{"id":"f3677d1e4c9be060a85b0f539eabfb97","text":"10.0.0.130","correct":true}]},{"id":"c2e1b0c2-2dd7-479b-a78e-8ddd1c6d2448","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a problem.  The client has several Ubuntu Linux servers in a private subnet within a VPC.  The servers are configured to use IPv6 only and must periodically communicate to the Internet to get security patches for applications installed on them.  Unfortunately, the servers are unable to reach the internet.  An internet gateway has been deployed in the public subnet in the VPC and default routes are configured.  Which of the following could fix the issue?","explanation":"With IPv6 you only requires an Egress-Only Internet Gateway and an IPv6 route to reach the internet from within a VPC.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/configure-private-ipv6-subnet/","title":"Configure Private IPv6 Subnets"}],"answers":[{"id":"024977f9219f0562163fc06724bc8c99","text":"Implement a Classic Load Balancer in front of the servers and register the servers to the load balancer.","correct":false},{"id":"8afd1250611c55c8d3301faba9716c97","text":"Implement a NAT Instance in the public subnet and configure the instance as the default route for the private subnet.","correct":false},{"id":"3528a1941badb3d7623a6df18c766d0a","text":"Create an Internet Gateway in the private subnet and configure the default route for the private subnet to the gateway.","correct":false},{"id":"5250cb1aeb48afd18fee3003d4798203","text":"Assign IPv6 EIP's to the servers and configure a default route to the existing internet gateway","correct":false},{"id":"3bd793f68f596bb23004279fc494f9a6","text":"Implement an Egress-only Gateway in the public subnet and configure an IPv6 default route for the private subnet to the gateway.","correct":true}]},{"id":"230f422f-7118-4096-8dce-59c642fb55c8","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a new Direct Connect connection.  The connection is up and you can ping the AWS peer IP address, but the BGP peering session cannot be established.  What should be your next logical troubleshooting steps?","explanation":"Because the connection is up and we can ping the AWS peer, the problem must be at a higher level on the OSI model than the Physical or Data layers.  BGP uses TCP port 179 to communicate routes so we should check that no NACL or SG is blocking it.  Additionally, we should make sure the ASNs are properly configured in the proper ranges.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/Troubleshooting.html","title":"Troubleshooting AWS Direct Connect - AWS Direct Connect"}],"answers":[{"id":"81977d7a1eb5714746851077b93f44d6","text":"Power cycle all the equipment to clear ARP table cache.","correct":false},{"id":"edd3f9408cecbbf9182678ccc51d7981","text":"Ask your network provider to provide you with a cross connect completion notice and compare the ports with those listed on your LOA-CFA","correct":false},{"id":"16e5aea88df69cc18f99e3f066ec99c1","text":"Ensure that the local ASNs and AWS-side ASNs are properly configured.","correct":true},{"id":"3d2a55832b90f19a2137e8715525d717","text":"Make sure no firewalls or ACLs are blocking TCP port 179 or any high-numbered ephemeral ports.","correct":true},{"id":"45d4c1753395277878b9a17343628c52","text":"Ensure that the VLAN is configured properly between your on-prem router the provider. ","correct":false},{"id":"8fc27418eee2ce07b64bc672007d2c1b","text":"Contact the co-location provider and request a written report for the Tx/Rx optical signal across the cross connect.","correct":false}]},{"id":"b401741c-5b37-4b47-8e61-7802fbc9d7d6","domain":"awscsapro-domain1","question":"You are helping a client consolidate several separate accounts into a single account.  This consolidation will result in approximately 50 new VPCs in their one account.  They want to continue to use Route 53 for DNS but only want it accessible privately. How can you accomplish this most efficiently?","explanation":"Private Hosted Zones provide DNS services to VPCs but cannot be access from the internet.  They can be associated with VPCs either by the console, CLI or programmatically via SDK.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html","title":"Associating More VPCs with a Private Hosted Zone - Amazon Route 53"}],"answers":[{"id":"f74daa2300ac594c111ca9fce198f19c","text":"Create a central DNS server using EC2 and BIND.  Configure Route 53 to reference this DNS server as a resolver.  Update DNS records at the registrar to point to the central DNS.","correct":false},{"id":"315372936e7ffba65896da15d0f45c2d","text":"Create a Private Hosted Zone within Route 53.  As the new VPCs are created, associate them with the Private Hosted Zone.","correct":true},{"id":"cbffb64b6b43e6fe45496b6e77ce17b8","text":"Create a Private Hosted Zone within Route 53 for each respective VPC.  Configure replication between the private hosted zones to keep records in sync.","correct":false},{"id":"3cc28b12f45b3dee8f7f16a0f93d00ce","text":"Install BIND on an EC2 instance in a single VPC.  Create VPC peering connections between the DNS VPC and any new VPCs.  Configure a DHCP Option Set to assign a DNS and link that to each VPC.","correct":false},{"id":"82d5ef7e7176200aa4350ef90dd4c354","text":"Create a Public Hosted Zone within Route 53 and associate it to each VPC.  Configure a NACL on each VPC to deny inbound DNS queries (UDP port 53).","correct":false}]},{"id":"9fc0785a-d5cb-47e3-bc2f-829b5a36ba26","domain":"awscsapro-domain3","question":"You work for a Genomics company which has decided to migrate its DNA Sequencing application to the AWS Cloud. The application is containerized. Currently, container image A works on genomics data residing on an on-premises file server, validating the data and updating the metadata in a local database. When it is done, engineers manually trigger 100 or more instances of container image B that process this data in parallel by reading the metadata, creating output files. When all these container instances have done their job, engineers manually trigger container image C that validates the results, cleans up and sends notifications.\nThe CTO has decided to use S3 for storing the input and output data files. She has also mandated that the parallel processing phase should run on a fleet of Spot EC2 instances to reduce compute costs. She also wants to automate the workflow, so that engineers do not have to manually trigger the next set of actions. The requirement is to minimize administrative overhead and custom development for the migration.\nAs the AWS Architect, which of the following approaches should you recommend?","explanation":"AWS ECS does not natively provide workflow management. In an ECS service definition file, you cannot specify a sequence of tasks with execution dependencies such that one will be run only after the previous one completes. Hence, the two ECS choices are ruled out.\nDistraction warning - Fargate does not allow you to specify Spot instances as it is serverless in nature (it absolves you from specifying server details). This effectively creates a distraction - when the candidate rules out ECS Fargate due to this reason, they may be relieved to see the ECS EC2 choice and jump to a conclusion because it is relatively easy to remember that EC2 launch type actually lets you select Spot instances. However, this distraction is designed to take focus away from the fact that neither of these two choices is correct. Both of the choices require service definition files to set up execution workflows. Task instances mentioned in an ECS service definition file are executed in parallel - ECS does not control the sequence of tasks.\nAWS SWF does not let you specify Spot instances either. Also, SWF is usually used in cases where human intervention is needed in the workflow.\nThis leaves AWS Batch as the correct answer. AWS Batch is indeed the most suitable AWS service for this scenario as it meets all requirements.","links":[{"url":"https://docs.aws.amazon.com/batch/latest/userguide/create-compute-environment.html","title":"How to create a compute environment for AWS Batch"},{"url":"https://docs.aws.amazon.com/batch/latest/userguide/example_array_job.html","title":"Example AWS Batch Array Job Workflow"},{"url":"https://aws.amazon.com/ec2/spot/containers-for-less/get-started/","title":"How to run ECS clusters in EC2 Spot Instances"}],"answers":[{"id":"757ddde350053553e44844d066c91386","text":"Use AWS ECS with Fargate Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"49755d6c34da495b8c91964f52946d29","text":"Use AWS SWF workers and deciders to manage the workflow. Configure the workers to use EC2 Spot Instances","correct":false},{"id":"ceb4c03a526e8ddb01ada7a40bb60001","text":"Use AWS Batch, setting up an array job with 100 or more copies preceded by pre-requisite and follow-up jobs where the workflow is controlled by dependencies between jobs. Also, use Spot as the Provisioning Model for compute environment","correct":true},{"id":"e46ada36d33a9e5b23aa37ee94c4c5d6","text":"Use AWS ECS with EC2 Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false}]},{"id":"49f16801-2cc1-48c8-a517-f9192f516318","domain":"awscsapro-domain3","question":"A tire manufacturing company needs to migrate a .NET simulation application to the AWS cloud. The application runs on a single Windows Application Server in their datacentre. It reads large quantities of data from local disks that are attached to the on-premises Application Server. The output from the application is small in size and posted in a queue for downstream processing. On the upstream side, the data acting as the input for the .NET simulation app is generated in the tire-testing Lab during the daytime by processes running on a Linux Lab Server. This data is then copied from the Linux Lab Server to the Windows Application Servers by a nightly process that also runs on the Linux Lab Server. This nightly process mounts the Application Server disks using a Samba client, this is made possible by the Application Server also acting as a Windows File Share Server. When the nightly process runs, it overwrites the input data from last night because of disk space constraint on the Application Server. This is undesirable as the data is permanently lost on a daily basis.\nThe migration is being undertaken because the .NET simulation application needs more CPU and RAM. The company does not want to spend on expensive hardware any more. However, the nightly process is not migrating, nor is the Linux Lab Server. The code of the simulation applications, as well as the nightly process, may change a little as a result of the migration, but leadership wants to keep these changes to a minimum. They also want to stop losing the daily test data and keep it somewhere for possible analytical processing later on.\nAs the AWS architect hired to shepherd this migration and many more possible migrations in the future, which of the following architectures would you choose as the best one, considering the minimization of code changes as the topmost goal, followed by cost-effectiveness as the second but important priority? The data has no security requirement.","explanation":"The two parts of this question are - (a) Do I use EFS or EBS for storing the data from the Lab Servers? (b) Do I copy data from each night to S3 using a NAT Gateway (thereby using the public internet) or a VPC Endpoint (thereby using the private network to copy)?\nThe answer to the first question is EBS because Windows EC2 instances cannot mount EFS, as EFS only supports Linux EC2 instances.\nThe answer to the second question is VPC Endpoint because NAT Gateways are very costly - they are charged 24-7 for just running, in addition to having data transfer rates. S3 VPC Endpoints are a cost-effective mechanism to copy data to S3. Note that the S3 put-object cost will be the same for both cases. The question tries to distract the candidate by stating that there is no security requirement, trying to confuse the candidate into selecting NAT Gateway in case they perceive the only distinction between NAT Gateway and S3 VPC Endpoint to be the usage of public network versus private.\nThis is an example of highly verbose question describing a complex scenario. There will definitely be quite a few such questions in the AWS SA-P exam that are challenging in terms of time management. You may use vertical scanning of the answer choices to spot the differences first. That way, you can focus on determining which of the variations is correct because you would know what is different between them.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html","title":"EFS Support for Windows EC2 Instances"},{"url":"https://aws.amazon.com/vpc/pricing/","title":"Search for NAT Gateway Pricing here"}],"answers":[{"id":"c7311a2612bfc63a966317d468a5b4dd","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"3ae5ae41663ec801882e10e7fa394613","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":true},{"id":"fabdb7e05e5bb1c78dd6e166134202ca","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"220d73c7cceb53661a402d59038118ee","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false}]},{"id":"dd8b46c7-d1d5-4326-a092-927b9333fd2a","domain":"awscsapro-domain5","question":"You are helping a company transition their website assets over to AWS.  The project is nearing completion with one major portion left.  They want to be able to direct traffic to specific regional EC2 web servers based on which country the end user is located.  At present, the domain name they use is registered with a third-party registrar.  What can they do?","explanation":"You can use Route 53 if the domain is registered under a third-party registrar.  When using Geolocation routing policies in Route 53, you always want to specify a default option in case the country cannot be identified.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy - Amazon Route 53"}],"answers":[{"id":"294ce2854fbe727bd4f4917543d45bec","text":"Create a private hosted zone for the domain in Route 53.  Update the DNS record entries in the registrars database to use AWS DNS Servers.  Once the DNS changes are fully propagated across the internet and the TTL has expired, convert the private hosted zone to a public hosted zone.  Create A-type records for all the regional EC2 instances and configure a Geo-proximity policy for each record, ensuring the bias across all records sums to 100.","correct":false},{"id":"724dab26998c86ab7ddc7faa063285b8","text":"You cannot use Route 53 routing policies unless AWS is the registrar of record for your domain.  A workaround could be to configure your own top-level DNS server using BIND.  Ensure the NS and SOA records point to this instances.  Create A-type records pointing to the IP addresses of the regional EC2 web servers.  Dynamically redirect requests using customized BIND rules and a third-party IP geolocation database.","correct":false},{"id":"81efc0b140e618c378c9e9bc59dd4ca8","text":"Create a public hosted zone for the domain in Route 53.  Update the DNS entries in the registrars database to use AWS DNS Servers as defined in the NS record on Route 53.  Create A-type records for all EC2 instances. Configure CNAME records for the main FQDN that point to regional A records using a Geolocation routing policy.  Create another CNAME record as a default route.","correct":true},{"id":"381d1621ab4c93aca8cc780f05e98c50","text":"Initiate a domain transfer request with the current registrar.  Once the request goes through, create a public hosted zone in Route 53.  Create SRV records for each regional EC2 instance using a Geolocation routing policy.  Create an alias record for the top-level domain and link that to the SRV records.","correct":false}]},{"id":"e3a59454-94fa-4b98-8d8a-80882a7d0e30","domain":"awscsapro-domain5","question":"You are setting up a new EC2 instance for an ERP upgrade project.  You have taken a snapshot and built an AMI from your production landscape and will be creating a duplicate of that system for testing purposes in a different VPC and AZ.  Because you will only be testing an upgrade process on this new landscape and it will not have the user volume of your production landscape, you select an EC2 instance that is smaller than the size of your production instance.  You create some EBS volumes from your snapshots but when you go to mount those on the EC2 instances, you notice they are not available.  What is the most likely cause?","explanation":"In order to mount an EBS volume on an EC2 instance, both must be in the same AZ.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html","title":"Amazon EBS Volumes - Amazon Elastic Compute Cloud"}],"answers":[{"id":"7a11f6b6797c8dd005c9ce25c77a37fe","text":"An SCP applied to the account you are in has restricted you from attaching EBS volumes to instances outside the original VPC","correct":false},{"id":"a9e147ffe118e514fd5069020e86cca6","text":"The instance that you selected for your testing landscape is too small.  It must be equal to or larger than the source of the AMI.","correct":false},{"id":"c2f8c791750ad6ce4c8c41ac45b246a0","text":"You have reached your account limit for EBS volumes.  You will need to create a support ticket to request an increase to the limit.","correct":false},{"id":"134c64ea3d25d70a667400e778a13c1a","text":"You created them in a different availability zone than your testing EC2 instance.","correct":true},{"id":"5a9ffc3875f9e3f50c9fc2684a6006b2","text":"The original volume is encrypted and you failed to check the encryption flag when creating the new volume.","correct":false}]},{"id":"f19a95ac-c0b9-4d00-a84a-67f71b7e2a76","domain":"awscsapro-domain2","question":"You are advising a client on some recommendations to increase performance of their web farm.  You notice that traffic seems to usually spike on the days after public holidays and unfortunately the responsiveness of the web server as collected by a third-party analytics company reflects a customer experience that is slower than targets.  Of these choices, which is the best way to improve performance with minimal cost?","explanation":"Of these options, only one meets the question requirements of performance at minimal cost.  Simply scheduling a scale event during a known period of traffic is a perfectly valid way to address the requirement and does not incur unnecessary cost. CloudTrail records API access and is not suitable for network alarms.  Route 53 would not be able to \"consolidate\" dynamic and static web resources.","links":[{"url":"https://docs.aws.amazon.com/auto scaling/ec2/userguide/schedule_time.html","title":"#N/A"}],"answers":[{"id":"3c6b1f2e20a3204df3886f680991b76d","text":"Use CloudTrail and SNS to trigger a Lambda function to scale the web farm when network traffic spikes over a configured threshold.  Create an additional Internet Gateway and split the traffic equally between the two gateways using an additional route table.  ","correct":false},{"id":"24c302b442af96b7c1eedb04e2c4069b","text":"Configure a dynamic scaling policy based on network traffic or CPU utilization.  Migrate static assets from EBS volumes to S3.  Configure two Cloudfront distributions--one for static content and one for dynamic content.  Use Route 53 to consolidate both Cloudfront distributions under one alias.","correct":false},{"id":"247053f8b211aeace0894f849838ef6f","text":"Configure a scheduled scaling policy to increase server capacity on days after public holidays.  ","correct":true},{"id":"8a2f77fe64a24871e80eae971ce2c877","text":"Create replicas of the existing web farm in multiple regions.  Migrate static assets to S3 and use cross-region replication to synchronize across regions.  Create CloudFront distributions in each region.  Use Route 53 to direct traffic to the closest CloudFront alias based on a geolocation routing policy.","correct":false}]},{"id":"e720cd54-de67-42de-ba10-593dee0582e6","domain":"awscsapro-domain3","question":"You are the Enterprise Architect in a Risk Quantification firm. The firm has a website which end-users can use to apply for loans and also track the status of their loan application if they log in. When a loan application comes in, several downstream systems need to independently process the application. Right now, the website server-side code invokes these systems one after the other, synchronously, in a tight loop. If one of these downstream systems times out or throws an exception, the entire loan application processing errors out. Even if none of these downstream systems fail, the time it takes to process a loan application is very high due to the serial nature of these systems being invoked. Your CTO wants only the loan-processing application moved to the AWS cloud and re-architected at the same time.\nThe downstream systems are all hosted on-premises and will continue to remain on-premises. They expose REST endpoints that accept POST HTTPS requests, use self-signed certificates and respond synchronously only when they are done processing an application. After re-architecture, all downstream systems must independently start processing an incoming loan application simultaneously.\nYour CTO wants to know how the loan-processing website application can be architected in the AWS Cloud, and what supporting changes will be needed in the downstream systems on-premises. He wants to minimize code changes to the downstream on-premises systems. Choose the best option","explanation":"This is an example of a verbose question with verbose answer choices. You can expect a few such questions in the exam, testing your time management skills. Try to vertically scan the answers to see which parts differ between them. Sometimes, though the answers seem big, a large part of each is identical. You can ignore those parts, as there is nothing to choose between the.\nAmong the four choices, two use SQS and two use SNS to feed the incoming loan applications to the downstream systems. You cannot automatically eliminate either SQS or SNS, as a working solution can be designed with either.\nLet us see how we can achieve this using SNS first. The basic requirement here is fan-out - a single loan application must be processed by several downstream systems, so there are multiple consumers. Hence, SNS is a natural fit. SNS supports multiple subscribers for a topic. SNS also supports HTTP/HTTPS subscribers. SNS makes POST REST API call to as many HTTP/HTTPS subscribers exist on the topic, so it fits the bill. However, there is a small problem - the requirement states that the downstream systems must be changed as little as possible. If we follow this design, we must change the HTTP Listening part of the downstream systems significantly. Because SNS is directly calling them now, SNS will use its own headers and body format. In fact, SNS POST-s two kinds of messages - one is Subscription Confirmation and one is Notification. A special HTTP header (x-amz-sns-message-type) has the right type in its value. The server side now must parse this header out and look for only the Notification type of message. The body itself will then be JSON formatted with the payload. While the server is probably used to process just the core payload (loan application data) as the HTTP body, the same will now be hidden inside a JSON field called Message inside the request body. Additionally, the downstream systems will have to deal with SNS retries, thus the loan application part must be made idempotent (if the same loan application lands twice, it will ignore the duplicates). Thus, though it is technically possible to design the solution using SNS, it will result in a lot of changes in the downstream systems. Hence, though the SNS option will work, it is not the correct answer because of this reason.\nNow, let us see how we can design this using SQS. While SQS does not support fan-out (multiple consumers for the same message), the proposed solution uses a Lambda function to achieve fan-out. The Lambda function will pick up the message, and then call the downstream systems one by one. The key to making this work is, of course, to modify the downstream systems from synchronous monolithic beasts to asynchronous servers so that they can instantly respond to the Lambda function and then continue to process the application. We will then have to provide a callback for when it is done. The solution uses an API Gateway for that purpose. Overall, the solution is elegant, and changes to the downstream systems are less than what SNS requires. Hence, SQS is the correct answer.\nNote that one version of the SNS design proposes to retain the synchronous nature of the downstream systems. That will not work as SNS will not wait more than 15 seconds for a response. The response will then be lost and the main website app will never know the results from the downstream systems.\nAlso, note that though SNS requires the HTTPS subscriber to present a trusted CA-signed certificate, there is no such requirement for Lambda because Lambda is basically your code, you can decide to trust anyone.","links":[{"url":"https://docs.aws.amazon.com/sns/latest/dg/sns-http-https-endpoint-as-subscriber.html","title":"Using Amazon SNS for System-to-System Messaging with an HTTP/S Endpoint as a Subscriber"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","title":"Using AWS Lambda with Amazon SQS"}],"answers":[{"id":"3d032e65493c0733ebe65683fb66a562","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems and (c) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as your Lambda function will not be able to POST to a server with self-signed certificate","correct":false},{"id":"eec2740374df8038093d636a17252168","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Override the default delivery policy on the subscriber endpoint to remove retries so that downstream systems do not have to worry about synchronous responses taking time or idempotency of retries. Make the following changes in the downstream systems - (a) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (b) Procure server certificates from a trusted Certificate Authority (CA) instead of using the self-signed certificate as SNS will not be able to POST to a server with a self-signed certificate","correct":false},{"id":"3fb725a8e71fa96168f18e50a146b4f0","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed and (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems","correct":true},{"id":"48d42d3290142cbfc8207e042690b35f","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting so that SNS does not retry, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (c) Make them idempotent for the same loan application as SNS may retry in case of lost messages or timeouts (d) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as SNS will not be able to POST to a server with self-signed certificate","correct":false}]},{"id":"8d0d69dd-35f2-468a-883b-18ad1135b564","domain":"awscsapro-domain2","question":"You are providing a security administrator with some on-the-job training regarding IAM, roles and policies.  The security administrator asks just what the policy evaluation logic is when a request is made?  For example, when a user tries to use the AWS Management Console, what is the process that AWS goes through to determine if that request is allowed?","explanation":"Knowing this logic flow can help troubleshoot security issues.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html","title":"Policy Evaluation Logic - AWS Identity and Access Management"}],"answers":[{"id":"eb4b98badf50f00af2105b3132c620af","text":"First AWS evaluates organizational boundaries of the request in context of the principal.  Next, any STS assumed role policies are evaluated.  Then, permission boundaries are evaluated against the principals request.  Finally, AWS then issues a decision of allow if there were no explicit deny.","correct":false},{"id":"5afa15bfdd702c70736460429675418c","text":"The AWS service receives the request from the principal.  AWS first evaluates the authority of the principal in context of the service.  Next, any identity-based or resource-based policies are evaluated.  Then, explicit Action statements are evaluated in context of the request.  Finally, AWS issues a decision on whether to allow or deny the request.","correct":false},{"id":"f3087f569306369e9cfa2b1f6806acbc","text":"The AWS service receives the request.  AWS first authenticates the principal.  Next, AWS determines which policy to apply to the request.  Then, AWS evaluates the policy types and arranges an order of evaluation. Finally, AWS then processes the policies against the request context to determine if it is allowed.","correct":true},{"id":"e13db2078457fb9b19c98ad9ae06563c","text":"Upon receiving the request, AWS will first evaluate the permissions of the principal and the request.  Next, any STS assumed role policies are evaluate.  Then, specific user or role permission boundaries are evaluated against the request and the principal.  Finally, any Organizational boundaries (SCPs) are evaluated against the request.  If there are no explicit deny actions, the request is allowed.","correct":false}]},{"id":"dac73d1f-8c64-48b1-90be-3432e789933d","domain":"awscsapro-domain2","question":"Your company is bringing to market a new Windows-based application for Computer Aided Manufacturing.  As part of the promotion campaign, you want to allow users an opportunity to try the software without having to purchase it.  The software is quite complex and requires specialized drivers so it's not conducive to allowing the public to download and install in their own systems.  Rather you want to control the installation and configuration.  Therefore, you want something such as a VDI concept.  You'll also need to have a landing page as well as a custom subdomain (demo.company.com) and limit users to 1 hour of use at a time to contain costs.  Which of the following would you recommend to minimize cost and complexity?","explanation":"AppStream is a way to deploy an application on a virtual desktop and allow anyone with a browser to use the application.  This is the most efficient and simplest option given the other choices.","links":[{"url":"https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html","title":"What Is Amazon AppStream 2.0? - Amazon AppStream 2.0"}],"answers":[{"id":"129d13b81f52a8ab01ce9e8ea7009289","text":"Create a landing page in HTML and deploy to an S3 bucket configured as a Static Web Host.  Use Route 53 to create a DNS record for the \"demo\" subdomain as alias record for the S3 bucket.  Deploy your application using Amazon AppStream.  Set Maximum Session Duration for 1 hour.","correct":true},{"id":"9b85d56999fa276cfa4a01df8195700c","text":"Create a landing page in HTML and deploy to an S3 bucket configured as a Static Web Host.  Embed in the HTML a Javascript-based RDP client that is downloaded with the webpage.  Create a CloudFront distribution with the S3 bucket as the origin.  Use an S3 Event to launch a Lambda function which starts up an EC2 instance with your golden AMI.  Once the instance is up and running, use a web socket call from the Lambda function to initiate the RDP client and log the user in.  After 1 hour, have the Lambda function issue a shutdown command to the EC2 instance.","correct":false},{"id":"bdc9c29dcecb6611cd71c4660fc235ca","text":"Deploy your application as an app in the Workspaces Application Manager.  Spin up several Workspaces and configure them to automatically install your application via WAM. Create the landing page such that it redirects to the web client for Workspaces and deploy the landing page via S3 configured as a web host.  Use Route 53 to create a DNS record for the demo subdomain as an alias record for the S3 bucket.  Configure the Workspaces for a 1 hour timeout.  ","correct":false},{"id":"a1c67181ea5765383a7b477e821391b4","text":"Configure an EC2 auto scaling fleet of spot instances with your golden AMI.  Create security groups to allow inbound RDP for the auto scaling group.  Deploy Apache Guacamole on an EC2 instance and place your landing page in its web server directory.  Use Guacamole to provide an RDP session into one of the EC2 instances directly in the users browser.  Use AWS Batch to reboot the EC2 instances after 1 hour of runtime.  ","correct":false}]},{"id":"0bfd631e-c08c-406a-acf5-a07416aab129","domain":"awscsapro-domain5","question":"Your team uses a CloudFormation stack to manage AWS infrastructure resources in production. As the AWS resources are used by a large number of customers, the update to the CloudFormation stack should be very cautious. Your manager asks for additional insight into the changes that CloudFormation is planning to perform when it updates the stack with a new template. The change needs to be reviewed before being applied by a DevOps engineer. What is the best method to achieve this requirement?","explanation":"CloudFormation Change Sets are able to provide the information on how the running resources are affected by a stack update. The outputs can be reviewed before being executed. Users can view the Change Set through AWS Console or CLI. The Retain option in the DeletionPolicy, CloudFormation stack policy or termination protection helps on protecting the stack resources. However, they cannot provide a summary of  changes in a stack update.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"550c80eaf15eeb89d49aa2a86eb747a6","text":"Enable termination protection in the CloudFormation stack so that the AWS resources cannot be accidentally deleted or modified. Disable the protection only if the changes are approved. Execute the changes in a maintenance window.","correct":false},{"id":"935df51ce2c7f07994c2b8a257489e00","text":"Create a CloudFormation Change Set using AWS Management Console or CLI, review the changes to see if the modifications are as expected and execute the changes to update the stack.","correct":true},{"id":"4657e544cc1daf4315865e230d92dd00","text":"For key AWS resources in the CloudFormation stack, add a Retain option in the DeletionPolicy attribute, which prevents the resources from being accidentally deleted by a stack update. Add a Delete option for the resources that you want to delete along with the stack.","correct":false},{"id":"700aa7cb0f0e8cfb417b67ae5d49e962","text":"Add a CloudFormation stack policy to prevent updates to stack resources. Only after the changes are reviewed and approved, change the stack policy to allow the stack update. Revert the stack policy after the change.","correct":false}]},{"id":"35fc536d-d968-472d-84d5-a7ae5d343564","domain":"awscsapro-domain1","question":"You work for a genetics company that has extremely large datasets stored in S3. You need to minimize storage costs, while maintaining mandated restore times that depend on the age of the data. Data 30-59 days old must be available immediately, and data ≥ 60 days old must be available within 12 hours. Which of the following options below should you consider?","explanation":"You should use S3 - IA for the data that needs to be accessed immediately, and you should use Glacier for the data that must be recovered within 12 hours. S3 - RRS and 1Zone-IA would not be suitable solution for irreplaceable data or data that required immediate access (reduced Durability or Availability), and CloudFront is a CDN service, not a storage solution.  The use of absolute words like 'Must' is an important clue as it will eliminate options where the case may not be possible such as with OneZone-IA.","links":[{"url":"https://aws.amazon.com/s3/faqs/#sia","title":"S3 - Infrequent Access"},{"url":"https://aws.amazon.com/s3/faqs/#glacier","title":"About Glacier"}],"answers":[{"id":"e9a5105fa288ef2b71c037e42d665d91","text":"S3 - OneZone-IA","correct":false},{"id":"4340570ba672bfa48cd45e3f026c01d1","text":"S3 - IA","correct":true},{"id":"31e831ec49678aed7f467f791d1f8704","text":"S3 - RRS","correct":false},{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":true},{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false}]},{"id":"7eebbdef-e751-4d76-be2a-1e3a746b87f6","domain":"awscsapro-domain5","question":"You are a database administrator for a company in the process of changing over from RDS MySQL to Amazon Aurora for MySQL.  You setup the new Aurora database in a similar fashion to how your pre-existing RDS MySQL landscape was setup:  Multi-AZ with Read Replica in a backup region.  You have just completed the migration of data and verified that the new Aurora landscape is performing like it should.  You are now in the process of decommissioning the old RDS MySQL landscape.  First, you decide to disable automatic backups.  Via the console, you try to set the Retention Period to 0 but receive an error saying \"Cannot Set Backup Retention Period to 0\".  How can you disable automatic backups?","explanation":" For RDS, Read Replicas require backups for managing read replica logs and thus you cannot set the retention period to 0.  You must first remove the read replicas and then you can disable backups.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Troubleshooting.html#CHAP_Troubleshooting.Backup.Retention","title":"Troubleshooting - Amazon Relational Database Service"}],"answers":[{"id":"898042b8eeafc1bbb4dd146c98dbb919","text":"Remove the Read Replicas first.","correct":true},{"id":"5698f7c6bb0530fc1c0ad18e6911f528","text":"You must first reprovision the database as a single AZ instances.  Multi-AZ replication requires backups to be enabled.","correct":false},{"id":"fb2c22782c91868676981ff65332f1d5","text":"You cannot disable backups via the console.  You must do this via the AWS CLI or SDK.","correct":false},{"id":"c61323c176e6d23809f0d467770ed205","text":"Automatic backups are enabled and disabled at the database engine level.  You need to login using a MySQL client to turn off automatic backups.","correct":false},{"id":"7086f25af0d0410de9d8826003760752","text":"You cannot disable automatic backups on RDS instances.  This feature is built into the platform as a failsafe.","correct":false}]},{"id":"73708d6f-e6cb-4b8f-90d9-723a2961496e","domain":"awscsapro-domain2","question":"Your team is architecting an application for an insurance company.  The application will use a series of machine learning methods encapsulated in an API call to evaluate claims submitted by customers.  Whenever possible, the claim is approved automatically but in some cases were the ML API is unable to determine approval, the claim is routed to a human for evaluation.  Given this scenario, which of the following architectures would most aligned with current AWS best practices?","explanation":"Formerly, AWS recommended SWF for human-involved workflows.  Now AWS recommends Step Functions be used as it requires less programmatic work to build workflows and is more tightly integrated into other AWS services.","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html","title":"Starting a State Machine Execution in Response to Amazon S3 Events - AWS  Step Functions"}],"answers":[{"id":"5298caaea3a55734efa8c62e637d0d40","text":"Use Kinesis to take in the claims and save them on S3 using Firehose.  Use Sagemaker to analyze the claims on S3 as a training set and devise a decider function.  Save the approved claims to another S3 bucket setup with an Event to trigger an SES message to a reviewer.","correct":false},{"id":"fa669ed2f0666420f19ad9c8836509f6","text":"Take in the claims into an SQS queue.  Create a Lambda function to poll the SQS queue, fetch the claim and submit the API call.  Use another Lambda function to evaluate the API results and if the claim is not approved, place the claim in a dead letter queue.  Train a person to periodically log into the SQS console and read the dead letter queue for review.","correct":false},{"id":"837fa3e7c6b347eddfa7fefd2a092017","text":"Create a workflow using Simple Workflow Service and an EC2 fleet to host worker and decider programs.  Create worker programs for each processing step and ML API call.  Create decider program to receive the output of the API and decide if the claim is approved.  For unapproved claims, create a worker program use the WorkMail SDK to place the unapproved claim into a mailbox to be reviewed by a human.","correct":false},{"id":"dfc86c259de4881886ffdac5b8106777","text":"Create a State Machine using Step Functions and a Lambda function for calling the API.  Intake the claims into an S3 bucket configured with a CloudWatch Event.  Trigger the Step Function from the CloudWatch Event.  Create an Activity Task after the API check to email an unapproved claim to a human.","correct":true}]},{"id":"0ee4566a-508e-472d-9789-3318e3284aca","domain":"awscsapro-domain5","question":"You are an AWS Solutions Architect and you maintain a CloudFormation stack that includes the resources of a network load balancer and an Auto Scaling group. The ASG has one running instance. A developer uses the instance for feature development and testing. However, after he adds some configurations and restarts an application process, the instance is terminated by the Auto Scaling group and a new instance is created. The new configurations are lost in the new instance. You need to modify the resource settings to make sure that the instance is not terminated by the ASG when application processes are restarted. Which of the following methods would best achieve this?","explanation":"The instance fails the health check in the ELB target group and is then terminated by ASG whenever the application processes are restarted. The prevent the ASG from terminating the EC2 instance you need to modify the health check type from ELB to EC2. As a result, even if the instance fails the health check in the ELB target group, it will not be terminated by the Auto Scaling group. You do not need to create an AMI or a new launch configuration to address the issue. And the custom health check script that runs every minute cannot prevent the instance from being terminated.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health checks for Auto Scaling instances"}],"answers":[{"id":"745390da2b4433786bf4cdf17df3d2d3","text":"Edit a health check shell script that performs some sanity checks in the EC2 instance. If the sanity checks pass, the shell script uses AWS CLI “aws autoscaling set-instance-health” to set its status to be healthy. Run the script every minute.","correct":false},{"id":"1584209c598c68a956227b3770a97fb2","text":"Create an AMI and configure a new launch configuration with the AMI. Then modify the Auto Scaling group to use the new launch configuration and launch a new instance.","correct":false},{"id":"8fad414c494de200ee3990b334d22b13","text":"Update the CloudFormation script and modify the health check type from ELB to EC2.","correct":true},{"id":"c66bf36d77a2aa4cf5ef75a4ac494df8","text":"Store all custom configuration scripts in an S3 bucket and create a new launch configuration. In its user data section, download the scripts from the S3 bucket and execute them. Whenever a new instance is launched, the configurations can be installed automatically. ","correct":false}]},{"id":"3440b6ff-6fe9-495d-a765-f69f6b82a628","domain":"awscsapro-domain3","question":"You work for a health record management company which operates a view-only portal for end-users to check their health records online. Users can also raise disputes if anything is incorrect. This 2-tier website that supports only HTTPS will be moved to the AWS Cloud. There are 5 web servers on-premises and an F5 Load Balancer that controls traffic to these web servers. SSL is terminated at the web servers. An Oracle Real Application Cluster (RAC) serves as the database tier. Due to the sensitive nature of personal health information, the web site uses mutual authentication - the server requests browsers for a valid client certificate before establishing a trusted session.\nSelect two alternate working architectures for this application to be migrated to AWS so that code changes are minimized. Choose two responses each of which can act as an independent and functional solution, and also result in minimum application code changes.","explanation":"The areas tested by this question are as follows.\nFirst, if an ELB is not terminating SSL, its listener protocol cannot be HTTPS - it has to be TCP. Additionally, AWS ELB does not support terminating client-side certificates. Therefore, if a website requires client SSL certificates, and if it also uses AWS ELB, the ELB must let the target EC2 instances to terminate SSL and validate the client certificate. This requires the protocol to be TCP/443. Both these facts (one, SSL is not terminated at the Load Balancer level, and two, mutual authentication is used) are stated explicitly in the question so that the candidate identifies at least one of them and is thus able to conclude that HTTPS is not the correct protocol choice for ELB. Hence, amongst the two choices that use ELB, the one that says TCP is the correct one.\nSecondly, RDS does not support Oracle RAC. Hence, the database tier must use EC2 instances. Thus, for the second alternate solution that uses Route 53 multi-value answer records instead of ELB, we should select the option that deploys EC2 instances for the database tier.","links":[{"url":"https://forums.aws.amazon.com/thread.jspa?threadID=109180","title":"Discussion Forums - HTTPS Client certificate validation while using client ELB"},{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"RDS FAQ-s, search for phrase - Is Oracle RAC supported on Amazon RDS"}],"answers":[{"id":"11a4ad1f948d7b2fd3631cb763fa8a00","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring TCP/443 as listener","correct":true},{"id":"307f49d40547367c203a0fcfa1a46be8","text":"Migrate the database to RDS Oracle and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":false},{"id":"e7a1d88af1880a82062437a4c1005adf","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":true},{"id":"7250b61db9c9e69abf4f9e7bd2bfb268","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring HTTPS/443 as listener","correct":false}]},{"id":"5af539b7-b132-4a3a-bc80-406c620e7325","domain":"awscsapro-domain1","question":"A food service business has begun an initiative to migrate all applications and data to the AWS cloud. Governance needs to be established before any migrations can occur. Business units such as sales, marketing, and product management have fluctuating infrastructure capacity and security requirements, while other business units like finance, operations, and human resources have more static demand. Security policies and compliance needs vary by project group within each business units. Each business unit is responsible for it's own cost center, and the finance group would like cost reporting to be as streamlined as possible. Which AWS account structure will best satisfy the company's governance needs?","explanation":"Leveraging AWS Organizations to manage an account structure with a core Organizational Unit and Organizational Units for each business unit provides flexibility for future organizational changes. Creating an account for each project group facilitates security policy differences within business units, and limits the exposure of a single security event. Managing differing security requirements by project group in a single account will require more governance maintenance. Creating billing, shared services, and log archive accounts in multiple Organizational Units will result in duplication of services, and can be done at the core level.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-laying-the-foundation/introduction.html","title":"Laying the Foundation: Setting Up Your Environment for Cost Optimization"},{"url":"https://aws.amazon.com/solutions/aws-landing-zone/?did=sl_card&trk=sl_card","title":"AWS Landing Zone"}],"answers":[{"id":"a8f7d8fbb7c6c3a1a14c91577dff42e1","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Place business units with similar security requirements in shared Organizational Units. Create accounts for each business unit in the shared Organizational Units. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"a66d8391267460b5800c5c3d07921767","text":"Use AWS Organizations to create Organizational Units for each business unit. Create a billing account, a shared services account, and a log archive account in each Organizational Unit. Create accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"bd400ff0d22480599228a0442d2bb8d4","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Create an Organizational Unit for each business unit that contains accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":true},{"id":"03705913700b8d76205d4203c58dc5e1","text":"Use AWS Organizations with a single Organizational Unit to consolidate costs. Create a billing account, a shared services account, and a log archive account in the Organizational Unit. Create individual accounts for each business unit. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs","correct":false}]},{"id":"63da01c2-9c4d-4abd-a482-06ede2baf728","domain":"awscsapro-domain2","question":"A popular royalty free photography website has decided to run their business on AWS. They receive hundreds of images from photographers each week to be included in their catalog. Amazon S3 has been selected as the image repository. As the business has grown, the task of creating catalog entries manually has become unsustainable. They'd like to automate the process and store the catalog information in Amazon DynamoDB. Which architecture will provide the most scalable solution for automatically adding content to their image catalog going forward?","explanation":"Calling the S3 API to upload the images will suffice for this use case. Streaming ingest is not needed for this volume of data. AWS Step Functions will orchestrate the process of discovering both the image metadata with a Lambda function and the image object data with Rekognition. Rekognition will not return the image metadata. AWS Elemental MediaStore is used for originating and storing video assets for live or on-demand media workflows, not image recognition. Kinesis Video Analytics is not a currently supported service.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://github.com/aws-samples/lambda-refarch-imagerecognition","title":"Serverless Reference Architecture: Image Recognition and Processing Backend"}],"answers":[{"id":"f98112f4f94f4a8d66edee560976acc2","text":"Programmatically call the S3 API to upload the images. Trigger an AWS Lambda function to send the image's S3 key to AWS Elemental MediaStore, which will extract the image's metadata, discover image patterns through machine learning, and deposit artifacts back into S3. Invoke a Lambda function to write the artifact data to DynamoDB.","correct":false},{"id":"dd8565c85d19c2867d3a6768f512b404","text":"Programmatically call the S3 API to upload the images. Trigger an AWS Lambda function to kick off execution of a state machine in AWS Step Functions. Create state machine sub-steps to invoke Lambda functions which extract image metadata, detect objects in the image with Amazon Rekognition, and store the discovered data in DynamoDB.","correct":true},{"id":"0710b6a1f2a807da2cfc1a940e7014e9","text":"Deploy Amazon Kinesis Data Streams to ingest the images with two consumers. Setup Amazon Kinesis Firehose as the first consumer to deposit the images into S3. Configure Amazon Kinesis Video Analytics as the second consumer to extract the image's metadata and object information. Invoke a Lambda function to store the discovered information in DynamoDB.","correct":false},{"id":"42367a003a702450700b58798073122b","text":"Deploy Amazon Kinesis Data Firehose to ingest images into S3. Invoke a Lambda function to pass the image's S3 key to Amazon Rekognition, which will extract the image metadata and detect objects in the image. Invoke a Lambda function to store the discovered data in DynamoDB.","correct":false}]},{"id":"f02ba751-479b-4ff0-a09b-8f18a63177b5","domain":"awscsapro-domain3","question":"An automotive supply company has decided to migrate their online ordering application to AWS. The application leverages a Model-View-Controller architecture with the user interface handled by a Tomcat server and twenty thousand lines of Java Servlet code. Business logic also resides in two thousand lines of PL/SQL stored procedure code in an Oracle database. The company's technology leadership has directed your team to move the database to a more cost-effective offering, and to adopt a more cloud-native architecture. Business objectives dictate that the application must be live in the AWS cloud in sixty days. Which migration approach will provide the most scalable architecture and meet the schedule objectives?","explanation":"This solution will require trade-offs between schedule requirements and architectural desires. Converting twenty thousand lines of Model-View-Controller code to a serverless architecture in sixty days is unreasonable, so moving the Tomcat MVC as-is to EC2 for the initial migration is the best approach. We can migrate to a serverless user interface in a later phase. Database Migration Service will suit our needs well for moving the application data to Aurora, but the most scalable architecture strategy is to migrate the stored procedure code out of the database so that database nodes won't need to be resized when the business logic needs more compute resources. Under normal circumstances, recoding two thousand lines of PL/SQL code to Python Lambda functions within a sixty day time frame will not be a problem.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/blogs/database/migrate-your-procedural-sql-code-with-the-aws-schema-conversion-tool/","title":"Migrate Your Procedural SQL Code with the AWS Schema Conversion Tool"},{"url":"https://aws.amazon.com/lambda/","title":"AWS Lambda"}],"answers":[{"id":"db222d8a15bda541fc4147908131cfd6","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service to move the application data into Amazon Aurora. Convert the stored procedure code to AWS Lambda Python functions, and modify the Servlet code to invoke them","correct":true},{"id":"97a348ed01424c357958b49bcc030935","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"5ab40b2a54c4e82a7aafa05c8fc9a458","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service to migrate the application data and stored procedures to an Amazon RDS Oracle instance","correct":false},{"id":"2cf84f7f8daee7548143ad181423c7cb","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false}]},{"id":"6d93e859-e1a9-468f-9a05-61a2dbc2be9c","domain":"awscsapro-domain5","question":"You manage a group of EC2 instances that host a critical business application.  You are concerned about the stability of the underlying hardware and want to reduce the risk of a single hardware failure impacting multiple nodes.  Regarding Placement Groups, which of the following would be the best course of action in this case?","explanation":"Spread Placement Groups ensure your instances are each placed on separate underlying hardware so this reduces the risk of a single hardware failure taking down multiple instances.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-spread","title":"Placement Groups - Amazon Elastic Compute Cloud"}],"answers":[{"id":"c8b809a2fef3f21146774b82e8f03f12","text":"You would move the instances onto a Dedicated Host.","correct":false},{"id":"b6153af57a0b8805fc28d93a2859fb9e","text":"You cannot move existing instances into a new placement group.  You would create AMIs from the existing instances and redeploy them into a clustered placement group.","correct":false},{"id":"3bde2d7fff7ead39c4035ab0600275f1","text":"You would use the AWS CLI to move the existing instances into a diversified placement group.","correct":false},{"id":"e6d129a06320300aaf95634a6691b7cb","text":"You would the AWS Console to move the existing instances into a clustered placement group.","correct":false},{"id":"e346d1667489b5726c0367eff4ea4a34","text":"You would use the AWS CLI to move the existing instances into a spread placement group.","correct":true}]},{"id":"482e75c9-071e-4a10-83f4-575f9c15b885","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They have just accidentally deleted the private key portion of their EC2 key pair.  Now, they are unable to SSH into their Amazon Linux servers.  Unfortunately the keys were not backed up and are considered gone for good.  What can this customer do to regain access to their instances?","explanation":"The two methods that AWS recommends if you lose a private key for an EC2 key pair are using Systems Manager Automation or using a secondary instance to edit the authorized_keys file.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2reset.html","title":"Reset Passwords and SSH Keys on Amazon EC2 Instances - AWS Systems Manager"}],"answers":[{"id":"bec3d01a56c851f61a1a09c852635db7","text":"Generate and upload a new key pair.  Stop the instances and select the new key pair from the dropdown on the Instance Settings sub-menu in the Console.","correct":false},{"id":"f17aa014620843abd81fa849982566b0","text":"Create a new key pair in KMS then assign the new public key to the required EC2 instance.","correct":false},{"id":"492c38d2fe2c3b96608bb8436592fe26","text":"Use the AWS CLI with the EC2 ModifyInstance action to enable SSH password-only access for the ec2-user account.  Attach using a password rather than an SSH key.  Modify the authorized_key file for the new public key.","correct":false},{"id":"79457a1b908d4a36cfeba625be909d40","text":"Use AWS Systems Manager Automation with the AWSSupport-ResetAccess document to create a new SSH key for your current instance.","correct":true},{"id":"509a77ae9827c5fcb60ccecc62fc9853","text":"Stop the instances, detach its root volume and attach it as a data volume to another instances.  Modify the authorized_keys file, move the volume back to the original instance and restart the instances.","correct":true},{"id":"be45655cb1d64dff71a97aa729bc4e4a","text":"Open the TELNET port (port 23) on the Security Group for the server.  Use a TELNET client to attach to the instances using the root account and password.  Modify the authorized_key file with the new public key.","correct":false}]},{"id":"c29d0343-9d60-4882-b1f3-2897ef7e889a","domain":"awscsapro-domain2","question":"Your team starts using Docker to manage a web application. The Docker image is pushed to AWS ECR and the application is hosted in Amazon Elastic Container Service (Amazon ECS) containers. After the application is deployed, you find that there are occasions where the application is attacked by SQL injection or cross-site scripting. You would like to set up certain rules to protect the web application from the common application-layer exploits so that specific traffic patterns that you define are filtered out. How would you implement this?","explanation":"AWS WAF is a web application firewall that can protect an Amazon CloudFront distribution, an Amazon API Gateway API, or an Application Load Balancer. It cannot associate with an ECS cluster. However you can configure the Amazon ECS to use an Application Load Balancer and then associate the WAF ACL with the ELB. CloudFront distribution can not use the ECS cluster as the origin. AWS Shield Advanced is only for DDoS attacks and is not suitable.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html","title":"Use AWS WAF to protect applications that are hosted in Amazon Elastic Container"}],"answers":[{"id":"d804d87fdada57fe73d5fb20b76721f4","text":"Configure rules to protect the application from common attacks including SQL injection and cross-site scripting in AWS Firewall Manager. Use an Application Load Balancer to distribute traffic to the ECS cluster. In AWS Firewall Manager, apply the rules in the Application Load Balancer.","correct":false},{"id":"7db2f0b98cf72e4dcc0d93100a6230cd","text":"Enable AWS Shield Advanced to protect the application from common attacks such as SQL injection, DDoS and cross-site scripting. In AWS Firewall Manager, associate AWS Shield with the ECS cluster name so that the ECS cluster is monitored by AWS Shield and the traffic is filtered based on the created rules.","correct":false},{"id":"9ee0fc90381d7cb22768fda1080073af","text":"Configure an ACL in AWS WAF and the ACL contains rules to block common attack patterns, such as SQL injection or cross-site scripting. Setup a CloudFront distribution to use the ECS cluster as the origin. Associate the WAF ACL with the CloudFront distribution.","correct":false},{"id":"51f6c122390f4f18e845faf465045044","text":"Set up access control lists (ACLs), rules, and conditions in AWS WAF to define acceptable or unacceptable traffic. Configure Amazon ECS to use an Application Load Balancer to distribute the traffic. Use WAF to protect the application behind the Application Load balancer.","correct":true}]},{"id":"6da286f8-23a6-4e8a-a3a4-c7b496a06523","domain":"awscsapro-domain5","question":"An online health foods retailer stores its product catalog in an Amazon Aurora database. The catalog contains over 6,000 products. They'd like to offer a product search engine on the website using Amazon Elasticsearch Service. They'll use AWS Database Migration Service (DMS) to perform the initial load of the Elasticsearch indexes, and to handle change data capture (CDC) going forward. During the initial load of the indexes, the DMS job terminates with an Elasticsearch return code of 429 and a message stating 'Too many requests'. What must be done to load the Elasticsearch indexes successfully?","explanation":"When the ElasticSearch indexing queue is full, a 429 response code is returned and an es_rejected_execution_exception is thrown. The DMS load task then terminates. Throttling the DMS input stream based on the number of Elasticsearch indexes, shards, and replicas to be loaded will result in a successfully completed job. The DMS MaxFullLoadSubTasks parameter indicates how many source tables to load in parallel, and the ParallelLoadThreads parameter determines the number of threads that can be allocated for a given table. Increasing Elasticsearch shards without modifying DMS subtask and thread parameters could still overrun the request queue. Changing the DMS stream buffer count won't help with this issue. Amazon Elasticsearch currently doesn't provide support for AWS Glue as a source, so integration would require significant effort. Increasing Elasticsearch EBS volume IOPS won't solve an ingress queue overrun problem. The DMS batch split size parameter sets the maximum number of changes applied in a single batch, but doesn't reduce the total number of requests.","links":[{"url":"https://aws.amazon.com/dms/","title":"Amazon Database Migration Service"},{"url":"https://aws.amazon.com/elasticsearch-service/","title":"Amazon Elasticsearch Service"},{"url":"https://aws.amazon.com/blogs/database/scale-amazon-elasticsearch-service-for-aws-database-migration-service-migrations/","title":"Scale Amazon Elasticsearch Service for AWS Database Migration Service migrations"}],"answers":[{"id":"c1e1a85b26a30433a6af5d30c8bb8d76","text":"Calculate the number of queue slots required for the Elasticsearch bulk request as a product of the number of indexes, shards, and replicas. Adjust DMS subtask and thread parameters accordingly","correct":true},{"id":"ee750f72f1e70b83f6d83819f2d504f5","text":"Raise the baseline IOPS performance of the Elasticsearch cluster EBS volumes to enable more throughput. Increase the DMS batch split size parameter to send more data in each request and reduce the number of total requests","correct":false},{"id":"62ac117860ebe5397e04bad8ea29a5fb","text":"Replace DMS with AWS Glue for the initial index load and ongoing change data capture. Enable parallel reads when the ETL methods are called in the Glue jobs","correct":false},{"id":"157e8733386382289486b3592774442f","text":"Increase the number of Elasticsearch shards for each index to increase distribution of the load. Change the DMS stream buffer count parameter to match the number of Elasticsearch shards","correct":false}]},{"id":"0c3d85ee-ff65-46e7-86d4-9fb7dcd21176","domain":"awscsapro-domain4","question":"Per the requirements of a government contract your company recently won, you must encrypt all data at rest.  Additionally, the material used to generate the encryption key cannot be produced by a third-party because that could result in a vulnerability.  You are making use of S3, EBS and RDS as data stores, so these must be encrypted.  Which of the following will meet the requirements at the least cost?","explanation":"When possible, making use of KMS is much more cost-effective than CloudHSM.  We can import our own key material into KMS for creating Customer Master Keys.  Because KMS works natively with the services we will be using, we save on any sort of custom integration that CloudHSM would have required.","links":[{"url":"https://aws.amazon.com/kms/faqs/","title":"FAQs | AWS Key Management Service (KMS) | Amazon Web Services (AWS)"},{"url":"https://aws.amazon.com/cloudhsm/faqs/","title":"AWS CloudHSM FAQs - Amazon Web Services"}],"answers":[{"id":"66a66f8efdd420438db68069645b1ae7","text":"Initialize a CloudHSM instance.  Use it to generate custom encryption keys for each service you will use.  When creating an S3 bucket, EBS volume or RDS instance, select the custom CloudHSM key from the dropdown in the setup wizard.","correct":false},{"id":"9052cdd68b68935da91384397a18351c","text":"Use AMS KMS to create a 256-bit encryption key.  Use a grant to only allow access by S3, RDS and EBS.  When creating an S3 bucket, select the SSE-KMS option and pick the key from the dropdown.  For EBS and RDS, use the CLI to assign the KMS key when creating those instances.","correct":false},{"id":"473891e603ed3e2091e789a13094399c","text":"Use AMS KMS to create a customer-managed CMK.  Create a random 256-bit key and encrypt it with the wrapping key.  Import the encrypted key with the import token.  When creating S3 buckets, EBS volumes or RDS instances, select the CMK from the dropdown list.","correct":true},{"id":"a4e27f3b361ede72810f71b15168297e","text":"Generate a public and private key pair.  Upload the public key via the EC2 dashboard.  When creating EBS volumes, select encryption and select this public key.  When creating S3 buckets, implement a bucket policy which requires encryption at rest only, rejecting other files.  Create an RDS instance and select the public key from the dropdown in the setup wizard.","correct":false}]},{"id":"8765bd56-057b-488c-9a0a-f5bd413dd240","domain":"awscsapro-domain5","question":"Due to new corporate policies on data security, you are now required to use encryption at rest for all data.  You have some EC2 Linux instances on AWS that were created without encryption for the root EBS volume.  What can you do that meet the requirement and reduce administrative overhead?","explanation":"AWS does support encrypted root volumes but conversion from unencrypted root to an encrypted root requires a bit of a process. You must first create an AMI then copy that newly created AMI to the same region, specifying that you want to encrypt the EBS volumes during the copy.  You can then create a new instance with an encrypted root volume from the copied AMI.  You can use either a generated key from KMS or your own CMK imported into KMS.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html","title":"AMIs with Encrypted Snapshots - Amazon Elastic Compute Cloud"},{"url":"https://aws.amazon.com/blogs/aws/new-encrypted-ebs-boot-volumes/","title":"New – Encrypted EBS Boot Volumes | AWS News Blog"}],"answers":[{"id":"1e30ccf0b75e9e70fd76c6e041510c75","text":"At present, EC2 does not support encrypted root volumes.  Create new encrypted EBS data volumes and attach the new volumes to the existing instances.  Use RSYNC to migrate all the non-OS data over to the encrypted data volumes.","correct":false},{"id":"9fbbb2e71386cb7f7a7ed77129a1a960","text":"Create an encrypted EFS instance and mount-points in the respective subnets.  Log into the instance and mount an encrypted EFS mount-point.  Copy all the root files over to the EFS mount point.  Edit the FSTAB file to mount the EFS mount point as the root volume instead of the root EBS device and reboot.","correct":false},{"id":"4430b7492a6c058c3574ce4e8ea43955","text":"Stop the instances and temporarily detach the EBS volumes.  Attach the root volumes to another EC2 instance and mount them a data volume.  Use a encryption tool like GPG or OpenPGP to recursively encrypt all the files on the mounted root volumes.  Detach and reattach the encrypted EBS volumes to the original instances and restart.  Import the encryption keys in KMS as a CMK.","correct":false},{"id":"180e9aecdb74f204b1df00ffe6fa8b56","text":"Stop the instances and create AMIs from the instances.  Copy the AMIs to the same region and select the \"Encrypt target EBS snapshots\".  Redeploy the instances using the AMI copies you made with encrypted root volumes.","correct":true},{"id":"50c17b27f0bd0390ae321943f7db5c3d","text":"Create a certificate in CMS for the encryption key.  Stop the instances and temporarily detach the root volumes.  Via the AWS CLI, enable encryption on the root volumes using the \"ebs modify-volume\" argument with the flag of \"encryption=<CMS ARN>\" to specify the certificate.","correct":false}]},{"id":"a5ad7848-a002-475f-8cb1-eab23822846e","domain":"awscsapro-domain4","question":"A toy company needs to reduce customer service costs related to email handling. With the current process, representatives read the emails, determine the intent, identify next best action, and compile all information required for a response. The company would like to automate as much of the process as possible. They've already established connectivity to AWS for other applications, and they're planning to link their corporate email exchange to Amazon Simple Email Service (SES). Which architecture will provide them with the best cost reduction opportunity over their current solution?","explanation":"The process starts with the storing of the raw email text to S3 for use by the machine learning services later. The SNS notification triggers the email responder workflow, which employs Comprehend to extract keywords. The keywords serve as input parameters to the SageMaker models for formulation of the next best action. The confidence scores from the SageMaker inference drive the decision as to whether a Lambda function should generate an automated email response or route to a representative for handling. Using EC2 or ECS will generally be more expensive and require more operational effort than using Lambda. Transcribe is used for voice recognition, and Amazon Lex is used for creating chat bots. Neither is used for keyword extraction. Amazon Pinpoint provides additional customer contact capabilities not needed for this use case.","links":[{"url":"https://aws.amazon.com/comprehend/","title":"Amazon Comprehend"},{"url":"https://aws.amazon.com/sagemaker/","title":"Amazon SageMaker"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/ai-ml-based-intelligent-email-responder-ra.pdf?did=wp_card&trk=wp_card","title":"AI/ML Based Intelligent Email Responder"}],"answers":[{"id":"fe8b2750f1ebef6fc86d4bf7224b99cc","text":"Configure SES to write the emails to an Amazon Simple Queue Service queue. Have a program on EC2 read the queue and invoke Amazon Transcribe to perform keyword extraction. Call Amazon SageMaker intent determination and next-best-action model endpoints. Based on confidence scores, have the program either build and send a response email through Amazon Pinpoint, or route the original email to a representative for handling","correct":false},{"id":"c7ce297a851f2728e3e035b08aed9f8f","text":"Configure SES to write the emails to S3 and publish notifications to an Amazon Simple Notification Service topic. From the SNS publish, trigger an AWS Lambda function to invoke Amazon Comprehend to perform keyword extraction. Trigger another Lambda function to call Amazon SageMaker intent determination and next-best-action model endpoints. Based on confidence scores, have another Lambda function either build and send a response email through SES, or route the original email to a representative for handling","correct":true},{"id":"330f1889136383d262cb0d652843bd75","text":"Configure SES to write the emails to S3 and publish notifications to an Amazon Simple Notification Service topic. From the SNS publish, trigger an AWS Lambda function to invoke Amazon Lex to perform keyword extraction. Have the Lambda function forward the email text and keywords to a representative for intent and next-best-action determination. Give the representative a menu of pre-written response letters to be customized and sent automatically through SES.","correct":false},{"id":"36fb6500a6d540182f27959a12c1e9e5","text":"Configure SES to write the emails to an Amazon Simple Queue Service queue. Have a program in an Amazon Elastic Container Service container read the queue and invoke Amazon Transcribe to perform keyword extraction. Call Amazon SageMaker intent determination and next-best-action model endpoints. Based on confidence scores, have the program either build and send a response email through SES, or route the original email to a representative for handling","correct":false}]},{"id":"663fbd6a-87bd-4fa6-a0ea-428ba2de5b51","domain":"awscsapro-domain5","question":"You manage a relatively complex landscape across multiple AZs.  You notice that the incoming requests vary mostly depending on the time of day but also there is a more unpredictable component resulting in smaller spikes and valleys for your resources.  Fortunately, you manage this landscape via OpsWorks Stacks.  What options, if any, are available to you as part of the OpsWorks featureset.","explanation":"OpsWorks Stacks offers three types of scaling: 24/7 for instances that remain on all the time; time-based for instances that can be scheduled for a certain time of day and on certain days of the week; and load-based scaling which will add instances based on metrics.  All this can be configured from within the OpsWorks Stack console.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/best-practices-autoscale.html","title":"Best Practices: Optimizing the Number of Application Servers - AWS OpsWorks"}],"answers":[{"id":"216c997091da6e24174ad1b83d0be8b9","text":"You would define a baseline level of resources within the OpsWorks Stack Console to cover the average load.  But for the periodic load, that requires a scheduled auto-scaling policy.  Similarly, for the volatile spikes, you must use a stepped auto-scaling policy defined in an auto scaling group. ","correct":false},{"id":"b7ff5b06f51facca179494cb2bb00e55","text":"You can enabled CloudFormation Anticipated Scaling that uses past CloudWatch metrics and machine learning to automatically design a scaling policy optimized for the incoming request patterns.","correct":false},{"id":"3622d494dceb973760a46dea038d1dc2","text":"If you need the ability to dynamically scale, you will need to use OpsWorks for Chef Automate.  OpsWorks Stacks does not support scaling.","correct":false},{"id":"75ab4de4ea42c1971b0ee09ae04ca591","text":"You would define a baseline level of resources and configure them for 24/7 instances.  Then you could define a time-based instances to cover certain times of day.  Finally, you could cover the volatile spikes with a load-based instances.  All this can be done within OpsWorks Stacks.","correct":true}]},{"id":"f679d23d-14d4-4021-9749-481bbe11046c","domain":"awscsapro-domain3","question":"A telecommunications company has decided to migrate their entire application portfolio to AWS. They host their customer database and billing application on IBM mainframes. IBM AIX servers running WebSphere provide an API layer into the mainframes. Customer-facing online applications are hosted on Linux systems. The customer service backend application resides on Oracle Solaris and makes use of gigabytes of persistent information. Their ERP and CRM systems also run on Solaris boxes. Telecom switches send call records to Linux-based applications, and their employee productivity suite runs on Windows. They need to complete the project in twelve months to satisfy budgetary constraints. Which migration strategy will provide them with the most resilient, scalable, and operationally efficient cloud environment within the project time frame?","explanation":"Since the company has twelve months to complete the project, they can plan for a highly cloud-centric migration. Refactoring the mainframe billing application to EC2 and the customer database to Aurora will require significant cost and effort, but will result in significant intermediate to long-term business value for most companies. A number of AWS Partner Network (APN) solutions are available to assist with this. The WebSphere layer can be replaced by API Gateway with HTTP, REST, or WebSocket APIs that call modules on the EC2 instances. Refactoring the customer-facing online apps to Lambda serverless and Step Functions will provide high operational efficiency. Performing a replatform of the Solaris customer service application to EC2 with Auto Scaling will achieve elasticity to avoid the excess capacity inefficiencies that were most-likely present in the on-premises environment. Many robust ERP, CRM, and employee productivity SaaS solutions exist and should be leveraged rather than trying to manage these applications with in-house staff. The call record processing Linux system can simply be rehosted to EC2. Repurchasing mainframe capacity from a third party provider only extends the rigidness of making mainframe changes whenever new business requirements arise. A Lambda/Step Functions solution will provide all the functionality needed for the online apps, and will be more economical than Elastic Beanstalk. Refactoring the customer service application to Lambda presents issues with processing the gigabytes of persistent information, so replatforming to EC2 is a better choice.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud"},{"url":"https://aws.amazon.com/blogs/apn/automated-refactoring-of-a-new-york-times-mainframe-to-aws-with-modern-systems/","title":"Automated Refactoring of a New York Times Mainframe to AWS with Modern Systems"},{"url":"https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/","title":"Build a Serverless Web Application"}],"answers":[{"id":"290da60b3bdd11f28e175fa86972386a","text":"Repurchase mainframe capacity from a third party provider and run the customer database and billing application there. Replace the API layer with Amazon API Gateway. Rehost the customer-facing online applications to Amazon Elastic Beanstalk. Refactor the customer service application to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the ERP and CRM applications to EC2 Linux instances with Auto Scaling. Rehost the call record processing applications, and repurchase SaaS applications for the employee productivity suite.","correct":false},{"id":"57d6d888b4cee16e22852910fa09cdb8","text":"Refactor the mainframe applications onto Amazon EC2 Linux instances, and migrate the customer database to Amazon Aurora. Replace the API layer with Amazon API Gateway. Rehost the customer-facing online applications to Amazon Elastic Beanstalk. Refactor the customer service application to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Repurchase SaaS solutions for the ERP and CRM systems. Rehost the call record processing applications and the employee productivity suite onto EC2.","correct":false},{"id":"5a404284de876ce2a054dcd916ed80ec","text":"Refactor the mainframe applications onto Amazon EC2 Linux instances, and migrate the customer database to Amazon Aurora. Replace the API layer with Amazon API Gateway. Refactor the customer-facing online applications to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the customer service applications to EC2 Linux with Auto Scaling. Repurchase SaaS solutions for the ERP and CRM systems. Rehost the call record processing applications onto EC2, and repurchase SaaS applications for the employee productivity suite.","correct":true},{"id":"c875a6eb6d74f098c971af4a638f288f","text":"Repurchase mainframe capacity from a third party provider and run the customer database and billing application there. Replatform the API layer onto EC2 Linux instances. Refactor the customer-facing online applications to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the customer service, ERP, and CRM applications to EC2 Linux instances with Auto Scaling. Rehost the call record processing applications onto EC2, and repurchase SaaS applications for the employee productivity suite.","correct":false}]},{"id":"baf2349f-71ba-4583-bfe6-31fb5a555bbd","domain":"awscsapro-domain5","question":"The information security group at your company has implemented an automated approach to checking Amazon S3 object integrity for compliance reasons. The solution consists of scripts that launch an AWS Step Functions state machine to invoke AWS Lambda functions. These Lambda functions will retrieve an S3 object, compute its checksum, and validate the computed checksum against the entity tag checksum returned with the S3 object. However, an unexpected number of S3 objects are failing the integrity check. You discover the issue is with objects that where uploaded with S3 multipart upload. What would you recommend that the security group do to resolve this issue?","explanation":"For S3 objects, the entity tag (or ETag) contains an MD5 hash of the object in most cases. But if an object is created by either the Multipart Upload or Part Copy operation, the ETag is not an MD5 digest of the object. The ETag value returned by S3 for objects uploaded using the multipart upload API is computed differently than for objects uploaded with PUT object, and does not represent the MD5 of the object data. The checksum for an object created via multipart upload can be stored in a custom metadata parameter for later integrity checks. The Content-MD5 metadata parameter can not be modified by a user after the object has been created. The complete-multipart-upload API does not have an md5-rehash parameter. The list-multipart-uploads API will only return information about the multipart upload while the upload is running.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html","title":"Multipart Upload Overview"},{"url":"https://aws.amazon.com/solutions/serverless-fixity-for-digital-preservation-compliance/?did=sl_card&trk=sl_card","title":"Serverless Fixity for Digital Preservation Compliance"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/s3-multipart-upload-cli/","title":"How do I use the AWS CLI to perform a multipart upload of a file to Amazon S3?"}],"answers":[{"id":"2a27b97da81e5d7f237719319a3d85fc","text":"When performing S3 multipart uploads, after all upload-parts API calls have been made, call the complete-multipart-upload API and include the md5-rehash parameter to reset the entity tag checksum to the sum of the parts. Reload all objects that were written with S3 multipart upload that need to be included in the integrity check.","correct":false},{"id":"df361f3568504f0db56ea3faddc6928a","text":"For all S3 objects created with multipart upload, retrieve the object, compute it's checksum, and store the value in the Content-MD5 metadata parameter. Have the Lambda function that validates checksums use the Content-MD5 metadata parameter if it's present instead of the entity tag checksum.","correct":false},{"id":"15ca1d17e48531d75eda4298140756e6","text":"In the Lambda function that retrieves the objects, if the object was created with a multipart upload, call the list-multipart-uploads API and retrieve each part of the multipart upload along with the entire object. In the Lambda function that computes checksums, compute the checksum of each part along with the checksum of the entire object. In the Lambda function that validates checksums, compare the sum of the checksum parts to the checksum for the entire object.","correct":false},{"id":"fe2f446e0e84bb45be4130b327b8b47b","text":"When performing S3 multipart uploads, calculate the checksum of the source file and store it in a custom metadata parameter. Have the Lambda function that compares checksums use the custom metadata parameter if it's present instead of the entity tag checksum. Reload all objects that were written with multipart upload that need to be included in the integrity check.","correct":true}]},{"id":"1520156f-0918-4ab4-a759-ce33a931c744","domain":"awscsapro-domain5","question":"Your company has an online shopping web application. It has adopted a microservices architecture approach and a standard SQS queue is used to receive the orders placed by the customers. A Lambda function sends orders to the queue and another Lambda function fetches messages from the queue and processes them. On some occasions the message in the queue cannot be handled properly. For example, when an order has a deleted production ID, the message cannot be consumed successfully and is returned to the queue. The problematic messages in the queue keep growing and the ability to process normal messages is affected. You need a mechanism to handle the message failure and isolate error messages for further analysis. Which method would you choose?","explanation":"It is not a good idea to adjust the retention period or simply delete the messages that fail to be processed as the question asks for a mechanism to isolate the messages for further troubleshooting. A redrive policy should be used to auto-forward error message to a dead letter queue. Then you can analyze the contents of messages to diagnose the producer’s or consumer’s issues. One thing to note is that a standard queue can only have another standard queue as the dead letter queue. Therefore a FIFO dead letter queue is incorrect as this scenario uses a standard SQS queue and requires a standard dead letter queue.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","title":"Amazon SQS dead-letter queues"}],"answers":[{"id":"3c8453a6c57faf61e761f771bab6f1af","text":"Create a standard queue as the dead letter queue and configure a redrive policy to put error messages to the dead letter queue. Analyze the contents of messages in the dead letter queue to diagnose the issues.","correct":true},{"id":"a7eb53a7df09334677590165f666c58f","text":"Modify the error handling logic of the Lambda function to delete the messages whenever the processing is unsuccessful with an error or exception. The error messages do not return to the queue and the normal message handling is not blocked.","correct":false},{"id":"f7b3898cfcb4851b120c9b14d044ab90","text":"Decrease the message retention period of the queue to 1 day. When the messages are not processed properly and put back in the queue, they can be quickly deleted when the retention period expires.","correct":false},{"id":"64fd54fe78b534f0eac9222a6e32747d","text":"Create a FIFO (First-In-First-Out) queue as the dead letter queue and use a redrive policy to forward problematic messages to this new queue. Create a Lambda function to read the message contents in the FIFO queue for further analysis.","correct":false}]},{"id":"abc17e7b-6b75-45e3-a5c0-ea0f55d4df97","domain":"awscsapro-domain2","question":"Your client is a software company starting their initial architecture steps for their new multi-tenant CRM application.  They are concerned about responsiveness for companies with employees scattered around the globe.  Which of the following ideas should you suggest to help with the overall latency of the application?","explanation":"CloudFront can cache both static and dynamic content.  By setting a high TTL, we allow CloudFront to serve content longer before having to refresh from the origin.  Additionally, Lambda@Edge can intercept the request and direct the requester to a region based on the geographic origin of the request.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/11/lambda-at-edge-now-supports-content-based-dynamic-origin-selection-network-calls-from-viewer-events-and-advanced-response-generation/","title":"Lambda@Edge Now Supports Content-Based Dynamic Origin Selection, Network  Calls from Viewer Events, and Advanced Response Generation"}],"answers":[{"id":"702a6122527d0830134717e0e7323bd0","text":"Store the data in a DynamoDB Global Table.  Use an auto scaling ElastiCache cluster with Memcached as a caching layer.  Distribute static elements of the application via CloudFront.  Use Route 53 Weighted routing to dynamically route users to the nearest region.","correct":false},{"id":"afa9743126cd2b0644654f2439a2aa0a","text":"Install the application on several regions around the globe.  Use RDS cross-region read replication for PostgreSQL to ensure a strongly consistent data store.","correct":false},{"id":"0fc3951d630f285646b22cdd30f43eee","text":"Architect the system to use as many static objects as possible with high TTL.  Use CloudFront to retrieve both static and dynamic objects.  POST and PUT new data through CloudFront.","correct":true},{"id":"35483961564002569ee69763e24961fa","text":"Install the application in several regions around the globe.  As new customers and users are on-boarded, pre-cache their user data in CloudFront for that region.  Use AWS Batch to routinely expire the cache to ensure the latest updates are visible.","correct":false},{"id":"35638855dc45f62b3801906fd9a6d87c","text":"Install key parts of the application in multiple AWS regions chosen to balance latency for geographically diverse users.  Use Lambda@Edge to dynamically select the appropriate region based on the users location.","correct":true}]},{"id":"dc5b1869-9feb-4074-a582-f42e8f358272","domain":"awscsapro-domain4","question":"You have decided to make some changes to your landscape. Your landscape consists of four EC2 instances within a VPC interacting mostly with S3 buckets.  You decide to move your EC2 instances into a spread placement group.  You then create a VPC Gateway Endpoint for S3.  These changes have which of these impacts?","explanation":"Costs will decrease because you are using the VPC Gateway Endpoint to reach S3 rather than Internet egress.  Security will be improved because the traffic is not routed out through the Internet.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html","title":"Endpoints for Amazon S3 - Amazon Virtual Private Cloud"}],"answers":[{"id":"ec2facee172127252b37883f153ee675","text":"Costs will increase.","correct":false},{"id":"1de433430d814a079012ae9f2bb7b404","text":"Costs will decrease.","correct":true},{"id":"a98a8d72a35888659eaf4659936a6809","text":"Security profile will be reduced.","correct":false},{"id":"7a549fb0ca57680e691ab656b55d0277","text":"Costs will stay the same.","correct":false},{"id":"cdbabf3a8907b7fc48761f3c8c22864e","text":"Security profile will be improved.","correct":true},{"id":"1ea65cd7f8d031e9a625162d59461522","text":"Security profile will stay the same.","correct":false}]},{"id":"78111d9b-922f-435f-8e91-4ae84e990761","domain":"awscsapro-domain3","question":"A hotel chain has decided to migrate their business analytics functions to AWS to achieve higher agility when future analytics needs change, and to lower their costs. The primary data sources for their current on-premises solution are CSV downloads from Adobe Analytics and transactional records from an Oracle database. They've entered into a multi-year agreement with Tableau to be their visualization platform. For the time being, they will not be migrating their transactional systems to AWS. Which architecture will provide them with the most flexible analytics capability at the lowest cost?","explanation":"AWS Database Migration Service can be configured with an on-premises Oracle database as a source and S3 as a target. It can provide continuous replication between the two. AWS Glue can aggregate the data from S3 according to desired reporting dimensions and store the summaries in Redshift. Keeping the transactional detail in S3 and only keeping the aggregate information in Redshift will save on costs. The same is true for keeping transactional detail in S3 instead of RDS Oracle. AWS Glue is a great solution for transforming the Adobe Analytics CSV files to Parquet format in S3. Parquet's columnar organization will provide excellent performance for Redshift Spectrum queries that join between Redshift tables and S3. Tableau's Redshift connector supports Redshift Spectrum queries. For this use case, using Amazon QuickSight would not make sense since the company has already committed payments to Tableau via their multi-year agreement.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/glue/","title":"AWS Glue"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"e1f7ec66baca2c4e27cf072a8ca91424","text":"Employ AWS Database Migration Service to continuously replicate Oracle transactional data to Amazon S3. Configure AWS Glue to aggregate the transactional data from S3 for each dimension into Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Amazon S3 in Parquet format. Install Tableau on Amazon EC2 and write queries to Amazon Redshift Spectrum.","correct":true},{"id":"dc33d336682223190f2d8cb22449cf81","text":"Implement AWS Database Migration Service to continuously replicate Oracle transactional data to an Amazon RDS Oracle instance. Use AWS Glue to write the Adobe Analytics data to the RDS Oracle instance. Install Tableau on Amazon EC2 and write queries against the RDS Oracle database.","correct":false},{"id":"86c10f6cca438461e60f5c04886f57c9","text":"Configure AWS Database Migration Service to continuously replicate Oracle transactional data to Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Redshift. Use Amazon QuickSight to query the data for visualization.","correct":false},{"id":"1bc635be85a059a6135751bf21fd3550","text":"Use Oracle Data Guard to continuously replicate Oracle transactional data to an Oracle instance on Amazon EC2. Configure AWS Glue to aggregate the transactional data from the Oracle instance for each dimension into Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Redshift. Use Amazon QuickSight to query the data for visualization.","correct":false}]},{"id":"f43ec458-0ff5-4633-a57b-6bf82f60bd14","domain":"awscsapro-domain5","question":"You have a target group in an elastic load balancer (ELB) and its target type is \"instance\". You attach an Auto Scaling group (ASG) in the target group. All the instances pass the health check and have a healthy state in the target group. Due to a new requirement, the ELB target group needs to forward the incoming traffic to an IP address that belongs to an on-premise server. The ASG is no longer needed. There is already a VPN connection between the on-premise server and AWS VPC. How would you configure the target in the ELB target group?","explanation":"The target type of existing target groups cannot be changed from \"instance\" to \"IP\". Because of this, users have to create a new target group and set the target type to be \"IP\". After that, the on-premise IP address can be registered as a target. A domain name cannot be registered as a target in the target group. You also do not need to create a new elastic load balancer since you only need a new target group to register the IP address.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#target-type","title":"Target type in ELB target group"}],"answers":[{"id":"265de6fdcab5323b156774dcb949d309","text":"Create a new network load balancer with a new listener and target group. Configure the target type to be \"IP\" in the target group and attach the on-premise IP address to it. Set up the health check using the HTTP protocol.","correct":false},{"id":"e0b619a421d68626ffb82b1a6e1d22d5","text":"Remove the Auto Scaling group from the target group and modify the target type to be \"IP\". Attach the IP address to the target and set up the IP address and port in the health check configurations.","correct":false},{"id":"49646fca04901301d145a2a814a7e481","text":"In the elastic load balancer, create a new target group with an \"IP\" target type. Register the on-premise IP address as its target. Monitor if the target becomes healthy after some time. Remove the old target group.","correct":true},{"id":"99f63c1cf5bcfbcb188328714abb8ed3","text":"Register a record set in AWS Route 53 to forward a domain name to the on-premise IP address. Modify the target group to register the domain name as its target. Remove the previous Auto Scaling group from the target group.","correct":false}]},{"id":"a7c939f1-277e-469f-a209-9b290e8136c9","domain":"awscsapro-domain5","question":"Your company has contracted with a third-party Security Consulting company to perform some risk assessments on existing AWS resources.  As part of a routine list of activities, they inform you that they will be launching a simulated attack on one of your EC2 instances.  After the Security Group performed all their activities, they issue their report.  In their report, they claim that they were successful at taking the EC2 instance offline because it stopped responding soon after the simulated attack began.  However, you're quite certain that machine did not go offline and have the logs prove it.  What might explain the Security company's experience?","explanation":"AWS Shield and other counter-measure technologies work to protect all AWS customers from DDoS attacks.  Unless AWS was aware of the test time and expected duration, its likely the traffic was blocked as suspicious.  AWS Firewall Manager is used to manage WAF ACLs and not dynamically blacklist IPs.  Similarly, VPC Flow Logs cannot automatically implement NACL changes as described here. Despite being a permitted service, traffic suspected of being malicious will still be blocked","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/penetration-testing/","title":"Submit a Penetration Testing Request"}],"answers":[{"id":"e3facacbe52b6423f9cf2e700d8e0b81","text":"The Security Company's traffic was seen as a threat and blocked dynamically by AWS.  AWS must grant permission before any penetration testing is done.","correct":true},{"id":"7d4826f179b8dc854c9cfb6e43678373","text":"The VPC Flow Logs record the spike in suspicious traffic and implement an update to the inbound NACL to block the remote IP address.","correct":false},{"id":"f3f963b71e307f8c28109631df115418","text":"The EC2 instance is using an ENI and the Security Company temporarily exceeded the throughput limit resulting in a throttling of their connection.","correct":false},{"id":"82a05cb45adab6d248655e827de16c6f","text":"AWS Firewall Manager is dynamically adding a blacklist entry for the Security Company's testing machine because it sees the traffic as a threat.","correct":false}]},{"id":"768271e9-9fd0-4921-a473-49ec465a0b34","domain":"awscsapro-domain4","question":"Your company is preparing for a large sales promotion coming up in a few weeks.  This promotion is going to increase the load on your web server landscape substantially.  In past promotions, you've run into scaling issues because the region and AZ of your web landscape is very heavily used.  Being unable to scale due to lack of resources is a very real possibility.  You need some way to absolutely guarantee that resources will be available for this one-time event.  Which of the following would be the most cost-effective in this scenario.","explanation":"If we only need a short-term resource availability guarantee, it does not make sense to contract for a whole year worth of Reserved Instance.  We can instead use On-Demand Capacity Reservations.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html","title":"On-Demand Capacity Reservations - Amazon Elastic Compute Cloud"}],"answers":[{"id":"0b48767c7357e407fa6f1e958fe0f051","text":"Purchase a Dedicated Host.","correct":false},{"id":"08d50c618757e489c5ac563be91f25f7","text":"Purchase a regional Reserved Instance.","correct":false},{"id":"f10307b8e644773b68b43dade41382f1","text":"Purchase Dedicated Instances.","correct":false},{"id":"0862f1bd2009c6ca8b52898831991642","text":"Use an On-Demand Capacity Reservation.","correct":true},{"id":"9dc64814f005254de6e1a269849ae0b1","text":"Purchase zonal Reserved Instance.","correct":false}]}]}}}}
