{"data":{"createNewExamAttempt":{"attempt":{"id":"430b4fe5-750e-41da-9776-02835a7a26c3"},"exam":{"id":"61899ee7-0c3d-438e-9a54-200f6b952adc","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"758841a6-0db9-4a02-a85b-699092182451","domain":"awscsapro-domain4","question":"You have just finished a contract with your client where you have helped them fully migrate to AWS.  As you are preparing to transition out of the account, they would like to integrate their current help desk software, Jira, with the AWS support platform to be able to create and track tickets in one place.  Which of the following do you recommend?","explanation":"You must have subscribe to at least the Business Support Plan to gain access to the AWS Support API. ","links":[{"url":"https://aws.amazon.com/premiumsupport/compare-plans/","title":"AWS Support - Compare all support plans"}],"answers":[{"id":"8b3759a4758ab061f15798717c42c8e0","text":"Subscribe to the Platinum Support Plan and direct them to the AWS Support API documentation","correct":false},{"id":"55d01f0a938121dd662f6d3068168eec","text":"Subscribe to the Developer Support Plan and direct them to the AWS Support API documentation","correct":false},{"id":"6c97334691882e8f846aad300e6a7811","text":"Subscribe to the Business Support Plan and direct them to the AWS Support API documentation","correct":true},{"id":"7e007e2c40ad802633ef945f34b23230","text":"It is currently not possible to integrate third-party products into the AWS Support system.  Offer to contract with them to perform manual updates between Jira and AWS Support cases.","correct":false},{"id":"0846705e92c2fcc07e439ddc2698dd15","text":"Use API Gateway to create a proxy service for the AWS Support API to allow third-party access.  Direct them to the AWS Support API documentation.","correct":false}]},{"id":"33803c8a-b588-4dca-8067-e500383254f3","domain":"awscsapro-domain4","question":"You work for a retail services company that has 8 S3 buckets in us-east-1 region. Some of the buckets have a lot of objects in them. There are Lambda functions and EC2-hosted custom application code where the names of these buckets are hardcoded. Your manager is worried about disaster recovery. As part of her business continuity plan, she has requested you to set up Cross-Region Replication of these S3 buckets to us-west-1, ensuring that the replicated objects are using a less expensive Storage Class because they would not be accessed unless disaster strikes. You are worried that in the event of failover due to the entire us-east-1 region being unavailable, the application code, once deployed in us-west-1, must continue to work while trying to access the S3 buckets in the new region. She has also requested you to start taking periodic snapshots of EBS Volumes and make these snapshots available in the us-west-1 region so that EC2 instances can be launched in us-west-1 using these snapshots if needed. How would you ensure that (a) the launching of EC2 instances works in us-west-1 and (b) your application code works with the us-west-1 S3 buckets?","explanation":"This question presents two problems - (1) how to ensure that EBS snapshots are created periodically and are also made available in a different region for launching required EC2 instances in case of failure of the primary region (2) how to deal with application code where S3 bucket names are hardcoded and whether this hardcoding will impact disaster recovery while trying to run in a different region. Both of these problems are real-life issues AWS customers face when designing and planning their disaster recovery solutions.\n(1)Remember that Data Lifecycle Manager can only schedule snapshot creation in the same Region. If we want to copy that snapshot into a different region, we must write our own scripts or Lambda functions for doing that. Hence, the choices that state that DLM can be used to directly create the snapshot into different regions are eliminated. Additionally, only root volume snapshots can be used to create an AMI. Non-root EBS Volume snapshots cannot be used to generate an AMI. Hence, the choices that specify using non-root volume snapshots are eliminated.\n(2)Remember that S3 bucket names are globally unique. Hence, one cannot create a second S3 bucket in the DR Region with the same name as the bucket in the primary region. Hence, the options that hint the creation of S3 buckets by the same name are eliminated. This results in a problem if S3 names are hardcoded in the application - that application will simply not run in a new region, it will fail. Hence, it is best to avoid hardcoding, and fetch the S3 bucket name from a key-value storage service like AWS Systems Manager Parameter Store at runtime. Creating this Parameter Store in each region and storing the correct bucket names in them can help in designing this non-hardcoded solution. Additionally, enabling Cross-Region Replication does not copy pre-existing content. Hence, the choices that suggest that pre-existing content will be automatically copied are eliminated.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-launch-snapshot.html","title":"Launching an Instance from a Backup"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/ec2/copy-snapshot.html","title":"Copy-snapshot documentation"}],"answers":[{"id":"824dc187ab89444593b50521f60b8ff3","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":true},{"id":"fc1a2efe00ef610249a55dadb0dd64fe","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false},{"id":"fae496411f2d461e0926abfdf8ad8b64","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":false},{"id":"02e8aed40d51e4bf9256f1ed436aa069","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false}]},{"id":"3f7fa126-1155-4aa3-802d-e9eeb75f5e5a","domain":"awscsapro-domain3","question":"You work for a Clothing Retailer and have just been informed the company is planning a huge promotional sale in the coming weeks.  You are very concerned about the performance of your eCommerce site because you have reached capacity in your data center.  Just normal day-to-day traffic pushes your web servers to their limit.  Even your on-prem load balancer is maxed out, mostly because that's where you terminate SSL and use sticky sessions.  You have evaluated various options including buying new hardware but there just isn't enough time.  Your company is a current AWS customer with a nice large Direct Connect pipe between your data center and AWS.  You already use Route 53 to manage your public domains.  You currently use VMware to run your on-prem web servers and sadly, the decision was made long ago to move the eCommerce site over to AWS last.  Your eCommerce site can scale easily by just adding VMs, but you just don't have the capacity.  Given this scenario, what is the best choice that would leverage as much of your current infrastructure as possible but also allow the landscape to scale in a cost-effective manner?","explanation":"A Target Group for an ALB can contain instances or IP addresses.  In this case, we can define the private IP addresses of our on-prem web servers along side the private IP addresses of any EC2 instances we spin up.  The caveat is that we can only use private IP addresses when defining a target group in this way.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancing-via-ip-address-to-aws-on-premises-resources/","title":"New – Application Load Balancing via IP Address to AWS & On-Premises  Resources | AWS News Blog"}],"answers":[{"id":"3e47f65e4524f53faba23e6995b592f5","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":true},{"id":"77592781918fa63474b5efbd5cc9555f","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an NLB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the NLB as an alias record.","correct":false},{"id":"4df24111c113846bfe0505ad0c84d9a3","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define two target groups:  one containing the public IP addresses of your on-prem load balancer and one including an auto scaling group of additional EC2 instances created from the imported AMI.  Assign both target groups to the ALB using the same listener port.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"6d3db4c52e96931f925f17fe8e9fd50f","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using public IP addresses of your on-prem web servers and additional EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false}]},{"id":"8765bd56-057b-488c-9a0a-f5bd413dd240","domain":"awscsapro-domain5","question":"Due to new corporate policies on data security, you are now required to use encryption at rest for all data.  You have some EC2 Linux instances on AWS that were created without encryption for the root EBS volume.  What can you do that meet the requirement and reduce administrative overhead?","explanation":"AWS does support encrypted root volumes but conversion from unencrypted root to an encrypted root requires a bit of a process. You must first create an AMI then copy that newly created AMI to the same region, specifying that you want to encrypt the EBS volumes during the copy.  You can then create a new instance with an encrypted root volume from the copied AMI.  You can use either a generated key from KMS or your own CMK imported into KMS.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html","title":"AMIs with Encrypted Snapshots - Amazon Elastic Compute Cloud"},{"url":"https://aws.amazon.com/blogs/aws/new-encrypted-ebs-boot-volumes/","title":"New – Encrypted EBS Boot Volumes | AWS News Blog"}],"answers":[{"id":"180e9aecdb74f204b1df00ffe6fa8b56","text":"Stop the instances and create AMIs from the instances.  Copy the AMIs to the same region and select the \"Encrypt target EBS snapshots\".  Redeploy the instances using the AMI copies you made with encrypted root volumes.","correct":true},{"id":"4430b7492a6c058c3574ce4e8ea43955","text":"Stop the instances and temporarily detach the EBS volumes.  Attach the root volumes to another EC2 instance and mount them a data volume.  Use a encryption tool like GPG or OpenPGP to recursively encrypt all the files on the mounted root volumes.  Detach and reattach the encrypted EBS volumes to the original instances and restart.  Import the encryption keys in KMS as a CMK.","correct":false},{"id":"1e30ccf0b75e9e70fd76c6e041510c75","text":"At present, EC2 does not support encrypted root volumes.  Create new encrypted EBS data volumes and attach the new volumes to the existing instances.  Use RSYNC to migrate all the non-OS data over to the encrypted data volumes.","correct":false},{"id":"9fbbb2e71386cb7f7a7ed77129a1a960","text":"Create an encrypted EFS instance and mount-points in the respective subnets.  Log into the instance and mount an encrypted EFS mount-point.  Copy all the root files over to the EFS mount point.  Edit the FSTAB file to mount the EFS mount point as the root volume instead of the root EBS device and reboot.","correct":false},{"id":"50c17b27f0bd0390ae321943f7db5c3d","text":"Create a certificate in CMS for the encryption key.  Stop the instances and temporarily detach the root volumes.  Via the AWS CLI, enable encryption on the root volumes using the \"ebs modify-volume\" argument with the flag of \"encryption=<CMS ARN>\" to specify the certificate.","correct":false}]},{"id":"3ca7c5e0-432a-4d40-afee-ab996819b429","domain":"awscsapro-domain3","question":"You are consulting with a client to guide them on migration of an in-house data center to AWS.  The client has stipulated in the contract that the migration cannot require any more than 1 hour downtime at a time and that there is always a fallback path.  Additionally, they want an overall increase in business continuity capabilities when the migration is done.  Their landscape is as follows:  (1) Several databases with about 1TB of data combined which are heavily used 24x7 and considered mission critical; (2) About 40TB of historic files which are read sometimes but almost never updated; (3) About 150 web servers on VMware in various states of customization of which there is a current project underway to standardize them.  The client's team has suggested some next steps but because they aren't yet familiar with AWS, they are not using equivalent AWS terms.  Translating their suggestions, which of the following activities would you choose to meet the requirements, reducing costs and management where possible?","explanation":"The database migration suggestion aligns well with DMS as it can keep the databases in sync until cutover.  SAN replication sounds a lot like Storage Gateway which is a reasonable way to migrate data to AWS.  However, simply using K8s does not convert your VMs into containers or make them serverless.  We can't restore tapes to AWS.  Creating the same VM landscape on AWS just adds an additional layer of complexity that's not needed.","links":[{"url":"https://aws.amazon.com/dms/faqs/","title":"AWS Database Migration Service FAQs - Amazon Web Services"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"}],"answers":[{"id":"3a02ebcd33fe18255e4ce43e8babb730","text":"Over several months, at end of business on Friday, backup all the servers and data to tape and restore to new instances in AWS to prove out AWS capabilities and reliability.","correct":false},{"id":"d2dde578790a34d9e740015474ea23e4","text":"Migrate the majority of the 150 web servers to a serverless concept by moving the VMs to a Kubernetes cluster.","correct":false},{"id":"31ea0eccdcea45ea4fce3b9459de52d4","text":"Use some block-level SAN replication tool to gradually migrate the on-prem historic files to AWS.","correct":true},{"id":"75d528d2ec243c60d1478ae605c89f40","text":"Build a matching VMware environment on AWS and use third-party tools to backup and restore the VMs there.","correct":false},{"id":"801ce55cfc2a125e7d17c729ca3e2e93","text":"Create new high powered stand-alone database instances in AWS and migrate data from on-prem database.  Use log shipping to keep the databases in sync.  Once we better understand AWS, we'll rebuild the servers and repartition the tables. ","correct":true}]},{"id":"482e75c9-071e-4a10-83f4-575f9c15b885","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They have just accidentally deleted the private key portion of their EC2 key pair.  Now, they are unable to SSH into their Amazon Linux servers.  Unfortunately the keys were not backed up and are considered gone for good.  What can this customer do to regain access to their instances?","explanation":"The two methods that AWS recommends if you lose a private key for an EC2 key pair are using Systems Manager Automation or using a secondary instance to edit the authorized_keys file.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2reset.html","title":"Reset Passwords and SSH Keys on Amazon EC2 Instances - AWS Systems Manager"}],"answers":[{"id":"bec3d01a56c851f61a1a09c852635db7","text":"Generate and upload a new key pair.  Stop the instances and select the new key pair from the dropdown on the Instance Settings sub-menu in the Console.","correct":false},{"id":"f17aa014620843abd81fa849982566b0","text":"Create a new key pair in KMS then assign the new public key to the required EC2 instance.","correct":false},{"id":"be45655cb1d64dff71a97aa729bc4e4a","text":"Open the TELNET port (port 23) on the Security Group for the server.  Use a TELNET client to attach to the instances using the root account and password.  Modify the authorized_key file with the new public key.","correct":false},{"id":"492c38d2fe2c3b96608bb8436592fe26","text":"Use the AWS CLI with the EC2 ModifyInstance action to enable SSH password-only access for the ec2-user account.  Attach using a password rather than an SSH key.  Modify the authorized_key file for the new public key.","correct":false},{"id":"509a77ae9827c5fcb60ccecc62fc9853","text":"Stop the instances, detach its root volume and attach it as a data volume to another instances.  Modify the authorized_keys file, move the volume back to the original instance and restart the instances.","correct":true},{"id":"79457a1b908d4a36cfeba625be909d40","text":"Use AWS Systems Manager Automation with the AWSSupport-ResetAccess document to create a new SSH key for your current instance.","correct":true}]},{"id":"c9f44641-660b-4c42-9380-9e7f6b0a9ba4","domain":"awscsapro-domain4","question":"As the solution architect, you are assisting your customer design and develop a mobile application using API Gateway, Lambda and DynamoDB. S3 buckets are being used to serve static content. The API created using API Gateway is protected by WAF. The development team has just staged all components to the QA environment. They are using a load testing tool to generate short bursts of a high number of concurrent requests sent to the API Gateway method. During the load testing, some requests are failing with a response of 504 Endpoint Request Timed-out Exception.\nWhat is one possible reason for this error response from API Gateway endpoint?","explanation":"The SA-P exam sometimes focuses on knowledge of response codes from API Gateway and what each distinct HTTP response code could mean.\nThe key to answering this question correctly is being able to distinguish between 4XX and 5XX HTTP error response codes. Though AWS has not been entirely consistent in their error code assignment philosophy, 4XX usually happens any time throttling kicks in because the request in that case never makes to an instance of Lambda function. 5XX happens when a Lambda function is actually instantiated, but some error (like time out) happened inside the Lambda function. One sneaky way to remember this is the fact that 5XX errors are called server errors in HTTP-land, so to generate a 5XX a server process must exist (and must have failed). Of course, in this context, the HTTP server process is a Lambda function - so in scenarios where throttling prevented a Lambda function from getting spawned, the response code cannot be 5XX. This is not consistently followed by AWS API Gateway error design, though, as we can see that AUTHORIZER_CONFIGURATION_ERROR and AUTHORIZER_FAILURE are both 500, though no Lambda function is actually spawned in either case. However, the candidate must remember that throttling always results in 4XX codes. An Endpoint Request Timed-out Exception (504) suggests that the requests in question actually made its way past the API Gateway into a Lambda function instance.\nFor the scenario where request rate exceeds API Gateway limits, the request would be blocked by API Gateway itself. The response would be 429. The exact knowledge of the code 429, however, is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe scenario where 1000 Lambda functions are already running is a similar example of throttling - the 1001st Lambda function will not even be spawned. The response, again, will be 429. However, the exact knowledge of the code 429 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe WAF scenario is yet another example of the request not even crossing the protections placed at the gateway level. If WAF is activated on API Gateway, it will block requests when the rate exceeds the HTTP flood rate-based rule (provided all such requests come from a single client IP address). However, the response, again, will be in the 4XX area (specifically, 403 Forbidden) - however, the exact knowledge of the code 403 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThis leaves Lambda time-out as the only correct answer. The mention of 30 seconds or more is a diversion tactic, in case candidate believes that the relevant Lambda time-out is 5 minutes. A given Lambda function instance may have a time-out limit of 5 minutes, but when it is invoked from API Gateway, the timeout imposed by API Gateway is 29 seconds. If a Lambda function runs for longer than 29 seconds, API Gateway will stop waiting for it and return 504 Endpoint Request Timed-out Exception.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html","title":"Amazon API Gateway Limits and Important Notes"},{"url":"https://aws.amazon.com/blogs/compute/amazon-api-gateway-adds-support-for-aws-waf/","title":"Amazon API Gateway adds support for AWS WAF"}],"answers":[{"id":"8090c6b0fe9036ec84fce24b16a7dc10","text":"The load testing tool has exceeded the soft limit for request rate allowed by API Gateway","correct":false},{"id":"98daaa701198f7e1c541fe8051799129","text":"The Lambda function is sometimes taking 30 seconds or more to finish executing","correct":true},{"id":"370770015087b4ff70656089fc9e3316","text":"The test is triggering too many Lambda functions concurrently. AWS imposes a soft limit of 1000 concurrent Lambda functions per region","correct":false},{"id":"82f6a6eb1a4aeef5dd34f21fcd2069ef","text":"The number of requests generated by the load testing framework has exceeded the threshold for the HTTP flood rate-based rule set in the WAF settings for the stage in question","correct":false}]},{"id":"49107f33-5b31-4d7e-a2cb-95f3ce8a2d75","domain":"awscsapro-domain1","question":"Your customer has setup AWS Organizations to help manage a collection of AWS Accounts.  They are running into a problem though and need your help.  They have created accounts for each business unit and applied SCPs to those OUs. However, they notice that root accounts in in those sub-accounts can still change root access keys and disable MFA.  How do you instruct your customer?","explanation":"Service Control Policies can control many aspects but they cannot restrict root account actions of changing root access keys or disabling MFA.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"3d722952f024bcec9174a311c17dcc14","text":"You can not use SCPs to restrict root account activities of changing the root password or managing MFA settings.","correct":true},{"id":"194cc2d07b5c378b62b1e090f0aea956","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the entire OU in the root account.","correct":false},{"id":"94a2f948d3d2f9c317a6ebb1f5a24ea5","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the specific sub-accounts.","correct":false},{"id":"77df34553819fdc2e31fb79762948993","text":"You can establish a trust with the top-level account and use the \"organizations:ServicePrinciple\" condition key to restrict root access at the sub-account level.","correct":false}]},{"id":"de88bc69-44a8-4a12-b28f-0a5e86db3939","domain":"awscsapro-domain1","question":"You have been entrusted to act as the interim AWS Administrator following the departure of the erstwhile Administrator in your company. You notice that there are several existing roles called role-engineer, role-manager, role-qa, role-dba, role-data-scientist, etc. When a new person joins the company, the new IAM user simply assumes the right role while using AWS - this allows central management of permissions and eliminates the need to manage permissions on a per-user basis.\nA new QA hire joins the company a few days later. You create an IAM User for her. You attach a Policy to the new IAM User that allows Action STS AssumeRole on any Resource. However, when this employee logs in the same day and tries to switch roles to role-qa, she is denied and is unable to assume the role-qa Role.\nWhat could be one reason why this is happening and how can it be best fixed?","explanation":"In order to allow an IAM User to successfully assume an IAM Role, two things must happen. First, the Policy attached to the User must allow the action STS AssumeRole. This is already true according to the question. Second, the Trust Policy of the Role itself must allow the User in question to assume the Role. This second condition can be met if we specify the arn of the User in the Principal element of the Trust Policy. In general, this question can be answered if the candidate is familiar with the concept of Principal in a Role, see link - A Principal within an Amazon IAM Role specifies the user (IAM user, federated user, or assumed-role user), AWS account, AWS service, or other principal entity that is allowed or denied to assume or impersonate that Role. Trust Policy is different than the Policy permissions - think of Policy Permissions as [what can be accessed] and Trust Policy as [who can access].\nTrust Policy cannot belong to an IAM User, hence the choice that claims the problem to be an unmodified User Trust Policy is incorrect. IAM changes are instantly effective, so the choice that points at the need of a time delay is also incorrect. Among the other two choices, the knowledge needed to pick the right one is an awareness of the Principal element.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html","title":"AWS JSON Policy Elements - Principal"},{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html","title":"IAM Roles"}],"answers":[{"id":"dbb48c05e238c18bdb9c17ee265e387b","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Condition element of the Trust Policy of the Role","correct":false},{"id":"e6dacaf19a289e1f73855c5e904b21fb","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Principal element of the Trust Policy of the Role","correct":true},{"id":"b3bf261fca3a734ad312d3ac0e5d0589","text":"You have not modified the Trust Policy of the IAM User to trust the Role role-qa. To fix this, add a Condition to the IAM Policy attached to the new user that filters on the role and specify the arn of role-qa","correct":false},{"id":"765f20e64b35dbc08c2bc319bcbe7e1a","text":"Sufficient time has not passed since you made the changes. It takes up to 12 hours to propagate IAM role changes. To fix this, ask her to try again the next day.","correct":false}]},{"id":"945797fb-5147-44fc-a50c-aaa636c3705b","domain":"awscsapro-domain3","question":"Your organisation currently runs an on-premise Windows file server.  Your manager has requested that you utilise the existing Direct Connect connection into AWS, to provide a method of storing and accessing these files securely in the Cloud.  The method should be simple to configure, appear as a standard file share on the existing servers, use native Windows technology and also have an SLA.  Choose an option which meets these needs.","explanation":"To choose the correct option, we can start by eliminating services which don't have an SLA, in this case only Storage Gateway doesn't have an SLA so we can remove that as an option.  Next we can rule out EFS and S3 as they don't use native Windows technology or provide a standard Windows file share, therefore the only correct answer is to use Amazon FSx for Windows File Server.","links":[{"url":"https://aws.amazon.com/fsx/windows/faqs/","title":"Amazon FSx for Windows File Server FAQs"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs"},{"url":"https://aws.amazon.com/efs/faq/","title":"Amazon EFS FAQs"}],"answers":[{"id":"e953bc59adceffe1274c4bc66d83b365","text":"Create an AWS Storage Gateway for Files server and map the generated SMB share to the Windows file server, then synchronise the files","correct":false},{"id":"437caccc8e39a279a232207ec3ca741a","text":"Map an SMB share to the Windows file server using Amazon FSx for Windows File Server and use RoboCopy to copy the files across","correct":true},{"id":"5a76242c7735a5846218b183930c3a41","text":"Map an Amazon Elastic File System (EFS) share to the Windows file server and use RoboCopy to copy files across","correct":false},{"id":"af082c9581ff0bb5e1c9cc6daf7d72e0","text":"Write a Powershell script which uses the CLI to synchronise the files into an S3 bucket","correct":false}]},{"id":"1080e29c-045b-4714-8f41-6be669f865a5","domain":"awscsapro-domain4","question":"Your company stores financial transactions in a write-heavy MySQL 5.6 RDS database instance. The innovation team is developing a simulation algorithm. They have made a point-in-time copy of the production database for running tests. Each such test runs for a few minutes and changes the data. They need to run thousands of such tests and compare the results, where each test is run on the same test data. Therefore, after each test is completed, they want the test database back in its original state with the initial data, so that they can run the next test quickly. Currently, they use a DB Snapshot that contains the initial state. They restore the database after every test run. The restoration process takes a couple of hours, slowing them down. They want to automate the process of running tests and ideally run these tests every few minutes to be more productive. Suggest a suitable mechanism for quickly achieving this with least effort and lowest cost.","explanation":"This question tests the knowledge of Aurora Backtracking feature, which is the best answer. This question also tests the knowledge of EBS and RDS MySQL. An EBS restore will not necessarily be any quicker than a DB Snapshot restore. Also, spinning up thousands of instances of RDS will firstly be met with a soft-limit increasing resistance by AWS (the current per account per region limit is 40), not to mention the drastic cost implications of so many instances running. Also, MySQLDump is not necessarily a whole lot quicker, not like Aurora backtracking anyway.\nIt is important to remember the fundamental difference between backtracking and point-in-time restore from DB Snapshots. Backtracking rewinds the same database. Point-in-time restoration from snapshots creates a new database. Hence, the latter is more time-consuming. Backtracking is much faster, but it is only available in Aurora, that too, only for certain database engines and versions as of 2019. Remember that if you turn on backtracking, you are paying for the extra storage, as Aurora must keep change records in order to be able to backtrack. However, that extra cost for one instance is not much compared to running hundreds or thousands of RDS instances. Hence, the Aurora solution is also the most cost-effective.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","title":"Backtracking an Aurora DB Cluster"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Import.html","title":"Migrating an RDS MySQL Snapshot to Aurora"}],"answers":[{"id":"8ff1b66ec30e68f3b0af77ca834fa8b4","text":"Restore the database in its initial state on an EC2 instance with Provisioned IOPS EBS disks. Take an EBS Snapshot. After each test, restore an EBS Volume from the EBS Snapshot. EBS Snapshots are much quicker to restore. Run the test on the EC2 Instance with the restored EBS Volume","correct":false},{"id":"9d29133ab1fe457147ed07251f102d5b","text":"Use MySQLDump to make a dump of the database in its initial state. Instead of restoring the database from its RDS DB Snapshot each time, restore it from the dump. This will make the process much quicker","correct":false},{"id":"b29796f55566af78fb926244627009f8","text":"Migrate the initial RDS MySQL Snapshot to an AWS Aurora DB Cluster. Turn on backtracking while creating the test Aurora MySQL DB Cluster. After each test is completed, backtrack to the point in time before the test.","correct":true},{"id":"093d4a2bd5748fd50abe9ea0a1adc3b7","text":"Spin up as many RDS MySQL Instances from the initial snapshot as required to run all the tests in parallel. That way, the innovation team can save time and not have to wait for one test to complete to run the next","correct":false}]},{"id":"0a4b2449-9275-4c2f-af02-0f8c51614f3a","domain":"awscsapro-domain2","question":"You are part of a business continuity team at a consumer products manufacturer.  In scope for the current project is the company web server which serves up static content like product manuals and specification sheets which customers can download.  This landscape consists only of a single NGINX web server and 5TB of local attached storage for the static content.  In the case of a failover, RTO has been defined as 15 minutes with RPO as 24 hours as the content is only updated a few times a year.  Staff reductions and budget constraints for the year mean that you need to carefully evaluate and choose the most cost-effective and most automated solution in the case of a failover.  Which of the following would be the most appropriate given the situation?","explanation":"In this case, the most cost-effective and most automated way to ensure the reliability statistics would be to migrate the static content to S3.  This option has built-in robustness and will cost less than any other option presented.","links":[{"url":"http://d36cz9buwru1tt.cloudfront.net/AWS_Disaster_Recovery.pdf","title":"Using Amazon Web Services for Disaster Recovery"}],"answers":[{"id":"925fb4f48c2c90011e7e1f92d3412dcd","text":"Migrate the static content to an EFS share.  Mount the EFS share via NFS from on-prem to serve up the web content.  Configure another EC2 instances with NGINX to also mount the same share.  Upon fail-over, manually redirect the Route 53 record for the web server to the IP address of the EC2 instance.","correct":false},{"id":"46b84866da301243767946743c6024a1","text":"Download and configure the AWS Storage Gateway, creating a volume which can be replicated to AWS S3.  Attach that volume to the web server via iSCSI and migrate the content to that Storage Gateway volume.  Locate an AMI from the AWS Marketplace for NGINX.  If a failover is required, manually launch the AMI and run an RSYNC between the on-prem server and the EC2 server to migrate the content.","correct":false},{"id":"69fd92fc5be4948bfc0128d02ed2f392","text":"Install the CloudWatch agent on the web server and configure an alarm based on a health check.  Create an EC2 replica installation of the web server and stop the instances.  Create a Lambda function that is triggered by the health check alarm which starts the dormant EC2 instance and updates a DNS entry in Route 53 pointing to the new server.","correct":false},{"id":"bceaccaea071754d3724eaf31f0f6189","text":"Migrate the website content to an S3 bucket configured for static web hosting.  Create a Route 53 alias record for the web server domain.  End-of-life the on-prem web server.","correct":true},{"id":"2d14eb477a2f2c9dc3605ea5740297cd","text":"Create a small pilot-light EC2 instance and configure with NGINX. Configure a CRON job to run every 24 hours that syncs the data from the on-prem web server to the pilot-light EC2 EBS volumes.  Configure an Application Load Balancer to direct traffic to the on-prem web server until a health check fails.  Then, the ALB will redirect traffic to the pilot light EC2 instances. ","correct":false}]},{"id":"431e43bc-ccbc-480f-9915-210bc7773d2b","domain":"awscsapro-domain5","question":"You are in the process of migrating a large quantity of small log files to S3 for long-term storage.  To accelerate the process and just because you can, you have created quite sophisticated multi-threaded distributed process deployed across 100 VMs which can load hundreds of thousands of files at one time.  For some reason, the process seems to be throttled somewhere along the chain.  You try many things to try to uncover the source of the throttling but nothing works.  Reluctantly, you decide to turn off the KMS encryption setting for your S3 bucket and the throttling goes away.  You turn AMS-KMS back on and the throttling is back. Given the troubleshooting steps, what is the most likely cause of the throttling and how can you correct it?","explanation":"Through a process of elimination, it seems you have identified the variable that is causing the throttling.  KMS, like other AWS services, does have rate limiters which can be increased via Support Case.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/limits.html","title":"Limits - AWS Key Management Service"}],"answers":[{"id":"1dd4f25e52404e18ddec0b8711a82a13","text":"You are hitting the KMS encrypt request account limit.  You must request a limit increase via a Support Case.","correct":true},{"id":"fe629daf7473efc279d7d8ee6f5a5806","text":"You are maxing out your SYNC requests to S3.  You need to request a limit increase via a Support Case.","correct":false},{"id":"01148eae3319190a0b228c6d02c9572c","text":"You are maxing out your network connection.  You must split the traffic over multiple interfaces.","correct":false},{"id":"60ff77ab365c15bb11771e94e3dc271d","text":"You have exceeded the number of API calls for your account.  You must create a new account.","correct":false},{"id":"05aeb0bc36d7b53aa30bf9e22b6cd120","text":"You are maxing out your PUT requests to S3.  You need to change over to multi-part upload as a workaround.","correct":false}]},{"id":"071d48ba-80e7-420e-969e-98cb2bcfbaa3","domain":"awscsapro-domain2","question":"Across your industry, there has been a rise in activist hackers launching attacks on companies like yours.  You want to be prepared in case some group turns its attention toward you.  The most common attack, based on forensic work security researchers have done after other attacks, seems to be the TCP Syn Flood attack.  To better protect yourself from that style of attack, what is the least cost measure you can take?","explanation":"AWS Shield Standard is offered to all AWS customers automatically at no charge and will protect against TCP Syn Flood attacks without you having to do anything - this meets the requirements of protecting TCP Syn Flood attacks at the lowest cost possible, as described in the question. A more robust solution which is better aligned to best practice would involve a load balancer in the data path, however as this would provide more functionality than required at a higher cost, is not the correct option for this question.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"6f87c7b3eda4188070a6635a49710939","text":"Implement AWS WAF and configure filters to block cross-site scripting match conditions.","correct":false},{"id":"82f7ef9f4b3e8e05aef157162915fecf","text":"Subscribe to a Business or Enterprise Support Plan.  Engage AWS DDoS Response Team and arrange for a custom mitigation.","correct":false},{"id":"c50dd258b1fac18bd9368b07bf0fbc11","text":"Implement AWS Shield Advanced and configure it to generate CloudWatch alarms when malicious activity is detected.","correct":false},{"id":"7d8280921ead2af66cf214b774f94306","text":"Re-architect your landscape to use an application load balancer in front of any public facing services.","correct":false},{"id":"aec18df10393205d72a60b52cf05bab9","text":"This type of attack is automatically addressed by AWS.  You do not need to take additional action.","correct":true}]},{"id":"f19a95ac-c0b9-4d00-a84a-67f71b7e2a76","domain":"awscsapro-domain2","question":"You are advising a client on some recommendations to increase performance of their web farm.  You notice that traffic seems to usually spike on the days after public holidays and unfortunately the responsiveness of the web server as collected by a third-party analytics company reflects a customer experience that is slower than targets.  Of these choices, which is the best way to improve performance with minimal cost?","explanation":"Of these options, only one meets the question requirements of performance at minimal cost.  Simply scheduling a scale event during a known period of traffic is a perfectly valid way to address the requirement and does not incur unnecessary cost. CloudTrail records API access and is not suitable for network alarms.  Route 53 would not be able to \"consolidate\" dynamic and static web resources.","links":[{"url":"https://docs.aws.amazon.com/auto scaling/ec2/userguide/schedule_time.html","title":"#N/A"}],"answers":[{"id":"247053f8b211aeace0894f849838ef6f","text":"Configure a scheduled scaling policy to increase server capacity on days after public holidays.  ","correct":true},{"id":"8a2f77fe64a24871e80eae971ce2c877","text":"Create replicas of the existing web farm in multiple regions.  Migrate static assets to S3 and use cross-region replication to synchronize across regions.  Create CloudFront distributions in each region.  Use Route 53 to direct traffic to the closest CloudFront alias based on a geolocation routing policy.","correct":false},{"id":"24c302b442af96b7c1eedb04e2c4069b","text":"Configure a dynamic scaling policy based on network traffic or CPU utilization.  Migrate static assets from EBS volumes to S3.  Configure two Cloudfront distributions--one for static content and one for dynamic content.  Use Route 53 to consolidate both Cloudfront distributions under one alias.","correct":false},{"id":"3c6b1f2e20a3204df3886f680991b76d","text":"Use CloudTrail and SNS to trigger a Lambda function to scale the web farm when network traffic spikes over a configured threshold.  Create an additional Internet Gateway and split the traffic equally between the two gateways using an additional route table.  ","correct":false}]},{"id":"7eebbdef-e751-4d76-be2a-1e3a746b87f6","domain":"awscsapro-domain5","question":"You are a database administrator for a company in the process of changing over from RDS MySQL to Amazon Aurora for MySQL.  You setup the new Aurora database in a similar fashion to how your pre-existing RDS MySQL landscape was setup:  Multi-AZ with Read Replica in a backup region.  You have just completed the migration of data and verified that the new Aurora landscape is performing like it should.  You are now in the process of decommissioning the old RDS MySQL landscape.  First, you decide to disable automatic backups.  Via the console, you try to set the Retention Period to 0 but receive an error saying \"Cannot Set Backup Retention Period to 0\".  How can you disable automatic backups?","explanation":" For RDS, Read Replicas require backups for managing read replica logs and thus you cannot set the retention period to 0.  You must first remove the read replicas and then you can disable backups.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Troubleshooting.html#CHAP_Troubleshooting.Backup.Retention","title":"Troubleshooting - Amazon Relational Database Service"}],"answers":[{"id":"898042b8eeafc1bbb4dd146c98dbb919","text":"Remove the Read Replicas first.","correct":true},{"id":"7086f25af0d0410de9d8826003760752","text":"You cannot disable automatic backups on RDS instances.  This feature is built into the platform as a failsafe.","correct":false},{"id":"c61323c176e6d23809f0d467770ed205","text":"Automatic backups are enabled and disabled at the database engine level.  You need to login using a MySQL client to turn off automatic backups.","correct":false},{"id":"fb2c22782c91868676981ff65332f1d5","text":"You cannot disable backups via the console.  You must do this via the AWS CLI or SDK.","correct":false},{"id":"5698f7c6bb0530fc1c0ad18e6911f528","text":"You must first reprovision the database as a single AZ instances.  Multi-AZ replication requires backups to be enabled.","correct":false}]},{"id":"6a0e9756-1b9e-495c-965b-a8c715843d4f","domain":"awscsapro-domain1","question":"A client has asked you to help troubleshoot a Service Control Policy.  Upon reviewing the policy, you notice that they have used multiple \"Statement\" elements for each Effect/Action/Resource object but the policy is not working. What would you suggest next?  ","explanation":"The syntax for an SCP requires only one Statement element.  You can have multiple objects within a single Statement element though. ","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/org_troubleshoot_policies.html","title":"Troubleshooting AWS Organizations Policies - AWS Organizations"}],"answers":[{"id":"c88df8df089061b0ce17c8aba3a63305","text":"Have them apply the same policy on another OU to eliminate any localized conflicts.","correct":false},{"id":"6d5becfe4962c177196b4dce486c4e58","text":"Look elsewhere as multiple Statement elements are used when multiple conditions are specified in SCPs.","correct":false},{"id":"4b06a8d83f7ea11c2700b91cae578fbb","text":"Split the SCP out into multiple policies and apply in a cascading manner to higher level OUs.","correct":false},{"id":"3410d9f58eea1a807fdc3d0f6194c3ce","text":"Change the policy to combine the multiple Statement elements into one element with an object array.","correct":true}]},{"id":"a5d03cc5-f156-4400-bb92-99f41b8da075","domain":"awscsapro-domain2","question":"You have configured a VPC Gateway Endpoint to S3 from your VPC named VPC1 with a CIDR block of 10.0.0.0/16.  You have lots of buckets but following least privilege, you want to only allow the instances in VPC1 access to the only two buckets they need.  What is the most efficient way of doing this?","explanation":"You cannot use an IAM policy or bucket policy to allow access from a VPC IPv4 CIDR range (the private IPv4 address range). VPC CIDR blocks can be overlapping or identical, which may lead to unexpected results. Therefore, you cannot use the aws:SourceIp condition in your IAM policies for requests to Amazon S3 through a VPC Gateway Endpoint.  The most efficient way is to use an endpoint policy to explicitly allow access to the two buckets which has the effect of denying access to the other buckets.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html#vpc-endpoints-policies-s3","title":"Endpoints for Amazon S3 - Amazon Virtual Private Cloud"}],"answers":[{"id":"656a72bc234934fce6cec26dd222fe22","text":"Create a bucket policy for the two required buckets using stringMatch to deny access for any system not coming from VPC1.","correct":false},{"id":"f972dfc0e001a9013cc0ca818705f14a","text":"Create an endpoint policy that uses a Condition key of sourceIP to only allow instances within the 10.0.0.0/16 block.","correct":false},{"id":"0dbe1d58efa8e52b477a87d218c8dee9","text":"Create a bucket policy for the two required buckets using aws:SourceIp to allow access for any system within the VPCs private CIDR block.","correct":false},{"id":"d614ddb85f137404469aec6d7b8e2422","text":"Create a endpoint policy that explicitly allows access to the two required buckets.","correct":true},{"id":"adcebcb93efce3a968f80ff194367109","text":"Create an endpoint policy that uses a Condition key of aws:SourceVpc equals VPC1 to allow access to the required buckets.","correct":false}]},{"id":"4eb19466-6d1a-4ccd-987c-3f8f0cc71479","domain":"awscsapro-domain1","question":"A client wants help setting up a way to manage access to the AWS Console and various services on AWS for their employees.  They are starting out small but expect to provide AWS-hosted services to their 20,000 employees within the year.  They currently have Active Directory on-premises, use VMware to host their VMs.  They want something that will allow for minimal administrative overhead and something that could scale out to work for their 20,000 employees when they have more services on AWS.  Due to audit requirements, they need to ensure that the solution can centrally log sign-in activity.  Which option is best for them?","explanation":"For userbases more than 5,000 and if they want to establish a trust relationship with on-prem directories, AWS recommends using AWS Directory Service for Microsoft Active Directory.  This is also compatible with AWS Single Sign-On which provides a simple way to provide SSO for your users across AWS Organizations.  Additionally, you can monitor and audit sign-in activity centrally using CloudTrail. ","links":[{"url":"https://aws.amazon.com/single-sign-on/faqs/","title":"AWS Single Sign-On FAQs – Amazon Web Services (AWS)"},{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ad_connector_best_practices.html","title":"Best Practices for AD Connector - AWS Directory Service"}],"answers":[{"id":"b1b70c7c05b814cf9685b3a64175cd80","text":"Connect the multiple accounts with AWS Organizations.  Deploy AWS Directory Service for Microsoft Active Directory on AWS and configure a trust with your on-premises AD.  Configure AWS Single Sign-On with the users and groups who are permitted to log into AWS.  Give the users the URL to the AWS SSO sign-in web page.","correct":true},{"id":"32479d3559d6ead0328c6419249c7859","text":"Connect the multiple accounts together using AWS Organizations.  Deploy AD Connector on AWS and configure their on-prem AD.  Create corresponding roles and groups in IAM and map those to their local AD groups.  Use STS to allow users to authenticate into AWS.","correct":false},{"id":"1eb0420847890f99ddd38092ac88c5f0","text":"Create a OAuth Identity Provider in IAM and create roles and policies with the appropriate level of permissions.  In AD, create groups which correspond to the roles you have created in IAM and populate the AD groups with the desired users.  Download and install the OAuth Identity Connector for AD.  Configure the connector for the OAuth Identity Provider on AWS.  ","correct":false},{"id":"a3c1cc18aa02ff66c06476aa7b8e78ed","text":"Download and install the AWS ActiveDirectory Sync appliance and install it in vCenter.  Configure the Sync appliance to connect to the local AD and replicate to an instance of Simple AD on AWS.  In IAM, create corresponding roles and policies for the permissions you want to allow on AWS.  Assign these roles to the synchronized Simple AD users in IAM.","correct":false},{"id":"878c950f9d14c2d81185f1950edda98c","text":"Configure Cognito with web federation against the on-prem Active Directory.  In IAM, create corresponding users corresponding to the Cognito accounts you want to allow on AWS.  Assign these roles to the user pools within Cognito.  Distribute the Cognito SSO client to your users.","correct":false}]},{"id":"6d93e859-e1a9-468f-9a05-61a2dbc2be9c","domain":"awscsapro-domain5","question":"You manage a group of EC2 instances that host a critical business application.  You are concerned about the stability of the underlying hardware and want to reduce the risk of a single hardware failure impacting multiple nodes.  Regarding Placement Groups, which of the following would be the best course of action in this case?","explanation":"Spread Placement Groups ensure your instances are each placed on separate underlying hardware so this reduces the risk of a single hardware failure taking down multiple instances.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-spread","title":"Placement Groups - Amazon Elastic Compute Cloud"}],"answers":[{"id":"b6153af57a0b8805fc28d93a2859fb9e","text":"You cannot move existing instances into a new placement group.  You would create AMIs from the existing instances and redeploy them into a clustered placement group.","correct":false},{"id":"e346d1667489b5726c0367eff4ea4a34","text":"You would use the AWS CLI to move the existing instances into a spread placement group.","correct":true},{"id":"c8b809a2fef3f21146774b82e8f03f12","text":"You would move the instances onto a Dedicated Host.","correct":false},{"id":"e6d129a06320300aaf95634a6691b7cb","text":"You would the AWS Console to move the existing instances into a clustered placement group.","correct":false},{"id":"3bde2d7fff7ead39c4035ab0600275f1","text":"You would use the AWS CLI to move the existing instances into a diversified placement group.","correct":false}]},{"id":"768271e9-9fd0-4921-a473-49ec465a0b34","domain":"awscsapro-domain4","question":"Your company is preparing for a large sales promotion coming up in a few weeks.  This promotion is going to increase the load on your web server landscape substantially.  In past promotions, you've run into scaling issues because the region and AZ of your web landscape is very heavily used.  Being unable to scale due to lack of resources is a very real possibility.  You need some way to absolutely guarantee that resources will be available for this one-time event.  Which of the following would be the most cost-effective in this scenario.","explanation":"If we only need a short-term resource availability guarantee, it does not make sense to contract for a whole year worth of Reserved Instance.  We can instead use On-Demand Capacity Reservations.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html","title":"On-Demand Capacity Reservations - Amazon Elastic Compute Cloud"}],"answers":[{"id":"f10307b8e644773b68b43dade41382f1","text":"Purchase Dedicated Instances.","correct":false},{"id":"0b48767c7357e407fa6f1e958fe0f051","text":"Purchase a Dedicated Host.","correct":false},{"id":"08d50c618757e489c5ac563be91f25f7","text":"Purchase a regional Reserved Instance.","correct":false},{"id":"9dc64814f005254de6e1a269849ae0b1","text":"Purchase zonal Reserved Instance.","correct":false},{"id":"0862f1bd2009c6ca8b52898831991642","text":"Use an On-Demand Capacity Reservation.","correct":true}]},{"id":"7c5f884f-c0f9-4028-a725-50819d704324","domain":"awscsapro-domain5","question":"You deploy an application load balancer and an Auto Scaling group (ASG) in production for a new project. When instances in the ASG have a high CPU utilization, a new instance is launched. However, the new instance fails the health check from the ASG and has been terminated after some time. You check the logs in the instance and find that the startup script does not finish yet before the instance is terminated. How would you resolve the problem?","explanation":"Amazon EC2 Auto Scaling waits until the health check grace period ends before checking the health status of the instance. The grace period timer should be increased to give the instance more time to finish the startup script. Increasing the healthy threshold makes the instance more difficult to become healthy. Decreasing the timeout value also does not help as the instance may become unhealthy very quickly. Modifying the health check type from ELB to EC2 is unsuitable as the ASG cannot get the instance status from the application level. Even if the instance shows as healthy in ASG, the application may not be ready yet.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Check Grace Period"}],"answers":[{"id":"e7ad9ea949ae87c6a6001a94f5a9bf48","text":"Increase the health check grace period in the Auto Scaling group configurations. When a new instance boots up, it is given more time to execute the startup scripts and run applications before the health check from ASG.","correct":true},{"id":"1d8964ced10da94d49d1a1efe02bbbca","text":"Modify the health check type from ELB to EC2 in the Auto Scaling group. Configure ASG to check the EC2 instance status. As long as the instance does not have a system level issue, it will not fail the health check in the ASG even when the startup script is still running.","correct":false},{"id":"e322fd6a55a044826665dfce4ae7020b","text":"Decrease the timeout value in the ELB health check from 5 seconds to 1 second so that when the ELB performs the health check on the backend instances, the instances are able to respond in time before a timeout occurs.","correct":false},{"id":"b5ca84726bf6fcfd6522ab188c8224ea","text":"Increase the default healthy threshold in the health check of elastic load balancer from 5 to 10 so that the instance will become healthy more quickly once the startup script finishes in the new instance.","correct":false}]},{"id":"c3ac5de9-a343-4cde-af1b-6c9f89824d2f","domain":"awscsapro-domain5","question":"An external auditor is reviewing your process documentation for a Payment Card Industry (PCI) audit.  The scope of this audit will extend to your immediate vendors where you store, transmit or process cardholder data.  Because you do store cardholder data in the AWS Cloud, the auditor would like to review AWS's PCI DSS Attestation of Compliance and Responsibility.  How would you go about getting this document? ","explanation":"AWS Artifact provides on-demand downloads of AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use. You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls.","links":[{"url":"https://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html?icmpid=docs_artifact_console","title":"What Is AWS Artifact? - AWS Artifact"}],"answers":[{"id":"d9208942349d1c6f7dbaba3661069bc1","text":"AWS WorkDocs","correct":false},{"id":"1d16d307ee989a80e421198a01993a9c","text":"AWS IAM Console","correct":false},{"id":"d7cb47dd1f374d3ed079b14cc6f2cd75","text":"Submit a Support Case requesting the document","correct":false},{"id":"09e838e873f25f954fef911d50b3d1ab","text":"AWS Pinpoint","correct":false},{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":true},{"id":"63df0d05cd43af35c95cf04d92aaf685","text":"AWS Legal Services website","correct":false},{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false}]},{"id":"b401741c-5b37-4b47-8e61-7802fbc9d7d6","domain":"awscsapro-domain1","question":"You are helping a client consolidate several separate accounts into a single account.  This consolidation will result in approximately 50 new VPCs in their one account.  They want to continue to use Route 53 for DNS but only want it accessible privately. How can you accomplish this most efficiently?","explanation":"Private Hosted Zones provide DNS services to VPCs but cannot be access from the internet.  They can be associated with VPCs either by the console, CLI or programmatically via SDK.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html","title":"Associating More VPCs with a Private Hosted Zone - Amazon Route 53"}],"answers":[{"id":"315372936e7ffba65896da15d0f45c2d","text":"Create a Private Hosted Zone within Route 53.  As the new VPCs are created, associate them with the Private Hosted Zone.","correct":true},{"id":"f74daa2300ac594c111ca9fce198f19c","text":"Create a central DNS server using EC2 and BIND.  Configure Route 53 to reference this DNS server as a resolver.  Update DNS records at the registrar to point to the central DNS.","correct":false},{"id":"82d5ef7e7176200aa4350ef90dd4c354","text":"Create a Public Hosted Zone within Route 53 and associate it to each VPC.  Configure a NACL on each VPC to deny inbound DNS queries (UDP port 53).","correct":false},{"id":"cbffb64b6b43e6fe45496b6e77ce17b8","text":"Create a Private Hosted Zone within Route 53 for each respective VPC.  Configure replication between the private hosted zones to keep records in sync.","correct":false},{"id":"3cc28b12f45b3dee8f7f16a0f93d00ce","text":"Install BIND on an EC2 instance in a single VPC.  Create VPC peering connections between the DNS VPC and any new VPCs.  Configure a DHCP Option Set to assign a DNS and link that to each VPC.","correct":false}]},{"id":"b00cb57f-7191-4f17-aa6d-ac687c418332","domain":"awscsapro-domain5","question":"You have a running EC2 instance and the name of its SSH key pair is \"adminKey\". The SSH private key file was accidentally put into a GitHub public repository by a junior developer and may get leaked. After you find this security issue, you immediately remove the file from the repository and also delete the SSH key pair in AWS EC2 Management Console. Which actions do you still need to do to prevent the running EC2 instance from unexpected SSH access?","explanation":"Although the SSH key pair is deleted in EC2, the public key content is still placed on the instance in an entry within ~/.ssh/authorized_keys. Someone can SSH to the instance if he has a copy of the leaked SSH private key. Users should not configure the instance to support another key pair as the old key pair still works. The correct method is deleting the instance immediately to prevent it from being compromised and launching another instance with a new SSH key pair. There is no need to use the AWS CLI command delete-key-pair as the key is already deleted from AWS EC2.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html","title":"Amazon EC2 Key Pairs"}],"answers":[{"id":"ee7464becbe88068a5f848419e621bba","text":"Stop and terminate the instance immediately as someone can still SSH to the instance using the key. Launch a new instance with another SSH key pair. SSH to the EC2 instance using the new key.","correct":true},{"id":"bc01758fc758191425928382b18697ca","text":"Create another SSH key pair via AWS EC2 or a third party tool such as ssh-keygen. Stop the instance and configure the instance with this new key pair in AWS Management Console. Restart the instance to activate the key pair.","correct":false},{"id":"4f9620d4caefa25fafabc98f00c6b192","text":"Use AWS CLI delete-key-pair to completely delete the key pair so that no one can use it to SSH to the instance. Configure CloudWatch logs to monitor the SSH logging events and filter the logs with the SSH key ID to see if the key pair is still used by someone.","correct":false},{"id":"a928b9732684f81d9ce046842965f1f6","text":"No action is required as the SSH key pair \"adminKey\" is already deleted from AWS EC2. Even if someone has the SSH private key, he still cannot use the key to access the instance.","correct":false}]},{"id":"338ce579-a236-4282-9670-8da3b0baf2e9","domain":"awscsapro-domain3","question":"You are helping a Retail client migrate some of their assets over to AWS.  Presently, they are in the process of moving their Enterprise Data Warehouse.  They are planning to re-host their very large Oracle data warehouse on EC2 in a high availability configuration across AZs.  They presently have several Scala scripts that process some detailed Point of Sale data that is collected each day.  The scripts perform some aggregation on the data and import the aggregate into their Oracle database.  They want to move this process to AWS as well.  Which option would be the most cost-effective way for them to do this?","explanation":"AWS Glue is a fully managed extract, translate and loading service and is compatible with Scala.  EMR could do this but represents more overhead than necessary.  Lambda is not compatible with Scala and migrating to Redshift does not bring anything in this case if the customer wants to retain their Oracle database.","links":[{"url":"https://aws.amazon.com/glue/faqs/","title":"AWS Glue Features - Amazon Web Services"}],"answers":[{"id":"4f1ff8b853c3ba363bdd2bda53538ab4","text":"Migrate from Oracle to Redshift and use Kinesis Firehose.","correct":false},{"id":"8b01d948d5ad2f4b1c8e817c2d98e7c2","text":"Migrate the processing to AWS Glue.","correct":true},{"id":"a445a1a877009cd7c31858687a818116","text":"Import your Scala scripts into AWS SCT for processing.","correct":false},{"id":"04578ae8419780f9dc441d01fe11582d","text":"Create Lambda functions using your Scala scripts.","correct":false},{"id":"1a4de6676c8c078310e08aad71d9dce6","text":"Migrate the processing to AWS EMR.","correct":false}]},{"id":"599dee9a-6ae7-4c85-a7c6-49edc6ae7d6b","domain":"awscsapro-domain5","question":"A development team is comprised of 20 different developers working remotely around the globe all in different timezones.  They are currently practicing Continuous Delivery and desperately want to mature to true Continuous Deployment.  Given a very large codebase and distributed nature of the team, enforcing consistent coding standards has become the top priority.  Which of the following would be the most effective to address this problem and get them closer to Continuous Deployment?","explanation":"Including an automated style check prior to the build can move them closer to a fully automated Continuous Deployment process.  A style check only before UI testing is too far in the SDLC.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"628453003287afe2200912bb38d0456b","text":"After integrating and load testing, run a code compliance check against the binary created during the build.","correct":false},{"id":"ec61b60c7eeb3bf9ca9c4149c09c5f3d","text":"Issue a department directive that standards must be followed and require the developers to sign the document.","correct":false},{"id":"1f40591b9d9dbe7a2371e5e82ec05997","text":"Introduce a peer review step into their deployment pipeline during the daily stand-up, requiring sign off for each commit.","correct":false},{"id":"f4cd7f15eb32d8ddd77234b38d0b35b8","text":"Incorporate a code style check right before user interface testing to ensure standards are being followed.","correct":false},{"id":"db4ecdbd1c7c8fda5d3e0792a15411ab","text":"Include code style check in the build stage of the deployment pipeline using a linting tool.  ","correct":true},{"id":"e60e97c4fb6cbf6c1dcf3e806624762f","text":"Require all developers to use the Pair Programming feature of Cloud9.  The commits must be signed by both developers before merging.","correct":false}]},{"id":"e720cd54-de67-42de-ba10-593dee0582e6","domain":"awscsapro-domain3","question":"You are the Enterprise Architect in a Risk Quantification firm. The firm has a website which end-users can use to apply for loans and also track the status of their loan application if they log in. When a loan application comes in, several downstream systems need to independently process the application. Right now, the website server-side code invokes these systems one after the other, synchronously, in a tight loop. If one of these downstream systems times out or throws an exception, the entire loan application processing errors out. Even if none of these downstream systems fail, the time it takes to process a loan application is very high due to the serial nature of these systems being invoked. Your CTO wants only the loan-processing application moved to the AWS cloud and re-architected at the same time.\nThe downstream systems are all hosted on-premises and will continue to remain on-premises. They expose REST endpoints that accept POST HTTPS requests, use self-signed certificates and respond synchronously only when they are done processing an application. After re-architecture, all downstream systems must independently start processing an incoming loan application simultaneously.\nYour CTO wants to know how the loan-processing website application can be architected in the AWS Cloud, and what supporting changes will be needed in the downstream systems on-premises. He wants to minimize code changes to the downstream on-premises systems. Choose the best option","explanation":"This is an example of a verbose question with verbose answer choices. You can expect a few such questions in the exam, testing your time management skills. Try to vertically scan the answers to see which parts differ between them. Sometimes, though the answers seem big, a large part of each is identical. You can ignore those parts, as there is nothing to choose between the.\nAmong the four choices, two use SQS and two use SNS to feed the incoming loan applications to the downstream systems. You cannot automatically eliminate either SQS or SNS, as a working solution can be designed with either.\nLet us see how we can achieve this using SNS first. The basic requirement here is fan-out - a single loan application must be processed by several downstream systems, so there are multiple consumers. Hence, SNS is a natural fit. SNS supports multiple subscribers for a topic. SNS also supports HTTP/HTTPS subscribers. SNS makes POST REST API call to as many HTTP/HTTPS subscribers exist on the topic, so it fits the bill. However, there is a small problem - the requirement states that the downstream systems must be changed as little as possible. If we follow this design, we must change the HTTP Listening part of the downstream systems significantly. Because SNS is directly calling them now, SNS will use its own headers and body format. In fact, SNS POST-s two kinds of messages - one is Subscription Confirmation and one is Notification. A special HTTP header (x-amz-sns-message-type) has the right type in its value. The server side now must parse this header out and look for only the Notification type of message. The body itself will then be JSON formatted with the payload. While the server is probably used to process just the core payload (loan application data) as the HTTP body, the same will now be hidden inside a JSON field called Message inside the request body. Additionally, the downstream systems will have to deal with SNS retries, thus the loan application part must be made idempotent (if the same loan application lands twice, it will ignore the duplicates). Thus, though it is technically possible to design the solution using SNS, it will result in a lot of changes in the downstream systems. Hence, though the SNS option will work, it is not the correct answer because of this reason.\nNow, let us see how we can design this using SQS. While SQS does not support fan-out (multiple consumers for the same message), the proposed solution uses a Lambda function to achieve fan-out. The Lambda function will pick up the message, and then call the downstream systems one by one. The key to making this work is, of course, to modify the downstream systems from synchronous monolithic beasts to asynchronous servers so that they can instantly respond to the Lambda function and then continue to process the application. We will then have to provide a callback for when it is done. The solution uses an API Gateway for that purpose. Overall, the solution is elegant, and changes to the downstream systems are less than what SNS requires. Hence, SQS is the correct answer.\nNote that one version of the SNS design proposes to retain the synchronous nature of the downstream systems. That will not work as SNS will not wait more than 15 seconds for a response. The response will then be lost and the main website app will never know the results from the downstream systems.\nAlso, note that though SNS requires the HTTPS subscriber to present a trusted CA-signed certificate, there is no such requirement for Lambda because Lambda is basically your code, you can decide to trust anyone.","links":[{"url":"https://docs.aws.amazon.com/sns/latest/dg/sns-http-https-endpoint-as-subscriber.html","title":"Using Amazon SNS for System-to-System Messaging with an HTTP/S Endpoint as a Subscriber"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","title":"Using AWS Lambda with Amazon SQS"}],"answers":[{"id":"3d032e65493c0733ebe65683fb66a562","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems and (c) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as your Lambda function will not be able to POST to a server with self-signed certificate","correct":false},{"id":"48d42d3290142cbfc8207e042690b35f","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting so that SNS does not retry, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (c) Make them idempotent for the same loan application as SNS may retry in case of lost messages or timeouts (d) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as SNS will not be able to POST to a server with self-signed certificate","correct":false},{"id":"eec2740374df8038093d636a17252168","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Override the default delivery policy on the subscriber endpoint to remove retries so that downstream systems do not have to worry about synchronous responses taking time or idempotency of retries. Make the following changes in the downstream systems - (a) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (b) Procure server certificates from a trusted Certificate Authority (CA) instead of using the self-signed certificate as SNS will not be able to POST to a server with a self-signed certificate","correct":false},{"id":"3fb725a8e71fa96168f18e50a146b4f0","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed and (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems","correct":true}]},{"id":"9882b1ed-c0de-4205-8a00-54731bef3109","domain":"awscsapro-domain2","question":"You work for an organic produce importer and the company is trying to find ways to better engage with its supply chain.  One idea is to create a public ledger that all members of the supply chain could update and query as products changed hands along the journey to the customer.  Then, your company could create an app that would allow consumers to view the chain from producer to end retailer and have confidence in the product sourcing.  Which AWS service or services could most directly help realize this vision?","explanation":"Amazon Quantum Ledger Database (QLDB) is a fully-managed ledger database that provides a transparent, immutable and verifiable transaction log.  While other products could be used to create such a supply chain logging solution, QLDB is the closest to a ready-made solution.","links":[{"url":"https://aws.amazon.com/qldb/","title":"Amazon QLDB"}],"answers":[{"id":"35b417ac310d92018a8db5515d4fae60","text":"Amazon DynamoDB and Lambda","correct":false},{"id":"a9d83c7f8f0b0f2a8f67b7097ee73e3a","text":"Amazon QLDB","correct":true},{"id":"a8312fbd65c49606c59b53a8a062ecff","text":"Amazon Managed Blockchain","correct":false},{"id":"ca649f7447f846ed8add8c396187b83a","text":"Amazon P2PShare and API Gateway","correct":false},{"id":"680da36fb45c26a1d8e7996c6f5014cd","text":"Amazon CloudTrail and API Gateway","correct":false}]},{"id":"0906c4cf-83a1-4cec-b2ab-c010dcdee73f","domain":"awscsapro-domain1","question":"The alternative energy company you work for has four different business units, each of which would like to run workloads on AWS. Each business unit has it's own AWS account, and a shared services AWS account has been created. An established process for tracking software license usage exists for on-premises applications, but the finance department has concerns that the self-serve nature of the cloud may result in license overages for applications deployed on AWS. You've been tasked with setting up a governance model whereby users are only given access to a standard list of products. Which architecture will provide an effective way to implement the governance requirements and manage software license usage on AWS?","explanation":"AWS Service Catalog allows organizations to create and manage catalogs of approved products for use on AWS. Products are defined as CloudFormation Templates. Software license information can be associated with Service Catalog products through tags. AWS Step Functions can orchestrate the process of incrementing usage counts and notifying of over-usage situations when products are launched by users. AWS License Manager is a robust solution for managing software licenses, but it needs to be coupled with Service Catalog to meet the requirement for limiting access to a standard set of products. A Lambda trigger is not currently available for Service Catalog product deployments. Elastic Container Registry provides tagging at the repository level, not at the individual container image level.","links":[{"url":"https://aws.amazon.com/servicecatalog/","title":"AWS Service Catalog"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://aws.amazon.com/blogs/mt/tracking-software-licenses-with-aws-service-catalog-and-aws-step-functions/","title":"Tracking software licenses with AWS Service Catalog and AWS Step Functions"}],"answers":[{"id":"33db838110cfb8b002590cbff630b825","text":"Create Amazon Machine Images for all of the instance configurations that will be deployed. Implement AWS License Manager license configurations and attach them to the AMIs. Create AWS CloudFormation StackSets for the AMIs in the shared AWS account and make them available to users in each business unit","correct":false},{"id":"e2441352bdc2d4db956149f0a10b6738","text":"Create Docker images for each of the standardized applications that will be deployed and register them with Amazon Elastic Container Registry (ECR). Populate ECR tags with software license metadata. Create an Amazon DynamoDB table to store software license usage counts. Whenever a container is launched in Amazon Elastic Container Service, trigger an AWS Lambda function to increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"62788b95bb6b249e6511d14023a20364","text":"Implement AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Create an Amazon DynamoDB table to store software license usage counts. Trigger an AWS Lambda function to run each time a Service Catalog product is launched. Have the Lambda function increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"41d11d1142468e8b0d2a29674d1eaa2c","text":"Deploy AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Populate Service Catalog product tags with software license information. Create an Amazon DynamoDB table to store software license usage counts. Have Amazon CloudWatch detect when a user deploys a Service Catalog product. Launch an AWS Step Functions process to increment license counts in the DynamoDB table, and send notifications when overage thresholds are met","correct":true}]},{"id":"dd8b46c7-d1d5-4326-a092-927b9333fd2a","domain":"awscsapro-domain5","question":"You are helping a company transition their website assets over to AWS.  The project is nearing completion with one major portion left.  They want to be able to direct traffic to specific regional EC2 web servers based on which country the end user is located.  At present, the domain name they use is registered with a third-party registrar.  What can they do?","explanation":"You can use Route 53 if the domain is registered under a third-party registrar.  When using Geolocation routing policies in Route 53, you always want to specify a default option in case the country cannot be identified.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy - Amazon Route 53"}],"answers":[{"id":"294ce2854fbe727bd4f4917543d45bec","text":"Create a private hosted zone for the domain in Route 53.  Update the DNS record entries in the registrars database to use AWS DNS Servers.  Once the DNS changes are fully propagated across the internet and the TTL has expired, convert the private hosted zone to a public hosted zone.  Create A-type records for all the regional EC2 instances and configure a Geo-proximity policy for each record, ensuring the bias across all records sums to 100.","correct":false},{"id":"381d1621ab4c93aca8cc780f05e98c50","text":"Initiate a domain transfer request with the current registrar.  Once the request goes through, create a public hosted zone in Route 53.  Create SRV records for each regional EC2 instance using a Geolocation routing policy.  Create an alias record for the top-level domain and link that to the SRV records.","correct":false},{"id":"724dab26998c86ab7ddc7faa063285b8","text":"You cannot use Route 53 routing policies unless AWS is the registrar of record for your domain.  A workaround could be to configure your own top-level DNS server using BIND.  Ensure the NS and SOA records point to this instances.  Create A-type records pointing to the IP addresses of the regional EC2 web servers.  Dynamically redirect requests using customized BIND rules and a third-party IP geolocation database.","correct":false},{"id":"81efc0b140e618c378c9e9bc59dd4ca8","text":"Create a public hosted zone for the domain in Route 53.  Update the DNS entries in the registrars database to use AWS DNS Servers as defined in the NS record on Route 53.  Create A-type records for all EC2 instances. Configure CNAME records for the main FQDN that point to regional A records using a Geolocation routing policy.  Create another CNAME record as a default route.","correct":true}]},{"id":"ef90df82-b36a-41f5-9294-904edfb5830e","domain":"awscsapro-domain1","question":"You are helping a client design their AWS network for the first time.  They have a fleet of servers that run a very precise and proprietary data analysis program.  It is highly dependent on keeping the system time across the servers in sync.  As a result, the company has invested in a high-precision stratum-0 atomic clock and network appliance which all servers sync to using NTP.  They would like any new AWS-based EC2 instances to also be in sync as close as possible to the on-prem atomic clock as well.  What is the most cost-effective, lowest maintenance way to design for this requirement?","explanation":"DHCP Option Sets provide a way to customize certain parameters that are issued to clients upon a DHCP request.  Setting the NTP server is one of those parameters.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_DHCP_Options.html","title":"DHCP Options Sets - Amazon Virtual Private Cloud"}],"answers":[{"id":"d5ea0e4895105bc1137c03c8bfb28ebe","text":"Configure a DHCP Option Set with the on-prem NTP server address and assign it to each VPC.  Ensure NTP (UDP port 123) is allowed between AWS and your on-prem network.","correct":true},{"id":"d94eaf4f8670e4d3f42fbae47250e3e6","text":"Create a dedicated host instance on AWS and place it within a transit VPC.  Configure the server to run NTP as a stratum-2 server.  Ensure NTP (UDP port 123) is allowed inbound and outbound in the Security Groups local to the stratum-2 server.","correct":false},{"id":"2b2197df0ae72d4d462432d7c2f65c98","text":"Create a bridged network tunnel from the on-prem time server to the VPCs on AWS.  Configure the VPC route tables to route NTP (UDP 123) over the tunnel.","correct":false},{"id":"40efb671ff85878c28d1fa99329f15c7","text":"Configure your Golden AMI to use Amazon Time Sync Server at 169.254.169.123 and require this AMI to be used.  Use AWS Config to periodically audit the NTP configuration of all AWS assets.","correct":false},{"id":"c950359dab3520d6c8c8f4e16a925860","text":"Deploy a third-party time server from the AWS Marketplace.  Configure it to sync from the on-prem time server.  Ensure NTP (UDP port 123) is allow inbound in the NACLs for the VPC containing the third-party server.","correct":false}]},{"id":"343601ea-0262-46dc-baab-511550237b8f","domain":"awscsapro-domain2","question":"You work for a distributed large enterprise that uses Splunk as their log aggregation, management and analysis software. The company has recently shown a keen interest in adopting a microservice based architecture and wants to convert some of its applications to Docker containers. They have selected AWS ECS as the orchestration platform for these containers. They are interested only in EC2 Launch Type as the Security Team wants to harden the EC2 instances with policies they want to control. However, the Chief Architect is concerned about hardcoding the Splunk token inside the container code or configuration file. Splunk requires any logging request to include this token. The Chief Architect wants to know if there is a way to pass the production Splunk token to ECS at runtime so that the container tasks can continue logging to Splunk without exposing the production secret to all developers, and if yes, how. Select the best answer.","explanation":"ECS Task Definition file supports two ways of specifying secretOptions in its logConfiguration element - AWS Systems Manager Parameter Store and AWS Secrets Manager. This question only deals with only one of these two ways, as it does not mention AWS Secrets Manager at all. However, the real focus of the question is how KMS is used by AWS Systems Manager Parameter Store. There is actually nothing special about how Parameter Store deals with KMS. Hence, the question actually is just about knowing the ways KMS Keys can be used - they can either be customer-managed, or AWS managed or both AWS and customer-managed.\nAnother aspect tested by the question is whether to use Task Role or EC2 Instance Role while granting an ECS Cluster permission to access resources during container execution. AWS recommends using the Task Role in this scenario, hence we can eliminate the two choices that specify using EC2 Instance Roles.\nThe option that says that the encryption key must only be stored by the customer is incorrect, as we can use KMS to store the key. The options that state the only supported management options for the KMS-stored keys are either only customer-managed or only AWS managed are both incorrect. The correct option identifies that KMS keys can either be customer-managed or AWS managed.\nOne thing to note is that this question does not need any prior knowledge of Splunk. Everything that needs to be known about Splunk is stated as part of the question. The AWS SA-P exam normally does not require the candidate to know about any 3rd-party product or services, but such services can be explicitly named.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html#secrets-logconfig","title":"Injecting Sensitive Data in a Log Configuration"},{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html","title":"How AWS Systems Manager Parameter Store Uses AWS KMS"},{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html","title":"IAM Roles for Tasks"}],"answers":[{"id":"ee244c2ca9783d96974f0c603a4aa548","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS but must be solely AWS-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. The task definition must include a task role that allows the container task to read AWS Systems Manager Parameter Store and decrypt using an AWS-managed KMS key","correct":false},{"id":"a161521ba68b3fbb6a60532e1bf664c0","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS and can either be customer-managed or be AWS-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. The task definition must include a task role that allows the container task to read AWS Systems Manager Parameter Store and decrypt using a KMS key, this latter permission being required only if the key is customer-managed and not AWS managed","correct":true},{"id":"8a8783b1fab2fb72bc61295df2630a46","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can only be stored and managed by the customer - therefore the value must be encrypted by the customer before writing to the Parameter Store and decrypted by the customer after reading from the Parameter Store. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. Each EC2 node must have an associated EC2 Instance Role that allows it to read AWS Systems Manager Parameter Store","correct":false},{"id":"c3037df22b608e2533a356125fab5322","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS but must be solely customer-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. Each EC2 node must have an associated EC2 Instance Role that allows it to read AWS Systems Manager Parameter Store and decrypt using a customer-managed KMS key","correct":false}]},{"id":"bdd6d6a9-48a5-45f8-bb74-873f266d85df","domain":"awscsapro-domain2","question":"A hospital would like to reduce the number of readmissions for high risk patients by implementing an interactive voice response system to provide reminders about follow up visit requirements after patients are discharged. The hospital has the capability to automatically send HL7 messages that include the patient's phone number and follow up visit information from its medical records application via Apache Camel. They've chosen to deploy the solution on AWS. They already have a VPN connection to AWS, and all aspects of the application need to be HIPAA eligible. Which architecture will provide the most resilient and cost effective solution for the automated call system?","explanation":"S3 provides a low cost repository for the HL7 messages received. Having Lambda write the object keys to SQS, and having another Lambda function retrieve and parse the messages gives the architecture asynchronous workflow. Amazon Connect provides the capability to define call flows and perform IVR functions. Each of these services is HIPAA eligible. DynamoDB is also a good option for storing message information, but will be more expensive than S3. Amazon Pinpoint can place outbound calls, but is not able to perform interactive voice response functions. Amazon Comprehend Medical doesn't create call flow sequences.","links":[{"url":"https://aws.amazon.com/compliance/hipaa-eligible-services-reference/","title":"HIPAA Eligible Services Reference"},{"url":"https://aws.amazon.com/connect/","title":"Amazon Connect"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/reduce-hospital-readmissions-ra.pdf?did=wp_card&trk=wp_card","title":"Reducing Hospital Readmissions"}],"answers":[{"id":"9585e2e5994ba5cdc2db33c22d9230cf","text":"Set up Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Connect to place the call to the patient.","correct":false},{"id":"c9bee4c5e4463b67e2726d3bed2ceed1","text":"Configure Apache Camel to write the HL7 messages to Amazon Kineses Data Firehose, which stores the patient information in Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"a8c26f088ade23063b2bf5f202a74cce","text":"Have Apache Camel write the HL7 messages to Amazon Kineses Data Streams. Configure a Lambda function as a consumer of the stream to parse the HL7 message and write the information to Amazon DynamoDB. Trigger another Lambda function to pull the patient data from DynamoDB and send it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"8121c4e6e285161311cacf7b8031d5af","text":"Configure Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to write each HL7 message object key to Amazon Simple Queue Service FIFO. Have another Lambda function read messages in sequence from the SQS queue and use the object key to retrieve and parse the HL7 messages. Use that same Lambda function to write patient information to Amazon Connect to place the call using an established call flow.","correct":true}]},{"id":"2eee6f1c-96d7-4d2b-821f-4ce8acaf3de3","domain":"awscsapro-domain5","question":"You've deployed a mobile app for a dance competition television show's viewers to vote on performances. The app's backend leverages Amazon API Gateway, AWS Lambda, and Amazon RDS Oracle, with voting activity going from devices directly to API Gateway. In the middle of the broadcast, you begin receiving errors in CloudWatch indicating that the database connection pool has been exhausted. You also see log entries in CloudWatch with a 429 status code. After the show concludes, ratings for the app indicate a very poor user experience, with multiple retries needed to cast a vote. What would be the best way to increase the scalability of the app going forward?","explanation":"Placing Kineses between API Gateway and Lambda decouples the architecture, making use of an intermediary service to buffer incoming requests. The 429 status code indicates a Lambda concurrency throttling error, which you can resolve by controlling the Kinesis batch size per batch delivery. Database sharding will increase scalability, but will still have an upper limit of capacity. Increasing available Lambda memory will have no effect. Inserting a Lambda traffic manager doesn't address the database scalability issues, nor does increasing the regional Lambda concurrency limit. Modifying RDS DB Parameter Group values will require a database restart to take effect, which won't be feasible during live voting activity.","links":[{"url":"https://aws.amazon.com/blogs/architecture/how-to-design-your-serverless-apps-for-massive-scale/","title":"How to Design Your Serverless Apps for Massive Scale"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/scaling.html","title":"AWS Lambda Function Scaling"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","title":"Using AWS Lambda with Amazon Kinesis"}],"answers":[{"id":"a4051078cf0289e689e79fe70eab1f14","text":"Create a separate Lambda function to increase the maximum DB connection value in the RDS DB Parameter Group when a CloudWatch Metrics DB connection threshold is exceeded. Invoke Lambda functions with an 'event' invocation type to retry failed events automatically","correct":false},{"id":"6b6e6fb8b54db42df4d343376ccd8c60","text":"Insert Amazon Kinesis between API Gateway and Lambda, and configure Kinesis as an event source for Lambda. Set the number of records to be read from a Kinesis shard to an optimal value based on volume projections","correct":true},{"id":"8f962935878f87ebc9523dc54f505886","text":"Scale the database horizontally by creating additional instances and use sharding to distribute the data across them. Provide the Lambda function with a mapping of the sharding scheme in DynamoDB. Increase the amount of memory available to the Lambda function during execution","correct":false},{"id":"2f66b519925956d6a91d0026056904d9","text":"Have API Gateway route requests to a new Lambda function that manages traffic and retries for the voting logic Lambda function. Request that the regional function concurrency limit be increased based on volume projections","correct":false}]},{"id":"edb30172-3f76-4423-a6bb-78a3d2fdeb42","domain":"awscsapro-domain2","question":"Your team is managing hundreds of Linux and Windows EC2 instances in different environments such as development, QA, staging and production. You need a tool to help you automate the process of patching instances so that the operating systems have latest patches and meet the compliance policies. You want to manage the patching in different groups depending on the environment. For example, patches should be deployed and tested in the QA environment first before the production environment. How would you achieve this requirement through an AWS service?","explanation":"AWS Systems Manager Patch Manager is the most appropriate tool to manage the patching for large groups of EC2 or on-premises instances. For different environments, users can configure patch groups using the \"Patch Group\" tag and then establish a patch baseline for each patch group. It is better to manage instances with the \"Patch Group\" tag rather than other customized tags. AWS SSM Session Manager and AWS SSM Run Command are not suitable to deploy patches across a large number of instances. The AWS-RunRemoteScript command is also incorrect as it is used to execute scripts stored in a remote location.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-patchgroups.html","title":"Patch Manager Patch Groups"}],"answers":[{"id":"08b9feba2a12ba67141a0ffe02938798","text":"Add patch group tags in the EC2 instances. Perform the patching using the command AWS-RunRemoteScript in AWS SSM Run Command. Patch on the QA environment first by selecting the QA patch group tag.","correct":false},{"id":"6c4763d45c6cb24622b0db9533f95e0c","text":"Create environmental tags in EC2 instances such as a tag key named \"env\". In AWS SSM Patch Manager, configure patching activities by selecting the instances using the tag. Patch on the QA environment first and perform the necessary testing.","correct":false},{"id":"32ac8d27221ec8e699eb4e872d3f6ed0","text":"Centrally manage the instances in AWS SSM Managed Instances and divide them into different categories. Perform the patching activities from AWS SSM Session Manager in a maintenance window.","correct":false},{"id":"b99f4271e073e1e030f3c26c383c5959","text":"In AWS Systems Manager Patch Manager, create different patch groups using the tag key \"Patch Group\" and configure a patch baseline for each patch group. Schedule the patching in a maintenance window by selecting a patch group.","correct":true}]},{"id":"663fbd6a-87bd-4fa6-a0ea-428ba2de5b51","domain":"awscsapro-domain5","question":"You manage a relatively complex landscape across multiple AZs.  You notice that the incoming requests vary mostly depending on the time of day but also there is a more unpredictable component resulting in smaller spikes and valleys for your resources.  Fortunately, you manage this landscape via OpsWorks Stacks.  What options, if any, are available to you as part of the OpsWorks featureset.","explanation":"OpsWorks Stacks offers three types of scaling: 24/7 for instances that remain on all the time; time-based for instances that can be scheduled for a certain time of day and on certain days of the week; and load-based scaling which will add instances based on metrics.  All this can be configured from within the OpsWorks Stack console.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/best-practices-autoscale.html","title":"Best Practices: Optimizing the Number of Application Servers - AWS OpsWorks"}],"answers":[{"id":"216c997091da6e24174ad1b83d0be8b9","text":"You would define a baseline level of resources within the OpsWorks Stack Console to cover the average load.  But for the periodic load, that requires a scheduled auto-scaling policy.  Similarly, for the volatile spikes, you must use a stepped auto-scaling policy defined in an auto scaling group. ","correct":false},{"id":"b7ff5b06f51facca179494cb2bb00e55","text":"You can enabled CloudFormation Anticipated Scaling that uses past CloudWatch metrics and machine learning to automatically design a scaling policy optimized for the incoming request patterns.","correct":false},{"id":"75ab4de4ea42c1971b0ee09ae04ca591","text":"You would define a baseline level of resources and configure them for 24/7 instances.  Then you could define a time-based instances to cover certain times of day.  Finally, you could cover the volatile spikes with a load-based instances.  All this can be done within OpsWorks Stacks.","correct":true},{"id":"3622d494dceb973760a46dea038d1dc2","text":"If you need the ability to dynamically scale, you will need to use OpsWorks for Chef Automate.  OpsWorks Stacks does not support scaling.","correct":false}]},{"id":"0bfd631e-c08c-406a-acf5-a07416aab129","domain":"awscsapro-domain5","question":"Your team uses a CloudFormation stack to manage AWS infrastructure resources in production. As the AWS resources are used by a large number of customers, the update to the CloudFormation stack should be very cautious. Your manager asks for additional insight into the changes that CloudFormation is planning to perform when it updates the stack with a new template. The change needs to be reviewed before being applied by a DevOps engineer. What is the best method to achieve this requirement?","explanation":"CloudFormation Change Sets are able to provide the information on how the running resources are affected by a stack update. The outputs can be reviewed before being executed. Users can view the Change Set through AWS Console or CLI. The Retain option in the DeletionPolicy, CloudFormation stack policy or termination protection helps on protecting the stack resources. However, they cannot provide a summary of  changes in a stack update.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"935df51ce2c7f07994c2b8a257489e00","text":"Create a CloudFormation Change Set using AWS Management Console or CLI, review the changes to see if the modifications are as expected and execute the changes to update the stack.","correct":true},{"id":"4657e544cc1daf4315865e230d92dd00","text":"For key AWS resources in the CloudFormation stack, add a Retain option in the DeletionPolicy attribute, which prevents the resources from being accidentally deleted by a stack update. Add a Delete option for the resources that you want to delete along with the stack.","correct":false},{"id":"550c80eaf15eeb89d49aa2a86eb747a6","text":"Enable termination protection in the CloudFormation stack so that the AWS resources cannot be accidentally deleted or modified. Disable the protection only if the changes are approved. Execute the changes in a maintenance window.","correct":false},{"id":"700aa7cb0f0e8cfb417b67ae5d49e962","text":"Add a CloudFormation stack policy to prevent updates to stack resources. Only after the changes are reviewed and approved, change the stack policy to allow the stack update. Revert the stack policy after the change.","correct":false}]},{"id":"19591d08-60c8-494e-9c39-d69c6c3390f0","domain":"awscsapro-domain4","question":"Your company's AWS migration was not planned out very well across the enterprise.  As as result, different business units created their own accounts and managed their own resources.  Recently, an internal audit of costs show that there may be some room for improvement with regard to how reserved instances are being used throughout the enterprise.  What is the most efficient way to ensure that the reserved instance spend is being best used?","explanation":"The discounts for Reserved Instances can be shared across accounts that are linked with Consolidated Billing but Reserved Instance Sharing must be enabled.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-ri-consolidated-billing/","title":"EC2 Reserved Instance Consolidated Billing"}],"answers":[{"id":"af8967525080f0a545bb6fbafc8a8f3c","text":"Setup Consolidated Billing for a single account and link all the other various accounts in the organization.  Ensure that Reserved Instance Sharing is turned on.  The discounts for specific reserved instances will automatically be applied to the consolidated invoice.","correct":true},{"id":"f58169ec8f86bed7c84e4510ed491661","text":"Load CloudTrail instance usage data into Redshift for analytics.  Use QuickSight to create some utilization reports on existing reserved instances.  Relocate on-demand instances into regions where reserved instances are underutilized.","correct":false},{"id":"617beec9b1a6fd3ca1cec7764ddfd9fb","text":"Use AWS Organizations to organize accounts under one organizational unit.  Use AWS Budgets to analyze utilization of reserved instances.  Reassign those RIs to other accounts that are currently using On-Demand instances.","correct":false},{"id":"34d8e4aeeb67918e8bd27daba0923994","text":"Use Cost Explorer reports to analyze coverage of reserved instances.  Where there are coverage gaps, purchase more reserved instance capacity for that account. Where there is excess, place those on the Reserved Instance Marketplace.","correct":false}]},{"id":"115e1b30-23e4-4f3f-9c13-a0086f6af223","domain":"awscsapro-domain2","question":"You are working with a pharmaceutical company on designing a workflow for processing data.  Once a day, a large 2TB dataset is dropped off at a pre-defined file share where the file is processed by a Python script containing some proprietary data aggregation routines.  On average, it takes 20-30 minutes to complete the processing.  At the end, a notification has to be sent to the submitter of the dataset letting them know processing is complete.  Which of the following architectures will work in this scenario?","explanation":"While it may not be the most cost-effective, the EFS option is the only one that can work.  A processing time of 20-30 minutes rules out Lambda (at present with a processing limit of 15 minutes).  If we create an EBS volume with a full OS on it and mount as root for a new instance with the data set included, we still would not be able to dismount the root volume without shutting down the instance.  This would not let us issue an SES SDK call.  The database is also far too large for SQS.","links":[{"url":"https://aws.amazon.com/efs/features/","title":"Amazon Elastic File System (EFS) | Cloud File Storage | Product Features"}],"answers":[{"id":"5f8da9106eba72e74c1a0d6415a235af","text":"Create an S3 bucket to store the incoming dataset.  Once the dataset has been fully received, use S3 Events to launch a Lambda function with the Python script to process the data.  When finished, use an SDK call to SNS to notify when the processing is complete.  Store the processed data back on S3.","correct":false},{"id":"1831d929918e3c425b20e476c1716ccc","text":"Load inbound dataset on an EBS volume.  Stand up an EBS-optimized instance and mount the data volume as the root volume.  Once the data processing is complete, unmount the EBS volume and issue an SDK call to SES to notify of completion.  Configure SES to trigger an instance shutdown after the notification is sent.","correct":false},{"id":"dee7711bae75cac018761467694d89e3","text":"Use SQS to take in the data set.  Use a Step Function to Launch Lambda functions in a fan-out architecture for data processing and then send an SNS message to notify when the processing is complete.  Store the processed data on S3.","correct":false},{"id":"3e1542fcef251cf86c8bdcc89d83aaa7","text":"Stand up memory optimized instances and provision an EFS volume. Pre-load the data on the EFS volume.  Use a User Data script to sync the data from the EFS share to the local instance store.  Use an SDK call to SNS to notify when the processing is complete, sync the processed data back to the EFS volume and shutdown the instance. ","correct":true}]},{"id":"722221be-beb9-4a2b-8ae1-dff52b80125c","domain":"awscsapro-domain3","question":"You work for a technology company with two leased data centres (one on the east coast and one on the west coast) and one owned on-premises data centre. Management has decided to move the two leased data centres to the AWS cloud - one to us-east-1 and the other to us-west-1. The on-premises data centre will still continue running workloads which are not ready to move to the cloud.\nThis on-premises data centre must be always connected to the VPC-s in us-east-1 and us-west-1 for (a) the continuous replication of several databases and (b) the need to access some data residing on the on-premises data centre from applications running in both the AWS regions. The peak bandwidth required for these connections is (a) 500 Mbps between us-east-1 and on-premises, and (b) 8 Gbps between us-west-1 and on-premises. The applications would still be able to function at lower bandwidth, but the experience will be poor, which is not desirable. Both these connections must be Highly Available with 99.999% uptime. The connectivity solution must be cost-effective as well.\nAs the AWS Architect, what connectivity solution would you propose, so that all Bandwidth, HA and cost-effectiveness requirements are met?","explanation":"We can eliminate the VPC Peering solution immediately, as VPC Peering is for connecting two VPC-s on AWS. VPC Peering cannot be used to connect an AWS VPC with an on-premises network.\nOut of the remaining choices, the one that proposes connecting to us-east-1 using VPN and us-west-1 using Direct Connect comes very close to fulfilling all requirements. It suffers from two problems, however. One - the peak bandwidth requirement for us-east-1 is 500 Mbps. A VPN connection cannot be expected to provide 500 Mbps most of the time, as the true bandwidth someone can get from a VPN connection depends on a lot of factors including internet traffic it is sharing the route with. Secondly, if we are paying for a Direct Connect connection for the other region anyway, why not just use that one for this region too? Now, there is something called Direct Connect Gateways that makes it possible to share multiple AWS Regions using the same Direct Connect connection. The knowledge of Direct Connect Gateways is important for the AWS SA-P exam. Hence, this question tests this knowledge. The correct answer is the only one that uses Direct Connect Gateway.\nThe other choice that uses two separate Direct COnnect connections (one for each region) is not cost-effective, especially because since 2017, Direct Connect Gateways make it possible to connect to multiple AWS Regions using the same Direct Connect connection.\nRegarding HA, it is always a good practice to set up a VPN connection as a back-up for Direct Connect. The only requirement to do this is that the back-up VPN connection must also use the same Virtual Private Gateway on the AWS VPC side, otherwise traffic cannot fail over easily.\nNote about Direct Connect Gateways - they not only allow a customer to connect to two AWS Regions using a single Direct Connect connection, they also let the connected Regions communicate with each other! (This is why the VPC CIDR-s in us-east-1 and us-west-1 in the correct answer have to be non-overlapping.) There may be questions testing this aspect as well. Before Direct Connect Gateways existed, VPC Peering would be the only way for Inter-Region VPC Access. There is also another solution now - Transit Gateway, but this was announced late 2018. Usually, topics do not start appearing on the exam unless they have been more than 6 months in GA. Expect Transit Gateways to start appearing in questions now as well!","links":[{"url":"https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/","title":"AWS Direct Connect Gateway for Inter-Region VPC Access"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/","title":"Configuring VPN as back up of Direct Connect"},{"url":"https://aws.amazon.com/directconnect/sla/","title":"Direct Connect SLA puts uptime target at 99.9%. Therefore, if we need more than that, we should set up VPN as back up"}],"answers":[{"id":"608dc6c035921ef09c640ebb70de9ddd","text":"Use VPC Peering to connect the on-premises network with both the us-east-1 and us-west-1 VPC-s independently. Bandwidth provided by VPC Peering is virtually unlimited, limited only by the instance sizes used. Also, VPC peering connections are fault-tolerant and scalable, so no back-up connectivity is needed","correct":false},{"id":"85b6b49a01a28a90d71ef3c20ca9da8d","text":"Connect the on-premises data centre and us-east-1 using redundant site-to-site VPN connections as its bandwidth requirements do not require a costly Direct Connect connection. The redundant VPN connections must use different customer gateways and will provide an HA solution for that region. Connect the on-premises data centre with us-west-1 using a 10 Gbps Direct Connect circuit. Set up a back-up VPN connection for this region such that it uses the same Virtual Private Gateway as the Direct Connect circuit","correct":false},{"id":"f0671c30ae58d58c0d64162640e69527","text":"Use two Direct Connect connections - an 1 Gbps one between the on-premises data centre and us-east-1, and a 10 Gbps one between the on-premises data centre and us-west-1. For each Direct Connect connection, set up a back-up VPN connection that must use the same Virtual Private Gateway as the Direct Connect circuit","correct":false},{"id":"3ea4a9645ac22c0b2c6c427764a7ae01","text":"Set up a Direct Connect Gateway. Associate the Virtual Private Gateways from both the us-east-1 and us-west-1 VPC-s with this Direct Connect Gateway. Then set up a single 10 Gbps Direct Connect connection between the on-premises data centre and the Direct Connect Gateway, using a Private Virtual Interface. Ensure that the VPC CIDR-s in the two AWS Regions are non-overlapping. To increase HA, set up separate back-up VPN connections between the on-premises data centre and each of the two AWS Regions","correct":true}]},{"id":"e70b0410-8628-4150-845e-a4bba1f66ec6","domain":"awscsapro-domain2","question":"You work for a credit union which has two VPC-s in us-west-1 - VPC A and VPC B. The CIDR of VPC A is 172.16.0.128/25, and 50% of its available private IP addresses are already used up. The CIDR of VPC B is 172.16.0.0/25, and 5% of its available private IP addresses are already used up.\nThe development team is creating a new service that will deploy an API Gateway and use a Lambda function as its back-end. The Lambda function must read an S3 bucket using an S3 VPC Endpoint deployed in VPC A. Then, it must write to a Cassandra database hosted on an EC2 instance in VPC B.\nThe Lambda function must be deployed in either VPC A or VPC B. Also, the expected number of peak concurrent Lambda function instances is 300, with each instance needing 1 GB of memory. The development team expects that at peak, 100 free IP addresses will be needed to accommodate all the Lambda function instances.\nAs the AWS Architect, you need to advise the development team of the right AWS Architecture to make this possible. What should you suggest?","explanation":"This question requires the knowledge of the following:\n(a)Counting the number of the available private IP address in a CIDR. A CIDR ending with /25 has 128 IP addresses available, minus whatever number AWS reserves for its own (which is 5 per subnet, so we cannot really tell that number here as we do not know how many subnets are there in VPC A or B). For VPC A, half of these are already used, so around 64 are remaining. For VPC B, 5% is used, so 6 or 7 are used, leaving more than 120 available. Thus, VPC A clearly does not have enough space left for 100 Lambda functions, while VPC B surely does. This eliminates the choice that wants to deploy the Lambda function in VPC A without adding a secondary CIDR to it. Though we can eliminate the choice because of this reason, it can be worthwhile to note that it does state something important and true - S3 VPC Endpoint cannot be accessed from a different VPC using a peering connection.\n(b)Determining if CIDR-s are overlapping. 172.16.0.0/25 and 172.16.0.128/25 are not overlapping. Hence, the choice that uses the overlapping argument against peering is eliminated.\n(c)Whether an S3 VPC Endpoint in one VPC can be accessed via a VPC-peered network from a second VPC. The answer is no. Though most VPC Endpoints (that are of the type Interface Endpoint, also called PrivateLink) do not have this limitation, S3 and DynamoDB VPC Endpoints are not of this type. They are of type Gateway Endpoint. Gateway Endpoints cannot be accessed from another VPC if the VPC-s are peered. This eliminates VPC B as the hosting choice for the Lambda function. Remember that the Cassandra Database Instance can be accessed easily over a VPC Peering Connection, so VPC A and B needs to be peered, and VPC A must house the Lambda function, provided we make enough space in VPC A.\nThus, the only correct answer is to host the Lambda function in VPC A after adding a secondary CIDR to make space for the 100 ENI-s needed by the Lambda function at its peak","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html","title":"Gateway VPC Endpoints"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html","title":"Interface VPC Endpoints (AWS PrivateLink)"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html","title":"Search for VPC and Subnet Sizing"}],"answers":[{"id":"45c82d2dc99061d7e4f27c276f170b4b","text":"Peer VPC A and VPC B. Deploy the Lambda function in VPC A after adding a secondary CIDR Range to VPC A so that the available number of free IP addresses in VPC A expands to accommodate the Lambda function's peak usage demands. The Lambda function can then access the S3 VPC Endpoint in the same VPC, and also access the Cassandra Database Instance over the Peering connection using appropriate routes added to the Routing Table","correct":true},{"id":"b281cc03c30d338041be0e55efeba980","text":"Peer VPC A and VPC B. Deploy the Lambda function in VPC A because S3 VPC Endpoint cannot be accessed from a different VPC using a peering connection. The Lambda function can then access the S3 VPC Endpoint in the same VPC, and also access the Cassandra Database Instance over the Peering connection using appropriate routes added to the Routing Table","correct":false},{"id":"d49117fbb6670c3f47211d6c4410e9e8","text":"Peer VPC A and VPC B. Deploy the Lambda function in VPC B because only VPC B has enough private IP addresses left to accommodate the Lambda function's peak usage demands. The Lambda function can then access the Cassandra Database Instance in the same VPC, and also access the S3 VPC Endpoint over the Peering connection using appropriate routes added to the Routing Table","correct":false},{"id":"9d99afaedfe56a6a5a3e43972f59eb43","text":"VPC A and VPC B cannot be peered owing to their overlapping IP Address ranges. Therefore, it is impossible to deploy the Lambda Function in either VPC because it needs to access some resources in both the VPC-s, unless a new S3 VPC Endpoint is deployed in VPC B. In that case, VPC B will house both the new S3 Endpoint and the Cassandra Database Instance, and therefore, the Lambda function can be deployed in VPC B","correct":false}]},{"id":"694c2138-4b6e-466d-b5b2-7a34cc0a3360","domain":"awscsapro-domain1","question":"AWS Cost Management encompasses a number of services to help you to organize, control and optimize your AWS costs and usage.  Which of the following Cost Management related tools gives you the ability to set alerts when costs or usage are exceeded?","explanation":"The correct answer is AWS Budgets.  AWS Cost Explorer lets you visualize, understand, and manage your AWS costs and usage over time. AWS Cost & Usage Report lists AWS usage for each service category used by an account and its IAM users and finally, Reserved Instance Reporting provides a number of RI-specific cost management solutions to help you better understand and manage RI Utilization and Coverage.","links":[{"url":"https://aws.amazon.com/aws-cost-management/aws-budgets/","title":"AWS Budgets"}],"answers":[{"id":"eef79d956328d5e4ec426d448cc53c74","text":"Reserved Instance Reporting","correct":false},{"id":"824fd559c917b4ae56f36787b886eb81","text":"AWS Cost & Usage Report","correct":false},{"id":"e32a801c8e0beab6abb9361e937365be","text":"AWS Budgets","correct":true},{"id":"c7f176d72688fd87853e31b84159d541","text":"AWS Cost Explorer","correct":false}]},{"id":"34351bd0-7925-4246-bb61-c64bbf4d5baf","domain":"awscsapro-domain4","question":"An application in your company that requires extremely high disk IO is running on m3.2xlarge EC2 instances with Provisioned IOPS SSD EBS Volumes. The EC2 instances have been EBS-optimized to provide up to 8000 IOPS. During a period of heavy usage, the EBS volume on an instance failed, and the volume was completely non-functional. The AWS Operations Team restored the volume from the latest snapshot as quickly as possible, re-attached it to the affected instance and put the instance back into production. However, the performance of the restored volume was found to be extremely poor right after it went live, during which period the latency of I/O operations was significantly high. Thousands of incoming requests timed out during this phase of poor performance.\nYou are the AWS Architect. The CTO wants to know why this happened and how the poor performance from a freshly restored EBS Volume can be prevented in the future. Which answer best reflects the reason and mitigation strategy?","explanation":"Data gap cannot be the reason for high disk I/O latency. Whether the data being requested is on the disk or not cannot be responsible for the extended period of high disk I/O latency, as all operating systems index the contents in some way. They do not scan the whole disk to conclude that something is missing. Hence, the choice that suggests data gap as the reason is eliminated.\nEBS Optimization works straight away after a freshly restored volume is attached to an EBS optimized instance. Hence, the choice that suggests that EBS Optimization takes some time to kick in is eliminated.\nThere is nothing called set-up-cache command. The option that suggests that there is an inbuilt caching mechanism that needs to be activated is completely fictional, and is eliminated.\nThe only correct option is the one that correctly states that every new block read from a freshly restored EBS Volume must first be downloaded from S3. This is because EBS Snapshots are saved in S3. Remember that EBS Snapshots are incremental in nature. Every time a new snapshot is taken, only the data that changed is written to that particular snapshot. Internally, it maintains the pointers to older data that was written to S3 as part of previous snapshots. These blocks of data continue to reside on S3 even after an EBS Volume is restored, and is read the first time they are accessed. Linux utilities like dd or fio can be used after restoring an EBS Volume to read the whole volume first to get rid of this latency problem when the instance is put back in production.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-initialize.html","title":"Initializing Amazon EBS Volumes"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html","title":"Amazon EBS–Optimized Instances"}],"answers":[{"id":"ae32d192c3829287819a74ded72e0da7","text":"The latest snapshot did not have the most current data. It only had the data from the last time a snapshot was taken. The requests timed out because of this data gap. To mitigate this, increase the frequency of taking EBS snapshots.","correct":false},{"id":"1a47200401a925a2ad1df3d3286e26dc","text":"When a data block is accessed for the first time on a freshly restored EBS Volume, EBS has to download the block from S3 first. This increases the I/O latency until all blocks are accessed at least once. To fix this, update the restoration process to run tools to read the entire volume before putting the instance back to production.","correct":true},{"id":"51e87b2dbdfcb476b026779380119b06","text":"A freshly restored EBS Volume cannot utilize EBS Optimization Instances straight away, as the network traffic and EBS traffic traverse the same 10-gigabit network interface. Only after the entire volume is scanned by an asynchronous process, EBS Optimization kicks in. This increases the I/O latency until the volume is ready to utilize EBS Optimization. To fix this, update the restoration process to wait and run random I/O tests on a freshly restored EBS Volume. Put the instance back to production only after the desired I/O levels are reached.","correct":false},{"id":"d3a1c3b2669127dabe2eaf2c490fcd30","text":"A freshly restored EBS Volume needs pre-warming to activate the inbuilt caching mechanism. To fix this, update the restoration process to run the set-up-cache command on the freshly restored EBS Volume first before the instance is put back in production. Also, include random I/O tests to ensure that desired I/O levels are reached before putting the instance back to production.","correct":false}]},{"id":"2b66bd04-756d-4e2f-a628-1e9b76a57066","domain":"awscsapro-domain2","question":"A media company is producing a live streaming video broadcast of a sporting event for a customer. The announcers will be delivering play-by-play analysis in English. The broadcast will be aired over the Internet, and will require real-time subtitles in Spanish. The company has decided to run all aspects of the production on AWS. Which architecture will provide the functionality needed to deliver the broadcast with real-time subtitles?","explanation":"AWS Elemental MediaLive is a broadcast-grade live video processing service that lets you create high-quality video streams for delivery to televisions and internet-connected devices. Storing it's output in S3 can trigger a Lambda function to extract the unsigned PCM audio from the video segments. A second Lambda function can use Amazon Transcribe to convert the audio to text, which can then be run through Amazon Translate to create the Spanish subtitles. The first Lambda function can send the Spanish subtitle files, the manifests, and the video files to AWS Elemental MediaPackage for distribution through CloudFront. AWS Elemental MediaTailor is used to insert targeted advertising into video streams, not enhance video with subtitles. Amazon Comprehend provides text sentiment analysis, not speech to text conversion.","links":[{"url":"https://aws.amazon.com/medialive/","title":"AWS Elemental MediaLive"},{"url":"https://aws.amazon.com/transcribe/","title":"AWS Transcribe"},{"url":"https://aws.amazon.com/translate/","title":"AWS Translate"},{"url":"https://aws.amazon.com/mediapackage/","title":"AWS Elemental MediaPackage"},{"url":"https://aws.amazon.com/solutions/live-streaming-with-automated-multi-language-subtitling/?did=sl_card&trk=sl_card","title":"Live Streaming with Automated Multi-Language Subtitling"}],"answers":[{"id":"1cc4dd5872e2deead228ed9c6651c445","text":"Deliver the live video to AWS Kinesis Data Streams and make Amazon S3 the consumer. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Invoke a second Lambda function, which uses Amazon Comprehend to convert the audio files to text and return the text to the first Lambda function. Have the first Lambda function use Amazon Translate to create the Spanish transcript. Send the subtitle files, manifests, and video files to AWS Elemental MediaPackage. Create an Amazon CloudFront distribution with MediaPackage as its origin.","correct":false},{"id":"58eec57699d2f82a27ad049e949eb09e","text":"Send the live video to AWS Elemental MediaLive and store it's output in Amazon S3. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Have the Lambda function call Amazon Transcribe to convert the audio files to text, and then use Amazon Translate to create the Spanish transcript. Use AWS Elemental MediaTailor to insert the subtitles into the video segments. Send the video files to AWS Elemental MediaStore. Create an Amazon CloudFront distribution with MediaStore as its origin.","correct":false},{"id":"2d6eb42c2e56159dbe9d0eb1c06b9681","text":"Transmit the live video to AWS Elemental MediaLive and deliver it's output to Amazon Kinesis Video Streams. Configure S3 and Amazon Comprehend as consumers of the stream. Have Comprehend write the text files to a different S3 bucket than the video files, and trigger a Lambda function on that bucket to have Amazon Translate create the Spanish transcripts. Use AWS Elemental MediaTailor to insert the subtitles into the video segments. Send the video files to AWS Elemental MediaStore. Create an Amazon CloudFront distribution with MediaStore as its origin.","correct":false},{"id":"a8d998390ff1bff69d4a734c8c8747e6","text":"Feed the live video into AWS Elemental MediaLive and deliver it's output to Amazon S3. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Invoke a second Lambda function, which uses Amazon Transcribe to convert the audio files to text and return the text to the first Lambda function. Have the first Lambda function use Amazon Translate to create the Spanish transcript. Send the subtitle files, manifests, and video files to AWS Elemental MediaPackage. Create an Amazon CloudFront distribution with MediaPackage as its origin.","correct":true}]},{"id":"90c0c2e2-4a39-4731-80b8-5f4d64c3d896","domain":"awscsapro-domain2","question":"You have been asked to design a landscape that can facilitate the upload very high resolution photos from mobile devices, gather metadata on objects in the photos and store that metadata for analysis.  Which of the following components would you use for this use-case for quickest implementation and best scalability?","explanation":"DynamoDB and S3 represent the most reasonable and scalable choices in this list for metadata storage (DynamoDB) and file upload (S3).  Kinesis has size limits on the inbound object so it would not be appropriate for use cases that involve potentially large files like photos.  Amazon Rekognition is image processing service that can extract metadata on objects in a photograph.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition – Video and Image - AWS"}],"answers":[{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true},{"id":"6fa977a31b941055e5cc04cc2000fb84","text":"Rekognition","correct":true},{"id":"803b0d23fbee2cf79d83376ef09a3eee","text":"Polly","correct":false},{"id":"8d4c0b2cef256d21ab680366c8b1c6bf","text":"EMR","correct":false},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true},{"id":"594025cae6dfa6b9073dc25de93ddb56","text":"Kinesis","correct":false}]},{"id":"11c28d09-1ccf-46ac-a56e-3998bff9c4e4","domain":"awscsapro-domain2","question":"You bought a domain name \"example.com\" from GoDaddy which is a domain registrar and the domain name will expire in several months. You plan to start using AWS Route 53 to manage the domain and resolve its DNS queries. The transferred domain in Route 53 should be automatically renewed every year so that the domain name will never expire and you do not need to renew it manually. Which method would you use to transfer the domain name properly?","explanation":"Users can register their new domain names in Route 53 or transfer existing domain names from other registrars such as GoDaddy to Route 53. After the transfer, the domain can automatically renew every year if the Auto Renew feature is enabled. To transfer the domain name, you do not need to wait until the domain name expires. And you cannot register the same domain name in both GoDaddy and Route 53 at the same time.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-transfer-to-route-53.html","title":"Transferring Registration for a Domain to Amazon Route 53"}],"answers":[{"id":"e27ac2c31c859905f6a51f22ab3a34bf","text":"Confirm that the domain is transferable in GoDaddy. In the Route 53 AWS Management Console, click \"Transfer Domain\" to transfer registration for the domain name from GoDaddy to Route 53. Enable the automatic renewal for this domain name.","correct":true},{"id":"d8a7c0a25d22a5534a30c668de1be1d1","text":"Register the same domain name \"example.com\" in Route 53 three months before it expires in GoDaddy. Enable the feature of Transfer Lock in Route 53 to prevent it from being transferred to another registrar. Do not renew the original domain name in GoDaddy.","correct":false},{"id":"32c733a4bf01f533d38704f340cb7eb5","text":"Login in the GoDaddy admin account, unlock the domain transfer and request the domain transfer to Route 53. Accept the domain transfer in Route 53 and extend the expiration date to 10 years as transferred domains cannot automatically renew.","correct":false},{"id":"bd0fd22629d8016f6941db326aeb4ba4","text":"Wait until the domain name expires in GoDaddy and then register the domain in AWS Route 53 by clicking the \"Register Domain\" button in AWS management console. Turn on the features of Auto Renew and Transfer Lock for the new domain.","correct":false}]},{"id":"489eea6e-bdaa-42a9-a80c-bfd209129fda","domain":"awscsapro-domain5","question":"You are working in a site reliability engineering team. There are dozens of EC2 instances in production that your team needs to maintain. When issues happen and online troubleshooting is required, the team needs to connect to a bastion host in order to login into an EC2 instance. You want to use the AWS Session Manager in Systems Manager to bypass the bastion host when accessing the instances. Which benefits can Session Manager bring to the team?","explanation":"Session Manager is a service in AWS Systems Manager that lets you access and manage your Amazon EC2 instances. Session Manager offers several benefits to the organization. With Session Manager, no SSH ports need to be open and no SSH keys are required. CloudTrail can also capture information about the Session Manager activities. However, the System Manager agent must be installed for Session Manager. For the Session Manager connections, you can choose to encrypt them with a CMK in KMS instead of an SSH key. And you should associate the IAM policies with IAM entities as an IAM policy cannot be attached in Session Manager.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html","title":"AWS Systems Manager Session Manager"}],"answers":[{"id":"8660dde753cd22508fc49e487f99b4dd","text":"Session Manager provides highly secure connections via encryptions in transit. You can choose to create an SSH key pair and use the private key to encrypt the connections.","correct":false},{"id":"1a123e85219205112eef2029de78d167","text":"AWS Session Manager handles the connections to EC2 instances and provides an interactive on-click shell. No agents need to be installed in the EC2 instances.","correct":false},{"id":"3166b29d289b61b2a127864f52105929","text":"Session Manager integrates with IAM and you can attach an IAM policy in Session Manager to control who can initiate sessions to instances.","correct":false},{"id":"e097db56681740bcf0785fb6555a2a2d","text":"The Session Manager API calls made in the AWS account can be tracked by AWS CloudTrail. You can have a record of the connections made to the instances.","correct":true},{"id":"77650e8a40264f564a70afe948a6b602","text":"You do not need to open inbound SSH ports or PowerShell ports for the remote connections and no SSH keys are required for the connections with Session Manager.","correct":true}]},{"id":"f3fef147-7b9d-45b6-8b2b-d943c90e8920","domain":"awscsapro-domain5","question":"You are assisting a company in the migration of their container-based web landscape over to Amazon.  They have a total of 21 containers which comprise their DEV, QA and Production environments.  All environment are identical in design and size.  Each environment consists of 3 web servers, 3 app servers and 1 datastore server.  Given the landscape, which of the provided options would be best for them to minimize maintenance?","explanation":"Deploying containers via ECS is a good option but we would want to use the EC2 hosted path.  Fargate is generally used for transient workloads and our datastore would be something we'd want to persist.  We might be able to deploy the data store with RDS, but the question does not make it clear if the data store is an RDS-supported database.  It could be a NoSQL data store or some other database unsupported by RDS.  Similarly, a MEAN stack under Elastic Beanstalk might not be compatible with our landscape either.","links":[{"url":"https://aws.amazon.com/ecs/resources/","title":"Resources for Amazon ECS - run containers in production"}],"answers":[{"id":"f05469f4c7578263f4271e7514c338ef","text":"Deploy the web and app servers in each environment using ECS.  Provision an RDS instance for each environment.  Use AWS Systems Manager to provide a common management console.","correct":false},{"id":"619957021a43a829fbb6228467323ca1","text":"Deploy the web, app and database servers using ECS on EC2.  Purchase 1-year reserved instance contracts for the required EC2 instances.","correct":true},{"id":"c1354e6d48fedccbf7b4e9c18854d980","text":"Redeploy the web landscape on a MEAN stack under Elastic Beanstalk, making use of auto-scaling groups to right-size the respective environments.  ","correct":false},{"id":"e0ea997f77cb156d35ec716cf772c49c","text":"Deploy the web, app and database containers using ECS.  Make use of Fargate for the underlying ECS infrastructure.","correct":false}]},{"id":"63da01c2-9c4d-4abd-a482-06ede2baf728","domain":"awscsapro-domain2","question":"A popular royalty free photography website has decided to run their business on AWS. They receive hundreds of images from photographers each week to be included in their catalog. Amazon S3 has been selected as the image repository. As the business has grown, the task of creating catalog entries manually has become unsustainable. They'd like to automate the process and store the catalog information in Amazon DynamoDB. Which architecture will provide the most scalable solution for automatically adding content to their image catalog going forward?","explanation":"Calling the S3 API to upload the images will suffice for this use case. Streaming ingest is not needed for this volume of data. AWS Step Functions will orchestrate the process of discovering both the image metadata with a Lambda function and the image object data with Rekognition. Rekognition will not return the image metadata. AWS Elemental MediaStore is used for originating and storing video assets for live or on-demand media workflows, not image recognition. Kinesis Video Analytics is not a currently supported service.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://github.com/aws-samples/lambda-refarch-imagerecognition","title":"Serverless Reference Architecture: Image Recognition and Processing Backend"}],"answers":[{"id":"0710b6a1f2a807da2cfc1a940e7014e9","text":"Deploy Amazon Kinesis Data Streams to ingest the images with two consumers. Setup Amazon Kinesis Firehose as the first consumer to deposit the images into S3. Configure Amazon Kinesis Video Analytics as the second consumer to extract the image's metadata and object information. Invoke a Lambda function to store the discovered information in DynamoDB.","correct":false},{"id":"42367a003a702450700b58798073122b","text":"Deploy Amazon Kinesis Data Firehose to ingest images into S3. Invoke a Lambda function to pass the image's S3 key to Amazon Rekognition, which will extract the image metadata and detect objects in the image. Invoke a Lambda function to store the discovered data in DynamoDB.","correct":false},{"id":"f98112f4f94f4a8d66edee560976acc2","text":"Programmatically call the S3 API to upload the images. Trigger an AWS Lambda function to send the image's S3 key to AWS Elemental MediaStore, which will extract the image's metadata, discover image patterns through machine learning, and deposit artifacts back into S3. Invoke a Lambda function to write the artifact data to DynamoDB.","correct":false},{"id":"dd8565c85d19c2867d3a6768f512b404","text":"Programmatically call the S3 API to upload the images. Trigger an AWS Lambda function to kick off execution of a state machine in AWS Step Functions. Create state machine sub-steps to invoke Lambda functions which extract image metadata, detect objects in the image with Amazon Rekognition, and store the discovered data in DynamoDB.","correct":true}]},{"id":"baf2349f-71ba-4583-bfe6-31fb5a555bbd","domain":"awscsapro-domain5","question":"The information security group at your company has implemented an automated approach to checking Amazon S3 object integrity for compliance reasons. The solution consists of scripts that launch an AWS Step Functions state machine to invoke AWS Lambda functions. These Lambda functions will retrieve an S3 object, compute its checksum, and validate the computed checksum against the entity tag checksum returned with the S3 object. However, an unexpected number of S3 objects are failing the integrity check. You discover the issue is with objects that where uploaded with S3 multipart upload. What would you recommend that the security group do to resolve this issue?","explanation":"For S3 objects, the entity tag (or ETag) contains an MD5 hash of the object in most cases. But if an object is created by either the Multipart Upload or Part Copy operation, the ETag is not an MD5 digest of the object. The ETag value returned by S3 for objects uploaded using the multipart upload API is computed differently than for objects uploaded with PUT object, and does not represent the MD5 of the object data. The checksum for an object created via multipart upload can be stored in a custom metadata parameter for later integrity checks. The Content-MD5 metadata parameter can not be modified by a user after the object has been created. The complete-multipart-upload API does not have an md5-rehash parameter. The list-multipart-uploads API will only return information about the multipart upload while the upload is running.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html","title":"Multipart Upload Overview"},{"url":"https://aws.amazon.com/solutions/serverless-fixity-for-digital-preservation-compliance/?did=sl_card&trk=sl_card","title":"Serverless Fixity for Digital Preservation Compliance"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/s3-multipart-upload-cli/","title":"How do I use the AWS CLI to perform a multipart upload of a file to Amazon S3?"}],"answers":[{"id":"fe2f446e0e84bb45be4130b327b8b47b","text":"When performing S3 multipart uploads, calculate the checksum of the source file and store it in a custom metadata parameter. Have the Lambda function that compares checksums use the custom metadata parameter if it's present instead of the entity tag checksum. Reload all objects that were written with multipart upload that need to be included in the integrity check.","correct":true},{"id":"df361f3568504f0db56ea3faddc6928a","text":"For all S3 objects created with multipart upload, retrieve the object, compute it's checksum, and store the value in the Content-MD5 metadata parameter. Have the Lambda function that validates checksums use the Content-MD5 metadata parameter if it's present instead of the entity tag checksum.","correct":false},{"id":"2a27b97da81e5d7f237719319a3d85fc","text":"When performing S3 multipart uploads, after all upload-parts API calls have been made, call the complete-multipart-upload API and include the md5-rehash parameter to reset the entity tag checksum to the sum of the parts. Reload all objects that were written with S3 multipart upload that need to be included in the integrity check.","correct":false},{"id":"15ca1d17e48531d75eda4298140756e6","text":"In the Lambda function that retrieves the objects, if the object was created with a multipart upload, call the list-multipart-uploads API and retrieve each part of the multipart upload along with the entire object. In the Lambda function that computes checksums, compute the checksum of each part along with the checksum of the entire object. In the Lambda function that validates checksums, compare the sum of the checksum parts to the checksum for the entire object.","correct":false}]},{"id":"d8b88385-e15f-4313-bc53-e9fb82f89cc3","domain":"awscsapro-domain2","question":"You are developing an application to be hosted on EC2. The application uses some environmental configuration data and other necessary parameters when running. For example, the application needs to get the correct username and password in order to communicate with a RDS database. You want to find a free AWS service to store these parameters. To meet security requirements, these stored parameters must be encrypted by the AWS Key Management Service. Which of the following methods is the best?","explanation":"You can manage parameters in Systems Manager Parameter Store. The type of the parameters must be SecureString so that they are encrypted by KMS. Parameter Store has standard tier and advanced tier. In this scenario, standard tier is enough and advanced tier is not a free service. AWS Secrets Manager does not have the concept of standard or secure parameter. It also charges you per secret per month.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html","title":"How AWS Systems Manager Parameter Store uses AWS KMS"}],"answers":[{"id":"4f357d94d3f20ee0d0c3c7c892ef0d70","text":"Create standard parameters in AWS Secrets Manager. Use the advanced tier as it uses envelope encryption to encrypt the stored parameters with KMS. You can also configure Secrets Manager to rotate the stored secrets or API keys automatically.","correct":false},{"id":"3295f034025fe6d21155d228ee3dd0a2","text":"Store secure string parameters in AWS Systems Manager Parameter Store so that the parameters are encrypted by KMS. Use the standard tier as there is no additional charge for it. Use AWS Encryption SDK in the application to get the parameters.","correct":true},{"id":"41e6f55938cd5c32278c6f4b6708641e","text":"Create standard string parameters in AWS Systems Manager Parameter Store as it is a free service. The parameters are automatically encrypted with envelope encryption by the default AWS managed key (aws/ssm) in KMS. Use AWS Encryption SDK in the application to fetch the parameters.","correct":false},{"id":"c08527f2eeacd9f56133d1dfed701f1a","text":"Create secure parameters in AWS Secrets Manager. Secrets Manager protects secrets by integrating with KMS and all stored parameters are automatically encrypted by the AWS managed key \"aws/secretsmanager\". You can also configure Secrets Manager to rotate the secrets.","correct":false}]},{"id":"95f1d7a8-c3d4-4fec-952a-72385aa8b4c8","domain":"awscsapro-domain5","question":"You are consulting for a company that performs specialized customer data analytics.  Their customers can upload raw customer data to a website and receive back demographic statistics.  Their application consists of a REST API created using PHP and Apache.  The application is self-contained and works in real-time to return results as a JSON response to the REST API call.  Because there is customer data involved, company policy states that data must be encrypted in transit and at rest.  Sometimes, there are data quality issues and the PHP application will throw an error.  The company wants to be notified immediately when this occurs so they can proactively reach out to the customer.  Additionally, many of the company's customers use very old mainframe systems that can only access internet resources using IP address rather than a FQDN.  Which architecture will meet these requirements fully?","explanation":"The requirement of a static IP leads us to a Network Load Balancer with an EIP.","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/","title":"Elastic Load Balancing Features"}],"answers":[{"id":"434ce04b2c3a4d2e679d37df43de2585","text":"Provision an Application Load Balancer with an EIP in front of your EC2 target group and terminate SSL at the ALB.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"e53df806e37b325d7f61be27772875f1","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Enabled SSL termination on the API Gateway using Certificate Manager.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"0c8e5c32f081b0484e86b71651ae3642","text":"Provision a Network Load Balancer in front of your EC2 target group and terminate SSL at the load balancer using Certificate Manager.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"569eec0061a1a97be77e3bdab43a1756","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Offload SSL termination using AWS KMS.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"5e4c5230c7e08202a0ea0575d5412d57","text":"Provision a Network Load Balancer with an EIP in front of your EC2 target group.  Install the CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.  Terminate SSL on the EC2 instances.","correct":true},{"id":"a9ed133e35b8332aea2bf603521b891a","text":"Provision an Application Load Balancer in front of your EC2 target group and offload SSL to CloudHSM.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch and configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from CloudHSM.","correct":false}]},{"id":"dac73d1f-8c64-48b1-90be-3432e789933d","domain":"awscsapro-domain2","question":"Your company is bringing to market a new Windows-based application for Computer Aided Manufacturing.  As part of the promotion campaign, you want to allow users an opportunity to try the software without having to purchase it.  The software is quite complex and requires specialized drivers so it's not conducive to allowing the public to download and install in their own systems.  Rather you want to control the installation and configuration.  Therefore, you want something such as a VDI concept.  You'll also need to have a landing page as well as a custom subdomain (demo.company.com) and limit users to 1 hour of use at a time to contain costs.  Which of the following would you recommend to minimize cost and complexity?","explanation":"AppStream is a way to deploy an application on a virtual desktop and allow anyone with a browser to use the application.  This is the most efficient and simplest option given the other choices.","links":[{"url":"https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html","title":"What Is Amazon AppStream 2.0? - Amazon AppStream 2.0"}],"answers":[{"id":"9b85d56999fa276cfa4a01df8195700c","text":"Create a landing page in HTML and deploy to an S3 bucket configured as a Static Web Host.  Embed in the HTML a Javascript-based RDP client that is downloaded with the webpage.  Create a CloudFront distribution with the S3 bucket as the origin.  Use an S3 Event to launch a Lambda function which starts up an EC2 instance with your golden AMI.  Once the instance is up and running, use a web socket call from the Lambda function to initiate the RDP client and log the user in.  After 1 hour, have the Lambda function issue a shutdown command to the EC2 instance.","correct":false},{"id":"129d13b81f52a8ab01ce9e8ea7009289","text":"Create a landing page in HTML and deploy to an S3 bucket configured as a Static Web Host.  Use Route 53 to create a DNS record for the \"demo\" subdomain as alias record for the S3 bucket.  Deploy your application using Amazon AppStream.  Set Maximum Session Duration for 1 hour.","correct":true},{"id":"bdc9c29dcecb6611cd71c4660fc235ca","text":"Deploy your application as an app in the Workspaces Application Manager.  Spin up several Workspaces and configure them to automatically install your application via WAM. Create the landing page such that it redirects to the web client for Workspaces and deploy the landing page via S3 configured as a web host.  Use Route 53 to create a DNS record for the demo subdomain as an alias record for the S3 bucket.  Configure the Workspaces for a 1 hour timeout.  ","correct":false},{"id":"a1c67181ea5765383a7b477e821391b4","text":"Configure an EC2 auto scaling fleet of spot instances with your golden AMI.  Create security groups to allow inbound RDP for the auto scaling group.  Deploy Apache Guacamole on an EC2 instance and place your landing page in its web server directory.  Use Guacamole to provide an RDP session into one of the EC2 instances directly in the users browser.  Use AWS Batch to reboot the EC2 instances after 1 hour of runtime.  ","correct":false}]},{"id":"c2d46981-3dac-4e68-81d1-9eedf0cbf264","domain":"awscsapro-domain2","question":"Your company is preparing a special event for its 100th year in business.  As part of that event, the event committee would like to create a kiosk where employees can browse the thousands of photographs captured over the years of the employees and company events.  In a brainstorming session, one event staff member suggests the crazy idea of allowing employees to quickly pull up photographs which they appear in.  What AWS service might be able to make this a reality?","explanation":"AWS Rekognition is a service that can detect and match faces in a photograph.  The kiosk could include a camera that allows event-goers to snap a picture of themselves and then it could scan the photo archive for facial matches.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition – Video and Image - AWS"}],"answers":[{"id":"ade161cea9b509c72570ba6ae5238e5f","text":"Kinesis for Video","correct":false},{"id":"cbe32e918d6a248e3e8ed74e6f0b72f6","text":"AWS Comprehend","correct":false},{"id":"438e06c02cba48ce4ffbf026d97488b4","text":"Amazon DeepView","correct":false},{"id":"42e6f83b4b8205e6c2e62ddafdd3bbe3","text":"Amazon Chime","correct":false},{"id":"07c025c347c7483abdd039cd36be4220","text":"AWS Rekognition","correct":true}]},{"id":"6dc7fe81-03aa-45d6-b8e1-6dc3b70914e0","domain":"awscsapro-domain1","question":"A company owns multiple AWS accounts managed in an AWS Organization. You need to generate daily cost and usage reports that include the activities of all the member accounts. The reports should track the AWS usage for each resource type and provide estimated charges. The report files also need to be delivered to an Amazon S3 bucket for storage. How would you create the required reports?","explanation":"The consolidated billing feature in AWS Organization does not generate billing reports automatically. You need to configure the AWS Cost and Usage Reports in the master account and use an S3 bucket to store the reports. The generated reports include activities for all the member accounts and it is not required to create a report in each member's account. The option of CloudWatch Event rule and Lambda function may work however it is not a straightforward solution.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-reports-costusage.html","title":"AWS Cost and Usage Report"}],"answers":[{"id":"149237c5674e21794204a4a0ca00bee2","text":"Create a CloudWatch Event rule that runs every day. Register a Lambda function target which calls the PutReportDefinition API to get cost reports of all AWS accounts and store them in an S3 bucket.","correct":false},{"id":"967b7874a080c033470777ce955a4550","text":"In the master account of the AWS Organization, generate the AWS Cost and Usage Reports and save the reports in an S3 bucket. Modify the bucket policy to allow the billing reports service to put objects.","correct":true},{"id":"b66bf1016802f0edb247437b5fda31cb","text":"Login in each AWS account using the root IAM user, configure the daily Cost and Usage Report and set up a central S3 bucket to save the reports from all AWS accounts. Store the reports in different folders in the S3 bucket.","correct":false},{"id":"033f176fcb8f66b1ee9fb950c8741cda","text":"Enable the consolidated billing feature in the AWS Organization which automatically generates a daily billing report. Predefine an S3 bucket to store the reports. Make sure the S3 bucket has a bucket policy to allow the AWS Organization service to write files.","correct":false}]},{"id":"49f16801-2cc1-48c8-a517-f9192f516318","domain":"awscsapro-domain3","question":"A tire manufacturing company needs to migrate a .NET simulation application to the AWS cloud. The application runs on a single Windows Application Server in their datacentre. It reads large quantities of data from local disks that are attached to the on-premises Application Server. The output from the application is small in size and posted in a queue for downstream processing. On the upstream side, the data acting as the input for the .NET simulation app is generated in the tire-testing Lab during the daytime by processes running on a Linux Lab Server. This data is then copied from the Linux Lab Server to the Windows Application Servers by a nightly process that also runs on the Linux Lab Server. This nightly process mounts the Application Server disks using a Samba client, this is made possible by the Application Server also acting as a Windows File Share Server. When the nightly process runs, it overwrites the input data from last night because of disk space constraint on the Application Server. This is undesirable as the data is permanently lost on a daily basis.\nThe migration is being undertaken because the .NET simulation application needs more CPU and RAM. The company does not want to spend on expensive hardware any more. However, the nightly process is not migrating, nor is the Linux Lab Server. The code of the simulation applications, as well as the nightly process, may change a little as a result of the migration, but leadership wants to keep these changes to a minimum. They also want to stop losing the daily test data and keep it somewhere for possible analytical processing later on.\nAs the AWS architect hired to shepherd this migration and many more possible migrations in the future, which of the following architectures would you choose as the best one, considering the minimization of code changes as the topmost goal, followed by cost-effectiveness as the second but important priority? The data has no security requirement.","explanation":"The two parts of this question are - (a) Do I use EFS or EBS for storing the data from the Lab Servers? (b) Do I copy data from each night to S3 using a NAT Gateway (thereby using the public internet) or a VPC Endpoint (thereby using the private network to copy)?\nThe answer to the first question is EBS because Windows EC2 instances cannot mount EFS, as EFS only supports Linux EC2 instances.\nThe answer to the second question is VPC Endpoint because NAT Gateways are very costly - they are charged 24-7 for just running, in addition to having data transfer rates. S3 VPC Endpoints are a cost-effective mechanism to copy data to S3. Note that the S3 put-object cost will be the same for both cases. The question tries to distract the candidate by stating that there is no security requirement, trying to confuse the candidate into selecting NAT Gateway in case they perceive the only distinction between NAT Gateway and S3 VPC Endpoint to be the usage of public network versus private.\nThis is an example of highly verbose question describing a complex scenario. There will definitely be quite a few such questions in the AWS SA-P exam that are challenging in terms of time management. You may use vertical scanning of the answer choices to spot the differences first. That way, you can focus on determining which of the variations is correct because you would know what is different between them.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html","title":"EFS Support for Windows EC2 Instances"},{"url":"https://aws.amazon.com/vpc/pricing/","title":"Search for NAT Gateway Pricing here"}],"answers":[{"id":"fabdb7e05e5bb1c78dd6e166134202ca","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"220d73c7cceb53661a402d59038118ee","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"c7311a2612bfc63a966317d468a5b4dd","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"3ae5ae41663ec801882e10e7fa394613","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":true}]},{"id":"e24f7f76-6908-4ad9-820a-11790bfdcec6","domain":"awscsapro-domain3","question":"You are helping a client migrate over an internal application from on-prem to AWS.  The application landscape on AWS will consist of a fleet of EC2 instances behind an Application Load Balancer.  The application client is an in-house custom application that communicates to the server via HTTPS and is used by around 40,000 users globally across several business units.  The same exact application and landscape will be deployed in US-WEST-2 as well as EU-CENTRAL-1.  Route 53 will then be used to redirect users to the closest region.  When the application was originally built, they chose to use a self-signed 2048-bit RSA X.509 certificate (SSL/TLS server certificate) and embedded the self-signed certificate information into the in-house custom client application.  Regarding the SSL certificate, which activities are both feasible and minimize extra administrative work?","explanation":"You can import private certificates into Certificate Manager and assign them to all the same resources you can with generated certificates, including an ALB.  Also note that Certificate Manager is a regional service so certificates must be imported in each region where they will be used.  The other options in this question would either require you to update the certificate on the client or requires unnecessary steps to resolve the challenge.","links":[{"url":"https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html","title":"Importing Certificates into AWS Certificate Manager - AWS Certificate  Manager"}],"answers":[{"id":"0663551d15f5b15af587ac8bf75a2566","text":"Purchase a new public SSL/TLS certificate from a third-party CA.  Upload the certificate to Certificate Manager and assign that certificate to the Application Load Balancers.","correct":false},{"id":"28694bd7f280694a43741563f6933ad6","text":"Create a new public SSL/TLS certificate using Certificate Manager and configure the common name and OU to match the existing certificate.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false},{"id":"111997579381183b07a22fad8574e76c","text":"Create a new Certificate Authority within Certificate Manager and import the existing certificate.  Generate a new certificate, CA chain and private key and push an update for the application.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false},{"id":"d4976a6c33ee189e6b681dffc83cbac5","text":"Import the existing certificate and private key into Certificate Manager in both regions.  Assign that imported certificate to the Application Load Balancers using their respective regionally imported certificate.","correct":true},{"id":"0c4320d1dd787a5bae2b43479b645d94","text":"Use Service Catalog to push an update of the in-house app which includes an updated certificate and CA chain.  Generate a new private certificate using OpenSSL. Import the new certificate to Certificate Manager in US-EAST-1.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false}]},{"id":"abc17e7b-6b75-45e3-a5c0-ea0f55d4df97","domain":"awscsapro-domain2","question":"Your client is a software company starting their initial architecture steps for their new multi-tenant CRM application.  They are concerned about responsiveness for companies with employees scattered around the globe.  Which of the following ideas should you suggest to help with the overall latency of the application?","explanation":"CloudFront can cache both static and dynamic content.  By setting a high TTL, we allow CloudFront to serve content longer before having to refresh from the origin.  Additionally, Lambda@Edge can intercept the request and direct the requester to a region based on the geographic origin of the request.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/11/lambda-at-edge-now-supports-content-based-dynamic-origin-selection-network-calls-from-viewer-events-and-advanced-response-generation/","title":"Lambda@Edge Now Supports Content-Based Dynamic Origin Selection, Network  Calls from Viewer Events, and Advanced Response Generation"}],"answers":[{"id":"35483961564002569ee69763e24961fa","text":"Install the application in several regions around the globe.  As new customers and users are on-boarded, pre-cache their user data in CloudFront for that region.  Use AWS Batch to routinely expire the cache to ensure the latest updates are visible.","correct":false},{"id":"0fc3951d630f285646b22cdd30f43eee","text":"Architect the system to use as many static objects as possible with high TTL.  Use CloudFront to retrieve both static and dynamic objects.  POST and PUT new data through CloudFront.","correct":true},{"id":"35638855dc45f62b3801906fd9a6d87c","text":"Install key parts of the application in multiple AWS regions chosen to balance latency for geographically diverse users.  Use Lambda@Edge to dynamically select the appropriate region based on the users location.","correct":true},{"id":"702a6122527d0830134717e0e7323bd0","text":"Store the data in a DynamoDB Global Table.  Use an auto scaling ElastiCache cluster with Memcached as a caching layer.  Distribute static elements of the application via CloudFront.  Use Route 53 Weighted routing to dynamically route users to the nearest region.","correct":false},{"id":"afa9743126cd2b0644654f2439a2aa0a","text":"Install the application on several regions around the globe.  Use RDS cross-region read replication for PostgreSQL to ensure a strongly consistent data store.","correct":false}]},{"id":"4eb7884e-f6fd-4803-83bf-8ab6faca4dd5","domain":"awscsapro-domain1","question":"You have been asked to give employees the simplest way of accessing the corporate intranet and other internal resources, from their iPhone or iPad.  The solution should allow access via a Web browser, authentication via SAML integration and you need to ensure that no corporate data is cached on their device. Which option would meet all of these requirements?","explanation":"Amazon WorkLink is a fully managed, cloud-based service that enables secure access to internal websites and apps from mobile devices. It provides single URL access to the applications and also links to existing SAML-based identity providers.  Amazon WorkLink does not store or cache data on user devices as the web content is rendered in AWS and sent to user devices as encrypted Scalable Vector Graphics (SVG).  WorkLink meets all of the requirements in the question and is therefore the only correct answer.","links":[{"url":"https://aws.amazon.com/worklink/faqs/","title":"Amazon WorkLink FAQs"}],"answers":[{"id":"0d0cb2140013a20863f643412ebd4698","text":"Tunnel through a Bastion Host into your VPC and view all internal servers via a Web Browser","correct":false},{"id":"8b3531ac066ba672af41cfd6c438fdb9","text":"Place all internal servers in a public subnet and lock down access via Security Groups to the IP address of each mobile user","correct":false},{"id":"668cfaeb2878db8e709660735f0ff009","text":"Configure Amazon WorkLink and connect to the servers using a Web Browser with the link provided","correct":true},{"id":"2ca6fccac916b71e240a465c8caf457e","text":"Connect into the VPC where the internal servers are located using Amazon Client VPN and view the sites using a Web Browser","correct":false}]},{"id":"1eb605a0-e0bc-4666-9fe9-aa249901bcb5","domain":"awscsapro-domain3","question":"Your company currently runs SharePoint as it's internal collaboration platform. It's hosted in the corporate data center on VMware ESXi virtual machines. To reduce costs, IT leadership has decided not to renew its VMware license agreement for the coming year. They've also decided on an AWS cloud-first approach going forward, and have ordered an AWS Direct Connect for connectivity back to the corporate network. On-premises Active Directory handles SharePoint authentication, and will continue to do so in the future. You've been tasked with determining the best way to deliver SharePoint to the company's users after the VMware agreement expires. How will you architect the solution in a cost effective and operationally efficient way?","explanation":"Deploying SharePoint Web Front Ends in separate Availability Zones behind a Network Load Balancer, with SharePoint App Servers in those same subnets, provides a highly reliable solution. RDS SQL Server supports Always On Availability Groups. Since RDS is a managed service, operational efficiency is achieved. Amazon Workspaces also provides managed service benefits for remote desktops, and gives the company the opportunity to have users use lower cost hardware. It can all be authenticated through an AWS Managed AD trust relationship with the on-premises Active Directory. The Managed AD managed service provides better operational efficiency than creating Domain Controllers on EC2. Introducing VMware Cloud on AWS for the database layer results in more networking complexity, and is not necessary since RDS supports Always On clusters. Remote Desktop Gateways will require higher cost end-user hardware.","links":[{"url":"https://d1.awsstatic.com/VMwareCloudonAWS/SharePoint-Hybrid_Reference-Architecture.pdf?did=wp_card&trk=wp_card","title":"SharePoint Reference Architecture - AWS and VMware Cloud on AWS"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html","title":"Multi-AZ Deployments for Microsoft SQL Server"},{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_microsoft_ad.html","title":"AWS Managed Microsoft AD"}],"answers":[{"id":"1ff58ba530d63513506d1a8a58cb91b9","text":"Deploy a Network Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Run an Amazon RDS SQL Server Always On cluster. Use AWS Directory Service Managed AD for authentication in a trust relationship with the on-premises Active Directory. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":true},{"id":"56ace4242f33e5150f0a40b3627f9568","text":"Use an Application Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers and SQL Server instances on EC2 instances in the same subnets as the SharePoint Web Front Ends. Configure the SQL Server instances as Always On clusters. Use AWS Directory Service AD Connector for authentication from the on-premises Active Directory. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":false},{"id":"140b5814920d29ba818858f73c97577b","text":"Implement a Network Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers, SQL Server instances, and Active Directory Domain Controllers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Configure the SQL Server instances as Always On clusters. Join the Domain Controllers to the on-premises AD forest. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":false},{"id":"c2b436befa546d5de6bf07d1d3eb3766","text":"Configure an Application Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Run SQL Server Always On clusters on VMware Cloud on AWS. Use AWS Directory Service AD Connector for authentication from the on-premises Active Directory. Implement Remote Desktop Gateways in each subnet to provide connectivity for Windows desktops.","correct":false}]},{"id":"6c770228-3fa1-4020-94a8-119c67a7e2d1","domain":"awscsapro-domain2","question":"You are volunteering with a local STEM (Science, Technology, Engineering and Math) program for youth.  You have decided that you'd like to help them learn about AWS by spinning up their very own WordPress site.  Given that the youth have no experience with AWS and the program, you want to choose the easiest way for students to spin up a simple webserver.  Which AWS technologies would you choose?","explanation":"AWS Lightsail is designed to be a very easy entry-level experience for those just starting out with virtual private servers.  A WordPress site can be deployed with literally a single click and does not require AWS Console access or knowledge of EC2 or VPCs.","links":[{"url":"https://aws.amazon.com/lightsail/","title":"Lightsail"}],"answers":[{"id":"cab27dc53edb571cac663ce2e16450dc","text":"AWS Marketplace","correct":false},{"id":"b21eea42e76007ac061cf37a5a41037d","text":"Lightsail","correct":true},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":false},{"id":"c8f63ecaff5e983a2441126a241c4cfa","text":"ECS","correct":false},{"id":"81b22456f78954c460ce2f531b5e048f","text":"EC2","correct":false},{"id":"d85c80578ad0849a611c1056b63e385c","text":"VPC","correct":false}]},{"id":"0ee4566a-508e-472d-9789-3318e3284aca","domain":"awscsapro-domain5","question":"You are an AWS Solutions Architect and you maintain a CloudFormation stack that includes the resources of a network load balancer and an Auto Scaling group. The ASG has one running instance. A developer uses the instance for feature development and testing. However, after he adds some configurations and restarts an application process, the instance is terminated by the Auto Scaling group and a new instance is created. The new configurations are lost in the new instance. You need to modify the resource settings to make sure that the instance is not terminated by the ASG when application processes are restarted. Which of the following methods would best achieve this?","explanation":"The instance fails the health check in the ELB target group and is then terminated by ASG whenever the application processes are restarted. The prevent the ASG from terminating the EC2 instance you need to modify the health check type from ELB to EC2. As a result, even if the instance fails the health check in the ELB target group, it will not be terminated by the Auto Scaling group. You do not need to create an AMI or a new launch configuration to address the issue. And the custom health check script that runs every minute cannot prevent the instance from being terminated.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health checks for Auto Scaling instances"}],"answers":[{"id":"1584209c598c68a956227b3770a97fb2","text":"Create an AMI and configure a new launch configuration with the AMI. Then modify the Auto Scaling group to use the new launch configuration and launch a new instance.","correct":false},{"id":"8fad414c494de200ee3990b334d22b13","text":"Update the CloudFormation script and modify the health check type from ELB to EC2.","correct":true},{"id":"c66bf36d77a2aa4cf5ef75a4ac494df8","text":"Store all custom configuration scripts in an S3 bucket and create a new launch configuration. In its user data section, download the scripts from the S3 bucket and execute them. Whenever a new instance is launched, the configurations can be installed automatically. ","correct":false},{"id":"745390da2b4433786bf4cdf17df3d2d3","text":"Edit a health check shell script that performs some sanity checks in the EC2 instance. If the sanity checks pass, the shell script uses AWS CLI “aws autoscaling set-instance-health” to set its status to be healthy. Run the script every minute.","correct":false}]},{"id":"2c034786-9b7e-4933-aad2-d0c4b1d89ca8","domain":"awscsapro-domain2","question":"A beach apparel company has begun an initiative to improve their sales analytics capabilities using AWS services. They'll need to be able to visualize summary sales data by product line, territory, and sales channel for each day, month, and year, and they'll need to be able to drill-down with ad-hoc queries on individual sales records. There are multiple data sources that provide transactional information in different formats. The company has chosen Amazon QuickSight as their visualization tool for the summary information. Visualizations and drill-down queries will require three years of rolling sales history, which estimates to seven petabytes of data. Which architecture will provide the best performance and cost efficiency?","explanation":"Using S3 to store the detailed sales transaction data and using Lambda to standardize data formats is the most cost effective option. Storing the summary data in Redshift provides a high performance option for reads from QuickSight, and keeping the detailed transaction data out of Redshift allows for smaller node sizes and lower cost. Amazon Redshift Spectrum can be used for drill-down queries that join tables from both Redshift and S3. For answer number two, Redshift will be a better option than Aurora for OLAP query performance due to it's columnar organization. Answer number four provides no simple way to perform ad-hoc drill down queries.","links":[{"url":"https://aws.amazon.com/redshift/","title":"Amazon Redshift"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"ebf4a6c248510d05a046c8f0ea4298b7","text":"Use Amazon Kinesis Data Analytics to format the data source transactions in a standard way and load it into Amazon Aurora. Invoke Lambda functions to aggregate the data and write it into summary tables in Aurora","correct":false},{"id":"a08e38f138d23c0f1759ab2d1801f67e","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Firehose and load them into Amazon Redshift. Run AWS Glue jobs to format the transaction data in a standard way and perform aggregate functions to write the data into summary tables in Redshift","correct":false},{"id":"4740b70569f15040edf0916e47386757","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Streams and write them to Amazon Elastic Block Store on EC2 instances in Auto Scaling Groups. Perform data format standardization and summary aggregation on EC2, and write the summary results to Amazon Redshift tables","correct":false},{"id":"b3deac265c2195cb988c345d096800fd","text":"Ingest individual sales transactions from each data source into Amazon S3 with Amazon Kineses Data Firehose. Trigger an AWS Lambda function to format the transaction data in a standard way and redeposit the results in S3. Run AWS Glue jobs to aggregate the summary data into Amazon Redshift","correct":true}]},{"id":"06504582-ce03-4252-b1dc-29654ff427bb","domain":"awscsapro-domain5","question":"You have just set up a Service Catalog portfolio and collection of products for your users.  Unfortunately, the users are having difficulty launching one of the products and are getting \"access denied\" messages.  What could be the cause of this?","explanation":"For Service Catalog products to be successfully launched, either a launch constraint must be assigned and have sufficient permission to deploy the product or the user must have the same required permissions.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-launch.html","title":"AWS Service Catalog Launch Constraints - AWS Service Catalog"}],"answers":[{"id":"779fec840f0f81e772ba3137d7ac28ad","text":"The notification constraint did not have access to the S3 location for the product's CloudFront template.","correct":false},{"id":"a987914548e48bb64be70c11a97ec644","text":"A Service Catalog Policy has not yet been applied to the account.","correct":false},{"id":"d11a80651ecb668bdbe507d4e7398b6a","text":"The launch constraint does not have permissions to CloudFormation.","correct":true},{"id":"43d54acb2dbc9f2dc8a0d793553b965e","text":"The user launching the product does not have required permissions to launch the product.","correct":true},{"id":"8bc518c42ab2dfa122390f1a497349a2","text":"The template constraint assigned to the product does not have the proper permissions.","correct":false},{"id":"7e0083aafd999688d628f67e003d79be","text":"The product does not have a launch constraint assigned.","correct":true}]},{"id":"5f6d53a1-1b9c-46fe-9a8e-a5706e72914d","domain":"awscsapro-domain4","question":"Which of the following is an example of buffer-based approach to controlling costs?","explanation":"The buffer-based approach to controlling costs is discussed in the Cost Optimization Pillar of the AWS Well-Architected Framework.  A buffer is a mechanism to ensure that applications can communicate with each other when they are running at different rates over time.  By decoupling the throughput rate of a process, you can better govern and smooth demand--creating a less volatile and reactionary landscape.  As a result, costs can be reduced by optimizing for the steady state. ","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"5bc62ebd024ca5594793ca76f08cd960","text":"A public-facing API is created using API Gateway and Lambda.  As a serverless architecture, it scales seamlessly in step with demand.","correct":false},{"id":"878d4fde3965b2e5f84c543b2cca1dfc","text":"A production ERP landscape is scaled up during the month-end financial close period to provide some padding for the additional processing and reports so they do not impact the normal business processes.","correct":false},{"id":"f867e23e60a3917c1ebe5e2c4ced818c","text":"An auto-scaling fleet is created to dynamically adjust available compute resources based network connection events as reported by CloudWatch.","correct":false},{"id":"26437f21964d56c3e373f55d997101ed","text":"A mobile image upload and processing service makes use of SQS to smooth an erratic demand curve.","correct":true}]},{"id":"74aec97e-c092-4588-8da4-43dca3ddd0eb","domain":"awscsapro-domain5","question":"You are trying to help a customer figure out a puzzling issue they recently experienced during a Disaster Recovery Drill.  They wanted to test the failover capability of their Multi-AZ RDS instance.  They initiated a reboot with failover for the instance and expected only a short outage while the standby replica was promoted and the DNS path was updated.  Unfortunately after the failover, they could not reach the database from their on-prem network despite the database being in an \"Available\" state.  Only when they initiated a second reboot with failover were they again able to access the database.  What is the most likely cause for this?","explanation":"The routes for all subnets in an RDS subnet group for a Multi-AZ deployment should be the same to ensure all master and stand-by units can be reached in the event of a failover.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/avoid-route-table-issues-rds/","title":"Avoid route table issues RDS Multi-AZ fail over"}],"answers":[{"id":"1105b24fe7a3c1eb0a86d0fa9a3cdc62","text":"This was most likely an AWS error.  They should submit a support ticket with the RDS instance identifier and the approximate time of the failover test via the Support Center.","correct":false},{"id":"f637d9886a1ea7b52d608046e233e504","text":"The subnets in the subnet group did not have the same routing rules.  The standby subnet did not have a valid route back to the on-prem network so the database could not be reached despite being available.","correct":true},{"id":"46b9b5188d94533190c30de816355165","text":"They used the AWS Console to issue the reboot.  You can only force a failover of RDS by using the AWS CLI and adding the --force-failover parameter to the \"aws rds reboot-db-instance\" command.","correct":false},{"id":"1dbb40d7bdb88fbe54fead30b1bf5f12","text":"There was a lag in the state update on the AWS console showing \"Available\".  If they would have waited longer, it likely would have changed to \"Degraded\".  A failover can take 30 minutes or more because AWS automatically creates a snapshot before promoting the standby.","correct":false},{"id":"260ae29233a6047971894eff9d05fa55","text":"They initiated a failover with an IAM account that did not have sufficient rights to perform the reboot.  This resulted in an incomplete failover that was only corrected by executing the failover again to reset the DNS entries.","correct":false}]},{"id":"9fc0785a-d5cb-47e3-bc2f-829b5a36ba26","domain":"awscsapro-domain3","question":"You work for a Genomics company which has decided to migrate its DNA Sequencing application to the AWS Cloud. The application is containerized. Currently, container image A works on genomics data residing on an on-premises file server, validating the data and updating the metadata in a local database. When it is done, engineers manually trigger 100 or more instances of container image B that process this data in parallel by reading the metadata, creating output files. When all these container instances have done their job, engineers manually trigger container image C that validates the results, cleans up and sends notifications.\nThe CTO has decided to use S3 for storing the input and output data files. She has also mandated that the parallel processing phase should run on a fleet of Spot EC2 instances to reduce compute costs. She also wants to automate the workflow, so that engineers do not have to manually trigger the next set of actions. The requirement is to minimize administrative overhead and custom development for the migration.\nAs the AWS Architect, which of the following approaches should you recommend?","explanation":"AWS ECS does not natively provide workflow management. In an ECS service definition file, you cannot specify a sequence of tasks with execution dependencies such that one will be run only after the previous one completes. Hence, the two ECS choices are ruled out.\nDistraction warning - Fargate does not allow you to specify Spot instances as it is serverless in nature (it absolves you from specifying server details). This effectively creates a distraction - when the candidate rules out ECS Fargate due to this reason, they may be relieved to see the ECS EC2 choice and jump to a conclusion because it is relatively easy to remember that EC2 launch type actually lets you select Spot instances. However, this distraction is designed to take focus away from the fact that neither of these two choices is correct. Both of the choices require service definition files to set up execution workflows. Task instances mentioned in an ECS service definition file are executed in parallel - ECS does not control the sequence of tasks.\nAWS SWF does not let you specify Spot instances either. Also, SWF is usually used in cases where human intervention is needed in the workflow.\nThis leaves AWS Batch as the correct answer. AWS Batch is indeed the most suitable AWS service for this scenario as it meets all requirements.","links":[{"url":"https://docs.aws.amazon.com/batch/latest/userguide/create-compute-environment.html","title":"How to create a compute environment for AWS Batch"},{"url":"https://docs.aws.amazon.com/batch/latest/userguide/example_array_job.html","title":"Example AWS Batch Array Job Workflow"},{"url":"https://aws.amazon.com/ec2/spot/containers-for-less/get-started/","title":"How to run ECS clusters in EC2 Spot Instances"}],"answers":[{"id":"ceb4c03a526e8ddb01ada7a40bb60001","text":"Use AWS Batch, setting up an array job with 100 or more copies preceded by pre-requisite and follow-up jobs where the workflow is controlled by dependencies between jobs. Also, use Spot as the Provisioning Model for compute environment","correct":true},{"id":"49755d6c34da495b8c91964f52946d29","text":"Use AWS SWF workers and deciders to manage the workflow. Configure the workers to use EC2 Spot Instances","correct":false},{"id":"e46ada36d33a9e5b23aa37ee94c4c5d6","text":"Use AWS ECS with EC2 Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"757ddde350053553e44844d066c91386","text":"Use AWS ECS with Fargate Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false}]},{"id":"79f2f5be-b591-44e6-957b-eb0383640d7d","domain":"awscsapro-domain5","question":"You have a standard SQS queue to receive messages from the frontend application. The backend application is JAVA based and the AWS SDK is used to get the messages from the queue for processing. The SQS queue is not busy most of the time. According to the backend application logs, there is a high number of empty ReceiveMessageResponse instances returned. You want to adjust the settings to minimize the number of empty responses and reduce the cost. How would you implement this? ","explanation":"Amazon SQS long polling is preferable to short polling in most of the cases. Long polling requests let the consumers receive messages as soon as they arrive in the queue. It can help to reduce the number of empty responses. In order to enable long polling, the attribute ReceiveMessageWaitTimeSeconds should be more than 0. Short polling is incorrect. Visibility timeout and delivery delay do not address the problem of empty responses.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html","title":"Amazon SQS short and long polling"}],"answers":[{"id":"a5cdcd2968c3566cbb7fc7bcd5fef01a","text":"Add a delivery delay in the SQS queue such as 1 minute. The delay helps to postpone the delivery of new messages to the queue for some time. When the JAVA application polls the messages from the queue, there will be a lower chance to get an empty response.","correct":false},{"id":"c3cbf51c591cb7fa17bd023ab814f95c","text":"Consume the messages in the SQS queue using long polling. Set the queue attribute ReceiveMessageWaitTimeSeconds to be more than 0. Amazon SQS will wait until there is an available message in a queue before sending a response.","correct":true},{"id":"203fa6faf2e6bf53939b43300ec6dac2","text":"Increase the default visibility timeout of the queue to reduce the possibilities that the messages become visible to consumers again. The application can also use the ChangeMessageVisibility API to specify a suitable timeout value.","correct":false},{"id":"220352e5b3779c1f2030cfd4b391b19e","text":"Modify AWS SDK to get the messages in the SQS queue by short polling. The ReceiveMessage call from the consumer sets the WaitTimeSeconds attribute to 0. As a result, the empty responses are eliminated.","correct":false}]},{"id":"a4822e4f-9c97-4726-9e05-df333bf77889","domain":"awscsapro-domain2","question":"A new AWS cloud-native social-networking web application is being developed in your company. As the AWS Architect, you are guiding the technical team in making architectural and design decisions. Though the web application will enable end-users to interact with the server-side using long-running sessions, the application itself is being designed to be completely stateless so that any HTTP request made from any browser can be handled by any instance of the web application. The application must scale to handle tens of thousands of peak concurrent users, but the steady-state is expected to be less than 100 users. The incoming requests are not idempotent. In other words, if an error happens while processing a given request, it cannot be re-processed by another instance. The application processes a large amount of data in every request, and needs around 1 GB of temporary disk space. It also needs to store and retrieve information from a back-end database that must be optimized to store and navigate two-way relationships. It must also be protected from advanced DDoS attacks.\nWhich architecture is the most suitable for hosting this web application on AWS?","explanation":"This question tests the knowledge of quite a few different areas related to developing and deploying enterprise web applications on AWS. Firstly, the AWS managed database service optimized for storing and navigating social-networking style relationships is AWS Neptune. Therefore, the option that picks DynamoDB instead of Neptune is eliminated. Secondly, the same option also picks Lambda, which, again, is not the suitable technology because of the temporary disk space requirements, which is 1 GB. Lambda only provides 512 MB of temporary hard disk space and this is a hard limit. Thirdly, Spot Instances cannot be used because of the non-idempotent nature of the requests. The question clearly states that if there is an error processing an incoming request, it cannot be re-tried. Using Spot EC2 Instances is only justified when another EC2 instance can re-process a given request that could be interrupted in case a Spot EC2 instance disappears suddenly. This eliminates two out of the remaining three choices. Fourthly, ALB Sticky Sessions should not be used for session management, as the design goal is very clear - any server instance should be able to process any incoming HTTP request in spite of lengthy user sessions. This suggests that the sessions must be saved externally to some database, which any server instance can read and resume from where the last server instance left off. This is not possible with Sticky Sessions in the ALB. Thus, the option suggesting the usage of Sticky Sessions is eliminated. Instead, the session data should be saved in Elasticache cluster. After eliminating all these choices, only one choice remains.","links":[{"url":"https://aws.amazon.com/caching/session-management/","title":"Session Management in web applications on AWS"},{"url":"https://aws.amazon.com/neptune/","title":"Amazon Neptune"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/limits.html","title":"AWS Lambda Limits"}],"answers":[{"id":"a052133db9f20c8e24a3ed1c8c1d4e90","text":"Run the web application on EC2 instances added as targets to an Application Load Balancer. Store user session data in Elasticache Redis cluster. Use an Auto Scaling Group consisting of EC2 instances using a mix of Reserved and On-demand instances for handling steady-state and peak traffic respectively. Use AWS Neptune as the managed database layer. Use and configure AWS Shield Advanced for DDoS protection.","correct":true},{"id":"84fe8968d1e32cd0473785de13e7e230","text":"Run the web application on EC2 instances added as targets to an Application Load Balancer. Turn on Sticky Session for the ALB for user session management. Use an Auto Scaling Group consisting of Spot EC2 instances for handling peak traffic and Reserved Instances for handling steady-state traffic. Use AWS Neptune as the managed database layer. Use and configure AWS Shield Advanced for DDoS protection.","correct":false},{"id":"cc351cceb99405f26e33b0c2d2905144","text":"Run the web application on EC2 instances added as targets to an Application Load Balancer. Store user session data in Elasticache Redis cluster. Use an Auto Scaling Group consisting of EC2 instances using a mix of Reserved and Spot instances for handling steady-state and peak traffic respectively. Use AWS Neptune as the managed database layer. Use and configure AWS Shield Advanced for DDoS protection.","correct":false},{"id":"6d0972c01bd5270a10a1cf8eb72d18c2","text":"Run the entire web application using Lambda functions behind an API Gateway API. Request an increase to the soft limit of 1000 concurrent Lambda functions per region to accommodate the peak. Store user session data in Elasticache Redis cluster. Use AWS DynamoDB as the managed database layer. Use and configure AWS Shield Advanced for DDoS protection.","correct":false}]},{"id":"6da286f8-23a6-4e8a-a3a4-c7b496a06523","domain":"awscsapro-domain5","question":"An online health foods retailer stores its product catalog in an Amazon Aurora database. The catalog contains over 6,000 products. They'd like to offer a product search engine on the website using Amazon Elasticsearch Service. They'll use AWS Database Migration Service (DMS) to perform the initial load of the Elasticsearch indexes, and to handle change data capture (CDC) going forward. During the initial load of the indexes, the DMS job terminates with an Elasticsearch return code of 429 and a message stating 'Too many requests'. What must be done to load the Elasticsearch indexes successfully?","explanation":"When the ElasticSearch indexing queue is full, a 429 response code is returned and an es_rejected_execution_exception is thrown. The DMS load task then terminates. Throttling the DMS input stream based on the number of Elasticsearch indexes, shards, and replicas to be loaded will result in a successfully completed job. The DMS MaxFullLoadSubTasks parameter indicates how many source tables to load in parallel, and the ParallelLoadThreads parameter determines the number of threads that can be allocated for a given table. Increasing Elasticsearch shards without modifying DMS subtask and thread parameters could still overrun the request queue. Changing the DMS stream buffer count won't help with this issue. Amazon Elasticsearch currently doesn't provide support for AWS Glue as a source, so integration would require significant effort. Increasing Elasticsearch EBS volume IOPS won't solve an ingress queue overrun problem. The DMS batch split size parameter sets the maximum number of changes applied in a single batch, but doesn't reduce the total number of requests.","links":[{"url":"https://aws.amazon.com/dms/","title":"Amazon Database Migration Service"},{"url":"https://aws.amazon.com/elasticsearch-service/","title":"Amazon Elasticsearch Service"},{"url":"https://aws.amazon.com/blogs/database/scale-amazon-elasticsearch-service-for-aws-database-migration-service-migrations/","title":"Scale Amazon Elasticsearch Service for AWS Database Migration Service migrations"}],"answers":[{"id":"157e8733386382289486b3592774442f","text":"Increase the number of Elasticsearch shards for each index to increase distribution of the load. Change the DMS stream buffer count parameter to match the number of Elasticsearch shards","correct":false},{"id":"c1e1a85b26a30433a6af5d30c8bb8d76","text":"Calculate the number of queue slots required for the Elasticsearch bulk request as a product of the number of indexes, shards, and replicas. Adjust DMS subtask and thread parameters accordingly","correct":true},{"id":"ee750f72f1e70b83f6d83819f2d504f5","text":"Raise the baseline IOPS performance of the Elasticsearch cluster EBS volumes to enable more throughput. Increase the DMS batch split size parameter to send more data in each request and reduce the number of total requests","correct":false},{"id":"62ac117860ebe5397e04bad8ea29a5fb","text":"Replace DMS with AWS Glue for the initial index load and ongoing change data capture. Enable parallel reads when the ETL methods are called in the Glue jobs","correct":false}]},{"id":"312b233b-ecc8-4e70-ae91-665159c7f77b","domain":"awscsapro-domain2","question":"You work for an automotive parts manufacturer as a Cloud Solutions Architect and you are in the middle of a design project for a new quality vision system.  To \"help out\", your parent company has insisted on contracting with a very expensive consultant to review your application design.  (You suspect that the consultant has more theoretical knowledge than practical knowledge however.)  You explain that the system uses video cameras and special polarizing filters to identify defects on fuel injectors.  As the part passes each station, an embedded RFID serial number is read and included with the PASS/FAIL vision test result in a JSON record written to DynamoDB.  The DynamoDB table is exported to Redshift on a monthly basis.  If a flaw is detected, the part can sometimes be reworked and sent back through the process--but it does retain its unique RFID tag.  Only the latest tests need to be kept for the part.  The consultant reviews your design and seems slightly frustrated that he is unable to recommend any improvement.  Then, he smiles and asks \"How are you ensuring idempotency?  In case a part is reprocessed?\"    ","explanation":"Idempotency or idempotent capability is a design pattern that allows your application to deal with the potential of duplicate records.  This can happen when interfaces fail and some records need to be reprocessed.  In this case, we are using a unique RFID serial number as our identifier for the part.  In DynamoDB, we would just overwrite the record with the latest record using a UpdateItem SDK method.  For Redshift, an UPSERT function allows us to either insert as a new record or update if a record of the same key already exists.  Redshift can do this using a merge operation with a staging table.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateItem.html","title":"UpdateItem - Amazon DynamoDB"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-upsert.html","title":"Use a Staging Table to Perform a Merge (Upsert) - Amazon Redshift"}],"answers":[{"id":"5a65801823c8e80af868a9ca05e34e18","text":"You will be using API Gateway and Lambda for the insert into DynamoDB so scaling is not a concern. The part can be processed as many times as needed and Lambda will scale as needed.","correct":false},{"id":"d87a9f08720c0fd91dee999f81f6f0ed","text":"You could change your design to write the message first to an SQS queue with FIFO enabled.  The records would then be guaranteed to process in the order they arrived.","correct":false},{"id":"9dc68479e32f89bfe0afde00f41ae6c3","text":"For the target DynamoDB table, you have defined the unique RFID string as the partition key.  When copying to Redshift, you use table merge method to perform a type of UPSERT operation.","correct":true},{"id":"8f594d180a595027ddef4e33f2784b0f","text":"You will be using CloudWatch to monitor the DynamoDB tables for capacity concerns.  If needed, you can enable DynamoDB auto scaling to accommodate the extra volume that reprocessing might introduce.","correct":false}]},{"id":"8aa313fe-cd0f-4899-a2f4-e8f2fd64c245","domain":"awscsapro-domain4","question":"Your business depends on AWS S3 to host three kinds of files - images, documents and compressed installation packages. These files are accessed and downloaded by end-users from all US regions and west EU, though the compressed installation packages are downloaded rarely as users tend to access the service from their browsers instead of installing anything on their machines. Each installation package bundles several images and documents, and also includes binaries that are downloaded from a 3rd party service while creating the package files.\nThe images and documents range from a few KBs to a few hundred KBs in size and they are mostly static in nature. However, the compressed installation package files are generated every few hours because of changes done by the 3rd party service to their binaries, and some of them are as large as a few hundred GB-s. The installation package files can be regenerated from the images and documents fairly quickly if required. It is important to be able to retrieve older versions of the images and documents.\nWhich of the following storage solutions is the most cost-effective approach to design the storage for these files?","explanation":"The areas tested by this question are:\\n1. Versioning cannot be enabled at the object level. It is a bucket-level feature. This rules out the choice where we have a single bucket and selectively turn on versioning on for some objects only.\\n2. If you enable Versioning for a bucket containing large objects that are frequently created/uploaded, it will result in higher storage cost as all the previous versions will result in storage volume growing quickly because of frequent writes. In the given scenario, the compressed installation package files are large and also frequently generated (every few hours). There is no requirement to version them, as they can be quickly generated on-demand. Hence, putting them in a bucket that has Versioning enabled is not a good cost-effective solution. This rules out two choices - one where we have a single versioned bucket, the other where we enable versioning for both buckets.\\n3. Note that all options except one correctly identify the storage class requirements - the compressed installation package files should be stored as One-Zone IA because durability is not a prime requirement for these files (simply because they can be regenerated on-demand easily). They are rarely downloaded, hence IA is the correct class. Combined with low durability, One Zone IA is the most cost-effective solution. Only one option uses the incorrect storage tier for these files - note that IA is more expensive than One-Zone IA, and the question is about cost-effectiveness.\nHence, the only correct answer is the one that addresses both Versioning and Storage Class requirements correctly.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html","title":"Documentation on Object Versioning"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-howtoset","title":"Setting the Storage Class of an Object"}],"answers":[{"id":"90f6b37d063a0158682b034d578bf29b","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for both the buckets. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":false},{"id":"fd1d330daf4d05b4fa4c888a0584130f","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the bucket. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to One-Zone Infrequent Access while uploading compressed installation package files","correct":false},{"id":"9e88398b12bdf0dd2fdc12a19f4962a1","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for Bucket A only. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":true},{"id":"2e4e12f367286b12f14ae74b1fd4e350","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the image and document objects only, but not for the compressed installation package files. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to Infrequent Access while uploading compressed installation package files","correct":false}]},{"id":"1239c235-107c-4f5e-8bac-9dc824c00680","domain":"awscsapro-domain5","question":"You are helping a client with some process automation.  They have managed to get their website landscape and deployment process encapsulated in a large CloudFormation template.  They have recently contracted with a third-party service to provide some automated UI testing.  To initiate the test scripts, they need to make a call out to an external REST API.  They would like to integrate this into their existing CloudFormation template but not quite sure of the best way to do that.  Help them decide which of the following ideas is feasible and incurs the least extra cost.","explanation":"To integrate external services into a CloudFormation template, we can use a custom resource.  Lambda makes a very good choice for this scenario because it can handle some logic if needed and make a call out to an external API.  Using an EC2 instances to make this call is excessive and we likely would not have the ability to configure the third-party API to poll an SQS queue.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html","title":"AWS Lambda-backed Custom Resources - AWS CloudFormation"}],"answers":[{"id":"97a123d11bbf3be0e3e1788e2f0874ac","text":"Add an API Gateway deployment to the CloudFormation template.  Add the DependsOn parameter to the API Gateway resource to ensure that the call to the external API only happens after all the other resources have been created.  Create a POST method and define it as a proxy for the external REST API endpoint.  Using SWF, call the API Gateway endpoint to trigger the testing process.","correct":false},{"id":"f761e84ee0cd0f689465458a41b69fae","text":"Include an SQS queue definition in the CloudFormation template.  Define a User Script on the deployed EC2 instance which will insert a message into the SQS queue only once it has fully booted.  Configure the external REST API to use long polling to check the queue for new messages in order to initiate the testing process.","correct":false},{"id":"43569df3ec4b7db0265dea4051c04644","text":"Add a small EC2 instance definition to the CloudFormation template.  Define a User Script for that instance which will install a custom application from S3 to call out to the external REST API endpoint using the POST method to trigger the testing process.  Add a CleanUp parameter to the EC2 instance definition that will shut down the instance once the activity has completed.","correct":false},{"id":"6b3b26e17f2323a91f04f792f0c2d20c","text":"Create a Lambda function which issues a call out to the external REST API using the POST method.  Define a custom resources in the CloudFormation template and associate the Lambda function and execution role with the custom resource.  Include DependsOn to ensure that the function is only called after the other instances are ready.","correct":true}]},{"id":"d2b0a9d5-1875-4a55-968d-3a2858601296","domain":"awscsapro-domain2","question":"You currently are using several CloudFormation templates. They are used to create stacks that include the resources of VPC subnets, Elastic Load Balancers, Auto Scaling groups, etc. You want to deploy all the stacks with a root stack so that all the resources can be configured at one time. Meanwhile, you need to isolate information sharing to within this stack group, which means other stacks outside of the stack group can not import its resources. For example, one stack creates a VPC subnet resource and this subnet can only be referenced by the stack group. What is the best way to implement this?","explanation":"As the stack outputs should be limited within the stack group, nested stacks should be chosen. The export stack outputs cannot prevent other stacks to use them. The AWS::CloudFormation::Stack resource type is used in nested stacks to provide dependencies. The DependsOn attribute is not used for configuring nested stacks.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html","title":"Exporting stack output values"}],"answers":[{"id":"e208251ba3e5646edab96df0da85794c","text":"Upload stack templates to an S3 bucket. Create a root CloudFormation stack to use the uploaded templates with the resource type of \"AWS::CloudFormation::Template\". Configure the \"TemplateURL\" field with the template location in S3.","correct":false},{"id":"fe3bf9eee211c59e9c64ad17a2d32e27","text":"Export output values for each child stack if needed. Create a parent stack to use the exported values from child stacks to deploy and manage all resources at one time.","correct":false},{"id":"aca679ff150a8a8176faf99cc057e825","text":"Create nested stacks with the \"AWS::CloudFormation::Stack\" resources. Use the outputs from one stack in the nested stack group as inputs to another stack in the group if needed.","correct":true},{"id":"e44006dac54e63b93a8804a4e632eeb5","text":"Upload the root and all child stack templates to an S3 bucket under the same directory. Use the \"DependsOn\" attribute in the root template to add dependencies. When the root stack is created, all the child stacks are created first. ","correct":false}]},{"id":"e3a59454-94fa-4b98-8d8a-80882a7d0e30","domain":"awscsapro-domain5","question":"You are setting up a new EC2 instance for an ERP upgrade project.  You have taken a snapshot and built an AMI from your production landscape and will be creating a duplicate of that system for testing purposes in a different VPC and AZ.  Because you will only be testing an upgrade process on this new landscape and it will not have the user volume of your production landscape, you select an EC2 instance that is smaller than the size of your production instance.  You create some EBS volumes from your snapshots but when you go to mount those on the EC2 instances, you notice they are not available.  What is the most likely cause?","explanation":"In order to mount an EBS volume on an EC2 instance, both must be in the same AZ.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html","title":"Amazon EBS Volumes - Amazon Elastic Compute Cloud"}],"answers":[{"id":"7a11f6b6797c8dd005c9ce25c77a37fe","text":"An SCP applied to the account you are in has restricted you from attaching EBS volumes to instances outside the original VPC","correct":false},{"id":"c2f8c791750ad6ce4c8c41ac45b246a0","text":"You have reached your account limit for EBS volumes.  You will need to create a support ticket to request an increase to the limit.","correct":false},{"id":"a9e147ffe118e514fd5069020e86cca6","text":"The instance that you selected for your testing landscape is too small.  It must be equal to or larger than the source of the AMI.","correct":false},{"id":"5a9ffc3875f9e3f50c9fc2684a6006b2","text":"The original volume is encrypted and you failed to check the encryption flag when creating the new volume.","correct":false},{"id":"134c64ea3d25d70a667400e778a13c1a","text":"You created them in a different availability zone than your testing EC2 instance.","correct":true}]},{"id":"b303f8e0-2c68-44aa-93bb-45b987b17d95","domain":"awscsapro-domain3","question":"You are helping a client build some internal training documentation to serve as architectural guidelines for their in-house Solutions Architects.  You suggest creating something inspired by the AWS Well-Architected Framework.  The client agrees and wants you to come up with some examples of each pillar.  Which of the following are examples of the Reliability pillar?","explanation":"The Reliability pillar includes five design principles:  Test recovery procedures, Automatically recovering from failure, Scaling horizontally to increase aggregate system availability, Manage change in automation and Stop guessing capacity.  By being able to closely monitor resource utilization, we can increase the Reliability and efficiency to right-size capacity.","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"3eb456756ecb767ab18179d87ec49a6b","text":"We can drive improvement through lessons learned from all operational events and failures. Share what is learned across teams and through the entire organization.","correct":false},{"id":"c57c8249b3885f2078d8dac40d695dec","text":"We can design workloads to allow components to be updated regularly, making changes in small increments that can be reversed if they fail.","correct":false},{"id":"e82a19c63c4c9fedccc997eece7eccdc","text":"With virtual and automatable resources, we can quickly carry out comparative testing using different types of instances, storage, or configurations.  This will allow us to experiment more often.","correct":false},{"id":"c257f1eec7a5a7f1eae9338f4de45cb0","text":"On AWS, we'll be able to monitor demand and system utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over or under-provisioning.  We can stop guessing on capacity needs.","correct":true}]}]}}}}
