{"data":{"createNewExamAttempt":{"attempt":{"id":"1a7cd555-cb80-4346-86e3-3e9989c49080"},"exam":{"id":"a576dc09-e7c3-4c95-baaa-9327c5aef8f6","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"91e4ebb5-18ac-45d6-867e-4c2eddefc075","domain":"awscsapro-domain4","question":"Your company has come under some hard times resulting in downsizing and cuts in operating budgets.  You have been asked to create a process that will increase expense awareness and your enhance your team's ability to contain costs.  Given the reduction in staff, any sort of manual analysis would not be popular so you need to leverage the AWS platform itself for automation.  What is the best design for this objective?","explanation":"AWS Budgets is specifically designed for creating awareness and transparency in your AWS spending rate and trends.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing Your Costs with Budgets - AWS Billing and Cost Management"}],"answers":[{"id":"ce7a1fa4194d00b76a1491e66b2a10ec","text":"Use AWS Budgets to create a budget.  Choose to be notified when monthly costs are forecasted to exceed your updated monthly target.","correct":true},{"id":"0e5c5be553400912c2517757d9aba6b2","text":"Provide access to Cost Explorer for your team for transparency.  Allow them to periodically review the accrued costs for the month and take the appropriate action.","correct":false},{"id":"aadb36f869250c141ae2eeec088d3bb3","text":"Implement a Cost Allocation Tagging strategy.  Create a periodic aggregation process in AWS Glue to read costs based on the tags.  Configure Glue to send the report to SNS where it can be either emailed or texted to the team depending on their own preference.","correct":false},{"id":"be1b52e799fe44ef89f78b167401c067","text":"Export AWS bill data into Redshift.  Use Quicksight to create reports per cost center.  Provide access for users to QuickSight to monitor their usage.","correct":false},{"id":"8e9e7154ed9b1f92015c5fde5ca7a2a3","text":"Use Kinesis Streams to Ingest CloudTrail log streams.  Create a Lambda function to parse the log stream and insert a record into DynamoDB whenever a pay-per-use activity happens.  Use DynamoDB streams to alert the team when a threshold has been exceeded.","correct":false}]},{"id":"e720cd54-de67-42de-ba10-593dee0582e6","domain":"awscsapro-domain3","question":"You are the Enterprise Architect in a Risk Quantification firm. The firm has a website which end-users can use to apply for loans and also track the status of their loan application if they log in. When a loan application comes in, several downstream systems need to independently process the application. Right now, the website server-side code invokes these systems one after the other, synchronously, in a tight loop. If one of these downstream systems times out or throws an exception, the entire loan application processing errors out. Even if none of these downstream systems fail, the time it takes to process a loan application is very high due to the serial nature of these systems being invoked. Your CTO wants only the loan-processing application moved to the AWS cloud and re-architected at the same time.\nThe downstream systems are all hosted on-premises and will continue to remain on-premises. They expose REST endpoints that accept POST HTTPS requests, use self-signed certificates and respond synchronously only when they are done processing an application. After re-architecture, all downstream systems must independently start processing an incoming loan application simultaneously.\nYour CTO wants to know how the loan-processing website application can be architected in the AWS Cloud, and what supporting changes will be needed in the downstream systems on-premises. He wants to minimize code changes to the downstream on-premises systems. Choose the best option","explanation":"This is an example of a verbose question with verbose answer choices. You can expect a few such questions in the exam, testing your time management skills. Try to vertically scan the answers to see which parts differ between them. Sometimes, though the answers seem big, a large part of each is identical. You can ignore those parts, as there is nothing to choose between the.\nAmong the four choices, two use SQS and two use SNS to feed the incoming loan applications to the downstream systems. You cannot automatically eliminate either SQS or SNS, as a working solution can be designed with either.\nLet us see how we can achieve this using SNS first. The basic requirement here is fan-out - a single loan application must be processed by several downstream systems, so there are multiple consumers. Hence, SNS is a natural fit. SNS supports multiple subscribers for a topic. SNS also supports HTTP/HTTPS subscribers. SNS makes POST REST API call to as many HTTP/HTTPS subscribers exist on the topic, so it fits the bill. However, there is a small problem - the requirement states that the downstream systems must be changed as little as possible. If we follow this design, we must change the HTTP Listening part of the downstream systems significantly. Because SNS is directly calling them now, SNS will use its own headers and body format. In fact, SNS POST-s two kinds of messages - one is Subscription Confirmation and one is Notification. A special HTTP header (x-amz-sns-message-type) has the right type in its value. The server side now must parse this header out and look for only the Notification type of message. The body itself will then be JSON formatted with the payload. While the server is probably used to process just the core payload (loan application data) as the HTTP body, the same will now be hidden inside a JSON field called Message inside the request body. Additionally, the downstream systems will have to deal with SNS retries, thus the loan application part must be made idempotent (if the same loan application lands twice, it will ignore the duplicates). Thus, though it is technically possible to design the solution using SNS, it will result in a lot of changes in the downstream systems. Hence, though the SNS option will work, it is not the correct answer because of this reason.\nNow, let us see how we can design this using SQS. While SQS does not support fan-out (multiple consumers for the same message), the proposed solution uses a Lambda function to achieve fan-out. The Lambda function will pick up the message, and then call the downstream systems one by one. The key to making this work is, of course, to modify the downstream systems from synchronous monolithic beasts to asynchronous servers so that they can instantly respond to the Lambda function and then continue to process the application. We will then have to provide a callback for when it is done. The solution uses an API Gateway for that purpose. Overall, the solution is elegant, and changes to the downstream systems are less than what SNS requires. Hence, SQS is the correct answer.\nNote that one version of the SNS design proposes to retain the synchronous nature of the downstream systems. That will not work as SNS will not wait more than 15 seconds for a response. The response will then be lost and the main website app will never know the results from the downstream systems.\nAlso, note that though SNS requires the HTTPS subscriber to present a trusted CA-signed certificate, there is no such requirement for Lambda because Lambda is basically your code, you can decide to trust anyone.","links":[{"url":"https://docs.aws.amazon.com/sns/latest/dg/sns-http-https-endpoint-as-subscriber.html","title":"Using Amazon SNS for System-to-System Messaging with an HTTP/S Endpoint as a Subscriber"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","title":"Using AWS Lambda with Amazon SQS"}],"answers":[{"id":"eec2740374df8038093d636a17252168","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Override the default delivery policy on the subscriber endpoint to remove retries so that downstream systems do not have to worry about synchronous responses taking time or idempotency of retries. Make the following changes in the downstream systems - (a) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (b) Procure server certificates from a trusted Certificate Authority (CA) instead of using the self-signed certificate as SNS will not be able to POST to a server with a self-signed certificate","correct":false},{"id":"3d032e65493c0733ebe65683fb66a562","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems and (c) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as your Lambda function will not be able to POST to a server with self-signed certificate","correct":false},{"id":"48d42d3290142cbfc8207e042690b35f","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting so that SNS does not retry, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (c) Make them idempotent for the same loan application as SNS may retry in case of lost messages or timeouts (d) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as SNS will not be able to POST to a server with self-signed certificate","correct":false},{"id":"3fb725a8e71fa96168f18e50a146b4f0","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed and (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems","correct":true}]},{"id":"482e75c9-071e-4a10-83f4-575f9c15b885","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They have just accidentally deleted the private key portion of their EC2 key pair.  Now, they are unable to SSH into their Amazon Linux servers.  Unfortunately the keys were not backed up and are considered gone for good.  What can this customer do to regain access to their instances?","explanation":"The two methods that AWS recommends if you lose a private key for an EC2 key pair are using Systems Manager Automation or using a secondary instance to edit the authorized_keys file.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2reset.html","title":"Reset Passwords and SSH Keys on Amazon EC2 Instances - AWS Systems Manager"}],"answers":[{"id":"509a77ae9827c5fcb60ccecc62fc9853","text":"Stop the instances, detach its root volume and attach it as a data volume to another instances.  Modify the authorized_keys file, move the volume back to the original instance and restart the instances.","correct":true},{"id":"bec3d01a56c851f61a1a09c852635db7","text":"Generate and upload a new key pair.  Stop the instances and select the new key pair from the dropdown on the Instance Settings sub-menu in the Console.","correct":false},{"id":"79457a1b908d4a36cfeba625be909d40","text":"Use AWS Systems Manager Automation with the AWSSupport-ResetAccess document to create a new SSH key for your current instance.","correct":true},{"id":"492c38d2fe2c3b96608bb8436592fe26","text":"Use the AWS CLI with the EC2 ModifyInstance action to enable SSH password-only access for the ec2-user account.  Attach using a password rather than an SSH key.  Modify the authorized_key file for the new public key.","correct":false},{"id":"f17aa014620843abd81fa849982566b0","text":"Create a new key pair in KMS then assign the new public key to the required EC2 instance.","correct":false},{"id":"be45655cb1d64dff71a97aa729bc4e4a","text":"Open the TELNET port (port 23) on the Security Group for the server.  Use a TELNET client to attach to the instances using the root account and password.  Modify the authorized_key file with the new public key.","correct":false}]},{"id":"2afb0db9-a43f-4e97-8272-5ce423ded162","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They notice on their RDS console that one of their mission-critical production databases has an \"Available\" listed under the Maintenance column.  They are extremely concerned that any sort of updates to the database will negatively impact their DB-intensive mission-critical application.  They at least want to review the update before it gets applied, but they are not sure when they will get around to that.  What do you suggest they do?","explanation":"For RDS, certain OS updates are marked as Required. If you defer a required update, you receive a notice from Amazon RDS indicating when the update will be performed. Other updates are marked as Available, and these you can defer indefinitely.  You can also apply the maintenance items immediately or schedule the maintenance for your next maintenance window.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html","title":"Maintaining a DB Instance - Amazon Relational Database Service"}],"answers":[{"id":"35c3b3d1cb8d9866e1abc96dec8bfa3f","text":"Apply the maintenance items immediately.  AWS validates each update with each customer's RDS instances using a shadow image so there is little risk here.","correct":false},{"id":"66ed278812c2a52913954afa52952b97","text":"The maintenance will be automatically performed during the next maintenance window.  They have no choice in the matter.","correct":false},{"id":"b9f81cfac73d5c4d2871a7f969ecb9f3","text":"Disable the Maintenance Window so the updates will not be applied.","correct":false},{"id":"6aeada3a9046319bc858239b15031f66","text":"Backup the database immediate because the updates could come at any time.  If possible, create a Read Replica to act as a standby in case problems are introduced with the update.","correct":false},{"id":"93df9eb71cb48a0c821fe555e35f5b62","text":"Defer the updates indefinitely until they are comfortable.","correct":true}]},{"id":"512696b6-6160-45f7-8000-d664f78a86aa","domain":"awscsapro-domain2","question":"You have been contracted by a Security Company to build a face recognition service for its customer, Department of Corrections, in the AWS Cloud. Whenever a new inmate or personnel joins a facility, their facial image will be taken by an application running on a laptop, and stored centrally, along with metadata like their name. They will have a second application getting a live image feed from cameras installed throughout the secure areas of the facility. Whenever the second application receives an image from a camera, it needs to check against the pool of images stored centrally to check if there is a match. If a match is found, the location must be saved along with the timestamp and name in a database which can later be used to query the location of a person at or near a given time-period.\nHow will you, as the AWS Architect, design this suite of applications?","explanation":"This question tests the knowledge of the various Machine Learning technologies and services offered by AWS. Amazon Rekognition is the AWS AI service for image and video analysis. The question also offers AWS Sagemaker, AWS Personalize and AWS Comprehend as alternatives. AWS Sagemaker is used to build and train Machine Learning models. Hence, it is not relevant in this scenario, as the use case is not about training a model. Amazon Personalize is an ML service that enables developers to create individualized recommendations for customers using their applications. This use case is not related to recommendations, hence we can eliminate AWS Personalize. Amazon Comprehend is a natural language processing (NLP) service that uses ML to find relationships and insights in text. We can eliminate Comprehend as this is an image analysis scenario, as opposed to text analysis.","links":[{"url":"https://aws.amazon.com/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/","title":"Build Your Own Face Recognition Service Using Amazon Rekognition"},{"url":"https://aws.amazon.com/machine-learning/","title":"Various components of Machine Learning on AWS"}],"answers":[{"id":"3c168275751e0bed48552554f05bad14","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Personalize from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in a Dynamodb database. Later, when a match is required, query Amazon Personalize with the image taken by security cameras. Personalize will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the Dynamodb database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to an RDS database for later querying","correct":false},{"id":"3fac11da1f4ec1911a5b1c4262c16bb4","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Comprehend from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in an RDS Postgresql database. Later, when a match is required, query Amazon Comprehend with the image taken by security cameras. Comprehend will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the RDS database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to a second RDS database for later querying","correct":false},{"id":"a24bc02aa584b134a2abecb810d5b32c","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Rekognition from the Lambda function to index the face in the image as a Collection, returning a Face id. Store this face id and Name of the person in a Dynamodb database. Later, when a match is required, query the Amazon Rekognition Collection with the image taken by security cameras. Rekognition will return a set of face ids that have potentially matched. If a face id is found whose confidence value is higher than a predefined set value, query the Dynamodb database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to an RDS database for later querying","correct":true},{"id":"cc5d32350e93736f1e3ec5e359029c80","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Sagemaker Image Classification Algorithm from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in an RDS Postgresql database. Later, when a match is required, query Amazon Sagemaker with the image taken by security cameras. Sagemaker will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the RDS database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to a second RDS database for later querying","correct":false}]},{"id":"0ca782d3-466e-4376-ab92-e4eec0374e13","domain":"awscsapro-domain2","question":"You've begun deploying EC2 and VMware Cloud on AWS instances to host various applications which you'd like to make accessible to those who authenticate via an on-premises Active Directory domain. You've configured AWS Managed Microsoft AD in the same region as the EC2 and VMware Cloud on AWS instances with a one-way trust back to the corporate AD domain. You're able to seamlessly join new EC2 Windows instances to the Managed AD domain at launch, but the EC2 Linux and VMware Cloud on AWS instances don't show up in the domain when launched. What additional actions need to take place in order to seamlessly join all the instances to the domain at launch?","explanation":"A bootstrap script that installs a Kerberos client package and performs a Realm Join will successfully join an EC2 Linux instance to an Active Directory domain. Active Directory uses Kerberos as it's authentication protocol between a server and a client. VMC Compute Gateway (CGW) Firewall Rules block traffic to all uplinks by default, so 'allow' rules need to be added. This particular issue would not be due to a role or Security Group configuration problem because the EC2 Windows instances are able to join the domain successfully. The instance's SSH service does need to be configured to allow password authentication, but this is not necessary for the domain join operation.","links":[{"url":"https://d1.awsstatic.com/VMwareCloudonAWS/aws_reference_architecture_hybrid_active_directory_trusted_domains.pdf?did=wp_card&trk=wp_card","title":"Hybrid Active Directory Trusted Domains"},{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/join_linux_instance.html","title":"Manually Join a Linux Instance"},{"url":"https://docs.vmware.com/en/VMware-Cloud-on-AWS/services/com.vmware.vmc-aws.networking-security/GUID-A5114A98-C885-4244-809B-151068D6A7D7.html","title":"Add or Modify Compute Gateway Firewall Rules"}],"answers":[{"id":"5d3e04eaf8f190a5e4314afbf01a181e","text":"Create a bootstrap script to install a Kerberos client package and perform a Realm Join command for the EC2 Linux instances, and add a VMware Cloud NSX Compute Gateway (CGW) Firewall Rule for the VMware Cloud on AWS instances","correct":true},{"id":"89f2d086e3f1e6302636559551197099","text":"Have EC2 Linux instances configure SSH services to allow password authentication, and configure the VMware Cloud NSX Compute Gateway (CGW) to not perform Network Address Translation on the domain server's IP address","correct":false},{"id":"43bbc2a5c48b9ae653075577b0f2555d","text":"Have EC2 Linux instances assume a role with permissions to write to the Managed AD domain, and configure the VMware Cloud NSX Compute Gateway (CGW) to pass Active Directory requests through without VMware Tunneling","correct":false},{"id":"aa2adec42d76f83b2f45af8ab617e0bb","text":"Add inbound rules for the EC2 Linux and the VMware Cloud on AWS instances to allow traffic on port 389 for LDAP","correct":false}]},{"id":"447b50dd-cf00-4688-8181-2d87c302c538","domain":"awscsapro-domain2","question":"An application that collects time-series data uses DynamoDB as its data store and has amassed quite a collection of data--1TB in all.  Over time, you have noticed a regular query has slowed down to the point where it is causing issues.  You have verified that the query is optimized to use the partition key so you need to look elsewhere for performance improvements.  Which of the following when done together could you do to improve performance without increasing AWS costs?","explanation":"As a DynamoDB table grows, it will spit into more partitions.  If the RCU and WCU remain constant, they are divided equally across the partitions.  When the allocation of that partition is used up, you risk throttling.  AWS will permit burst RCU and WCU at times but it is not assured.  You could increase the RCU and WCU but this would increase cost. Therefore, we can archive off as much data as possible but also need to shrink the partitions down to something more reasonable.  We can do this by backing up and recreating/restoring the table.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html","title":"Best Practices for DynamoDB - Amazon DynamoDB"}],"answers":[{"id":"28004d3db4c1cf8f98d735e3893f14d0","text":"Export the data then import it into a newly created table.","correct":true},{"id":"21bb1c71dec739e9fc76b69fb51ee515","text":"Create a secondary global index on the most common fields to increase performance. ","correct":false},{"id":"e132b6523705a41bc5ed88930a62a21c","text":"Increase the provisioned read and write capacity units.","correct":false},{"id":"be0c8b120e1730a1fc73ec1b0ae38d50","text":"Archive off as much old data as possible to reduce the size of the table.","correct":true},{"id":"59b19c32945103a6f4da6ef17acd7f75","text":"Change the query to use a secondary local index instead of the partition key.","correct":false}]},{"id":"78111d9b-922f-435f-8e91-4ae84e990761","domain":"awscsapro-domain3","question":"A hotel chain has decided to migrate their business analytics functions to AWS to achieve higher agility when future analytics needs change, and to lower their costs. The primary data sources for their current on-premises solution are CSV downloads from Adobe Analytics and transactional records from an Oracle database. They've entered into a multi-year agreement with Tableau to be their visualization platform. For the time being, they will not be migrating their transactional systems to AWS. Which architecture will provide them with the most flexible analytics capability at the lowest cost?","explanation":"AWS Database Migration Service can be configured with an on-premises Oracle database as a source and S3 as a target. It can provide continuous replication between the two. AWS Glue can aggregate the data from S3 according to desired reporting dimensions and store the summaries in Redshift. Keeping the transactional detail in S3 and only keeping the aggregate information in Redshift will save on costs. The same is true for keeping transactional detail in S3 instead of RDS Oracle. AWS Glue is a great solution for transforming the Adobe Analytics CSV files to Parquet format in S3. Parquet's columnar organization will provide excellent performance for Redshift Spectrum queries that join between Redshift tables and S3. Tableau's Redshift connector supports Redshift Spectrum queries. For this use case, using Amazon QuickSight would not make sense since the company has already committed payments to Tableau via their multi-year agreement.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/glue/","title":"AWS Glue"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"1bc635be85a059a6135751bf21fd3550","text":"Use Oracle Data Guard to continuously replicate Oracle transactional data to an Oracle instance on Amazon EC2. Configure AWS Glue to aggregate the transactional data from the Oracle instance for each dimension into Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Redshift. Use Amazon QuickSight to query the data for visualization.","correct":false},{"id":"e1f7ec66baca2c4e27cf072a8ca91424","text":"Employ AWS Database Migration Service to continuously replicate Oracle transactional data to Amazon S3. Configure AWS Glue to aggregate the transactional data from S3 for each dimension into Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Amazon S3 in Parquet format. Install Tableau on Amazon EC2 and write queries to Amazon Redshift Spectrum.","correct":true},{"id":"86c10f6cca438461e60f5c04886f57c9","text":"Configure AWS Database Migration Service to continuously replicate Oracle transactional data to Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Redshift. Use Amazon QuickSight to query the data for visualization.","correct":false},{"id":"dc33d336682223190f2d8cb22449cf81","text":"Implement AWS Database Migration Service to continuously replicate Oracle transactional data to an Amazon RDS Oracle instance. Use AWS Glue to write the Adobe Analytics data to the RDS Oracle instance. Install Tableau on Amazon EC2 and write queries against the RDS Oracle database.","correct":false}]},{"id":"b06ef2a9-b122-4b47-b5be-b6d604e78405","domain":"awscsapro-domain2","question":"You are working with a customer to implement some better security policies.  They have a group of remote employees working on a confidential project that uses some proprietary Windows software and stores data in S3.  The Chief Information Security Officer is concerned about the threat of the desktop software or confidential data being smuggled out to a competitor.  What architecture would you recommend to best address this concern? ","explanation":"Using a locked down virtual desktop concept would be the best way to manage this.  AWS WorkSpaces provides this complete with client software to log into the desktops.  These Workspaces can be walled off from the Internet.  Using policies, you could allow access from only those in the Workspaces VPC.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html","title":"What Is Amazon WorkSpaces? - Amazon WorkSpaces"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html","title":"Endpoints for Amazon S3 - Amazon Virtual Private Cloud"}],"answers":[{"id":"21d0f58369770fb84d6bff8bfcf9c265","text":"Use Service Catalog to deploy and manage the proprietary Windows software to the remote employees.  Create an OpenVPN server instances within a VPC.  Create an VPC Interface Endpoint to S3 and use a security group to only permit traffic from the OpenVPN server security group.  Supply the remote employees with instructions to install and login using OpenVPN client software.","correct":false},{"id":"b5b5a93dca37986129c444b7654d600f","text":"Provision Amazon Workspaces in a secured private VPC.  Do not enable Internet access for the Workspaces.  Create a VPC Gateway Endpoint to S3 and implement an endpoint policy that explicitly allows access to the required bucket.  Assign an S3 bucket policy that denies access unless the sourceVpce matches the VPC endpoint.  Supply the users with instructions on downloading and login into the Workspaces instances.","correct":true},{"id":"36e6b9fb8ba6dd4b59c9f032ba6f78f7","text":"Provision Windows 2016 instances in a private subnet.  Create a specific security group for the Windows machines permitting only SSH inbound.  Create a NACL which allows traffic to S3 services and explicitly deny all other network traffic to and from the subnet.  Assign an S3 bucket policy that only allows access for members of the Windows machine security group.","correct":false},{"id":"f8c297565cec662fd215e6c551daca36","text":"Create a bucket policy using the sourceIP condition to only allow access from a specific VPC CIDR.  Apply a NACL which only permits inbound port 22 and outbound ephemeral ports.  Deploy Amazon Workspaces in the VPC and disable internet access.  Supply the users with instructions on downloading and login into the Workspaces instances.","correct":false}]},{"id":"6a0e9756-1b9e-495c-965b-a8c715843d4f","domain":"awscsapro-domain1","question":"A client has asked you to help troubleshoot a Service Control Policy.  Upon reviewing the policy, you notice that they have used multiple \"Statement\" elements for each Effect/Action/Resource object but the policy is not working. What would you suggest next?  ","explanation":"The syntax for an SCP requires only one Statement element.  You can have multiple objects within a single Statement element though. ","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/org_troubleshoot_policies.html","title":"Troubleshooting AWS Organizations Policies - AWS Organizations"}],"answers":[{"id":"6d5becfe4962c177196b4dce486c4e58","text":"Look elsewhere as multiple Statement elements are used when multiple conditions are specified in SCPs.","correct":false},{"id":"c88df8df089061b0ce17c8aba3a63305","text":"Have them apply the same policy on another OU to eliminate any localized conflicts.","correct":false},{"id":"4b06a8d83f7ea11c2700b91cae578fbb","text":"Split the SCP out into multiple policies and apply in a cascading manner to higher level OUs.","correct":false},{"id":"3410d9f58eea1a807fdc3d0f6194c3ce","text":"Change the policy to combine the multiple Statement elements into one element with an object array.","correct":true}]},{"id":"dc82c397-347d-4f69-bb06-03822238c7a0","domain":"awscsapro-domain1","question":"You are consulting for a large multi-national company that is designing their AWS account structure.  The company policy says that they must maintain a centralized logging repository but localized security management.  For economic efficiency, they also require all sub-account charges to roll up under one invoice.  Which of the following solutions most efficiently addresses these requirements?","explanation":"Service Control Policies are an effective way to broadly restrict access to certain features of sub-accounts.  Use of a single separate logging account is an effective way to create a secure logging repository.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"0c9b5a803a99a3d2ef53869b6857c0e0","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use ACLs to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.  Establish trust relationships between the Consolidated Billing account and all sub-accounts.","correct":false},{"id":"cbec34b5388f7f183659e82c20fb3abf","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use an SCP to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.","correct":true},{"id":"871870fa49beedfb95106595e4a1c9f4","text":"Configure billing for each account to load into a consolidated RedShift instance.  Create a centralized security account and establish trust relationships between each sub-account.  Configure admin roles within IAM of each sub-account for local administrators.  Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  ","correct":false},{"id":"74a6c6df518100b16da3f16e870b5d5c","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Create localized IAM policies to restrict modification of CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create a centralized security account and establish trust relationships between each sub-account.","correct":false}]},{"id":"375e7161-43df-4d2f-adab-75cc6166a453","domain":"awscsapro-domain5","question":"You build a CloudFormation stack for a new project. The CloudFormation template includes an AWS::EC2::Volume resource that specifies an Amazon Elastic Block Store (Amazon EBS) volume. The EBS volume is mounted in an EC2 instance and contains some important customer data and logs. However, when the CloudFormation stack is deleted, the EBS volume is deleted as well and the data is lost. You want to create a snapshot of the volume when the resource is deleted by CloudFormation. What is the easiest method for you to take?","explanation":"The easiest method is using the DeletePolicy attribute in the CloudFormation template. The \"Snapshot\" value ensures that a snapshot is created before the CloudFormation stack is deleted. The \"Retain\" value is incorrect as it keeps the volume rather than creates a snapshot. The EBS lifecycle manager can create daily snapshot however it is not required in the question. When the CloudWatch Event rule is triggered, the EBS volume may already be deleted and no snapshot can be taken. Besides, the CloudFormation deletion cannot be suspended by a Lambda function.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html","title":"DeletionPolicy Attribute"}],"answers":[{"id":"0719057e0f4aca49eb47cd94c333e599","text":"Add a DeletePolicy attribute in the CloudFormation template and specify \"Snapshot\" to have AWS CloudFormation create a snapshot for the EBS volume before deleting the resource.","correct":true},{"id":"061af6db30e4b65a1d544d4e64546085","text":"Modify the CloudFormation template by adding a DeletePolicy variable for the AWS::EC2::Volume resource. Specify the value of \"Retain\" to automatically create a snapshot of the EBS volume before the stack is deleted.","correct":false},{"id":"692ea271f5ada1d555a5889be933267e","text":"Modify the CloudFormation template to create an EBS snapshot strategy in EBS lifecycle manager which creates a daily snapshot as backup and also another snapshot when the EBS volume’s CloudFormation stack is being deleted.","correct":false},{"id":"555e7ef93e63402dd6a074edd5b8a16d","text":"Create a CloudWatch Event rule that checks the CloudFormation delete-stack event. Trigger a Lambda function that pauses the CloudFormation stack deletion, creates the EBS snapshot of the volume and resumes the stack deletion after the snapshot is created successfully.","correct":false}]},{"id":"0bfd631e-c08c-406a-acf5-a07416aab129","domain":"awscsapro-domain5","question":"Your team uses a CloudFormation stack to manage AWS infrastructure resources in production. As the AWS resources are used by a large number of customers, the update to the CloudFormation stack should be very cautious. Your manager asks for additional insight into the changes that CloudFormation is planning to perform when it updates the stack with a new template. The change needs to be reviewed before being applied by a DevOps engineer. What is the best method to achieve this requirement?","explanation":"CloudFormation Change Sets are able to provide the information on how the running resources are affected by a stack update. The outputs can be reviewed before being executed. Users can view the Change Set through AWS Console or CLI. The Retain option in the DeletionPolicy, CloudFormation stack policy or termination protection helps on protecting the stack resources. However, they cannot provide a summary of  changes in a stack update.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"550c80eaf15eeb89d49aa2a86eb747a6","text":"Enable termination protection in the CloudFormation stack so that the AWS resources cannot be accidentally deleted or modified. Disable the protection only if the changes are approved. Execute the changes in a maintenance window.","correct":false},{"id":"935df51ce2c7f07994c2b8a257489e00","text":"Create a CloudFormation Change Set using AWS Management Console or CLI, review the changes to see if the modifications are as expected and execute the changes to update the stack.","correct":true},{"id":"700aa7cb0f0e8cfb417b67ae5d49e962","text":"Add a CloudFormation stack policy to prevent updates to stack resources. Only after the changes are reviewed and approved, change the stack policy to allow the stack update. Revert the stack policy after the change.","correct":false},{"id":"4657e544cc1daf4315865e230d92dd00","text":"For key AWS resources in the CloudFormation stack, add a Retain option in the DeletionPolicy attribute, which prevents the resources from being accidentally deleted by a stack update. Add a Delete option for the resources that you want to delete along with the stack.","correct":false}]},{"id":"254eeafd-1183-4117-936e-f6f7ffa9d88a","domain":"awscsapro-domain2","question":"A client is having a challenge with performance of a custom data collection application.  The application collects data from machines on their factory floor at up to 1000 records per second.  It uses a Python script to collect data from the machines and write records to a DynamoDB table.  Unfortunately, under times of peak data generation, which only last 1-2 minutes at a time, the Python application has timeouts when trying to write to DynamoDB.  They don't do any analytics on the data but only have to keep it for potential warranty issues.  They are willing to re-architect the whole solution if it will mean a more reliable process.  Which of the following options would you recommend to give them the most scalable and cost-efficient solution?","explanation":"The application is likely running into throttling when writing to DynamoDB.  Kinesis Firehose makes for a good option in this case to accommodate streaming records.  Since we do not have to perform any analytics, we can simply store it on S3 using Firehose.","links":[{"url":"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html","title":"What Is Amazon Kinesis Data Firehose? - Amazon Kinesis Data Firehose"}],"answers":[{"id":"0bd59f25e091570f7b171c3dd76b0206","text":"Change the application design to use SQS and a custom process on an EC2 spot fleet to throttle inbound messages into DynamoDB.","correct":false},{"id":"7e2676bbdbc658bd6a68274495e2376f","text":"Change the application design to use Kinesis to take in the data.  Use Kinesis Firehose to spool the data files out to S3.  Use S3 Lifecycle to transition the files to Glacier after a few days.","correct":true},{"id":"b30cd1d2aae071493c960798a422ade4","text":"Change the application design to use SWF to take in the data.  Use Amazon Elasticache in front of the DynamoDB database as a buffer to throttle the writes.","correct":false},{"id":"61f03e55a88efffb8060d5d3e07f247d","text":"Turn on DynamoDB Auto Scaling and configure appropriate upper and lower limits.","correct":false},{"id":"6ac1d0de8c41824a325523f050a3784e","text":"Change the application design to write the data records to EMR.  Use a Pig script to transfer the data from EMR to DynamoDB periodically.","correct":false},{"id":"80999a665ea6597ad65e91f9d4a84b3f","text":"Change the application design to use Kinesis Streams to take in the data.  Provision at least 5 shards to ensure enough peak capacity.  Configure the Kinesis Streams to load the data into DynamoDB.  Increase the RCU and WCU for the DynamoDB table to match peak needs.","correct":false}]},{"id":"374fde7d-232a-4cfe-b5d6-7755d564c6ca","domain":"awscsapro-domain1","question":"You are consulting for a company that has decided to partially migrate some resources to AWS from their two data centers (DC1 and DC2).  Their first order of business is to design a robust, redundant and cost-effective network connection between their data centers and AWS.  They already have redundant links between DC1 and DC2.  Which of the following architectures provides the highest availability at the least cost?","explanation":"A common and cost effective way to provide a redundant link to AWS with Direct Connect is a VPN connection.  In the event that the Direct Connect path fails at DC1, your on-prem router can redirect traffic over the VPN at DC2 via the DC1-DC2 link.  Having dual Direct Connect links is definitely redundant but more expensive than a VPN.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/","title":"Multiple Data Center HA Network Connectivity – AWS Answers"}],"answers":[{"id":"bcac2f1d84c5f367ede3342f0adda492","text":"Ensure that DC1 and DC2 have separate ISPs.  Setup VPN connections from DC1 and DC2 to a Virtual Private Gateway on AWS.  Create static routes at each DC to use the local VPN to AWS.  Use CloudTrail to monitor traffic on the Virtual Private Gateway and trigger a script to update the static route if one of the VPN connections goes down.","correct":false},{"id":"425adb8435a7c170eef3698fa729f5ea","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure a default route in both DC1 and DC2 to route traffic to the local Direct Connect link.","correct":false},{"id":"abd8b38f393b6a0d0b42f68dca5ec24d","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure BGP to dynamically route traffic across the nearest Direct Connect link.","correct":false},{"id":"77c9fb7459e1b19f6f655d63556016e8","text":"Configure a Direct Connect connection from DC1 to a Virtual Private Gateway on AWS.  Setup a VPN connection from DC2 to a Virtual Private Gateway on AWS.  Configure a dynamic route across DC1 and DC2 for both paths with a route priority favoring the Direct Connect path to AWS.","correct":true}]},{"id":"19591d08-60c8-494e-9c39-d69c6c3390f0","domain":"awscsapro-domain4","question":"Your company's AWS migration was not planned out very well across the enterprise.  As as result, different business units created their own accounts and managed their own resources.  Recently, an internal audit of costs show that there may be some room for improvement with regard to how reserved instances are being used throughout the enterprise.  What is the most efficient way to ensure that the reserved instance spend is being best used?","explanation":"The discounts for Reserved Instances can be shared across accounts that are linked with Consolidated Billing but Reserved Instance Sharing must be enabled.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-ri-consolidated-billing/","title":"EC2 Reserved Instance Consolidated Billing"}],"answers":[{"id":"af8967525080f0a545bb6fbafc8a8f3c","text":"Setup Consolidated Billing for a single account and link all the other various accounts in the organization.  Ensure that Reserved Instance Sharing is turned on.  The discounts for specific reserved instances will automatically be applied to the consolidated invoice.","correct":true},{"id":"617beec9b1a6fd3ca1cec7764ddfd9fb","text":"Use AWS Organizations to organize accounts under one organizational unit.  Use AWS Budgets to analyze utilization of reserved instances.  Reassign those RIs to other accounts that are currently using On-Demand instances.","correct":false},{"id":"f58169ec8f86bed7c84e4510ed491661","text":"Load CloudTrail instance usage data into Redshift for analytics.  Use QuickSight to create some utilization reports on existing reserved instances.  Relocate on-demand instances into regions where reserved instances are underutilized.","correct":false},{"id":"34d8e4aeeb67918e8bd27daba0923994","text":"Use Cost Explorer reports to analyze coverage of reserved instances.  Where there are coverage gaps, purchase more reserved instance capacity for that account. Where there is excess, place those on the Reserved Instance Marketplace.","correct":false}]},{"id":"ebcf9ff3-82a1-48f8-b2fc-5d2aeb1d018c","domain":"awscsapro-domain3","question":"You are consulting with a company who is at the very early stages of their cloud journey.  As a framework to help work through the process, you introduce them to the Cloud Adoption Framework.  They read over the CAF and come back with a list of activities as next steps.  They are asking you to validate these activities to keep them focused.  Of these activities, which would you recommend delaying until later in the project?","explanation":"External communication usually comes much later in the process once project plans are defined and specific customer impact is better understood.","links":[{"url":"https://aws.amazon.com/professional-services/CAF/","title":"The AWS Cloud Adoption Framework"}],"answers":[{"id":"937d0c376f475ce2eac7a3356601b8fb","text":"Investigate the need for training for Program and Project Management staff around agile project management.","correct":false},{"id":"153eadc71676701cd67fdf00dc6c4723","text":"Work with Marketing business partners to design an external communications strategy to be used during potential outages during the migration.","correct":true},{"id":"199907bba5306a10dbadf5a330d5f1f6","text":"Hold a workshop with IT business partners about the creation of an IT Service Catalog concept.","correct":false},{"id":"2f063bd85b14ae1bbeec72bf0f6c06f5","text":"Work with the Human Resources business partners to create new job roles, titles and compensation/remuneration scales.","correct":false},{"id":"525fde29088ab22597d7d8063c7dadf6","text":"Work with internal Finance business partners to design a transparent chargeback model.","correct":false}]},{"id":"2c034786-9b7e-4933-aad2-d0c4b1d89ca8","domain":"awscsapro-domain2","question":"A beach apparel company has begun an initiative to improve their sales analytics capabilities using AWS services. They'll need to be able to visualize summary sales data by product line, territory, and sales channel for each day, month, and year, and they'll need to be able to drill-down with ad-hoc queries on individual sales records. There are multiple data sources that provide transactional information in different formats. The company has chosen Amazon QuickSight as their visualization tool for the summary information. Visualizations and drill-down queries will require three years of rolling sales history, which estimates to seven petabytes of data. Which architecture will provide the best performance and cost efficiency?","explanation":"Using S3 to store the detailed sales transaction data and using Lambda to standardize data formats is the most cost effective option. Storing the summary data in Redshift provides a high performance option for reads from QuickSight, and keeping the detailed transaction data out of Redshift allows for smaller node sizes and lower cost. Amazon Redshift Spectrum can be used for drill-down queries that join tables from both Redshift and S3. For answer number two, Redshift will be a better option than Aurora for OLAP query performance due to it's columnar organization. Answer number four provides no simple way to perform ad-hoc drill down queries.","links":[{"url":"https://aws.amazon.com/redshift/","title":"Amazon Redshift"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"a08e38f138d23c0f1759ab2d1801f67e","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Firehose and load them into Amazon Redshift. Run AWS Glue jobs to format the transaction data in a standard way and perform aggregate functions to write the data into summary tables in Redshift","correct":false},{"id":"ebf4a6c248510d05a046c8f0ea4298b7","text":"Use Amazon Kinesis Data Analytics to format the data source transactions in a standard way and load it into Amazon Aurora. Invoke Lambda functions to aggregate the data and write it into summary tables in Aurora","correct":false},{"id":"b3deac265c2195cb988c345d096800fd","text":"Ingest individual sales transactions from each data source into Amazon S3 with Amazon Kineses Data Firehose. Trigger an AWS Lambda function to format the transaction data in a standard way and redeposit the results in S3. Run AWS Glue jobs to aggregate the summary data into Amazon Redshift","correct":true},{"id":"4740b70569f15040edf0916e47386757","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Streams and write them to Amazon Elastic Block Store on EC2 instances in Auto Scaling Groups. Perform data format standardization and summary aggregation on EC2, and write the summary results to Amazon Redshift tables","correct":false}]},{"id":"ba6576ca-a3da-4742-8917-cf5852a133bc","domain":"awscsapro-domain2","question":"You are in the process of porting over a Java application to Lambda.  You find that one Java application's code exceeds the size limit Lambda allows--even when compressed.  What can you do?","explanation":"If your code is too large for Lambda, it might indicate the need to break the code down into more atomic elements to support microservice best practices.  If breaking the code down is not possible, you should consider deploying in a different way like ECS or Elastic BeanStalk.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/limits.html","title":"AWS Lambda Limits - AWS Lambda"}],"answers":[{"id":"5fd2567315fb3689d9b56ea422cba642","text":"Evaluate the structure of the program and break it into more modular components.","correct":true},{"id":"2c12b2cfb5b150801f171e15a1e6bfb7","text":"Consider containerization and deploy using Elastic Beanstalk.","correct":true},{"id":"0d95df1e6f5e72a068c3fbcbc598b16d","text":"Use AWS CodeBuild to identify unused libraries and remove them from the package. ","correct":false},{"id":"91f37922d4b3b7e70c1f900cb95370e8","text":"Enable Extended Storage in the Lambda console to permit a larger codebase to be deployed.","correct":false},{"id":"a6bb1edf368a254d9604ca8e6419be1d","text":"Consider using API Gateway to offload some of the I/O your code requirements.","correct":false},{"id":"7c5f7ca2bf3fe981064a0883ba4d7158","text":"Change the Java Runtime Version in the Lambda function to one that supports BIGINT. ","correct":false}]},{"id":"d58acd29-561d-4017-a55d-bcff8eb40f90","domain":"awscsapro-domain4","question":"Last month's AWS service cost is much higher than the previous months. You check the billing information and find that the used hours of Elastic Load Balancer (ELB) increases dramatically. You manager asks you to plan and control the ELB usage. When the ELB service has been used for over 5000 hours in a month, the team should get an email notification immediately and further actions will be taken accordingly. Which of the following options is the easiest one for you to choose?","explanation":"AWS Budgets include the types of cost budget, usage budget, reservation budget and savings plan budget. The usage budget enables you to plan the usage of ELB service and receive budget alerts when the actual usage becomes more than a threshold (5000 hours in this scenario). The cost budget type is incorrect as it evaluates the cost instead of usage and you cannot receive budget alerts from either Cost Explorer or AWS Config.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing your costs with Budgets"}],"answers":[{"id":"baf09736ecf556311eb6e77579877ccd","text":"In AWS Config, monitor the usage of all ELB resources within the AWS account. Create a custom Config rule via a Lambda function that calculates the ELB usage and sends an alert message to an SNS topic when the usage is over 5000 hours.","correct":false},{"id":"15fe48e7010a90e06774d9aa9668704a","text":"Create a usage budget in AWS Budgets. Check the ELB running hours for every month and set the budgeted amount to be 5000 hours. Configure an email alert or an Amazon Simple Notification Service (SNS) notification when the actual usage is more than the threshold.","correct":true},{"id":"0f88bd3ee9fd3724922aa89baa9f656d","text":"Launch the Cost Explorer in AWS billing dashboard, filter the EC2 ELB service and configure a CloudWatch alert to track its actual monthly usage. When the monthly ELB usage grows more than 5000 hours, raise the CloudWatch alert and notify an SNS topic.","correct":false},{"id":"39f1073c8a5f51031cb743e7cf45dce2","text":"Calculate the estimated ELB cost when the total ELB usage is 5000 hours in a month. Configure a cost budget in AWS Budgets for the EC2 ELB service and set the number as the threshold. When the cost is greater than the user-defined threshold, send an email alert to the team.","correct":false}]},{"id":"c3ac5de9-a343-4cde-af1b-6c9f89824d2f","domain":"awscsapro-domain5","question":"An external auditor is reviewing your process documentation for a Payment Card Industry (PCI) audit.  The scope of this audit will extend to your immediate vendors where you store, transmit or process cardholder data.  Because you do store cardholder data in the AWS Cloud, the auditor would like to review AWS's PCI DSS Attestation of Compliance and Responsibility.  How would you go about getting this document? ","explanation":"AWS Artifact provides on-demand downloads of AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use. You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls.","links":[{"url":"https://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html?icmpid=docs_artifact_console","title":"What Is AWS Artifact? - AWS Artifact"}],"answers":[{"id":"1d16d307ee989a80e421198a01993a9c","text":"AWS IAM Console","correct":false},{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false},{"id":"d7cb47dd1f374d3ed079b14cc6f2cd75","text":"Submit a Support Case requesting the document","correct":false},{"id":"d9208942349d1c6f7dbaba3661069bc1","text":"AWS WorkDocs","correct":false},{"id":"63df0d05cd43af35c95cf04d92aaf685","text":"AWS Legal Services website","correct":false},{"id":"09e838e873f25f954fef911d50b3d1ab","text":"AWS Pinpoint","correct":false},{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":true}]},{"id":"694c2138-4b6e-466d-b5b2-7a34cc0a3360","domain":"awscsapro-domain1","question":"AWS Cost Management encompasses a number of services to help you to organize, control and optimize your AWS costs and usage.  Which of the following Cost Management related tools gives you the ability to set alerts when costs or usage are exceeded?","explanation":"The correct answer is AWS Budgets.  AWS Cost Explorer lets you visualize, understand, and manage your AWS costs and usage over time. AWS Cost & Usage Report lists AWS usage for each service category used by an account and its IAM users and finally, Reserved Instance Reporting provides a number of RI-specific cost management solutions to help you better understand and manage RI Utilization and Coverage.","links":[{"url":"https://aws.amazon.com/aws-cost-management/aws-budgets/","title":"AWS Budgets"}],"answers":[{"id":"e32a801c8e0beab6abb9361e937365be","text":"AWS Budgets","correct":true},{"id":"824fd559c917b4ae56f36787b886eb81","text":"AWS Cost & Usage Report","correct":false},{"id":"c7f176d72688fd87853e31b84159d541","text":"AWS Cost Explorer","correct":false},{"id":"eef79d956328d5e4ec426d448cc53c74","text":"Reserved Instance Reporting","correct":false}]},{"id":"4c49e888-8f76-4b15-b267-7f6ec35579ca","domain":"awscsapro-domain5","question":"A client has asked you to review their system architecture in advance of a compliance audit.  Their production environment is setup in a single AWS account that can only be accessed through a monitored and audited bastion host. Their EC2 Linux instances currently use AWS-encrypted EBS volumes and the web server instances sit in a private subnet behind an ALB that terminates TLS using a certificate from ACM. All their web servers share a single Security Group, and their application and data layer servers similarly share one Security Group each. Their S3 objects are stored with SSE-S3.  The auditors will require all data to be encrypted at rest and will expect the system to secure against the possibility that TLS certificates might be stolen by would-be spoofers.  How would you help this client pass their audit in a cost effective way? ","explanation":"All the measures they have taken with Certificate Manager, S3 encryption and the EBS volumes meet the audit requirements.  There is no need for LUKS, CloudHSM or client-side encryption.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html","title":"Amazon EBS Encryption - Amazon Elastic Compute Cloud"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"Protecting Data Using Server-Side Encryption with Amazon S3-Managed  Encryption Keys (SSE-S3) - Amazon Simple Storage Service"}],"answers":[{"id":"bdddd09d0832e16504afd5c88136cf7e","text":"Leave the S3 objects alone.","correct":true},{"id":"92194f6603feb3f83a46b00bda37de5a","text":"Deploy CloudHSM and migrate the TLS keys to that service.","correct":false},{"id":"0c31fb48e65443ba5bfa312a7dcc117c","text":"Encrypt the S3 objects with OpenPGP locally before re-uploading them to S3.","correct":false},{"id":"d9447af4853ab8736e49349138cac8fb","text":"Make no changes to the EBS volumes.","correct":true},{"id":"ab9ad7bfd57b97954a6f861d872c6137","text":"Continue to use the ACM for the TLS certificate.","correct":true},{"id":"93113c2b6ca9be67acbd3561eef56481","text":"Reconfigure the EC2 EBS volumes to use LUKS OS-Level encryption.","correct":false}]},{"id":"431e43bc-ccbc-480f-9915-210bc7773d2b","domain":"awscsapro-domain5","question":"You are in the process of migrating a large quantity of small log files to S3 for long-term storage.  To accelerate the process and just because you can, you have created quite sophisticated multi-threaded distributed process deployed across 100 VMs which can load hundreds of thousands of files at one time.  For some reason, the process seems to be throttled somewhere along the chain.  You try many things to try to uncover the source of the throttling but nothing works.  Reluctantly, you decide to turn off the KMS encryption setting for your S3 bucket and the throttling goes away.  You turn AMS-KMS back on and the throttling is back. Given the troubleshooting steps, what is the most likely cause of the throttling and how can you correct it?","explanation":"Through a process of elimination, it seems you have identified the variable that is causing the throttling.  KMS, like other AWS services, does have rate limiters which can be increased via Support Case.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/limits.html","title":"Limits - AWS Key Management Service"}],"answers":[{"id":"60ff77ab365c15bb11771e94e3dc271d","text":"You have exceeded the number of API calls for your account.  You must create a new account.","correct":false},{"id":"01148eae3319190a0b228c6d02c9572c","text":"You are maxing out your network connection.  You must split the traffic over multiple interfaces.","correct":false},{"id":"1dd4f25e52404e18ddec0b8711a82a13","text":"You are hitting the KMS encrypt request account limit.  You must request a limit increase via a Support Case.","correct":true},{"id":"fe629daf7473efc279d7d8ee6f5a5806","text":"You are maxing out your SYNC requests to S3.  You need to request a limit increase via a Support Case.","correct":false},{"id":"05aeb0bc36d7b53aa30bf9e22b6cd120","text":"You are maxing out your PUT requests to S3.  You need to change over to multi-part upload as a workaround.","correct":false}]},{"id":"63a6def8-9b52-4d89-8248-6079ca1393e2","domain":"awscsapro-domain3","question":"You are helping a client prepare a business case for cloud migration.  One of the required parts of the business case is an estimation of AWS costs per month.  The client has about 200 VMs in their landscape under VMware vCenter.  Due to security concerns, they will not allow any external agents to be installed on their VMs for discovery.  How might you most efficiently gather information about their VMs to build a cost estimate with the least amount of effort? ","explanation":"The Application Discover Service uses agent-based or agentless collection methods.  Agentless collection is only available for those customers using VMware.  The AWS Application Discovery Agentless Connector is delivered as an Open Virtual Appliance (OVA) package that can be deployed to a VMware host. Once configured with credentials to connect to vCenter, the Discovery Connector collects VM inventory, configuration, and performance history such as CPU, memory, and disk usage and uploads it to Application Discovery Service data store.  This data can then be used to estimate monthly costs.","links":[{"url":"https://aws.amazon.com/application-discovery/faqs/?nc=sn&loc=6","title":"AWS Application Discovery Service FAQs"}],"answers":[{"id":"b279286ed54b2394c29d2fbd0061b4c4","text":"Provision an S3 bucket for data collection.  Use SCT to scan the existing VMware landscape for VM hardware, network connection and performance parameters.  Retrieve the SCT CSV data from the data collection bucket and use it to align EC2 instance types with existing VM parameters.  Use this cross-reference to calculate estimated monthly costs for AWS.","correct":false},{"id":"09d153439d976dabcdd13a4a2f8a4a5f","text":"Use Application Discovery Service to gather details on the network connections, hardware and performance of the VMs.  Export this data as CSV and use it to approximate monthly AWS costs by aligning current VMs with similar EC2 instances types.","correct":true},{"id":"0a144eb3993e48693aab4c9744b6acb2","text":"Use AWS OpsWorks to remotely pull hardware, network connection and performance of the VMs.  Export the collected data from OpsWorks in Excel format.  Use the collected data to align current VMs with similar EC2 instance types and calculate an estimated monthly cost.","correct":false},{"id":"30cbd0a822a4905f8a795dcb7cc3d31e","text":"Use a custom script to iteratively log into each VM and pull network, hardware and performance details of the VM.  Write the data out to S3 in CSV format.  Use that data to select corresponding EC2 instance sizes and calculate estimated monthly cost.","correct":false}]},{"id":"6b6689f4-b150-482a-aa96-eab1674cb232","domain":"awscsapro-domain5","question":"Quality Auto Parts, Inc. has installed IoT sensors across all of their manufacturing lines. The devices send data to both AWS IoT Core and Amazon Kinesis Data Streams. Kinesis Data Streams triggers a Lambda function to format the data, and then forwards it to AWS IoT Analytics to perform monitoring and time-series analyses, and to take actions based on business processes. After an equipment failure on one of the manufacturing lines causes tens of thousands of dollars in revenue losses, it's determined that alarms for a specific piece of equipment where received seventy-five seconds after the issue originated, and that automated corrective action within a few seconds of the problem could have avoided the financial losses altogether. What changes should be made to the architecture to improve the latency of device alerts?","explanation":"AWS IoT Analytics is useful for understanding long-term device performance, performing business reporting, and identifying predictive fleet maintenance needs, but common latencies run from seconds to minutes. If you need to analyze IoT data in real-time for device monitoring, use Kinesis Data Analytics, which provides latencies in the millisecond to seconds range. A Lambda function can be used as the destination for Kinesis Data Analytics to perform corrective actions. IoT Core rules can write messages to a Kinesis stream, but not directly to Kinesis Data Analytics. Having a Lambda function perform anomaly detection will work, but will require more logic to be written for query setup and execution than using a specialized service like Kinesis Data Analytics. With Amazon CloudWatch Alarms, an alarm will watch a single metric over a period time, but will not provide the capabilities of SQL to detect complex anomaly conditions.","links":[{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/aws-reference-architecture-time-series-processing.pdf?did=wp_card&trk=wp_card","title":"Processing IoT Time Series Data on AWS"},{"url":"https://aws.amazon.com/iot-analytics/faq/","title":"AWS IoT Analytics FAQs"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/05/introducing-real-time-iot-device-monitoring-with-kinesis-data-analytics/","title":"Introducing Real-Time IoT Device Monitoring with Kinesis Data Analytics"}],"answers":[{"id":"fe8ff20697982ca413f81ea14472e603","text":"Add Amazon Kinesis Data Analytics as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":true},{"id":"7e2eb8f3a96a390aab88e66000821c26","text":"Create an AWS IoT Core rule to write the message to Amazon Kinesis Data Analytics to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":false},{"id":"522af3cd7d520d3e94e97c02d19c0672","text":"Create an AWS IoT Core rule to write the message to Amazon CloudWatch Alarms to detect anomalies in the data. Invoke another AWS Lambda function from CloudWatch Alarms to perform device corrective action when needed.","correct":false},{"id":"71ade679994c0c1e6e55b3853194e4c5","text":"Add another AWS Lambda function as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Have the Lambda function write the anomalies to Amazon DynamoDB and perform device corrective action when needed.","correct":false}]},{"id":"4ebf4191-8562-4f67-945d-ea5aae2f9f26","domain":"awscsapro-domain3","question":"You are consulting for a client who is trying to define a comprehensive cloud migration roadmap.  They have a legacy custom ERP system written in RPG running on an AS400 system.  RPG programmers are becoming rare so support is an issue.  They run Lotus Notes email which has not been upgraded in years and thus out of support.  They do have a web application that serves as their CRM created several years ago by a consulting group.  It is a Java and JSP-based application running on Tomcat with MySQL as the data layer hosted on a Red Hat Linux server. The company is in a real growth cycle and realizes their current platforms cannot sustain them.  So, they are about to launch a project to implement SAP as a replacement for their legacy ERP system over the next year.  What migration strategy would you recommend for their landscape that would allow them to modernize as soon as possible?","explanation":"In this case, retiring Lotus Notes is the better move because it would just prolong the inevitable by simply migrating to EC2.  The CRM system is fairly new and can be re-platformed on Elastic Beanstalk.  Due to the impending ERP upgrade, it makes no sense to do anything with the legacy ERP.  It would take lots of work to port over an RPG application to run on AWS--if it's even possible.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud | AWS Cloud Enterprise  Strategy Blog"}],"answers":[{"id":"20ef21af4ab4ecda70e90e90861b154c","text":"Retire the Lotus Notes email and implement AWS Workmail.  Replatform the CRM application Tomcat portion to Elastic Beanstalk and the data store to MySQL RDS.  Invest time in training Operations staff CloudFormation and spend time architecting the landscape for the new SAP platform.  Do nothing to the legacy ERP platform until the SAP implementation is complete.  ","correct":true},{"id":"7579c185856bdbbcffbe49a649866488","text":"Rehost Lotus Notes mail on EC2 instances.  Refactor the CRM application to make use of Lambda and DynamoDB.  Use a third-party RPG to Java conversion tool to create Java versions of the legacy ERP to make it more supportable. Invest time in training developers continuous integration and continuous deployment concepts.  Because SAP implementations always take longer than estimated, rehost the legacy ERP system on EC2 instances so the AS400 can be retired.","correct":false},{"id":"c7be9ce93a4a7a74b44e281cb697dcc4","text":"Begin a product search for a new CRM system that is cloud-ready.  Once identified, migrate the existing CRM into the new CRM system.  Migrate Lotus Notes to Workmail using the AWS Migration Hub.  Invest in training the IT staff about AWS through a Certified AWS Training Partner.  Provision and run the various SAP environments from scratch using EKS.  Do nothing to the legacy ERP until the SAP implementation is complete.","correct":false},{"id":"480496ebe4100958e2f466291752ae2d","text":"Retire the CRM application and migrate the MySQL data over to Aurora.  Use QuickSight to provide access to the application for users.  Pay back support agreements to bring Lotus Notes back into support so it can be upgraded.  Migrate Notes email to EC2 instances.  Invest time in training Operations staff CloudFormation.  Create the complete SAP landscape as scriptable elements.  Do nothing to the legacy ERP platform until the SAP implementation is complete.","correct":false}]},{"id":"12556935-2b08-4f73-b7f6-6b82bd7fa1a0","domain":"awscsapro-domain1","question":"Your company has recently acquired another business unit and is in the process of integrating it into the corporate structure.  Like your company, the acquisition's IT assets are fully hosted on AWS.  They have a mix of EC2 instances, RDS instances and Lambda-based applications but these will become end-of-life as the new business unit transitions to your company's standard applications over the next year.  Fortunately, the CIDR blocks of the respective VPCs do not overlap.  If the goal is to integrate the new network into your current hub-and-spoke network architecture to provide full access to each other's resource, what can you do that will require the least amount of disruption and management?","explanation":"VPC Peering provides a way to connect VPCs so they can communicate with each other.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/peering/invalid-peering-configurations.html","title":"Unsupported VPC Peering Configurations - Amazon Virtual Private Cloud"}],"answers":[{"id":"d8d7beaa505f84c757a93d64f70a0ae7","text":"Initiate a VPC peering request from each of your spoke VPCs to each new VPC.  Configure route tables in each spoke VPC and new VPC to route traffic to the respective VPC.","correct":false},{"id":"6088d94d774b3a3e9be7fc71331181d1","text":"setup a VPN connection via Internet Gateway between each new and existing VPC to create a mesh network.  Update route tables to direct traffic to the appropriate VPC.","correct":false},{"id":"8e438cf633d07a0d70f2d21978e083bb","text":"Configure a VPC Gateway Endpoint for EC2, RDS and Lambda services.  Configure route tables in your existing VPCs to use the endpoints to communicate with the new VPCs.","correct":false},{"id":"0d3332dc947d3085b33697562a70c9f5","text":"Initiate a VPC peering request between your hub VPC and all VPCs from the new business.  Setup routes in the hub to direct traffic to and from the new VPCs.","correct":true},{"id":"b4cd9cd07c83868a626224de0d92c70a","text":"Initiate a Transitive Peering request from each new VPC to your hub VPC.  Configure routes in the hub to direct traffic to and from the new VPCs.","correct":false}]},{"id":"79f2f5be-b591-44e6-957b-eb0383640d7d","domain":"awscsapro-domain5","question":"You have a standard SQS queue to receive messages from the frontend application. The backend application is JAVA based and the AWS SDK is used to get the messages from the queue for processing. The SQS queue is not busy most of the time. According to the backend application logs, there is a high number of empty ReceiveMessageResponse instances returned. You want to adjust the settings to minimize the number of empty responses and reduce the cost. How would you implement this? ","explanation":"Amazon SQS long polling is preferable to short polling in most of the cases. Long polling requests let the consumers receive messages as soon as they arrive in the queue. It can help to reduce the number of empty responses. In order to enable long polling, the attribute ReceiveMessageWaitTimeSeconds should be more than 0. Short polling is incorrect. Visibility timeout and delivery delay do not address the problem of empty responses.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html","title":"Amazon SQS short and long polling"}],"answers":[{"id":"220352e5b3779c1f2030cfd4b391b19e","text":"Modify AWS SDK to get the messages in the SQS queue by short polling. The ReceiveMessage call from the consumer sets the WaitTimeSeconds attribute to 0. As a result, the empty responses are eliminated.","correct":false},{"id":"a5cdcd2968c3566cbb7fc7bcd5fef01a","text":"Add a delivery delay in the SQS queue such as 1 minute. The delay helps to postpone the delivery of new messages to the queue for some time. When the JAVA application polls the messages from the queue, there will be a lower chance to get an empty response.","correct":false},{"id":"203fa6faf2e6bf53939b43300ec6dac2","text":"Increase the default visibility timeout of the queue to reduce the possibilities that the messages become visible to consumers again. The application can also use the ChangeMessageVisibility API to specify a suitable timeout value.","correct":false},{"id":"c3cbf51c591cb7fa17bd023ab814f95c","text":"Consume the messages in the SQS queue using long polling. Set the queue attribute ReceiveMessageWaitTimeSeconds to be more than 0. Amazon SQS will wait until there is an available message in a queue before sending a response.","correct":true}]},{"id":"e04f4064-644c-4e96-a040-90164b4f91b0","domain":"awscsapro-domain2","question":"A fast-food restaurant chain would like to automate their drive-thru capabilities with chatbot speech technology to capture full orders for the kitchen crew without human interaction. The automated service will need to be able to offer various up-sell options based on menu items selected (for example, suggesting fries if a burger is ordered). Line item and total order price will need to be displayed for the customer as the order progresses. When customers use vocabulary not recognized by the service, that speech will need to be handled properly in future interactions. Ordering volume increases dramatically around lunch and dinner-time at the fast-food restaurants. Which architecture will provide the most scalable solution for the automated ordering service?","explanation":"Lex provides the chatbot logic for the conversation, but Lex needs Polly to render the speech. Lex integrates seamlessly with Lambda, providing the capability to read and write database information. Product pricing would most likely be part of a larger corporate ERP system which would more likely be using a relational database like Aurora than DynamoDB. Up-sell logic can be coded directly in Lex. Lex captures unrecognized utterances, which can be fed to a SageMaker model to determine future actionable options. A Lambda function can apply the SageMaker inferences to Lex's order flow logic, but SageMaker can't do that on it's own. All of the services used in this solution are managed services which scale automatically, except for Aurora which is only used for reads in this use case. Transcribe is not needed to convert speech to text as Lex does this inherently.","links":[{"url":"https://aws.amazon.com/machine-learning/?nc2=h_ql_prod_ml","title":"Machine Learning on AWS"},{"url":"https://aws.amazon.com/lex/","title":"Amazon Lex"},{"url":"https://aws.amazon.com/polly/","title":"Amazon Polly"},{"url":"https://aws.amazon.com/sagemaker/","title":"Amazon SageMaker"}],"answers":[{"id":"c3610e508d769c91f319fbaef356928e","text":"Have the drive-thru device speak order transaction statements from Amazon Polly. Send customer utterances to Amazon Lex, which calls an AWS Lambda function to retrieve menu item pricing information from Amazon Aurora, and to deposit order line items in Amazon DynamoDB. Determine order up-sell options in Lex. Send unrecognized customer utterances to Amazon SageMaker, and regularly invoke a Lambda function to update Lex's order flow logic from SageMaker's inferences. Have Lex send order transaction statements to Polly.","correct":true},{"id":"c24a3f6c66e92b76fe292d174d28c80d","text":"Have the drive-thru device speak order transaction statements from Amazon Polly. Send customer utterances to Amazon Transcribe. Have Transcribe send the text of the speech to Amazon Lex, which calls an AWS Lambda function to retrieve menu item pricing and up-sell information from Amazon DynamoDB. The Lambda function also deposits order line items in Amazon DynamoDB. Determine order up-sell options in the Lambda function. Send unrecognized customer utterances to Amazon SageMaker, and have SageMaker update Lex's order flow logic. Have Lex send order transaction statements to Polly.","correct":false},{"id":"4b2c3233ba5ed40f66a5c98230dc85c0","text":"Have Amazon Lex handle all chatbot interaction with the drive-thru device. Configure Lex to call an AWS Lambda function to retrieve menu item pricing information from Amazon DynamoDB, and to deposit order line items in Amazon DynamoDB. Determine order up-sell options in the Lambda function. Send unrecognized customer utterances to Amazon SageMaker, and regularly invoke a Lambda function to update Lex's order flow logic from SageMaker's inferences.","correct":false},{"id":"e142e997b7988dd2168b7c5bcc72a973","text":"Have Amazon Lex handle all chatbot interaction with the drive-thru device. Configure Lex to call an AWS Lambda function to retrieve menu item pricing information from Amazon Aurora, and to deposit order line items in Amazon DynamoDB. Determine order up-sell options in Lex. Send unrecognized customer utterances to Amazon SageMaker, and have SageMaker update Lex's order flow logic.","correct":false}]},{"id":"0ee4566a-508e-472d-9789-3318e3284aca","domain":"awscsapro-domain5","question":"You are an AWS Solutions Architect and you maintain a CloudFormation stack that includes the resources of a network load balancer and an Auto Scaling group. The ASG has one running instance. A developer uses the instance for feature development and testing. However, after he adds some configurations and restarts an application process, the instance is terminated by the Auto Scaling group and a new instance is created. The new configurations are lost in the new instance. You need to modify the resource settings to make sure that the instance is not terminated by the ASG when application processes are restarted. Which of the following methods would best achieve this?","explanation":"The instance fails the health check in the ELB target group and is then terminated by ASG whenever the application processes are restarted. The prevent the ASG from terminating the EC2 instance you need to modify the health check type from ELB to EC2. As a result, even if the instance fails the health check in the ELB target group, it will not be terminated by the Auto Scaling group. You do not need to create an AMI or a new launch configuration to address the issue. And the custom health check script that runs every minute cannot prevent the instance from being terminated.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health checks for Auto Scaling instances"}],"answers":[{"id":"1584209c598c68a956227b3770a97fb2","text":"Create an AMI and configure a new launch configuration with the AMI. Then modify the Auto Scaling group to use the new launch configuration and launch a new instance.","correct":false},{"id":"c66bf36d77a2aa4cf5ef75a4ac494df8","text":"Store all custom configuration scripts in an S3 bucket and create a new launch configuration. In its user data section, download the scripts from the S3 bucket and execute them. Whenever a new instance is launched, the configurations can be installed automatically. ","correct":false},{"id":"745390da2b4433786bf4cdf17df3d2d3","text":"Edit a health check shell script that performs some sanity checks in the EC2 instance. If the sanity checks pass, the shell script uses AWS CLI “aws autoscaling set-instance-health” to set its status to be healthy. Run the script every minute.","correct":false},{"id":"8fad414c494de200ee3990b334d22b13","text":"Update the CloudFormation script and modify the health check type from ELB to EC2.","correct":true}]},{"id":"401cbed4-e977-4303-9344-586af01a4180","domain":"awscsapro-domain2","question":"You have been contracted by a manufacturing company to create an application that uses DynamoDB to store data collected in a automotive part machining process.  Sometimes this data will be used to replay a process for a given serial number but that's always done within 7 days or so of the manufacture date.  The record consists of a MACHINE_ID (partition key) and a SERIAL_NUMBER (sort key).  Additionally, there is a CREATE_TIMESTAMP attribute that contains the creation timestamp of the record and a DATA attribute that contains a BASE64 encoded stream of machine data.  To keep the DynamoDB table as small as possible, the industrial engineers have agreed that records older than 30 days can be purged on a continual basis.  Given this, what is the best way to implement this with the least impact on provisioned throughput.","explanation":"Using DynamoDB Time to Live feature is a perfect way to purge out old data and not consume any WCU or RCU.  Other methods of deleting records would impact the provisioned capacity units.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","title":"Time To Live - Amazon DynamoDB"}],"answers":[{"id":"ef62d6ec88d117a0ac0cb7c99cd1abbd","text":"Use DynamoDB Streams to trigger a Lambda function when the record ages past 30 days.  Use the DynamoDB SDK in the Lambda function to delete the record.","correct":false},{"id":"081f76fb4967f9d10a3799ae400ad898","text":"Update the table to add a attribute called EXPIRE  Change the application to store EXPIRE as CREATE_TIMESTAMP + 30 days.  Enable Time to Live on the DynamoDB table for the EXPIRE attribute.","correct":true},{"id":"7c29d6c8effb51be4df54033ce45d01f","text":"Enabled Lifecycle Management on the DynamoDB table.  Create a rule that deletes any records where CREATE_TIMESTAMP attribute is greater than 30 days old.","correct":false},{"id":"0a945c4865c940ffaddaeade6f6bbdaf","text":"Use Step Functions to track the lifecycle of DynamoDB records.  Once 30 days has elapsed, branch to a Delete step and trigger a Lambda function to remove the record.","correct":false},{"id":"41f68b5f48ef97cc437ebfe50ae10882","text":"Use AWS Batch to execute a daily custom script which queries the DynamoDB table and deletes those records where CREATE_TIMESTAMP is older than 30 days.  ","correct":false}]},{"id":"2eee6f1c-96d7-4d2b-821f-4ce8acaf3de3","domain":"awscsapro-domain5","question":"You've deployed a mobile app for a dance competition television show's viewers to vote on performances. The app's backend leverages Amazon API Gateway, AWS Lambda, and Amazon RDS Oracle, with voting activity going from devices directly to API Gateway. In the middle of the broadcast, you begin receiving errors in CloudWatch indicating that the database connection pool has been exhausted. You also see log entries in CloudWatch with a 429 status code. After the show concludes, ratings for the app indicate a very poor user experience, with multiple retries needed to cast a vote. What would be the best way to increase the scalability of the app going forward?","explanation":"Placing Kineses between API Gateway and Lambda decouples the architecture, making use of an intermediary service to buffer incoming requests. The 429 status code indicates a Lambda concurrency throttling error, which you can resolve by controlling the Kinesis batch size per batch delivery. Database sharding will increase scalability, but will still have an upper limit of capacity. Increasing available Lambda memory will have no effect. Inserting a Lambda traffic manager doesn't address the database scalability issues, nor does increasing the regional Lambda concurrency limit. Modifying RDS DB Parameter Group values will require a database restart to take effect, which won't be feasible during live voting activity.","links":[{"url":"https://aws.amazon.com/blogs/architecture/how-to-design-your-serverless-apps-for-massive-scale/","title":"How to Design Your Serverless Apps for Massive Scale"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/scaling.html","title":"AWS Lambda Function Scaling"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","title":"Using AWS Lambda with Amazon Kinesis"}],"answers":[{"id":"8f962935878f87ebc9523dc54f505886","text":"Scale the database horizontally by creating additional instances and use sharding to distribute the data across them. Provide the Lambda function with a mapping of the sharding scheme in DynamoDB. Increase the amount of memory available to the Lambda function during execution","correct":false},{"id":"2f66b519925956d6a91d0026056904d9","text":"Have API Gateway route requests to a new Lambda function that manages traffic and retries for the voting logic Lambda function. Request that the regional function concurrency limit be increased based on volume projections","correct":false},{"id":"a4051078cf0289e689e79fe70eab1f14","text":"Create a separate Lambda function to increase the maximum DB connection value in the RDS DB Parameter Group when a CloudWatch Metrics DB connection threshold is exceeded. Invoke Lambda functions with an 'event' invocation type to retry failed events automatically","correct":false},{"id":"6b6e6fb8b54db42df4d343376ccd8c60","text":"Insert Amazon Kinesis between API Gateway and Lambda, and configure Kinesis as an event source for Lambda. Set the number of records to be read from a Kinesis shard to an optimal value based on volume projections","correct":true}]},{"id":"a5ad7848-a002-475f-8cb1-eab23822846e","domain":"awscsapro-domain4","question":"A toy company needs to reduce customer service costs related to email handling. With the current process, representatives read the emails, determine the intent, identify next best action, and compile all information required for a response. The company would like to automate as much of the process as possible. They've already established connectivity to AWS for other applications, and they're planning to link their corporate email exchange to Amazon Simple Email Service (SES). Which architecture will provide them with the best cost reduction opportunity over their current solution?","explanation":"The process starts with the storing of the raw email text to S3 for use by the machine learning services later. The SNS notification triggers the email responder workflow, which employs Comprehend to extract keywords. The keywords serve as input parameters to the SageMaker models for formulation of the next best action. The confidence scores from the SageMaker inference drive the decision as to whether a Lambda function should generate an automated email response or route to a representative for handling. Using EC2 or ECS will generally be more expensive and require more operational effort than using Lambda. Transcribe is used for voice recognition, and Amazon Lex is used for creating chat bots. Neither is used for keyword extraction. Amazon Pinpoint provides additional customer contact capabilities not needed for this use case.","links":[{"url":"https://aws.amazon.com/comprehend/","title":"Amazon Comprehend"},{"url":"https://aws.amazon.com/sagemaker/","title":"Amazon SageMaker"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/ai-ml-based-intelligent-email-responder-ra.pdf?did=wp_card&trk=wp_card","title":"AI/ML Based Intelligent Email Responder"}],"answers":[{"id":"fe8b2750f1ebef6fc86d4bf7224b99cc","text":"Configure SES to write the emails to an Amazon Simple Queue Service queue. Have a program on EC2 read the queue and invoke Amazon Transcribe to perform keyword extraction. Call Amazon SageMaker intent determination and next-best-action model endpoints. Based on confidence scores, have the program either build and send a response email through Amazon Pinpoint, or route the original email to a representative for handling","correct":false},{"id":"330f1889136383d262cb0d652843bd75","text":"Configure SES to write the emails to S3 and publish notifications to an Amazon Simple Notification Service topic. From the SNS publish, trigger an AWS Lambda function to invoke Amazon Lex to perform keyword extraction. Have the Lambda function forward the email text and keywords to a representative for intent and next-best-action determination. Give the representative a menu of pre-written response letters to be customized and sent automatically through SES.","correct":false},{"id":"c7ce297a851f2728e3e035b08aed9f8f","text":"Configure SES to write the emails to S3 and publish notifications to an Amazon Simple Notification Service topic. From the SNS publish, trigger an AWS Lambda function to invoke Amazon Comprehend to perform keyword extraction. Trigger another Lambda function to call Amazon SageMaker intent determination and next-best-action model endpoints. Based on confidence scores, have another Lambda function either build and send a response email through SES, or route the original email to a representative for handling","correct":true},{"id":"36fb6500a6d540182f27959a12c1e9e5","text":"Configure SES to write the emails to an Amazon Simple Queue Service queue. Have a program in an Amazon Elastic Container Service container read the queue and invoke Amazon Transcribe to perform keyword extraction. Call Amazon SageMaker intent determination and next-best-action model endpoints. Based on confidence scores, have the program either build and send a response email through SES, or route the original email to a representative for handling","correct":false}]},{"id":"f02ba751-479b-4ff0-a09b-8f18a63177b5","domain":"awscsapro-domain3","question":"An automotive supply company has decided to migrate their online ordering application to AWS. The application leverages a Model-View-Controller architecture with the user interface handled by a Tomcat server and twenty thousand lines of Java Servlet code. Business logic also resides in two thousand lines of PL/SQL stored procedure code in an Oracle database. The company's technology leadership has directed your team to move the database to a more cost-effective offering, and to adopt a more cloud-native architecture. Business objectives dictate that the application must be live in the AWS cloud in sixty days. Which migration approach will provide the most scalable architecture and meet the schedule objectives?","explanation":"This solution will require trade-offs between schedule requirements and architectural desires. Converting twenty thousand lines of Model-View-Controller code to a serverless architecture in sixty days is unreasonable, so moving the Tomcat MVC as-is to EC2 for the initial migration is the best approach. We can migrate to a serverless user interface in a later phase. Database Migration Service will suit our needs well for moving the application data to Aurora, but the most scalable architecture strategy is to migrate the stored procedure code out of the database so that database nodes won't need to be resized when the business logic needs more compute resources. Under normal circumstances, recoding two thousand lines of PL/SQL code to Python Lambda functions within a sixty day time frame will not be a problem.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/blogs/database/migrate-your-procedural-sql-code-with-the-aws-schema-conversion-tool/","title":"Migrate Your Procedural SQL Code with the AWS Schema Conversion Tool"},{"url":"https://aws.amazon.com/lambda/","title":"AWS Lambda"}],"answers":[{"id":"97a348ed01424c357958b49bcc030935","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"2cf84f7f8daee7548143ad181423c7cb","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"db222d8a15bda541fc4147908131cfd6","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service to move the application data into Amazon Aurora. Convert the stored procedure code to AWS Lambda Python functions, and modify the Servlet code to invoke them","correct":true},{"id":"5ab40b2a54c4e82a7aafa05c8fc9a458","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service to migrate the application data and stored procedures to an Amazon RDS Oracle instance","correct":false}]},{"id":"bdd6d6a9-48a5-45f8-bb74-873f266d85df","domain":"awscsapro-domain2","question":"A hospital would like to reduce the number of readmissions for high risk patients by implementing an interactive voice response system to provide reminders about follow up visit requirements after patients are discharged. The hospital has the capability to automatically send HL7 messages that include the patient's phone number and follow up visit information from its medical records application via Apache Camel. They've chosen to deploy the solution on AWS. They already have a VPN connection to AWS, and all aspects of the application need to be HIPAA eligible. Which architecture will provide the most resilient and cost effective solution for the automated call system?","explanation":"S3 provides a low cost repository for the HL7 messages received. Having Lambda write the object keys to SQS, and having another Lambda function retrieve and parse the messages gives the architecture asynchronous workflow. Amazon Connect provides the capability to define call flows and perform IVR functions. Each of these services is HIPAA eligible. DynamoDB is also a good option for storing message information, but will be more expensive than S3. Amazon Pinpoint can place outbound calls, but is not able to perform interactive voice response functions. Amazon Comprehend Medical doesn't create call flow sequences.","links":[{"url":"https://aws.amazon.com/compliance/hipaa-eligible-services-reference/","title":"HIPAA Eligible Services Reference"},{"url":"https://aws.amazon.com/connect/","title":"Amazon Connect"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/reduce-hospital-readmissions-ra.pdf?did=wp_card&trk=wp_card","title":"Reducing Hospital Readmissions"}],"answers":[{"id":"9585e2e5994ba5cdc2db33c22d9230cf","text":"Set up Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Connect to place the call to the patient.","correct":false},{"id":"a8c26f088ade23063b2bf5f202a74cce","text":"Have Apache Camel write the HL7 messages to Amazon Kineses Data Streams. Configure a Lambda function as a consumer of the stream to parse the HL7 message and write the information to Amazon DynamoDB. Trigger another Lambda function to pull the patient data from DynamoDB and send it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"8121c4e6e285161311cacf7b8031d5af","text":"Configure Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to write each HL7 message object key to Amazon Simple Queue Service FIFO. Have another Lambda function read messages in sequence from the SQS queue and use the object key to retrieve and parse the HL7 messages. Use that same Lambda function to write patient information to Amazon Connect to place the call using an established call flow.","correct":true},{"id":"c9bee4c5e4463b67e2726d3bed2ceed1","text":"Configure Apache Camel to write the HL7 messages to Amazon Kineses Data Firehose, which stores the patient information in Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Pinpoint to place the outbound call.","correct":false}]},{"id":"4b00251a-a278-4d88-b715-955b4752a79a","domain":"awscsapro-domain2","question":"You'd like to create a more efficient process for your company employees to book a meeting room.  Which of the following is the most efficient path to enabling this improved business experience?","explanation":"With Alexa for Business, you can enlist Alexa-enabled devices to perform tasks for employees like retrieve information, start conference calls and book meeting rooms.","links":[{"url":"https://aws.amazon.com/blogs/business-productivity/announcing-room-booking-for-alexa-for-business/","title":"Announcing Room Booking for Alexa for Business | Business Productivity"}],"answers":[{"id":"e43eb2c75536fac2d714b862f4fec490","text":"Invest in a voice-to-text API from the AWS Marketplace.  Create a custom Lambda function that calls the API and books a conference room.  Equip each conference room with Amazon Dash buttons and configure them to invoke the Lambda function.","correct":false},{"id":"12cb35ea295a9a96bfba94741cb4f0df","text":"Sign-up for Amazon Chime.  Create conference rooms in the console and place speakerphones in each conference room.","correct":false},{"id":"5445641c568edaf760839d9f5bb7169c","text":"Configure an Alexa device with a custom skill backed by a Lambda function.  Use Amazon Lex to convert the audio sent to the Lambda function into an actionable skill.  ","correct":false},{"id":"a723a332c2ed052968becb9b6824e2a4","text":"Sign-up for AWS Alexa for Business. Create conference rooms in the console and place an Alexa device in each conference room.","correct":true}]},{"id":"ffae5615-188b-4023-aae2-71270158730a","domain":"awscsapro-domain5","question":"Several teams are using an AWS VPC at the same time. The VPC has three subnets (subnet-1a, subnet-1b, subnet-1c) in three availability zones (eu-west-1a, eu-west-1b, eu-west-1c) respectively. As there are more and more AWS resources created, there is a shortage of available IP addresses in subnet-1a and subnet-1b. The subnet subnet-1c in availability zone eu-west-1c still has plenty of IP addresses. You use a CloudFormation template to create an Auto Scaling group (ASG) and an application load balancer for the ASG. You enable three availability zones for the load balancer and the Auto Scaling group also spans all the three subnets. The CloudFormation stack usually fails to launch because there are not enough IP addresses. High availability is not required for your project since it is a proof of concept. Which of the following methods is the easiest one to resolve your problem?","explanation":"The easiest way is to enable only the eu-west-1c availability zone for ELB and launch ASG instances in subnet-1c. Only the IP addresses from eu-west-1c are required to create the resources. For a VPC, the existing CIDR cannot be modified and you also cannot add another CIDR IP range to an existing subnet. For an application load balancer, there is no IP balancing feature and IP addresses from a subnet cannot be reserved by other subnets.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-subnets.html","title":"Availability Zones for Your Application Load Balancer"}],"answers":[{"id":"9cb12513be8578fb017ebb7d1b302f9a","text":"Enable the IP balancing feature in the application load balancer so that the IP addresses are equally distributed among subnets. When elastic load balancer is created, available IP addresses in one subnet can be reserved by other subnets.","correct":false},{"id":"94887cfd8a06ae986d5721a940c3c52b","text":"Modify your CloudFormation template to enable only the availability zone eu-west-1c for the application load balancer and launch the Auto Scaling group in subnet-1c which belongs to eu-west-1c.","correct":true},{"id":"1f29e88227ce0771f9e43093bd53583a","text":"Add more IP addresses by extending the CIDR range for the VPC. Create a new subnet in each availability zone and reserve at least 128 IP addresses in a subnet. Modify the CloudFormation template to use the new subnets for the Auto Scaling group and ELB.","correct":false},{"id":"b8c040b89e07e5150d4a8a3899d41659","text":"Add another IPv4 CIDR to the VPC which should have at least 256 IP addresses. Add another IP CIDR block to subnet-1a and subnet-1b to increase the available IP addresses.","correct":false}]},{"id":"2d9a7fba-ee40-4128-8f36-b32ef55ce662","domain":"awscsapro-domain3","question":"You have just been informed that your company's data center has been struck by a meteor and it is a total loss.  Your company's applications were not capable of being deployed with high availability so everything is currently offline.  You do have a recent VM images and DB backup stored off-site.  Your CTO has made a crisis decision to migrate to AWS as soon as possible since it would take months to rebuild the data center.  Which of the following options will get your company's applications up and running again in the fastest way possible?","explanation":"The Server Migration Service uses the Server Migration Service Connector which is an appliance VM that needs to be loaded locally in vCenter.  We don't have a VMware system...only a backup of an image so this won't work.  The best thing we can do is import the VM and restore the database.","links":[{"url":"https://docs.aws.amazon.com/server-migration-service/latest/userguide/prereqs.html","title":"Server Migration Service (SMS) Requirements - AWS Server Migration Service"},{"url":"https://aws.amazon.com/ec2/vm-import/","title":"VM Import/Export"}],"answers":[{"id":"364a256b0572da0bd43823d35b22e01d","text":"Call your data communications provider and order a Direct Connect link to your main office.  Order a Snowball Edge to serve as a mobile data center.  Restore the VM image to the Snowball Edge device as an EC2 instance.  Restore the backup to an RDS instance on the Edge device.  When the Direct Connect link is installed, use that to smoothly migrate to AWS.","correct":false},{"id":"e50f3fc14c0feac5ff24b30bf605d687","text":"Use Server Migration Service to import the VM into EC2.  Use DMS to restore the backup to an RDS instance on AWS.","correct":false},{"id":"b8015467f854fe29575bd8fd26c819a5","text":"Use VM Import to upload the VM image to S3 and create the AMI of key servers.  Manually start them in a single AZ.  Stand-up a single AZ RDS instance and use the backup files to restore the database data.","correct":true},{"id":"c3692f609e87c0dc0416f0ad05897b3f","text":"Explain to company stakeholders that it is not possible to migrate from the backups directly to AWS.  Recommend that we first find a co-location site, procure similar hardware as before the disaster and restore everything there.  Then, we can carefully migrate to AWS.","correct":false},{"id":"af2dfb68e123596c40e5014f3e8dc491","text":"Copy the VMs into AWS and create new AMIs from them.  Create a clustered auto scaling group across multiple AZs for your application servers.  Provision a multi-AZ RDS instance to eliminate the single-point-of-failure problem.  Restore the data from the backups using the database admin tools.","correct":false}]},{"id":"3f7fa126-1155-4aa3-802d-e9eeb75f5e5a","domain":"awscsapro-domain3","question":"You work for a Clothing Retailer and have just been informed the company is planning a huge promotional sale in the coming weeks.  You are very concerned about the performance of your eCommerce site because you have reached capacity in your data center.  Just normal day-to-day traffic pushes your web servers to their limit.  Even your on-prem load balancer is maxed out, mostly because that's where you terminate SSL and use sticky sessions.  You have evaluated various options including buying new hardware but there just isn't enough time.  Your company is a current AWS customer with a nice large Direct Connect pipe between your data center and AWS.  You already use Route 53 to manage your public domains.  You currently use VMware to run your on-prem web servers and sadly, the decision was made long ago to move the eCommerce site over to AWS last.  Your eCommerce site can scale easily by just adding VMs, but you just don't have the capacity.  Given this scenario, what is the best choice that would leverage as much of your current infrastructure as possible but also allow the landscape to scale in a cost-effective manner?","explanation":"A Target Group for an ALB can contain instances or IP addresses.  In this case, we can define the private IP addresses of our on-prem web servers along side the private IP addresses of any EC2 instances we spin up.  The caveat is that we can only use private IP addresses when defining a target group in this way.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancing-via-ip-address-to-aws-on-premises-resources/","title":"New – Application Load Balancing via IP Address to AWS & On-Premises  Resources | AWS News Blog"}],"answers":[{"id":"77592781918fa63474b5efbd5cc9555f","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an NLB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the NLB as an alias record.","correct":false},{"id":"4df24111c113846bfe0505ad0c84d9a3","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define two target groups:  one containing the public IP addresses of your on-prem load balancer and one including an auto scaling group of additional EC2 instances created from the imported AMI.  Assign both target groups to the ALB using the same listener port.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"3e47f65e4524f53faba23e6995b592f5","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":true},{"id":"6d3db4c52e96931f925f17fe8e9fd50f","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using public IP addresses of your on-prem web servers and additional EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false}]},{"id":"07f91ae7-094b-48a9-8924-a4d142cbbcb6","domain":"awscsapro-domain5","question":"On your last Security Penetration Test Audit, the auditors noticed that you were not effectively protecting against SQL injection attacks.  Even though you don't have any resources that are vulnerable to that type of attack, your Chief Information Security Officer insists you do something.  Your organization consists of approximately 30 AWS accounts.  Which steps will allow you to most efficiently protect against SQL injection attacks?","explanation":"Firewall Manager is a very effective way of managing WAF rules across many WAF instances and accounts.  It does require that the accounts be linked as an AWS Organization.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"ee9c067e3dea2fa4cd2e3968abaf86de","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations.  Use CloudFormation to implement request restrictions for SQL code on the CloudFront distributions across all accounts.  Setup a CloudWatch event to notify administrators if requests with SQL code are seen.","correct":false},{"id":"9a5fa8e9a1a9be9149138c6307abde19","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service.  Use Firewall Manager to create an ACL rule to deny requests that contain SQL code.  Apply the ACL to WAF instances across all organizational accounts.","correct":true},{"id":"2238e9ae7489214f767fa479d013cd23","text":"Create a custom NACL filter using Lambda@Edge to check requests for SQL code.  Use OpsWorks to apply the NACL across all public subnets across the organization. ","correct":false},{"id":"3927b32bfb85cc040d2b7dcb21015fc5","text":"Use AWS WAF to create an ACL that denies requests that include SQL code.  Assign the ACL to Firewall Manager instances in each account using AWS OpsWorks.","correct":false},{"id":"e072f443d93c569cce94eb4946a912af","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service and use Consolidated Billing. Subscribe to AWS Shield Advanced to automatically enable SQL injection protection across all sub-accounts.","correct":false}]},{"id":"758841a6-0db9-4a02-a85b-699092182451","domain":"awscsapro-domain4","question":"You have just finished a contract with your client where you have helped them fully migrate to AWS.  As you are preparing to transition out of the account, they would like to integrate their current help desk software, Jira, with the AWS support platform to be able to create and track tickets in one place.  Which of the following do you recommend?","explanation":"You must have subscribe to at least the Business Support Plan to gain access to the AWS Support API. ","links":[{"url":"https://aws.amazon.com/premiumsupport/compare-plans/","title":"AWS Support - Compare all support plans"}],"answers":[{"id":"8b3759a4758ab061f15798717c42c8e0","text":"Subscribe to the Platinum Support Plan and direct them to the AWS Support API documentation","correct":false},{"id":"55d01f0a938121dd662f6d3068168eec","text":"Subscribe to the Developer Support Plan and direct them to the AWS Support API documentation","correct":false},{"id":"0846705e92c2fcc07e439ddc2698dd15","text":"Use API Gateway to create a proxy service for the AWS Support API to allow third-party access.  Direct them to the AWS Support API documentation.","correct":false},{"id":"7e007e2c40ad802633ef945f34b23230","text":"It is currently not possible to integrate third-party products into the AWS Support system.  Offer to contract with them to perform manual updates between Jira and AWS Support cases.","correct":false},{"id":"6c97334691882e8f846aad300e6a7811","text":"Subscribe to the Business Support Plan and direct them to the AWS Support API documentation","correct":true}]},{"id":"63da01c2-9c4d-4abd-a482-06ede2baf728","domain":"awscsapro-domain2","question":"A popular royalty free photography website has decided to run their business on AWS. They receive hundreds of images from photographers each week to be included in their catalog. Amazon S3 has been selected as the image repository. As the business has grown, the task of creating catalog entries manually has become unsustainable. They'd like to automate the process and store the catalog information in Amazon DynamoDB. Which architecture will provide the most scalable solution for automatically adding content to their image catalog going forward?","explanation":"Calling the S3 API to upload the images will suffice for this use case. Streaming ingest is not needed for this volume of data. AWS Step Functions will orchestrate the process of discovering both the image metadata with a Lambda function and the image object data with Rekognition. Rekognition will not return the image metadata. AWS Elemental MediaStore is used for originating and storing video assets for live or on-demand media workflows, not image recognition. Kinesis Video Analytics is not a currently supported service.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://github.com/aws-samples/lambda-refarch-imagerecognition","title":"Serverless Reference Architecture: Image Recognition and Processing Backend"}],"answers":[{"id":"f98112f4f94f4a8d66edee560976acc2","text":"Programmatically call the S3 API to upload the images. Trigger an AWS Lambda function to send the image's S3 key to AWS Elemental MediaStore, which will extract the image's metadata, discover image patterns through machine learning, and deposit artifacts back into S3. Invoke a Lambda function to write the artifact data to DynamoDB.","correct":false},{"id":"0710b6a1f2a807da2cfc1a940e7014e9","text":"Deploy Amazon Kinesis Data Streams to ingest the images with two consumers. Setup Amazon Kinesis Firehose as the first consumer to deposit the images into S3. Configure Amazon Kinesis Video Analytics as the second consumer to extract the image's metadata and object information. Invoke a Lambda function to store the discovered information in DynamoDB.","correct":false},{"id":"42367a003a702450700b58798073122b","text":"Deploy Amazon Kinesis Data Firehose to ingest images into S3. Invoke a Lambda function to pass the image's S3 key to Amazon Rekognition, which will extract the image metadata and detect objects in the image. Invoke a Lambda function to store the discovered data in DynamoDB.","correct":false},{"id":"dd8565c85d19c2867d3a6768f512b404","text":"Programmatically call the S3 API to upload the images. Trigger an AWS Lambda function to kick off execution of a state machine in AWS Step Functions. Create state machine sub-steps to invoke Lambda functions which extract image metadata, detect objects in the image with Amazon Rekognition, and store the discovered data in DynamoDB.","correct":true}]},{"id":"0906c4cf-83a1-4cec-b2ab-c010dcdee73f","domain":"awscsapro-domain1","question":"The alternative energy company you work for has four different business units, each of which would like to run workloads on AWS. Each business unit has it's own AWS account, and a shared services AWS account has been created. An established process for tracking software license usage exists for on-premises applications, but the finance department has concerns that the self-serve nature of the cloud may result in license overages for applications deployed on AWS. You've been tasked with setting up a governance model whereby users are only given access to a standard list of products. Which architecture will provide an effective way to implement the governance requirements and manage software license usage on AWS?","explanation":"AWS Service Catalog allows organizations to create and manage catalogs of approved products for use on AWS. Products are defined as CloudFormation Templates. Software license information can be associated with Service Catalog products through tags. AWS Step Functions can orchestrate the process of incrementing usage counts and notifying of over-usage situations when products are launched by users. AWS License Manager is a robust solution for managing software licenses, but it needs to be coupled with Service Catalog to meet the requirement for limiting access to a standard set of products. A Lambda trigger is not currently available for Service Catalog product deployments. Elastic Container Registry provides tagging at the repository level, not at the individual container image level.","links":[{"url":"https://aws.amazon.com/servicecatalog/","title":"AWS Service Catalog"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://aws.amazon.com/blogs/mt/tracking-software-licenses-with-aws-service-catalog-and-aws-step-functions/","title":"Tracking software licenses with AWS Service Catalog and AWS Step Functions"}],"answers":[{"id":"62788b95bb6b249e6511d14023a20364","text":"Implement AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Create an Amazon DynamoDB table to store software license usage counts. Trigger an AWS Lambda function to run each time a Service Catalog product is launched. Have the Lambda function increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"33db838110cfb8b002590cbff630b825","text":"Create Amazon Machine Images for all of the instance configurations that will be deployed. Implement AWS License Manager license configurations and attach them to the AMIs. Create AWS CloudFormation StackSets for the AMIs in the shared AWS account and make them available to users in each business unit","correct":false},{"id":"41d11d1142468e8b0d2a29674d1eaa2c","text":"Deploy AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Populate Service Catalog product tags with software license information. Create an Amazon DynamoDB table to store software license usage counts. Have Amazon CloudWatch detect when a user deploys a Service Catalog product. Launch an AWS Step Functions process to increment license counts in the DynamoDB table, and send notifications when overage thresholds are met","correct":true},{"id":"e2441352bdc2d4db956149f0a10b6738","text":"Create Docker images for each of the standardized applications that will be deployed and register them with Amazon Elastic Container Registry (ECR). Populate ECR tags with software license metadata. Create an Amazon DynamoDB table to store software license usage counts. Whenever a container is launched in Amazon Elastic Container Service, trigger an AWS Lambda function to increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false}]},{"id":"bbcb9a8c-f84d-4424-b199-9047a4625e15","domain":"awscsapro-domain2","question":"Your company's DevOps manager has asked you to implement a CI/CD methodology and tool chain for a new financial analysis application that will run on AWS. Code will be written by multiple teams, each team owning a separate AWS account. Each team will also be responsible for a Docker image for their piece of the application. Each team's Docker image will need to include code from other teams. Which approach will provide the most operationally efficient solution?","explanation":"AWS CodePipeline, AWS CodeCommit, and AWS CodeBuild all allow cross-account access once the appropriate resource-level permissions have been granted. Orchestrating deployments from a single DevOps account will provide the most operationally efficient solution, resulting in less need for coordination of services and configurations across development team accounts.","links":[{"url":"https://aws.amazon.com/products/developer-tools/","title":"Developer Tools on AWS"},{"url":"https://aws.amazon.com/blogs/devops/how-to-use-cross-account-ecr-images-in-aws-codebuild-for-your-build-environment/","title":"How to Use Cross-Account ECR Images in AWS CodeBuild for Your Build Environment"}],"answers":[{"id":"b9ae951b67f2fed4a145bd7f591c8631","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the DevOps account to AWS CodeCommit in the team accounts to get the latest code. Perform cross-account access from AWS CodeBuild in the DevOps account to AWS CodeBuild in the team accounts to get the Docker images. Perform deployments from AWS CodeDeploy in the DevOps account","correct":true},{"id":"4a4bd59860afb498d97f0d01cff52b7a","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in a single DevOps account","correct":false},{"id":"83443cdcabe51198b91ed96d84eed4a6","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform all deployments from AWS CodeDeploy in the DevOps account","correct":false},{"id":"c32df9c97fbc40e07515d4ca41de63e2","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in the team accounts","correct":false}]},{"id":"91006a08-8658-479c-9f96-4f9cd9c770c6","domain":"awscsapro-domain2","question":"Your customer is a commercial real-estate company who owns parking lots in all major cities in the world. They are building a website on AWS that will be globally accessed by international travellers. The website will provide near-real-time information on available parking lot spaces for all cities. You need a back-end comprising a multi-region multi-master data storage solution, with writes happening in all regions. Writes must be replicated between regions in near real-time so that different regional website compute instances can query the database instance in its own region and retrieve the data pertaining to all other regions. In addition, all users should be able to access the website using the same domain name, but they need to be routed to the Elastic Load Balancer that responds most quickly to their request (which should be the one closest to them most of the time). The website must also serve locally cached static content and have protection against malicious attacks. Which AWS-based architecture should you choose?","explanation":"The only AWS managed database service that offers multi-region multi-master is DynamoDB Global Tables. AWS Aurora does offer multi-master clusters, but all nodes of an Aurora multi-master must be in the same region. Aurora also offers cross-region replication, but the nodes, in that case, are Read Replicas, they are not Masters (in the database world, a master is an instance that accepts write requests). Similarly, AWS RDS does not currently have a multi-master multi-region solution to leverage. Thus, the only choice that can satisfy the multi-region multi-master nature of the requirement is DynamoDB Global Tables with DynamoDB Streams enabled for Replication. Additionally, the question tests knowledge in other areas. However, just focusing on the above fact about the database tier is enough to eliminate all incorrect answers.\nRoute 53 latency-based multi-region routing is the correct Routing solution here. Multi-value answering records results in randomly picking one, which is not the requirement here (in this scenario, the region must be picked based on latency, not randomly). Similarly, geolocation-based routing honours country boundaries, not latency. If we use geolocation-based routing, an end-user in country A will always go to the AWS Region in country A, but that may not be the one with least latency, especially if the user is close to an international boundary, and the AWS data centre happens to be on the other end of his country.\nLambda@Edge can be used with Cloudfront to re-write origins for requests. This is an important usage of Lambda@Edge. Similarly, WAF can be activated with a Cloudfront web distribution for protection against malicious traffic.","links":[{"url":"https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/","title":"How to use Amazon DynamoDB global tables to power multiregion architectures"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html","title":"Search for - all DB instances in a multi-master cluster must be in the same AWS Region"},{"url":"https://aws.amazon.com/blogs/aws/latency-based-multi-region-routing-now-available-for-aws/","title":"Multi-Region Latency Based Routing now Available for AWS"}],"answers":[{"id":"9faf11728364108db71381c81e091523","text":"For the database, use Amazon RDS with Cross Region Replication, with instances in each AWS Region configured as master. For routing, use Route 53 multi-region multi-value answering based routing. Use Cloudfront to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF with Cloudfront geo restriction feature.","correct":false},{"id":"c74e4146c5eb59b1a892f52ac51bfbd7","text":"For the database, use Amazon Aurora Cross-Region Replication, with instances in each AWS Region configured as master. For routing, use Route 53 geolocation-based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":false},{"id":"32667dba126e63f9c8c6adb288419974","text":"For the database, use DynamoDB Global Tables, with instances in each AWS Region configured as master, and replication enabled using DynamoDB Streams. For routing, use Route 53 multi-region latency based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":true},{"id":"25c4e1f274a66d3e2b5d67c75702ef4b","text":"In AWS, all cross-region replication solutions treat secondary instances as read-only, so multiple masters are not possible in a managed multi-region database solution. Use Cassandra on EC2 instances in different regions that are peered to each other and configure Cassandra in multi-master mode. For routing, use Route 53 multi-region latency based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":false}]},{"id":"6d93e859-e1a9-468f-9a05-61a2dbc2be9c","domain":"awscsapro-domain5","question":"You manage a group of EC2 instances that host a critical business application.  You are concerned about the stability of the underlying hardware and want to reduce the risk of a single hardware failure impacting multiple nodes.  Regarding Placement Groups, which of the following would be the best course of action in this case?","explanation":"Spread Placement Groups ensure your instances are each placed on separate underlying hardware so this reduces the risk of a single hardware failure taking down multiple instances.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-spread","title":"Placement Groups - Amazon Elastic Compute Cloud"}],"answers":[{"id":"e6d129a06320300aaf95634a6691b7cb","text":"You would the AWS Console to move the existing instances into a clustered placement group.","correct":false},{"id":"3bde2d7fff7ead39c4035ab0600275f1","text":"You would use the AWS CLI to move the existing instances into a diversified placement group.","correct":false},{"id":"b6153af57a0b8805fc28d93a2859fb9e","text":"You cannot move existing instances into a new placement group.  You would create AMIs from the existing instances and redeploy them into a clustered placement group.","correct":false},{"id":"c8b809a2fef3f21146774b82e8f03f12","text":"You would move the instances onto a Dedicated Host.","correct":false},{"id":"e346d1667489b5726c0367eff4ea4a34","text":"You would use the AWS CLI to move the existing instances into a spread placement group.","correct":true}]},{"id":"1aa1b2b9-bb81-44d6-bcee-ec9408859b70","domain":"awscsapro-domain4","question":"A small business owner owns two businesses - a restaurant and a pet grooming facility. Each of her businesses has its own website, respectively www.xyz-restaurant.com and www.pqr-pet-grooming.com, both supporting only HTTP (no HTTPS). One single EC2 instance on AWS serves both the websites. This Ec2 instance has an Elastic IP Address attached to it, with two Route 53 A records pointing at it. The owner wants to add credit card payment processing to both her websites. As a result, she wants to ensure that both sites are served on HTTPS. Also, due to the popularity of her businesses, she needs to run a second EC2 instance which will be an exact copy of the existing one. She plans to create the second instance from the AMI snapshot of the first one, hoping to not needing to make any website configuration or code changes on any of the instances. She wants the traffic for each site to be equally or randomly split between the two servers though all requests from the same user-session must reach the same server.\nAs an AWS architect, what approach would you recommend for functionality as well as cost-effectiveness?","explanation":"This question tests several aspects of Load Balancers, knowledge of SNI and SAN, Sticky Sessions and Route 53.\nApplication Load Balancer supports SNI - hence it can deal with multiple SSL certificates per Listener. Thus, the option that says that a single ALB can handle a single SSL certificate only is incorrect. Also, as the question states that no code or website configuration changes can be done, we cannot terminate SSL at the EC2 instances - as doing so would surely need changes to the website configuration if not both code and website configuration. This eliminates the options that propose the Classic Load Balancer or a completely Load-Balancer-less solution.\nThe Classic Load Balancer option would technically work as long as the website configuration can be updated to terminate SSL at the EC2 instance. In fact, that would be the lowest cost solution from a Load Balancer perspective as CLB costs less than ALB - however, terminating SSL at the EC2 instances usually requires greater compute capacity, so the cost of EC2 instance could go up slightly. Note that the CLB option correctly states that SAN must be used with CLB, as CLB does not support SNI. Also, when SAN is used with CLB, it correctly states that the listener must be configured as TCP instead of HTTPS as the SSL termination must occur at the EC2 instance in that case. However, the only reason this option cannot be selected as correct is the restriction imposed on not changing the AMI at all. We cannot terminate SSL on the EC2 instance without changing the AMI or configuring the website.\nThe Load-Balancer-less solution that achieves routing via Route 53 has two problems - one is terminating SSL at the EC2 instance. The other is the fact that Route 53 does not provide Sticky Sessions.\nHence, the right answer is to use SNI with both SSL certificates bound to the same ALB listener, use Sticky Sessions, and use Route 53 to CNAME the website domains to the ALB","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/","title":"SNI and ALB"},{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-https-load-balancers.html","title":"Classic Load Balancer Listener Configuration for SAN"}],"answers":[{"id":"d1e651efb4dab3eecdbcda26841cba22","text":"Procure an SSL certificate for one of the domains and add a Subject Alternative Name (SAN) for the other domain to the same certificate. Deploy a Classic Load Balancer with a single TCP Listener. Deploy the SAN certificate to the Classic Load Balancer. Add both the instances as Targets to the CLB. Turn on Session Affinity on the CLB. Terminate SSL on the EC2 instances. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the CLB","correct":false},{"id":"5344d22fe688e9bd0ec7d0ff5a2ca246","text":"Procure SSL certificates for both the domains. Deploy an Application Load Balancer. As a single ALB can only handle a single SSL certificate, SSL termination must be now done at the EC2 instance. Hence, create a TCP/443 Listener on the ALB. Add both the instances as Targets belonging to the same Target Group to the ALB, assigning the Target Group to the Listener. Turn on Sticky Sessions for this Target Group. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the ALB","correct":false},{"id":"8b618d3847d47090857df2e1dd20b574","text":"Procure SSL certificates for both the domains. Deploy an Application Load Balancer with a single HTTPS Listener. Bind both certificates to the Listener on the Load Balancer. Add both the instances as Targets belonging to the same Target Group to the ALB, assigning the Target Group to the Listener. Turn on Sticky Sessions for this Target Group. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the ALB","correct":true},{"id":"676aa985e464f22918d04ea588f87ae4","text":"Procure SSL certificates for both the domains. Attach a second Elastic IP Address to the second EC2 instance. Add two Multivalue-answer records to Route53, one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, each of type A Record, and each with both the Elastic IP addresses - one for each domain name, one domain per hosted zone. This will ensure traffic being equally split between the two instances. Terminate SSL at the EC2 instances. Turn on Session Affinity for both the Route 53 Hosted Zones","correct":false}]},{"id":"8765bd56-057b-488c-9a0a-f5bd413dd240","domain":"awscsapro-domain5","question":"Due to new corporate policies on data security, you are now required to use encryption at rest for all data.  You have some EC2 Linux instances on AWS that were created without encryption for the root EBS volume.  What can you do that meet the requirement and reduce administrative overhead?","explanation":"AWS does support encrypted root volumes but conversion from unencrypted root to an encrypted root requires a bit of a process. You must first create an AMI then copy that newly created AMI to the same region, specifying that you want to encrypt the EBS volumes during the copy.  You can then create a new instance with an encrypted root volume from the copied AMI.  You can use either a generated key from KMS or your own CMK imported into KMS.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html","title":"AMIs with Encrypted Snapshots - Amazon Elastic Compute Cloud"},{"url":"https://aws.amazon.com/blogs/aws/new-encrypted-ebs-boot-volumes/","title":"New – Encrypted EBS Boot Volumes | AWS News Blog"}],"answers":[{"id":"50c17b27f0bd0390ae321943f7db5c3d","text":"Create a certificate in CMS for the encryption key.  Stop the instances and temporarily detach the root volumes.  Via the AWS CLI, enable encryption on the root volumes using the \"ebs modify-volume\" argument with the flag of \"encryption=<CMS ARN>\" to specify the certificate.","correct":false},{"id":"1e30ccf0b75e9e70fd76c6e041510c75","text":"At present, EC2 does not support encrypted root volumes.  Create new encrypted EBS data volumes and attach the new volumes to the existing instances.  Use RSYNC to migrate all the non-OS data over to the encrypted data volumes.","correct":false},{"id":"180e9aecdb74f204b1df00ffe6fa8b56","text":"Stop the instances and create AMIs from the instances.  Copy the AMIs to the same region and select the \"Encrypt target EBS snapshots\".  Redeploy the instances using the AMI copies you made with encrypted root volumes.","correct":true},{"id":"4430b7492a6c058c3574ce4e8ea43955","text":"Stop the instances and temporarily detach the EBS volumes.  Attach the root volumes to another EC2 instance and mount them a data volume.  Use a encryption tool like GPG or OpenPGP to recursively encrypt all the files on the mounted root volumes.  Detach and reattach the encrypted EBS volumes to the original instances and restart.  Import the encryption keys in KMS as a CMK.","correct":false},{"id":"9fbbb2e71386cb7f7a7ed77129a1a960","text":"Create an encrypted EFS instance and mount-points in the respective subnets.  Log into the instance and mount an encrypted EFS mount-point.  Copy all the root files over to the EFS mount point.  Edit the FSTAB file to mount the EFS mount point as the root volume instead of the root EBS device and reboot.","correct":false}]},{"id":"e24f7f76-6908-4ad9-820a-11790bfdcec6","domain":"awscsapro-domain3","question":"You are helping a client migrate over an internal application from on-prem to AWS.  The application landscape on AWS will consist of a fleet of EC2 instances behind an Application Load Balancer.  The application client is an in-house custom application that communicates to the server via HTTPS and is used by around 40,000 users globally across several business units.  The same exact application and landscape will be deployed in US-WEST-2 as well as EU-CENTRAL-1.  Route 53 will then be used to redirect users to the closest region.  When the application was originally built, they chose to use a self-signed 2048-bit RSA X.509 certificate (SSL/TLS server certificate) and embedded the self-signed certificate information into the in-house custom client application.  Regarding the SSL certificate, which activities are both feasible and minimize extra administrative work?","explanation":"You can import private certificates into Certificate Manager and assign them to all the same resources you can with generated certificates, including an ALB.  Also note that Certificate Manager is a regional service so certificates must be imported in each region where they will be used.  The other options in this question would either require you to update the certificate on the client or requires unnecessary steps to resolve the challenge.","links":[{"url":"https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html","title":"Importing Certificates into AWS Certificate Manager - AWS Certificate  Manager"}],"answers":[{"id":"0663551d15f5b15af587ac8bf75a2566","text":"Purchase a new public SSL/TLS certificate from a third-party CA.  Upload the certificate to Certificate Manager and assign that certificate to the Application Load Balancers.","correct":false},{"id":"111997579381183b07a22fad8574e76c","text":"Create a new Certificate Authority within Certificate Manager and import the existing certificate.  Generate a new certificate, CA chain and private key and push an update for the application.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false},{"id":"0c4320d1dd787a5bae2b43479b645d94","text":"Use Service Catalog to push an update of the in-house app which includes an updated certificate and CA chain.  Generate a new private certificate using OpenSSL. Import the new certificate to Certificate Manager in US-EAST-1.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false},{"id":"d4976a6c33ee189e6b681dffc83cbac5","text":"Import the existing certificate and private key into Certificate Manager in both regions.  Assign that imported certificate to the Application Load Balancers using their respective regionally imported certificate.","correct":true},{"id":"28694bd7f280694a43741563f6933ad6","text":"Create a new public SSL/TLS certificate using Certificate Manager and configure the common name and OU to match the existing certificate.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false}]},{"id":"02a9611c-591c-4280-bb83-6c65c7c4921f","domain":"awscsapro-domain5","question":"A sporting goods retailer runs WordPress on Amazon EC2 Linux instances to host their customer-facing website. An ELB Application Load Balancer sits in front of the EC2 instances in Auto Scaling Groups in two different Availability Zones of a single AWS region. The load balancer serves as an origin for Amazon CloudFront. Amazon Aurora provides the database for WordPress with the master instance in one of the Availability Zones and a read replica in the other. Many custom and downloaded WordPress plugins have been installed. Much of the DevOps teams' time is spent manually updating plugins across the EC2 instances in the two Availability Zones. The website suffers from poor performance between the Thanksgiving and Christmas holidays due to a high occurrence of product catalog lookups. What should be done to increase ongoing operational efficiency and performance during high-volume periods?","explanation":"ElastiCache Memcached will provide in-memory access speeds for the catalog read transactions. A WordPress plugin is required to leverage caching. WordPress can access an EFS Mount Target for file sharing across all instances. Aurora offers a MySQL option, and WordPress requires MySQL, so the solution would have been set up that way already. CodeDeploy could update plugins on all instances, and will work well for the custom in-house code, but triggering the updates of downloaded plugins will need to be orchestrated. Aurora Auto Scaling will distribute catalog reads across multiple replicas for increased performance, but not to the extent of in-memory caching. Elastic File System is a managed service providing operational advantages over NFS file shares. ElastiCache Redis will provide the in-memory read performance desired, but changing the wp-config.php file won't provide access to it, as a plugin is needed for that. WordPress does work with S3, but a shared file system is easier to implement.","links":[{"url":"https://aws.amazon.com/getting-started/projects/build-wordpress-website/","title":"Build a WordPress Website"},{"url":"https://github.com/aws-samples/aws-refarch-wordpress?did=wp_card&trk=wp_card","title":"Hosting WordPress on AWS"}],"answers":[{"id":"f060160b20c4408b2442010d3ea4d387","text":"Use Amazon ElastiCache Redis as a caching layer between the EC2 instances and the database. Change wp-config.php to point to the Redis caching layer, and have Redis point to Aurora. Move the WordPress files to S3 and have WordPress access them there.","correct":false},{"id":"bc643d3342a5a675e65e5baed00e88b9","text":"Deploy Amazon ElastiCache Memcached as a caching layer between the EC2 instances and the database. Install a WordPress plugin to read from Memcached. Implement Amazon Elastic File System to store the WordPress files and create mount targets in each EC2 subnet.","correct":true},{"id":"17b12e57cd85610e888cda82b5a8a145","text":"Migrate the WordPress database to RDS MySQL since MySQL is WordPress's native database and WordPress is performance optimized for MySQL. Implement AWS CodeDeploy to update WordPress plugins on all EC2 instances.","correct":false},{"id":"aeb27370afc3b672eb0a1afcb28e9176","text":"Implement Aurora Auto Scaling to increase the number of replicas automatically as demand increases. Create an NFS file share to hold the WordPress files. Access the file share from the EC2 instances in both Availability Zones.","correct":false}]},{"id":"8b70203c-6d35-44fb-9575-9b4bb2958269","domain":"awscsapro-domain1","question":"A financial services company operates in all fifty U.S. states. They've decided to deploy part of their application portfolio in the AWS us-east-1, us-east-2, and us-west-2 regions. Multiple AWS accounts will be created, one for each of the company's four business units. The applications need to be able to communicate with each other across the accounts and across the regions. The applications also need to communicate with systems in the corporate data center. Which networking approach with provide the best operational efficiency?","explanation":"Creating a hub-and-spoke network topology minimizes network management overhead. Transit Gateway would be the best approach. Routing AWS traffic for many VPCs through Transit Gateway is now possible and will allow for smooth integration of environments without the technical overhead of managing separate VPN or multiple Direct Connect connections.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-multiple-region-multi-vpc-connectivity/","title":"Multiple Region Multi-VPC Connectivity"},{"url":"https://aws.amazon.com/transit-gateway/","title":"AWS Transit Gateway"},{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/multi-account-associate-tgw.html","title":"Associating a Transit Gateway Across Accounts"}],"answers":[{"id":"d8df048014675105d92ca9f84fa7b760","text":"Route the AWS cross-account, cross-region traffic through the corporate data center network via VPN connections to leverage existing network infrastructure","correct":false},{"id":"5e87ca266342ca7d4c0d7a033f63da87","text":"Deploy a transit VPC in a shared account with EC2-based appliances that create hub-and-spoke VPN connections to VPCs in the other accounts, the other regions, and the corporate data center network","correct":false},{"id":"6f2b464e20f643061f4f98c2c84d1df5","text":"Deploy an AWS Transit Gateway as a network hub to manage the connections between the VPCs in the different regions, the different accounts, and the corporate data center network","correct":true},{"id":"639783dcd838facfacfca10603c4ac87","text":"Create VPC Peering connections between the VPCs in the different regions and different accounts. Use AWS Direct Connect Gateway to interface the corporate data center network to the different AWS regions","correct":false}]},{"id":"73708d6f-e6cb-4b8f-90d9-723a2961496e","domain":"awscsapro-domain2","question":"Your team is architecting an application for an insurance company.  The application will use a series of machine learning methods encapsulated in an API call to evaluate claims submitted by customers.  Whenever possible, the claim is approved automatically but in some cases were the ML API is unable to determine approval, the claim is routed to a human for evaluation.  Given this scenario, which of the following architectures would most aligned with current AWS best practices?","explanation":"Formerly, AWS recommended SWF for human-involved workflows.  Now AWS recommends Step Functions be used as it requires less programmatic work to build workflows and is more tightly integrated into other AWS services.","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html","title":"Starting a State Machine Execution in Response to Amazon S3 Events - AWS  Step Functions"}],"answers":[{"id":"5298caaea3a55734efa8c62e637d0d40","text":"Use Kinesis to take in the claims and save them on S3 using Firehose.  Use Sagemaker to analyze the claims on S3 as a training set and devise a decider function.  Save the approved claims to another S3 bucket setup with an Event to trigger an SES message to a reviewer.","correct":false},{"id":"fa669ed2f0666420f19ad9c8836509f6","text":"Take in the claims into an SQS queue.  Create a Lambda function to poll the SQS queue, fetch the claim and submit the API call.  Use another Lambda function to evaluate the API results and if the claim is not approved, place the claim in a dead letter queue.  Train a person to periodically log into the SQS console and read the dead letter queue for review.","correct":false},{"id":"dfc86c259de4881886ffdac5b8106777","text":"Create a State Machine using Step Functions and a Lambda function for calling the API.  Intake the claims into an S3 bucket configured with a CloudWatch Event.  Trigger the Step Function from the CloudWatch Event.  Create an Activity Task after the API check to email an unapproved claim to a human.","correct":true},{"id":"837fa3e7c6b347eddfa7fefd2a092017","text":"Create a workflow using Simple Workflow Service and an EC2 fleet to host worker and decider programs.  Create worker programs for each processing step and ML API call.  Create decider program to receive the output of the API and decide if the claim is approved.  For unapproved claims, create a worker program use the WorkMail SDK to place the unapproved claim into a mailbox to be reviewed by a human.","correct":false}]},{"id":"cb24982e-2c2d-43d4-872f-2dabbdb7e367","domain":"awscsapro-domain2","question":"You are helping an IT Operations group transition into AWS.  They will be created several instances based off the latest Amazon Linux 2 AMI.  They are unsure of the best way to enable secure connections for all members of the group and certainly do not want to share credentials. Which of the following methods would you recommend?","explanation":"Of the provided options, the only one that upholds AWS best practices for providing secure access to EC2 instances is to use AWS Session Manager.  ","links":[{"url":"https://aws.amazon.com/blogs/aws/new-session-manager/","title":"New – AWS Systems Manager Session Manager for Shell Access to EC2 Instances  | AWS News Blog"}],"answers":[{"id":"b1ab01927f47a067694506df9d5249e3","text":"Configure IAM role access for AWS Systems Manager Session Manager.","correct":true},{"id":"cce617457cd8701c595d236b7fa4ed7c","text":"Allow each administrator to create their own SSH keypair and assign them all to the SSH Key for the instance upon each launch.","correct":false},{"id":"c594a26a8a9141cdd5615a0761fb2438","text":"Create a bastion host and use it like a jump-box.  Paste each administrators private key into the known_hosts file on the bastion host.","correct":false},{"id":"93ecb84b9fc90ec0bc4db84968ab5ebf","text":"Allow administrators to update the SSH key of the instance in the AWS console each time they need access to a system.","correct":false},{"id":"e36f501af80c8f03e08321d565cc900e","text":"Share the single private SSH key with each administrator in the group.","correct":false}]},{"id":"5af539b7-b132-4a3a-bc80-406c620e7325","domain":"awscsapro-domain1","question":"A food service business has begun an initiative to migrate all applications and data to the AWS cloud. Governance needs to be established before any migrations can occur. Business units such as sales, marketing, and product management have fluctuating infrastructure capacity and security requirements, while other business units like finance, operations, and human resources have more static demand. Security policies and compliance needs vary by project group within each business units. Each business unit is responsible for it's own cost center, and the finance group would like cost reporting to be as streamlined as possible. Which AWS account structure will best satisfy the company's governance needs?","explanation":"Leveraging AWS Organizations to manage an account structure with a core Organizational Unit and Organizational Units for each business unit provides flexibility for future organizational changes. Creating an account for each project group facilitates security policy differences within business units, and limits the exposure of a single security event. Managing differing security requirements by project group in a single account will require more governance maintenance. Creating billing, shared services, and log archive accounts in multiple Organizational Units will result in duplication of services, and can be done at the core level.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-laying-the-foundation/introduction.html","title":"Laying the Foundation: Setting Up Your Environment for Cost Optimization"},{"url":"https://aws.amazon.com/solutions/aws-landing-zone/?did=sl_card&trk=sl_card","title":"AWS Landing Zone"}],"answers":[{"id":"03705913700b8d76205d4203c58dc5e1","text":"Use AWS Organizations with a single Organizational Unit to consolidate costs. Create a billing account, a shared services account, and a log archive account in the Organizational Unit. Create individual accounts for each business unit. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs","correct":false},{"id":"a66d8391267460b5800c5c3d07921767","text":"Use AWS Organizations to create Organizational Units for each business unit. Create a billing account, a shared services account, and a log archive account in each Organizational Unit. Create accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"a8f7d8fbb7c6c3a1a14c91577dff42e1","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Place business units with similar security requirements in shared Organizational Units. Create accounts for each business unit in the shared Organizational Units. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"bd400ff0d22480599228a0442d2bb8d4","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Create an Organizational Unit for each business unit that contains accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":true}]},{"id":"edb30172-3f76-4423-a6bb-78a3d2fdeb42","domain":"awscsapro-domain2","question":"Your team is managing hundreds of Linux and Windows EC2 instances in different environments such as development, QA, staging and production. You need a tool to help you automate the process of patching instances so that the operating systems have latest patches and meet the compliance policies. You want to manage the patching in different groups depending on the environment. For example, patches should be deployed and tested in the QA environment first before the production environment. How would you achieve this requirement through an AWS service?","explanation":"AWS Systems Manager Patch Manager is the most appropriate tool to manage the patching for large groups of EC2 or on-premises instances. For different environments, users can configure patch groups using the \"Patch Group\" tag and then establish a patch baseline for each patch group. It is better to manage instances with the \"Patch Group\" tag rather than other customized tags. AWS SSM Session Manager and AWS SSM Run Command are not suitable to deploy patches across a large number of instances. The AWS-RunRemoteScript command is also incorrect as it is used to execute scripts stored in a remote location.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-patchgroups.html","title":"Patch Manager Patch Groups"}],"answers":[{"id":"32ac8d27221ec8e699eb4e872d3f6ed0","text":"Centrally manage the instances in AWS SSM Managed Instances and divide them into different categories. Perform the patching activities from AWS SSM Session Manager in a maintenance window.","correct":false},{"id":"6c4763d45c6cb24622b0db9533f95e0c","text":"Create environmental tags in EC2 instances such as a tag key named \"env\". In AWS SSM Patch Manager, configure patching activities by selecting the instances using the tag. Patch on the QA environment first and perform the necessary testing.","correct":false},{"id":"08b9feba2a12ba67141a0ffe02938798","text":"Add patch group tags in the EC2 instances. Perform the patching using the command AWS-RunRemoteScript in AWS SSM Run Command. Patch on the QA environment first by selecting the QA patch group tag.","correct":false},{"id":"b99f4271e073e1e030f3c26c383c5959","text":"In AWS Systems Manager Patch Manager, create different patch groups using the tag key \"Patch Group\" and configure a patch baseline for each patch group. Schedule the patching in a maintenance window by selecting a patch group.","correct":true}]},{"id":"4eb7884e-f6fd-4803-83bf-8ab6faca4dd5","domain":"awscsapro-domain1","question":"You have been asked to give employees the simplest way of accessing the corporate intranet and other internal resources, from their iPhone or iPad.  The solution should allow access via a Web browser, authentication via SAML integration and you need to ensure that no corporate data is cached on their device. Which option would meet all of these requirements?","explanation":"Amazon WorkLink is a fully managed, cloud-based service that enables secure access to internal websites and apps from mobile devices. It provides single URL access to the applications and also links to existing SAML-based identity providers.  Amazon WorkLink does not store or cache data on user devices as the web content is rendered in AWS and sent to user devices as encrypted Scalable Vector Graphics (SVG).  WorkLink meets all of the requirements in the question and is therefore the only correct answer.","links":[{"url":"https://aws.amazon.com/worklink/faqs/","title":"Amazon WorkLink FAQs"}],"answers":[{"id":"0d0cb2140013a20863f643412ebd4698","text":"Tunnel through a Bastion Host into your VPC and view all internal servers via a Web Browser","correct":false},{"id":"668cfaeb2878db8e709660735f0ff009","text":"Configure Amazon WorkLink and connect to the servers using a Web Browser with the link provided","correct":true},{"id":"2ca6fccac916b71e240a465c8caf457e","text":"Connect into the VPC where the internal servers are located using Amazon Client VPN and view the sites using a Web Browser","correct":false},{"id":"8b3531ac066ba672af41cfd6c438fdb9","text":"Place all internal servers in a public subnet and lock down access via Security Groups to the IP address of each mobile user","correct":false}]},{"id":"7eebbdef-e751-4d76-be2a-1e3a746b87f6","domain":"awscsapro-domain5","question":"You are a database administrator for a company in the process of changing over from RDS MySQL to Amazon Aurora for MySQL.  You setup the new Aurora database in a similar fashion to how your pre-existing RDS MySQL landscape was setup:  Multi-AZ with Read Replica in a backup region.  You have just completed the migration of data and verified that the new Aurora landscape is performing like it should.  You are now in the process of decommissioning the old RDS MySQL landscape.  First, you decide to disable automatic backups.  Via the console, you try to set the Retention Period to 0 but receive an error saying \"Cannot Set Backup Retention Period to 0\".  How can you disable automatic backups?","explanation":" For RDS, Read Replicas require backups for managing read replica logs and thus you cannot set the retention period to 0.  You must first remove the read replicas and then you can disable backups.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Troubleshooting.html#CHAP_Troubleshooting.Backup.Retention","title":"Troubleshooting - Amazon Relational Database Service"}],"answers":[{"id":"c61323c176e6d23809f0d467770ed205","text":"Automatic backups are enabled and disabled at the database engine level.  You need to login using a MySQL client to turn off automatic backups.","correct":false},{"id":"5698f7c6bb0530fc1c0ad18e6911f528","text":"You must first reprovision the database as a single AZ instances.  Multi-AZ replication requires backups to be enabled.","correct":false},{"id":"fb2c22782c91868676981ff65332f1d5","text":"You cannot disable backups via the console.  You must do this via the AWS CLI or SDK.","correct":false},{"id":"898042b8eeafc1bbb4dd146c98dbb919","text":"Remove the Read Replicas first.","correct":true},{"id":"7086f25af0d0410de9d8826003760752","text":"You cannot disable automatic backups on RDS instances.  This feature is built into the platform as a failsafe.","correct":false}]},{"id":"a2a38759-7505-4ece-bd74-965f9d589a08","domain":"awscsapro-domain4","question":"You have been asked to help a company with optimizing cost on AWS.  You notice in reviewing documentation that they have constructed a transit network to link around 30 VPCs in different regions.  When you review traffic logs, most of the traffic is across regions.  Given this information, what might you recommend to reduce costs?","explanation":"By smartly consolidating resources into fewer regions and AZs, you are able to reduce or potentially eliminate data transfer and thus lower your overall costs.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-global-transit-network/","title":"AWS Global Transit Network – AWS Answers"}],"answers":[{"id":"94085d779db3a238c9dabf1d52624a3c","text":"Consolidate resources into as few regions and AZs as necessary.","correct":true},{"id":"9eba5c209cd53cc1ac849178ebf5fa05","text":"Implement a host-based router in the Transit VPC to intelligently route traffic based on least cost.","correct":false},{"id":"0e3b0d145e1202cb34b4947f2f6ed8ef","text":"Create VPC Gateway Endpoints within each of the 30 VPCs and add the necessary prefix lists.","correct":false},{"id":"d053c2a500bfc32b54da5688d6f230a4","text":"Implement NAT rules to compensate for overlapping networks and permit more direct routes.","correct":false}]},{"id":"2c204ae8-6bba-49a1-b8f6-1aa4330c3d8c","domain":"awscsapro-domain5","question":"You are helping an IT organization meet some security audit requirements imposed on them by a prospective customer.  The customer wants to ensure their vendors uphold the same security practices as they do before they can become authorized vendors.  The organization's assets consist of around 50 EC2 instances all within a single private VPC.  The VPC is only accessible via an OpenVPN connection to an OpenVPN server hosted on an EC2 instance in the VPC.  The customer's audit requirements disallow any direct exposure to the public internet.  Additionally, prospective vendors must demonstrate that they have a proactive method in place to ensure OS-level vulnerability are remediated as soon as possible.  Which of the following AWS services will fulfill this requirement?","explanation":"AWS Macie is a service that attempts to detect confidential data rather than OS vulnerabilities.  Since there is no public internet access for the VPC, services like GuardDuty and Shield have limited usefulness. They help protect against external threats versus any OS-level needs.  AWS Artifact is simply a document repository and has no monitoring functions.  Only AWS Inspector will proactively monitor instances using a database of known vulnerabilities and suggest patches.","links":[{"url":"https://aws.amazon.com/inspector/faqs/","title":"FAQs - Amazon Inspector - Amazon Web Services (AWS)"},{"url":"https://aws.amazon.com/macie/","title":"Amazon Macie | Discover, classify, and protect sensitive data | Amazon Web  Services (AWS)"}],"answers":[{"id":"45d5e166ed185c1f7516650c714423dd","text":"Enable AWS Artifact to periodically scan my instances and prepare a report for the auditors.","correct":false},{"id":"7ed3972608faa6c3dfc6fda7f151889c","text":"Enable AWS GuardDuty to monitor and remediate threats to my instances.","correct":false},{"id":"178912f5fbd90ab710621756a2ba18ff","text":"Employ AWS Macie to periodically assess my instances for vulnerabilities and proactively correct gaps.","correct":false},{"id":"6a353f53758a3fd632209b5286a01086","text":"Enable AWS Shield to protect my instances from unauthorized access.","correct":false},{"id":"81133f0650fa1ca2fbe1b920a6c67cc9","text":"Employ Amazon Inspector to periodically assess applications for vulnerabilities or deviations from best practices.","correct":true}]},{"id":"edc23d5d-e9ea-4713-b8c2-e35a1aa13626","domain":"awscsapro-domain2","question":"You are helping a company design a fully cloud-based Customer Service application.  Over 50% of their Customer Service Representatives are remote and that number increases and decreases seasonally.  They need the ability to handle inbound and outbound calls as well as chatbot capabilities.  Additionally, they want to provide a self-service option using interactive voice response to customers who do not need to speak to a person.  Which design is feasible and makes most efficient use of AWS services?","explanation":"AWS Connect is Amazon's \"call center in a box\" solution that enabled interactive voice response with Lex and inbound and outbound calling.  Additionally, you can use Lex to build a chatbot.  AWS Workspaces is a managed DaaS that is we suited for deploying to remote workers.","links":[{"url":"https://aws.amazon.com/connect/","title":"Amazon Connect Overview"},{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html","title":"What Is Amazon WorkSpaces? - Amazon WorkSpaces"}],"answers":[{"id":"4086f40e467d8d2fadaac8b5b9a2d49b","text":"Setup AWS Connect for inbound and outbound calling.  Make use of Polly and Lex for interactive voice response components.  Create a standard Customer Service Rep desktop and deploy using AWS Workspaces.  Leverage Lex to create a chatbot component.","correct":true},{"id":"22128c4eb6db1f8d7e92b3fbb7155565","text":"Use AWS Comprehend to create the chatbot and interactive voice response components.  Use Asterisk PBX from AWS Marketplace to handle the inbound and outbound calling.  Create a standardized Customer Service Rep desktop and deploy using Service Catalog.","correct":false},{"id":"8ed07bd636bec143bc1f7e2ce888bb03","text":"Create a standard Customer Service Rep desktop and deploy using AWS Workspaces. Setup AWS Connect for inbound and outbound calling.  Leverage Alexa for Business to create chatbot and interactive voice response components.  Store call logs in Redshift and analyze using Quicksight.","correct":false},{"id":"29d3b6d296574b52ab6aa78419ca5aad","text":"Create a standardized Customer Service Rep desktop and deploy via CloudFront.  Use Translate and AWS Connect to create a chatbot component.  Leverage Polly to create an interactive voice response component.  Use Alexa for Business for the inbound and outbound calling.","correct":false},{"id":"da552d1c23cba5e3e05a5dca7bdc0ca5","text":"Setup Twilio with Lambda to manage inbound and outbound calling.  Create a standard Customer Service Rep desktop Windows AMI and deploy via Service Catalog.  Leverage Polly for creating a chatbot and Translate for an interactive voice response system.","correct":false}]},{"id":"a4d41d3b-abbe-4121-8e1d-5567b1ec7294","domain":"awscsapro-domain5","question":"You are an AWS administrator and you need to maintain multiple Amazon Linux EC2 instances. You can SSH to the instances with a .pem key file created by another colleague. However, as the colleague will leave the company shortly, the SSH key pair needs to be changed to a new one created by you. After the change, users should be able to access the instances only with the new .pem key file. The old key should not work. How would you get the key pair replaced properly?","explanation":"You do not need to launch new instances as you can simply paste the public key content in the \".ssh/authorized_keys\" file to enable the new key pair. You cannot directly change the key through AWS Management Console by clicking the \"change SSH key\" button. You are also not allowed to change the SSH key when stopping and starting instances. Users can only select an SSH key pair when they launch a new instance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#replacing-key-pair","title":"Adding or Replacing a Key Pair for Your Instance"}],"answers":[{"id":"4fa9ce5fdb3a2215ed8798d2e734fd42","text":"In AWS Management Console, create a new key pair and download the .pem file at a safe place. Select the Amazon EC2 instances and click \"change SSH key\" to get the key replaced. The EC2 instances are automatically restarted after the change.","correct":false},{"id":"521ec8da4f5fa8bee591740d44a8427e","text":"Create a new key pair locally or through AWS EC2 service. Take AMIs of the instances as backups. Stop the instances and choose the new key pair when restarting the instances. Notify the team to start using the private key for SSH connections.","correct":false},{"id":"b216d43c3e1e71b67af45a138184b181","text":"Create a new SSH key pair, get the public key information from it, paste it in the \".ssh/authorized_keys\" file in the Amazon Linux EC2 instances and remove the old public key.","correct":true},{"id":"be5d2e07f838b7752b235431feaae361","text":"The SSH key pair cannot be changed after EC2 instances are launched. Take AMIs or EBS snapshots from existing instances, terminate the instances, launch new ones with the AMIs/snapshots and select another SSH key pair.","correct":false}]},{"id":"872cd65b-287b-4fdb-b59f-f07f7bff707f","domain":"awscsapro-domain5","question":"Your client is a small engineering firm which has decided to migrate their engineering CAD files to the cloud.  They currently have an on-prem SAN with 30TB of CAD files and growing at about 1TB a month as they take on new projects.  Their engineering workstations are Windows-based and mount the SAN via SMB shares.  Propose a design solution that will make the best use of AWS services, be easy to manage and reduce costs where possible. ","explanation":"At present, EFS doesn't support Windows-based clients.  Storage Gateway-File Gateway does support SMB mount points.  The other options introduce additional unneeded costs.","links":[{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/limits.html","title":"Amazon EFS Limits - Amazon Elastic File System"}],"answers":[{"id":"340da8cc24330ac5b143b526c880e4f7","text":"Order a Snowball appliance to migrate the bulk of the data.  Setup an EFS share on AWS and configure the CAD workstations to mount via SMB.  ","correct":false},{"id":"c15496feac5aa7ce58d0c5d4813a5a29","text":"Use AWS CLI to sync the CAD files to S3.  Setup Storage Gateway-File Gateway locally and configure the CAD workstations to mount as SMB.","correct":true},{"id":"f52e177dc6a594ed2c0852c91b6133d3","text":"Order a Snowmobile to migrate the bulk of the data.  Setup S3 buckets on AWS to store the data.  Use AWS WorkDocs to mount the S3 buckets from the engineering workstations.","correct":false},{"id":"7cad520a624f4c3b174014f339f732df","text":"Use AWS CLI to sync the CAD files to S3.  Use EC2 and EBS to create an SMB file server.  Configure the CAD workstations to mount the EC2 instances.  Setup Direct Connect to ensure performance is acceptable.","correct":false},{"id":"9c4821e0d9178e80636a5d4c7d0c6441","text":"Setup Storage Gateway-File Gateway and configure the CAD workstations to mount as iSCSI.  Use a Snowball appliance to sync data daily to S3 buckets at AWS.","correct":false}]},{"id":"b533b3c1-222f-4f33-99da-2c828e98ff91","domain":"awscsapro-domain5","question":"You have run out of root disk space on your Windows EC2 instance.  What is the most efficient way to solve this?","explanation":"We can easily increase the size of an EBS from the console or the CLI (using modify-volume) but then we also need to allow the OS to expand the resized volume so we can use it.  For Windows Server, we could use Disk Manager.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/expand-ebs-root-volume-windows/","title":"Expand the EBS Root Volume of Your EC2 Windows Instance"}],"answers":[{"id":"b893490015da5047b17b1220d43f4a1c","text":"From the AWS CLI, use the \"modify-instance\" command for EC2 to resize the volume to a larger size.  Using RDP, connect to the Windows instances and use Disk Manager to expand the volume.","correct":false},{"id":"346ac05b4353d34c630eb6d233f8a35d","text":"Compress all files on the root volume using the built-in zip utility.  Modern versions of Windows will automatically unzip the files when they are accessed.","correct":false},{"id":"229b013eff2d5f53df7b9c3a60bd2418","text":"From the AWS Console, select Modify Volume for the EBS volume.  Enter the new size and confirm the change.  Connect to your Windows instance and use Disk Manager to extend the newly resized volume.","correct":true},{"id":"38862a719074689f75df6a20a42f7df7","text":"Use AWS System Manager Run service to remotely execute a PowerShell script using AWS Tools for PowerShell to expand the volume using the ModifyInstance command.","correct":false}]},{"id":"1727d24a-6bf1-4fc6-b99b-24c0bc4552e9","domain":"awscsapro-domain2","question":"NextGen Appliances Corporation is developing a new smart refrigerator that sends telemetry data to a backend application running on AWS. The refrigerator will also be able to receive commands from a mobile dashboard app that will be provided with the product. Features of the mobile dashboard app include data query, analytics, notifications, and settings commands. The mobile dashboard app will be developed using AWS Amplify and will be deployed to an Amazon S3 bucket configured for web hosting. The software on the refrigerator will be developed with the AWS IoT Device SDK, and will communicate with AWS IoT Core. How should the company architect the backend application to provide management capabilities for the refrigerator from the mobile dashboard app?","explanation":"The smart refrigerator solution has an AWS IoT Core rule set up to route messages to a Lambda function which stores event data in DynamoDB and sends desired notifications to an SNS topic for viewing in the mobile dashboard app. The mobile dashboard app can be developed in AWS Amplify, and deployed to an S3 bucket configured for web hosting. CloudFront can be used to provide public access to the bucket. The app can send RESTful API requests to the backend through API Gateway to a Lambda function that performs the DynamoDB queries and sends commands to the refrigerator through IoT Core. Analytics requests can be routed to QuickSight Mobile, which uses IOT Analytics as its data source. The mobile app won't be able to send settings commands to IoT Core directly. API Gateway and Lambda need to be involved for that. QuickSight can't use Kinesis Data Analytics as a data source. The data will need to be written to S3 for QuickSight queries. IoT Analytics will provide analytics capabilities more suited to this use case than general solutions like Kinesis Data Analytics and Redshift. Amazon Pinpoint is used for sending personalized marketing messages, not general notifications. ","links":[{"url":"https://aws.amazon.com/iot-analytics/","title":"AWS IoT Analytics"},{"url":"https://aws.amazon.com/amplify/","title":"AWS Amplify"},{"url":"https://docs.aws.amazon.com/quicksight/latest/user/using-quicksight-mobile.html","title":"Using the Amazon QuickSight Mobile App"},{"url":"https://aws.amazon.com/solutions/smart-product-solution/?did=sl_card&trk=sl_card","title":"Smart Product Solution"}],"answers":[{"id":"5840ee0fec04ee6fca146272a5e75892","text":"Write IoT Core messages to Amazon Kinesis Data Streams with Amazon Kinesis Data Analytics as one consumer and Amazon EC2 instances with Auto Scaling as a second consumer. Have the EC2 instances write product event data to Amazon DynamoDB and publish notifications to an Amazon Simple Notification Service topic. Configure the mobile dashboard app to call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses Kinesis Data Analytics as its source.","correct":false},{"id":"af8854f867f77abde2a38d7b3d1d84b7","text":"Route IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and sends notifications to Amazon Pinpoint. Also write IoT Core messages to an Amazon S3 bucket to be loaded into Amazon Redshift by AWS Glue. Have the mobile dashboard app call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses Redshift as its source.","correct":false},{"id":"f0ebf0f93a7448dab82c61c11dcc2c4a","text":"Deliver IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and publishes notifications to an Amazon Simple Notification Service topic. Also write IoT Core messages to AWS IoT Analytics. Have the mobile dashboard app send settings commands to AWS IoT Core. Configure the AWS Mobile Hub NoSQL Database option and register the DynamoDB event table for app data queries. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses IoT Analytics as its source.","correct":false},{"id":"57c04a93b4f460af88471726dcca35b2","text":"Send IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and publishes notifications to an Amazon Simple Notification Service topic. Also write IoT Core messages to AWS IoT Analytics. Have the mobile dashboard app call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses IoT Analytics as its source.","correct":true}]},{"id":"5d35c6d3-3eaf-49d0-b64e-611d74d40af0","domain":"awscsapro-domain2","question":"You need to design a new CloudFormation template for several security groups. The security groups are required in different environments such as QA, Dev and Production. The allowed CIDR range in the security group ingress rule depends on environments. For example, the allowed inbound address range is 10.0.0.0/16 for non-production and 10.1.0.0/16 for production. You prefer to maintain a single template for all the environments. What is the best method for you to choose?","explanation":"CloudFormation has an optional Conditions section that contains statements to determine whether or not entities should be created or configured. Then you can use the \"Fn::If\" function to check the condition and return different values. You do not need to use Jenkins to pre-process the template and there are no \"Fn::Case\" and \"Fn::Switch\" intrinsic functions.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html","title":"CloudFormation conditions"}],"answers":[{"id":"c4a393b1cce09df81069eb93f2d38054","text":"Use the environment type as an input parameter and create a condition based on the parameter. In the AWS::EC2::SecurityGroupIngress resource, use Fn::If to check the condition and return related source CIDR range.","correct":true},{"id":"d2661ed9bcd0f8b132885660604a665a","text":"Create a Jenkins pipeline with an environment variable. Depending on the variable value, modify the template script to use the correct CIDR IP range. Deploy the CloudFormation stack with the updated template.","correct":false},{"id":"d273c3216496e7d41cb97094f83d3e33","text":"Create an environment variable and pass it to the CloudFormation template. In the security group resources, use Fn::Case and Fn::Switch to check the variable value and return different CIDR IP range.","correct":false},{"id":"7afa8c2f0fa2ef8966917c74044516dd","text":"CloudFormation does not provide functions for conditions. Use Terraform instead. Create a variable file to manage different CIDR IP ranges. Pass in the environment variable value and use \"Terraform apply\" to deploy all the security group resources at one time.","correct":false}]},{"id":"3ca7c5e0-432a-4d40-afee-ab996819b429","domain":"awscsapro-domain3","question":"You are consulting with a client to guide them on migration of an in-house data center to AWS.  The client has stipulated in the contract that the migration cannot require any more than 1 hour downtime at a time and that there is always a fallback path.  Additionally, they want an overall increase in business continuity capabilities when the migration is done.  Their landscape is as follows:  (1) Several databases with about 1TB of data combined which are heavily used 24x7 and considered mission critical; (2) About 40TB of historic files which are read sometimes but almost never updated; (3) About 150 web servers on VMware in various states of customization of which there is a current project underway to standardize them.  The client's team has suggested some next steps but because they aren't yet familiar with AWS, they are not using equivalent AWS terms.  Translating their suggestions, which of the following activities would you choose to meet the requirements, reducing costs and management where possible?","explanation":"The database migration suggestion aligns well with DMS as it can keep the databases in sync until cutover.  SAN replication sounds a lot like Storage Gateway which is a reasonable way to migrate data to AWS.  However, simply using K8s does not convert your VMs into containers or make them serverless.  We can't restore tapes to AWS.  Creating the same VM landscape on AWS just adds an additional layer of complexity that's not needed.","links":[{"url":"https://aws.amazon.com/dms/faqs/","title":"AWS Database Migration Service FAQs - Amazon Web Services"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"}],"answers":[{"id":"31ea0eccdcea45ea4fce3b9459de52d4","text":"Use some block-level SAN replication tool to gradually migrate the on-prem historic files to AWS.","correct":true},{"id":"d2dde578790a34d9e740015474ea23e4","text":"Migrate the majority of the 150 web servers to a serverless concept by moving the VMs to a Kubernetes cluster.","correct":false},{"id":"3a02ebcd33fe18255e4ce43e8babb730","text":"Over several months, at end of business on Friday, backup all the servers and data to tape and restore to new instances in AWS to prove out AWS capabilities and reliability.","correct":false},{"id":"75d528d2ec243c60d1478ae605c89f40","text":"Build a matching VMware environment on AWS and use third-party tools to backup and restore the VMs there.","correct":false},{"id":"801ce55cfc2a125e7d17c729ca3e2e93","text":"Create new high powered stand-alone database instances in AWS and migrate data from on-prem database.  Use log shipping to keep the databases in sync.  Once we better understand AWS, we'll rebuild the servers and repartition the tables. ","correct":true}]},{"id":"a2fb4f91-4c73-4080-bbf3-6d07a1b2ce03","domain":"awscsapro-domain2","question":"You have been asked to investigate creating a production Oracle server in RDS.  You need to choose the correct options that will allow you to run the latest version of Oracle 12c with High Availability.  You do not currently have any Oracle licenses. Which of the below are valid options?","explanation":"To get to the correct answer, you must first disregard any option with Oracle Data Guard as this is not available in RDS, then remove any answer containing the editions SE or SE1 as they only allow version 11g to be deployed, not 12c.  The remaining two options are correct as they allow High Availability, 12c and either a Bring-You-Own or licence included option, so you can ensure you get the best value.","links":[{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"Amazon RDS for Oracle FAQs"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Oracle.html","title":"Oracle on Amazon RDS"}],"answers":[{"id":"1ed8c7e073924060dbe648fbdf8a9c17","text":"Choose the SE2 edition with licence included and enable Multi-AZ","correct":true},{"id":"0cb21536be9ea582178c7e54e365bd33","text":"Choose either the SE or SE1 editions, purchase your own licenses from Oracle and enable Multi-AZ","correct":false},{"id":"5d1af18d1bfbdb6d990f138d844f75dc","text":"Choose either the SE or SE1 editions, Bring-Your-Own-Licence and Oracle Data Guard","correct":false},{"id":"0d9a63f8cb9a74845f363b450fa5cf11","text":"Purchase your own licenses from Oracle, Choose either the Enterprise or SE2 editions, Bring-Your-Own-Licence and enable Multi-AZ","correct":true}]},{"id":"1520156f-0918-4ab4-a759-ce33a931c744","domain":"awscsapro-domain5","question":"Your company has an online shopping web application. It has adopted a microservices architecture approach and a standard SQS queue is used to receive the orders placed by the customers. A Lambda function sends orders to the queue and another Lambda function fetches messages from the queue and processes them. On some occasions the message in the queue cannot be handled properly. For example, when an order has a deleted production ID, the message cannot be consumed successfully and is returned to the queue. The problematic messages in the queue keep growing and the ability to process normal messages is affected. You need a mechanism to handle the message failure and isolate error messages for further analysis. Which method would you choose?","explanation":"It is not a good idea to adjust the retention period or simply delete the messages that fail to be processed as the question asks for a mechanism to isolate the messages for further troubleshooting. A redrive policy should be used to auto-forward error message to a dead letter queue. Then you can analyze the contents of messages to diagnose the producer’s or consumer’s issues. One thing to note is that a standard queue can only have another standard queue as the dead letter queue. Therefore a FIFO dead letter queue is incorrect as this scenario uses a standard SQS queue and requires a standard dead letter queue.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","title":"Amazon SQS dead-letter queues"}],"answers":[{"id":"a7eb53a7df09334677590165f666c58f","text":"Modify the error handling logic of the Lambda function to delete the messages whenever the processing is unsuccessful with an error or exception. The error messages do not return to the queue and the normal message handling is not blocked.","correct":false},{"id":"f7b3898cfcb4851b120c9b14d044ab90","text":"Decrease the message retention period of the queue to 1 day. When the messages are not processed properly and put back in the queue, they can be quickly deleted when the retention period expires.","correct":false},{"id":"3c8453a6c57faf61e761f771bab6f1af","text":"Create a standard queue as the dead letter queue and configure a redrive policy to put error messages to the dead letter queue. Analyze the contents of messages in the dead letter queue to diagnose the issues.","correct":true},{"id":"64fd54fe78b534f0eac9222a6e32747d","text":"Create a FIFO (First-In-First-Out) queue as the dead letter queue and use a redrive policy to forward problematic messages to this new queue. Create a Lambda function to read the message contents in the FIFO queue for further analysis.","correct":false}]},{"id":"08a68d51-48ba-43b7-b0c3-c24e04bb33a8","domain":"awscsapro-domain3","question":"You have just completed the move of a Microsoft SQL Server database over to a Windows Server EC2 instance.  Rather than logging in periodically to check for patches, you want something more proactive.  Which of the following would be the most appropriate for this?","explanation":"The default predefined patch baseline for Windows servers in Patch Manager is AWS-DefaultPatchBaseline.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html","title":"Default and Custom Patch Baselines - AWS Systems Manager"}],"answers":[{"id":"b40084bb5d5139b353505e5f942afc34","text":"Make use of Patch Manager to apply patches as you have defined in the Patch Groups","correct":false},{"id":"78beead90b3e8deb4bf1ee7e3544a309","text":"Make use of AWS Batch to apply patches as they appear on the RSS feed from Microsoft","correct":false},{"id":"42c46b2fcd3034cf79b83f0d5dc37f7d","text":"Make use of Patch Manager and the AWS-DefaultPatchBaseline pre-defined baseline","correct":true},{"id":"5ee836bac8680ff00d457a8d7f90fad6","text":"Make use of Patch Manager and the AWS-WindowsDefaultPatchBaseline pre-defined baseline","correct":false},{"id":"4b5715f588231d7445bb512181ea13a2","text":"Make use of Server Manager and the AWS-LinuxWindowsDefaultPatchBaseline pre-defined baseline","correct":false}]},{"id":"7c5f884f-c0f9-4028-a725-50819d704324","domain":"awscsapro-domain5","question":"You deploy an application load balancer and an Auto Scaling group (ASG) in production for a new project. When instances in the ASG have a high CPU utilization, a new instance is launched. However, the new instance fails the health check from the ASG and has been terminated after some time. You check the logs in the instance and find that the startup script does not finish yet before the instance is terminated. How would you resolve the problem?","explanation":"Amazon EC2 Auto Scaling waits until the health check grace period ends before checking the health status of the instance. The grace period timer should be increased to give the instance more time to finish the startup script. Increasing the healthy threshold makes the instance more difficult to become healthy. Decreasing the timeout value also does not help as the instance may become unhealthy very quickly. Modifying the health check type from ELB to EC2 is unsuitable as the ASG cannot get the instance status from the application level. Even if the instance shows as healthy in ASG, the application may not be ready yet.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Check Grace Period"}],"answers":[{"id":"1d8964ced10da94d49d1a1efe02bbbca","text":"Modify the health check type from ELB to EC2 in the Auto Scaling group. Configure ASG to check the EC2 instance status. As long as the instance does not have a system level issue, it will not fail the health check in the ASG even when the startup script is still running.","correct":false},{"id":"e7ad9ea949ae87c6a6001a94f5a9bf48","text":"Increase the health check grace period in the Auto Scaling group configurations. When a new instance boots up, it is given more time to execute the startup scripts and run applications before the health check from ASG.","correct":true},{"id":"e322fd6a55a044826665dfce4ae7020b","text":"Decrease the timeout value in the ELB health check from 5 seconds to 1 second so that when the ELB performs the health check on the backend instances, the instances are able to respond in time before a timeout occurs.","correct":false},{"id":"b5ca84726bf6fcfd6522ab188c8224ea","text":"Increase the default healthy threshold in the health check of elastic load balancer from 5 to 10 so that the instance will become healthy more quickly once the startup script finishes in the new instance.","correct":false}]},{"id":"abc17e7b-6b75-45e3-a5c0-ea0f55d4df97","domain":"awscsapro-domain2","question":"Your client is a software company starting their initial architecture steps for their new multi-tenant CRM application.  They are concerned about responsiveness for companies with employees scattered around the globe.  Which of the following ideas should you suggest to help with the overall latency of the application?","explanation":"CloudFront can cache both static and dynamic content.  By setting a high TTL, we allow CloudFront to serve content longer before having to refresh from the origin.  Additionally, Lambda@Edge can intercept the request and direct the requester to a region based on the geographic origin of the request.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/11/lambda-at-edge-now-supports-content-based-dynamic-origin-selection-network-calls-from-viewer-events-and-advanced-response-generation/","title":"Lambda@Edge Now Supports Content-Based Dynamic Origin Selection, Network  Calls from Viewer Events, and Advanced Response Generation"}],"answers":[{"id":"0fc3951d630f285646b22cdd30f43eee","text":"Architect the system to use as many static objects as possible with high TTL.  Use CloudFront to retrieve both static and dynamic objects.  POST and PUT new data through CloudFront.","correct":true},{"id":"35483961564002569ee69763e24961fa","text":"Install the application in several regions around the globe.  As new customers and users are on-boarded, pre-cache their user data in CloudFront for that region.  Use AWS Batch to routinely expire the cache to ensure the latest updates are visible.","correct":false},{"id":"702a6122527d0830134717e0e7323bd0","text":"Store the data in a DynamoDB Global Table.  Use an auto scaling ElastiCache cluster with Memcached as a caching layer.  Distribute static elements of the application via CloudFront.  Use Route 53 Weighted routing to dynamically route users to the nearest region.","correct":false},{"id":"35638855dc45f62b3801906fd9a6d87c","text":"Install key parts of the application in multiple AWS regions chosen to balance latency for geographically diverse users.  Use Lambda@Edge to dynamically select the appropriate region based on the users location.","correct":true},{"id":"afa9743126cd2b0644654f2439a2aa0a","text":"Install the application on several regions around the globe.  Use RDS cross-region read replication for PostgreSQL to ensure a strongly consistent data store.","correct":false}]},{"id":"0abe2292-3f6e-47e1-93d9-6af24d5ea4c2","domain":"awscsapro-domain4","question":"A graphic design company has purchased eighteen m5.xlarge regional Reserved Instances and sixteen c5.xlarge zonal Reserved Instances. They receive their monthly AWS bill and find the invoice amount to be significantly higher than expected. Upon investigation, they discover RI discounted and non-discounted charges for nine m5.xlarge instances, nine m5.2xlarge instances, eight c5.xlarge instances, and eight c5.2xlarge instances. The business will need all of this capacity for at least the next twelve months. As their consultant, what would you advise them to do to maximize and monitor their RI discounts?","explanation":"Regional Reserved Instances allow for application of RI discounts within instance families, so all of the m5 instances are covered. Zonal Reserved Instances only provide discounts for specific instance types and sizes. So purchase of additional RIs would lower costs on the eight c5.2xlarge instances. Unused Reserved Instances are contractual and cannot be cancelled, so looking for another place to use them is the right approach. They could possibly be sold on the Reserved Instance Marketplace. AWS Budgets reservation budgets provide visibility and alerting on RI coverage specifically. Cost budgets and usage budgets may be useful, but they won't target RI coverage specifically.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html","title":"Regional and Zonal Reserved Instances (Scope)"},{"url":"https://aws.amazon.com/blogs/aws-cost-management/launch-instance-family-coverage-budgets/","title":"Launch: Instance Family Coverage Budgets"}],"answers":[{"id":"8c1f6db4cd04a86fd43efa1bc97640ea","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances and eight c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":false},{"id":"8e0e716570fad7bcaf6fe8bebe4f9d49","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Cancel the Reserved Instances for the nine unused m5.xlarge instances and eight unused c5.xlarge instances. Create an AWS Budgets cost budget that sends notification whenever costs exceed 80% of usage expectation","correct":false},{"id":"1c87a2204ad8da355c8ca0890aa06785","text":"Purchase an additional nine m5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances. Create an AWS Budgets usage budget that sends notification whenever RI coverage drops below 60%","correct":false},{"id":"4b6d77d8423cd16b8e711b482dbcafbf","text":"Purchase an additional nine c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":true}]},{"id":"312b233b-ecc8-4e70-ae91-665159c7f77b","domain":"awscsapro-domain2","question":"You work for an automotive parts manufacturer as a Cloud Solutions Architect and you are in the middle of a design project for a new quality vision system.  To \"help out\", your parent company has insisted on contracting with a very expensive consultant to review your application design.  (You suspect that the consultant has more theoretical knowledge than practical knowledge however.)  You explain that the system uses video cameras and special polarizing filters to identify defects on fuel injectors.  As the part passes each station, an embedded RFID serial number is read and included with the PASS/FAIL vision test result in a JSON record written to DynamoDB.  The DynamoDB table is exported to Redshift on a monthly basis.  If a flaw is detected, the part can sometimes be reworked and sent back through the process--but it does retain its unique RFID tag.  Only the latest tests need to be kept for the part.  The consultant reviews your design and seems slightly frustrated that he is unable to recommend any improvement.  Then, he smiles and asks \"How are you ensuring idempotency?  In case a part is reprocessed?\"    ","explanation":"Idempotency or idempotent capability is a design pattern that allows your application to deal with the potential of duplicate records.  This can happen when interfaces fail and some records need to be reprocessed.  In this case, we are using a unique RFID serial number as our identifier for the part.  In DynamoDB, we would just overwrite the record with the latest record using a UpdateItem SDK method.  For Redshift, an UPSERT function allows us to either insert as a new record or update if a record of the same key already exists.  Redshift can do this using a merge operation with a staging table.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateItem.html","title":"UpdateItem - Amazon DynamoDB"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-upsert.html","title":"Use a Staging Table to Perform a Merge (Upsert) - Amazon Redshift"}],"answers":[{"id":"9dc68479e32f89bfe0afde00f41ae6c3","text":"For the target DynamoDB table, you have defined the unique RFID string as the partition key.  When copying to Redshift, you use table merge method to perform a type of UPSERT operation.","correct":true},{"id":"8f594d180a595027ddef4e33f2784b0f","text":"You will be using CloudWatch to monitor the DynamoDB tables for capacity concerns.  If needed, you can enable DynamoDB auto scaling to accommodate the extra volume that reprocessing might introduce.","correct":false},{"id":"5a65801823c8e80af868a9ca05e34e18","text":"You will be using API Gateway and Lambda for the insert into DynamoDB so scaling is not a concern. The part can be processed as many times as needed and Lambda will scale as needed.","correct":false},{"id":"d87a9f08720c0fd91dee999f81f6f0ed","text":"You could change your design to write the message first to an SQS queue with FIFO enabled.  The records would then be guaranteed to process in the order they arrived.","correct":false}]},{"id":"c1333471-d052-4710-bcdb-facadc095d70","domain":"awscsapro-domain5","question":"You are setting up a corporate newswire service for a global news company.  The service consists of a REST API deployed on EC2 instances where customers can retrieve the latest news articles in real-time that happen to contain their company name.  This allows companies to monitor all news sources for stories where they are mentioned.  Because of the worldwide reach of the new site, you want to position servers around the globe.  You want to publish one subdomain name globally (api.domain.com) and have the requesters directed to the nearest region based on latency.  In each region, you want to be able to accommodate blue-green deployments without downtime as well.  What steps do you take?","explanation":"We want to use weighted routing records for local instances so we have the ability to adjust weights and shift traffic during blue-green deployments.  Latency-based routing would take care of funneling requests to the site with the lowest latency.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html","title":"How Health Checks Work in Complex Amazon Route 53 Configurations - Amazon  Route 53"}],"answers":[{"id":"41e30b8e30cbd737ced85953d7e3e939","text":"Use CloudFormation to create a distribution of the website.  Create an alias record for the subdomain (api.domain.com) in Route 53 and assign it to the CloudFront distribution.  To ensure no lag in news retrieval, set the maximum TTL on the CloudFront distribution to 0.","correct":false},{"id":"b850f1c18972d022213271c5d673e07f","text":"First setup weighted routing records for the local instances in the region in Route 53.  Assign equal weights with all sharing the same regional subdomain name (us-east-2.api.domain.com).  Next, create latency alias records by creating multiple entries for api.domain.com--each pointing to the regional subdomains.","correct":true},{"id":"a60aaaf971cbd546c1ec57e08ea38274","text":"We would first create geo-spatial records for the local resources in each region (us-east-2.api.domain.com) and assign equal weights.  Next, we create latency-based routing records for the top level subdomain (api.domain.com) and direct those to the regional records as an alias.  We must also disable Health Check on the latency record to ensure the localized Health Check is used.","correct":false},{"id":"6e56101de6ed2ca97550d5025ddf559a","text":"Using Route 53, we first create the top-level api.domain.com with a geolocation policy.  We then create latency-based routing records for the instances in each region (us-east-2.api.domain.com).  Next, we configure the countries closest to each region in the geolocation policy to direct them to the regional records.","correct":false}]},{"id":"760cbc05-d8ba-4d58-a555-250f26815963","domain":"awscsapro-domain4","question":"Your team is working on a long-term government project. You have purchased several reserved instances (c5.18xlarge) to host the applications to reduce costs. There is a requirement to track the utilization status of these instances and the actual utilization should be always over 80% of the reservation. When the utilization falls below 80%, the team members need to receive email and SMS alerts immediately. Which AWS services would you use to achieve this requirement?","explanation":"The AWS Budgets service is able to track the reserved instances utilization. Several optional budget parameters can be configured such as linked account, region and instance type. You can use an SNS notification to receive alerts when the actual usage is less than a defined threshold. You do not need to maintain a new Lambda function to get the usage data and send alerts. Besides, either CloudWatch alarm or Trusted Advisor cannot provide alerts based on the reserved instances utilization.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-create.html#create-reservation-budget","title":"Create a reservation budget"}],"answers":[{"id":"a4024e0dbac3a9e9e371ebfe3a511ba7","text":"In the Cost Optimization category of AWS Trusted Advisor, monitor the usage of the EC2 reserved instances. If the usage is less than 80% of the reservation, Trusted Advisor raises an action recommended alarm and sends a message to an SNS topic which notifies the team via email and SMS.","correct":false},{"id":"9cb24caa236c81ae714987bb14b0faee","text":"Create a new reservation budget in AWS Budgets service. Set the reservation budget type to be RI Utilization and configure the utilization threshold to be 80%. Use an SNS topic to notify the team when the utilization drops below 80%.","correct":true},{"id":"ba6dc1f65ab53b3ac3bb10944df422b4","text":"Create a Lambda function using Python boto3 to get the utilization status of the reserved instances. Execute the Lambda function at a regular basis such as every day and notify the team by an SNS topic if the utilization drops below 80%.","correct":false},{"id":"c13e13d0cd86acac35a5358fff11066a","text":"Configure the reserved EC2 instances to send the utilization metrics to CloudWatch. Create a CloudWatch dashboard based on the metrics and set up a CloudWatch alarm. Use an SNS topic to receive the alert when the alarm is on.","correct":false}]},{"id":"230f422f-7118-4096-8dce-59c642fb55c8","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a new Direct Connect connection.  The connection is up and you can ping the AWS peer IP address, but the BGP peering session cannot be established.  What should be your next logical troubleshooting steps?","explanation":"Because the connection is up and we can ping the AWS peer, the problem must be at a higher level on the OSI model than the Physical or Data layers.  BGP uses TCP port 179 to communicate routes so we should check that no NACL or SG is blocking it.  Additionally, we should make sure the ASNs are properly configured in the proper ranges.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/Troubleshooting.html","title":"Troubleshooting AWS Direct Connect - AWS Direct Connect"}],"answers":[{"id":"3d2a55832b90f19a2137e8715525d717","text":"Make sure no firewalls or ACLs are blocking TCP port 179 or any high-numbered ephemeral ports.","correct":true},{"id":"81977d7a1eb5714746851077b93f44d6","text":"Power cycle all the equipment to clear ARP table cache.","correct":false},{"id":"8fc27418eee2ce07b64bc672007d2c1b","text":"Contact the co-location provider and request a written report for the Tx/Rx optical signal across the cross connect.","correct":false},{"id":"45d4c1753395277878b9a17343628c52","text":"Ensure that the VLAN is configured properly between your on-prem router the provider. ","correct":false},{"id":"16e5aea88df69cc18f99e3f066ec99c1","text":"Ensure that the local ASNs and AWS-side ASNs are properly configured.","correct":true},{"id":"edd3f9408cecbbf9182678ccc51d7981","text":"Ask your network provider to provide you with a cross connect completion notice and compare the ports with those listed on your LOA-CFA","correct":false}]}]}}}}
