{"data":{"createNewExamAttempt":{"attempt":{"id":"78700352-7f3e-4e54-b991-8cceb176cf93"},"exam":{"id":"017d30f3-5092-4c2e-8e27-1d9e4740d27c","title":"AWS Certified Developer - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"e466b1d0-1858-40d8-aeca-e48e5e44ae2d","domain":"deployment","question":"Your application is using Kinesis to ingest data from a number of environmental sensors which continuously monitor for pollution within a 1 mile radius of a local primary school. An EC2 instance consumes the data from the stream using the Kinesis Client Library. You have recently increased the number of shards in your stream to 6 and your project manager is now suggesting that you need to add at least 6 additional EC2 instances to cope with the new shards. What do you recommend?","explanation":"Re-sharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. You should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances. When re-sharding increases the number of shards in the stream, the corresponding increase in the number of record processors increases the load on the EC2 instances that are hosting them. If the instances are part of an Auto Scaling group, and the load increases sufficiently, the Auto Scaling group adds more instances to handle the increased load.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html","title":"Kinesis Data Streams Terminology"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html","title":"Resharding, Scaling, and Parallel Processing"}],"answers":[{"id":"c11271a6921d20363c624ddde1790781","text":"You should decrease the number of shards to match the number of consumer instances","correct":false},{"id":"6c565b117fa64cb3b8ddaf38c670efed","text":"You should increase the number of instances to match the number of shards","correct":false},{"id":"1c8f531c39885147b4935b0e64880b97","text":"The number of instances should be greater than number of shards","correct":false},{"id":"7561e4f93183899bc8757da9db13ae60","text":"One worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances","correct":true}]},{"id":"f89a5f8e-ae8f-4ee6-842e-02f3101c7c60","domain":"development","question":"An application uses DynamoDB table as its backend data store. Each item has size of 10KB.  The application needs to perform 100 strongly consistent read operations per second, and 50 write operations per second.  What is the provisioned RCU value required to meet these requirements?","explanation":"1 RCU is equivalent to one strongly consistent read per second of an item up to 4KB in size.  Thus, to calculate the required RCU in this scenario we need to: 1) Round up the item size to the nearest 4KB (12KB). 2) Divide by 4KB to calculate number of strongly consistent read units (12/4 = 3). 3) Multiple by operations per second to get the total RCU required (3*100 = 300 RCU).","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","title":"Provisioned Mode"}],"answers":[{"id":"94f6d7e04a4d452035300f18b984988c","text":"300","correct":true},{"id":"a9b7ba70783b617e9998dc4dd82eb3c5","text":"1000","correct":false},{"id":"7ef605fc8dba5425d6965fbd4c8fbe1f","text":"150","correct":false},{"id":"cee631121c2ec9232f3a2f028ad5c89b","text":"500","correct":false}]},{"id":"e4d5998a-073a-4507-991c-ac138ac609c5","domain":"mon-trb","question":"Your application reads data from an SQS queue. The reads are then forwarded to Lambda downstream for processing critical customer information including purchasing and inventory data. It is critical that this data is not lost. How would you accommodate failed Lambda captures of the data?","explanation":"A dead letter queue would allow you to prevent data loss. After a message is taken from the queue and returned for the maximum number of retries, it is automatically sent to a dead letter queue, if one has been configured. It stays there until you retrieve it for forensic purposes. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. DelaySeconds for individual messages are similar to visibility timeouts because both features make messages unavailable to consumers for a specific period of time. The difference between the two is that, for delay queues, a message is hidden when it is first added to queue, whereas for visibility timeouts a message is hidden only after it is consumed from the queue. A FIFO queue guarantees first-in-first-out.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","title":"Amazon SQS Dead-Letter Queues"}],"answers":[{"id":"029cff5be66415e342452d104b4e2e57","text":"Requeue the message with a VisibilityTimeout of 30 seconds.","correct":false},{"id":"7e66715cc7d52627f1372ab97dbd2350","text":"Requeue the message with DelaySeconds to 3.","correct":false},{"id":"5d63d8895abd795fa4b98f12304185dc","text":"Create a dead letter queue and set the Maximum Receives to 3.","correct":true},{"id":"b431614eebc36d55520274500ed9ba2d","text":"Create a FIFO queue and set DelaySeconds to 3.","correct":false}]},{"id":"0f02f4b2-9d11-4efb-b467-19bc558ef33d","domain":"security","question":"Your application on EC2 must write to an Aurora cluster to store user and purchasing data. Your CISO implements a new company-wide policy that requires all AWS credentials are encrypted and rotated monthly. How would you fulfill the new security policy with minimum administrative burden?","explanation":"AWS designed IAM roles so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. IAM roles are based on temporary security tokens, so they are rotated automatically. Credentials embedded in source code cannot be rotated without it being an administrative burden, and is a bad practice. It’s impossible to retrieve credentials from an S3 bucket if you don’t already have credentials for that bucket. IAM users cannot be associated with resources, and Active Directory authorization will not grant access to AWS resources.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html","title":"IAM Roles for Amazon EC2"}],"answers":[{"id":"5fa27e0170ca1f467557d0d8db5502ff","text":"Encrypt the Aurora clusters' credentials using SHA-256 hash function in the application code, and schedule a CRON job to rotate monthly.","correct":false},{"id":"c3e276ad3302f173e7d92b40534f2074","text":"Allow the application to fetch the credentials from an S3 bucket with SSE-S3. Upload new credentials monthly.","correct":false},{"id":"05a57da8fa8c802639a4f49ed0d59116","text":"Associate an IAM user with the application. Enroll that user with your Active Directory domain to use AD authorization.","correct":false},{"id":"fc14a6fe77ae1367c0f776ae96c7379c","text":"Attach an IAM role to the instance with proper credentials.","correct":true}]},{"id":"a4afc791-647b-40b6-8f2a-29feb6142a0d","domain":"development","question":"ABC corp runs a web application that uses API Gateway to provide their developer customers with access to data. To reduce load on their upstream systems, the ABC corp have enabled API Gateway caching. A small number of developer customers still need access to results directly from the integration endpoint. To prevent all developer customers from bypassing the cache, ABC corp has also enabled the requirement for cache invalidation to require authorization.\n\nWhat must a developer customer do to return a result that is not cached from the API Gateway?","explanation":"Setting a Cache-Control: max-age=0 HTTP header as part of the request tells API Gateway that you want a response directly from the integration endpoint, rather than a cached response. This header can be interpreted as the client stating the maximum age a cached result can be is 0 seconds - equivalent to saying it cannot be cached at all.\n\nAs the cache is configured to require authorization to be invalidated, the request must be signed with a user or role that allows the execute-api:InvalidateCache action to be performed on the API Gateway resource. An example of this policy is found in the documentation for API Gateway Caching. We have also included a link to how to sign a request using AWS Signature Version 4.\n\nIt is recommended that you require authorization to invalidate a cached response; otherwise, if a significant number of requests perform an invalidation, the cache is no longer helping reduce load on upstream systems.\n\nCalling flush-stage-cache is incorrect because this would delete all data in the entire API cache, rather than just for the response the client has requested. Called often, this will likely result in the cache not having sufficient data to be effective.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"API Gateway Cache developer guide"},{"url":"https://docs.aws.amazon.com/apigateway/api-reference/signing-requests/","title":"Signing API Gateway requests"}],"answers":[{"id":"cb28453003b461189ca4aec138e2d1bf","text":"Sign their request with a user or role that has the required execute-api:InvalidateCache permissions to invalidate the cache.","correct":true},{"id":"3dd3c69d480989554365826eaef7be2f","text":"Include a ?execute-api-invalidate-cache query string in their request.","correct":false},{"id":"0f148da68f4442eace5c89fe939ba9ac","text":"Call the flush-stage-cache command before making the request.","correct":false},{"id":"4f0d8679da16e6c12618e0c2b85739be","text":"Include a Cache-Control: max-age=0 HTTP header in their request.","correct":true},{"id":"54f0ac8f9360a2dfe63f22a0a503ae7e","text":"Call the /execute-api-invalidate-cache API Gateway endpoint before sending their request.","correct":false}]},{"id":"8ee26b88-194a-4069-85a1-e28a48bcca27","domain":"security","question":"An organization has mandated that all data within its DynamoDB tables must be encrypted at rest using an AWS owned key. What must a developer do to ensure this?","explanation":"All DynamoDB tables are encrypted at rest using an AWS owned CMK by default. Non-encrypted DynamoDB tables are no longer supported in AWS. You have the option to pick an alternative AWS or Customer Managed KMS key if required.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html","title":"DynamoDB Encryption at Rest"}],"answers":[{"id":"92c03c315e7b990388532214f2a73c62","text":"Enable DynamoDB encryption and select AWS owned CMK.","correct":false},{"id":"5744856c94411674b105aff56f20a6a3","text":"There's no need to do anything; all DynamoDB tables are encrypted at rest with an AWS owned key by default.","correct":true},{"id":"2a13575a2661e32db0bdf9ad7b95ffee","text":"This cannot be done; DynamoDB does not support encryption at rest.","correct":false},{"id":"852abdb4878eba09a03ca6e37eb1dd16","text":"Enable DynamoDB encryption and select AWS managed CMK.","correct":false}]},{"id":"36bd2e7a-adf7-4dae-ad6f-5e04d3ca873e","domain":"security","question":"You work for a large government agency which is conducting research for a top secret defense project. You are using SQS to handle messaging between components of a large, distributed application. You need to ensure that confidential data relating to your research is encrypted by the messaging system, which of the following services can you use to centrally manage your encryption keys?","explanation":"You can use a CMK to encrypt and decrypt up to 4 KB (4096 bytes) of data. Typically, you use CMKs to generate, encrypt, and decrypt the data keys that you use outside of AWS KMS to encrypt your data. This strategy is known as envelope encryption. CMKs are created in AWS KMS and never leave AWS KMS un-encrypted. To use or manage your CMK, you access them through AWS KMS.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys","title":"KMS Concepts"}],"answers":[{"id":"4a4df63c87b4f42081b846d9b9189984","text":"KMS","correct":true},{"id":"fa9c36d7e57eae51ce84cfd30a7346a3","text":"Systems Manager Parameter Store","correct":false},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":false},{"id":"0e8433f9a404f1f3ba601c14b026d321","text":"HTTPS","correct":false},{"id":"ea52c36203c5f99c3ce2442d531b1a22","text":"SSL","correct":false}]},{"id":"007a648d-faa9-4464-9816-0b646386047f","domain":"mon-trb","question":"A three-tier application consists of a presentation and application tier deployed on EC2 instances in a public VPC subnet, and a data tier hosted on an RDS databases in a private VPC subnet.  When attempting to establish a connection to the RDS database, the application times out.  What could be the source of this problem?","explanation":"A VPC Network Security Groups are a network security capability providing firewall functionality.  They control inbound and outbound traffic on EC2 or RDS instances. NSG rules applied on the RDS database must be configured to allow inbound traffic from the NSG on the EC2 instances on the database port. If the NSG's on the RDS database are not configured correctly, any connection attempting to access the RDS instance will time out as the database will be unreachable. If database credentials were incorrect, the application would not time out. A connection to the RDS instance would be established, an the database would return an error code.  The question specifically states that there is an issue establishing a connection. VPC peering is used to establish a secure connection between two VPC’s, and is not suitable in this scenario. Similarly, Internet Gateway is used in public subnets to route outbound traffic to the internet and is not relevant to the situation.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html","title":"Controlling Access with Security Groups"}],"answers":[{"id":"77a29c8aaf1cae7a01a6aa1960c59878","text":"Database credentials are incorrect.","correct":false},{"id":"2954caaf920be59b173a0e00624c47f0","text":"VPC Peering is not configured properly.","correct":false},{"id":"caf6ad7f31cbd20a32d3eea156a6cffb","text":"The public subnet does not have Internet Gateway configured.","correct":false},{"id":"b7adf16356fbe2a22cd0c74e0b333b26","text":"Database NSG is not configured to allow traffic from EC2 instances.","correct":true}]},{"id":"108f8299-0018-4835-91f6-329fc5f9c1de","domain":"deployment","question":"Which of the following are considered to be Serverless?","explanation":"The following AWS technologies are Serverless: DynamoDB, API Gateway, SNS, Lambda, Kinesis and S3. RDS and Elastic Beanstalk both deploy EC2 instances to run their services","links":[{"url":"https://aws.amazon.com/serverless/","title":"Serverless Computing"}],"answers":[{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true},{"id":"3d6cbd7db2a4fa389808ea6f4a5fc1bc","text":"Elastic Beanstalk","correct":false},{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":true},{"id":"2ef9dd82927a3196ca2df3fc0cdf2e0b","text":"API Gateway","correct":true},{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":false}]},{"id":"8856df48-5866-4ee3-a5a7-2033444e21eb","domain":"security","question":"You have provisioned an RDS database and then deployed your application servers using Elastic Beanstalk. You now need to connect your application servers to the database. What should you do?","explanation":"As you are connecting to a database that was not created within your Elastic Beanstalk environment, you will need to create the Security Group yourself and also provide connection string and credentials to allow your application servers to connect to the database","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html#rds-external-defaultvpc","title":"Elastic Beanstalk And RDS"}],"answers":[{"id":"26b897f7474a08158506b32e7c566323","text":"Provide the ip address of the RDS instance to Elastic Beanstalk","correct":false},{"id":"1aa1798c4c20c22832e9a949af74ada3","text":"Configure Elastic Beanstalk to install a database client on your application servers","correct":false},{"id":"50021ae41e50f1b10069ebcccab3c0ef","text":"Provide the database connection information to your application","correct":true},{"id":"24e204c62e09f215959e144cbb8611d4","text":"Configure a security group allowing access to the database and add it to your environments auto-scaling group","correct":true}]},{"id":"7db65ca0-ae58-41f9-8817-0bc22b41d4a7","domain":"development","question":"You are running a large distributed application using a mix of EC2 instances and Lambda. Your EC2 instances are spread across multiple availability zones for resilience and are configured inside a VPC. You have just developed a new Lambda function which you are testing. However, when you try to complete the testing, your function cannot access a number of application servers which are located in the same private subnet. Which of the following could be a possible reason for this?","explanation":"To connect to a VPC, your functions execution role must have the following permissions: ec2:CreateNetworkInterface, ec2:DescribeNetworkInterfaces, ec2:DeleteNetworkInterface. These permissions are included in the AWSLambdaVPCAccessExecutionRole managed policy.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/vpc.html","title":"Configuring a Lambda Function to Access Resources in a VPC"}],"answers":[{"id":"d8a24d4f6bf371cd385b9a03e8b561f5","text":"The EC2 instances are running in a different region to the Lambda function","correct":false},{"id":"3a66964403f88954ed77583314f3ae2f","text":"The function execution role does not include permission to connect to the VPC","correct":true},{"id":"518adaf29098983c5a9e4ed138960fdd","text":"Your security group does not allow connectivity from the AWS Lambda endpoint","correct":false},{"id":"fc84432518af302b6f9d1b1157cde9eb","text":"The EC2 instances need to be in the same subnet as the Lambda function","correct":false}]},{"id":"6e7680bf-f045-4583-b38b-b2ca3ae466ed","domain":"security","question":"An IT Auditor has started in your Security Team, they will need access to read files in S3 and DynamoDB as well as the ability to describe EC2 instances. You want to ensure that only the Auditor is granted this access and that the IAM policy you create cannot mistakenly be attached to any other user. Which IAM policy type should you use?","explanation":"When you use an inline policy, the permissions in the policy cannot be inadvertently attached to the wrong principal entity","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html","title":"Managed Policies and Inline Policies"}],"answers":[{"id":"ab18cb3ee046b7e5466057043f273700","text":"Custom Policy","correct":false},{"id":"449ff67db14fb547ec64ef11c4d33c40","text":"Inline Policy","correct":true},{"id":"05aaffc9a37013fcae1b36c4baffceff","text":"AWS Managed Policy","correct":false},{"id":"60a6d5742e8f4d09d2d0edb9b15fa80d","text":"Customer Managed Policy","correct":false}]},{"id":"c1fc5f56-f74e-405f-a974-d9bb2e2c57e6","domain":"deployment","question":"You have deployed a new version of your Lambda function, however during testing, you notice that  your application is not behaving as expected. How can you roll back to the previous version of your code?","explanation":"Remapping the PROD alias to the previous version will allow you to quickly roll back","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda versioning and Aliases"}],"answers":[{"id":"1ba636d1ad4c4e2a9239fc75d17ffc41","text":"Update the $LATEST alias to point to the previous version of your function","correct":false},{"id":"60682da7d7f6df421c71e7e42cf4b227","text":"Redeploy your original code to $LATEST","correct":false},{"id":"aa97bfdc9437ce44352de51699501c4d","text":"Make a new version of your function using the original Lambda code","correct":false},{"id":"28ab809ddc3a2aee352db2592bca020a","text":"Remap the PROD alias to point to the previous version of your function","correct":true}]},{"id":"22556a45-7db0-48f9-85cf-654ac74d729f","domain":"security","question":"When using the AWS REST API to upload an object to S3, which of the following request headers will ensure that your data must be encrypted using SSE?","explanation":"To request server-side encryption using the object creation REST APIs, provide the x-amz-server-side-encryption request header.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"Protecting Data Using Server-Side Encryption"}],"answers":[{"id":"f041d978ebb9256a023c1f9d263316ac","text":"amz-s3-server-side-encryption","correct":false},{"id":"6d38512683c3cf8052e7e47d9d12a9f6","text":"x-amz-server-side-encryption","correct":true},{"id":"6d1f5944b9a6aada7e00dc385d5373bd","text":"x-s3-server-side-encryption","correct":false},{"id":"63e1961675193e5c234f582f08632a28","text":"s3-amz-server-side-encryption","correct":false}]},{"id":"91e59ee8-028b-44f1-9d03-1081d09738d3","domain":"development","question":"A clothing company needs to build a REST service to allow salespeople quick access to stock levels. The service must be accessible from an HTTP request. Which of the following solutions addresses the company's requirements?","explanation":"In an AWS Lambda integration in Amazon API Gateway, the HTTP method request from the client is mapped to a backend Lambda function invocation. Depending on your use case, you may choose to use Lambda proxy integration, Lambda non-proxy integration, or both in your API Gateway API. In a Lambda proxy integration, the entire client request is sent to the backend Lambda function as is, except that the order of the request parameters isn't preserved. In a Lambda non-proxy integration (also called a custom integration), you configure the way the parameters, headers, and body of the client's request are translated into the format that your backend Lambda function requires. ","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-getting-started-with-rest-apis.html","title":"Create a REST API with Lambda Integrations in Amazon API Gateway"}],"answers":[{"id":"d87496b040e2581c8e89e43a7e5ed235","text":"Amazon SQS and DynamoDB","correct":false},{"id":"e8445389785666bd90da881eada7e373","text":"Amazon CloudFront and Amazon S3","correct":false},{"id":"01f2c91643488c78436cc962fca2f2d7","text":"Amazon EC2 and AWS Auto Scaling","correct":false},{"id":"e9f6666f3057d0c266ac855cbae770b8","text":"Amazon API Gateway and AWS Lambda","correct":true}]},{"id":"d38a2534-f7bb-4cc2-9b9a-0c50dfb7707b","domain":"security","question":"You are attempting to analyse the CloudWatch metrics for a number of your application servers, however when you try to view the metrics you cannot access them, however one of your colleagues is able to access them without any issues. What could be the problem?","explanation":"Access to Amazon CloudWatch Logs requires credentials that AWS can use to authenticate your requests. Those credentials must have permissions to access AWS resources, such as to retrieve CloudWatch Logs data about your cloud resources.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/auth-and-access-control-cw.html","title":"CloudWatch Access Control"}],"answers":[{"id":"2578745b37e7d610c9f6227f920c4cb5","text":"CloudWatch doesn't have permission to collect the metrics","correct":false},{"id":"691a929a32273fce2328658fc4c41fa6","text":"Your EC2 instance role does not have permission to push the metrics to CloudWatch","correct":false},{"id":"a0ff1e54243f914c3324d8826543cdd2","text":"Your IAM user doesn't have permission to view CloudWatch metrics","correct":true},{"id":"dcd1178c83830f41078c6b19fcd33f05","text":"The CloudWatch agent has stopped running","correct":false}]},{"id":"b476baa3-99dc-4d87-a37a-2d0b23a7375c","domain":"deployment","question":"What is the largest size file you can transfer to S3 using a PUT operation?","explanation":"The largest file you can transfer to S3 using a PUT operation is 5GB. ","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html","title":"Uploading Objects to S3"}],"answers":[{"id":"6df47862fbfbd67605dc294d3f41925a","text":"100MB","correct":false},{"id":"208b4367cf771ded0e29ff6f9282442e","text":"1GB","correct":false},{"id":"84fb7b98e6e50302bf6cc709c92a6192","text":"5TB","correct":false},{"id":"0be25e5b91d25f9db3b4d3dcaf2cfd1f","text":"5GB","correct":true}]},{"id":"71b8b773-eed1-4335-9631-5bfe132f16c7","domain":"development","question":"You are the development lead on a large project to launch a new e-commerce website specialising in fishing supplies. Your developers are located in India, USA and the Middle East. You need to find a source code repository that everyone can use, and that will allow developers to continue to work on their code even when they are not connected to the internet. Which of the following would you suggest to the team?","explanation":"CodeCommit is based on Git, which is a distributed version control system, meaning there is no single, central place where everything is stored. In a distributed system, there are multiple backups in the event that you need one. This approach also means that you can work offline and commit your changes when you are ready.","links":[{"url":"https://aws.amazon.com/devops/source-control/git/","title":"Source Control In AWS"}],"answers":[{"id":"c2a2ffe5e9352016e58fe01b3c304de3","text":"Install Git on 2 EC2 instances in an auto-scaling group","correct":false},{"id":"94efdca7e5940d3078c950c64e833082","text":"Use CodeCommit to manage your source code","correct":true},{"id":"c3a56d6c63ca88e19d3e6afc5b896c46","text":"Use CodeBuild in offline mode to manage your source code","correct":false},{"id":"4e1b47d595e44072d3890d332ead7f86","text":"Run an instance of Git in a docker container on AWS ECS","correct":false}]},{"id":"4becf869-2fee-4023-80b0-0cb3a21c2651","domain":"development","question":"You are using CodeBuild to build the source code for your new application and would like to reference a large number of environment variables in buildspec.yml. However when you try to run the build you see an error telling you that the parameters you have specified have exceeded the number of characters allowed by the buildspec file. You need to find an alternative way to store these parameters, which of the following options would you recommend?","explanation":"Use Amazon Systems Manager Parameter Store to store large environment variables and then retrieve them from your buildspec file. Amazon EC2 Systems Manager Parameter Store can store an individual environment variable (name and value added together) that is a combined 4,096 characters or less.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html","title":"Troubleshooting CodeBuild"},{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html","title":"Build Specification Reference for CodeBuild"}],"answers":[{"id":"fdc76a3ff4a2b80aefd90a5dd389f44c","text":"Store the variables as dependencies within the application code","correct":false},{"id":"18f72b8f598e9cc0d20295158be172dd","text":"Store the variables as key value pairs in DynamoDB","correct":false},{"id":"913d4a9f910c7f6b836d623b39131480","text":"Use Systems Manager Parameter Store","correct":true},{"id":"31a2f312e14f62be70158dff280e56cb","text":"Store the variables as key value pairs in S3","correct":false}]},{"id":"e629622d-44a7-4083-a9a7-fb3fdc722794","domain":"development","question":"An organization wishes to use CodeDeploy to automate its application deployments. The organization has asked a developer to advise on which of their services can integrate with CodeDeploy.\n\nWhich of the following services can the developer advise are compatible with CodeDeploy managed deployments?","explanation":"CodeDeploy supports EC2, ECS (both EC2 and Fargate), Lambda, and on-premise servers.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/primary-components.html","title":"CodeDeploy - supported compute platforms"}],"answers":[{"id":"43928686738b0bda305045d0551f9eb4","text":"S3 Static website hosting","correct":false},{"id":"a6cf28d340be2c50ea43df2e6e98d2d1","text":"Fargate","correct":true},{"id":"81b22456f78954c460ce2f531b5e048f","text":"EC2","correct":true},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":true},{"id":"1c94a6d4d0c7d6eb4ba63ba083edaab5","text":"Elastic Kubernetes Service pods","correct":false},{"id":"8f852744b4deb867329697cf35ef908a","text":"On-premises servers","correct":true}]},{"id":"c9035bad-12fc-4afa-805c-a9faa8f1078f","domain":"development","question":"Your application accesses data stored in an S3 bucket. The S3 bucket hosts human resources data and the application produces a summary of key metrics with the human resources data via dashboard. The data is also inserted into an RDS table for another downstream process. Given this environment, which operation could return temporarily inconsistent results?","explanation":"Amazon S3 provides read-after-write consistency for PUTS of new objects in your S3 bucket in all Regions with one caveat. The caveat is that if you make a HEAD or GET request to a key name before the object is created, then create the object shortly after that, a subsequent GET might not return the object due to eventual consistency. Amazon S3 offers eventual consistency for overwrite PUTS and DELETES in all Regions. Strict read-after-write consistency is available on the main DB Instance.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html","title":"Introduction to Amazon S3"}],"answers":[{"id":"b5bc268aa60bf81ed5116d16a002db71","text":"Reading employee data on the dashboard after the employee was deleted from the S3 bucket from termination.","correct":true},{"id":"d0cc2f897bd189b1c38cb5df31dd166d","text":"Running a SELECT statement in RDS after employee data was removed due to termination.","correct":false},{"id":"88b5992e95f695a0d9a8f5bc66376a6e","text":"Running a SELECT statement in RDS after new employee data was inserted.","correct":false},{"id":"d78b72603892e5cf166cd79f125685df","text":"Downloading employee data from S3 after it was created.","correct":false}]},{"id":"88a77a58-b901-4605-a657-98cdc90dff65","domain":"deployment","question":"A developer needs to compile Java code to produce a deployment artifact. Which Amazon service can the developer use for this task?","explanation":"Amazon CodeBuild is a service that compiles source code, runs tests, and produces software packages that are ready to deploy. Amazon CodeCommit is a source control service that hosts Git-based repositories. Amazon CodeDeploy is a deployment service that automates software deployments. Amazon CodePipeline is a continuous delivery service that helps you automate your release pipelines.","links":[{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild"}],"answers":[{"id":"56c20df941e069e1a35a1871f662298a","text":"CodeCommit","correct":false},{"id":"f8d1a2a80933c71d9a65e3d825976bda","text":"CodeBuild","correct":true},{"id":"5f6f48261d96567b3014b43c23382021","text":"CodePipeline","correct":false},{"id":"07929940c38c1e9e91ecf67c01ccfa73","text":"CodeDeploy","correct":false}]},{"id":"6a4ef89c-dbe0-491d-88d4-11b09f0a272a","domain":"development","question":"What is NOT the best practice when deploying production applications using Elastic Beanstalk?","explanation":"It is a good practice to decouple an Amazon RDS instance from an Elastic Beanstalk environment, especially in production environment.  Launching an RDS database as part of an Elastic Beanstalk environment may be suitable for development or PoC environments. However, in general, it isn’t ideal as this means that termination of the Elastic Beanstalk environment will result in termination of the database as well. To protect important data from potential data loss, Amazon RDS database should be launched outside of the Elastic Beanstalk environment. With this approach, we decouple the life-cycle of the database from the life-cycle of the Elastic Beanstalk environment. This method also allows us to connect multiple environments to the same RDS instance. This may be useful for performing advanced deployment scenarios such as blue-green deployments. Storing RDS connection string in an encrypted, secured, and controlled S3 bucket and using Elastic Beanstalk configuration files is a valid method that can be used to securely store this data outside of the application code. Protecting the RDS databases from accidental deletion by enabling Delete Protection is always a good practice.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html","title":"Using Elastic Beanstalk with Amazon RDS"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/","title":"How do I decouple an Amazon RDS instance from an Elastic Beanstalk environment without downtime, database sync issues, or data loss?"},{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/rds-external-credentials.html","title":"Storing the connection string in Amazon S3"}],"answers":[{"id":"24e8e8250f45def68ec55670ac2390e7","text":"Amazon RDS Delete Protection should be enabled.","correct":false},{"id":"f8ac7ea397ed3bf0d0dcf3a3d5123521","text":"Amazon RDS databases should be included in the Elastic Beanstalk environment as that maintains the same life cycle for all components of the environment.","correct":true},{"id":"1a265bcccdb4ad642b0a09989c931196","text":"Amazon RDS database should be launched outside of the Elastic Beanstalk environment as that provides more flexibility.","correct":false},{"id":"9e19947230060c5fdc4610175989a39e","text":"Amazon RDS Connection String should be stored in a controlled S3 bucket.","correct":false}]},{"id":"f7fd0d7a-5d45-4f72-aa8e-d9a65270360f","domain":"refactoring","question":"You are developing an online hotel booking application which makes an number of requests to different back end applications to get quotes for travel related add-on services. You are using API gateway handle all the API calls and you notice that the majority of requests are for the same 5 or 6 services. How can you optimize the configuration to ensure the best performance for your application?","explanation":"You can enable API caching to cache your endpoint's responses, this reduces the number of calls made to your endpoint and improves the latency of requests to your API.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"API Caching"}],"answers":[{"id":"59a69a5bb20a3dbe9214ef93c38041f7","text":"Configure auto-scaling for the API Gateway","correct":false},{"id":"336a729744a0b4682222e3a4a1cdc750","text":"Implement API Caching to cache the endpoint's response for the most popular requests","correct":true},{"id":"b255bca6117b1096664ab7b1415fae80","text":"Add an ElastiCache cluster in front of your database to cache the most frequently accessed data","correct":false},{"id":"9a86454fefc71c7430655ce2e18ac716","text":"Configure a CloudFront CDN in front of the API Gateway to cache the most frequent HTTP requests","correct":false}]},{"id":"99e1be6c-c87a-45fd-b1a4-80ee4e4e1fe9","domain":"deployment","question":"You are creating a DynamoDB table to manage your customer orders, which of the following attributes would make a good Sort Key?","explanation":"A well designed Sort key allows you to retrieve groups of related items and query based on a range of values, e.g. a range of dates. In this case, Order Date is the best choice as it will allow users to search based on a range of dates","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-sort-keys.html","title":"DynamoDB Sort Keys"}],"answers":[{"id":"f6a56b98d7e5fa6571618bdc13db0c77","text":"OrderNumber","correct":false},{"id":"b5a34ba8eab605ce7cebca74791768c8","text":"OrderDate","correct":true},{"id":"bb96cafb784d4109a905317afdff8f71","text":"ProductID","correct":false},{"id":"8deaf45b8d4daa4827510e469b744fdb","text":"CustomerID","correct":false}]},{"id":"e516825d-bec2-463c-9a64-63be7bc91509","domain":"refactoring","question":"You are developing a serverless application which runs on Lambda, DynamoDB and API Gateway. The application needs to support an average of 5,000 requests per second. During testing, the Test Team want to test for peaks of 2.5 x the average load (12,500 requests per second). Shortly after testing begins, your application crashes with API Gateway generating a 429 error code. What could be the reason for this?","explanation":"By default, API Gateway limits the steady-state request rate to 10,000 requests per second. The 429 error means that the application is generating too many requests and is being throttled.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html","title":"API Request Throttling"}],"answers":[{"id":"e33a82eb6c47467dbddcc3cebfdd8dae","text":"Your tests have caused the application to hit the default limit of 10,000 requests per second for Lambda","correct":false},{"id":"71cfc755a5cb85f74a6c94a83c5a102b","text":"Your Lambda function has run out of CPU, you need to increase the memory allocation in order to increase CPU capacity","correct":false},{"id":"998c50932ec426f49890c7568b789ab0","text":"Your tests have caused the application to hit the default limit of 10,000 requests per second for API Gateway","correct":true},{"id":"c44bbd8c68bc540a9f4ec1eb6aef1f14","text":"Your Lambda function has run out of memory, you need to increase CPU capacity in order to increase memory capacity","correct":false}]},{"id":"00544151-fdaf-446d-ade3-7d6988c9133d","domain":"deployment","question":"You are building a distributed application, which is made up of a number of Docker instances running on an ECS cluster. You would like to configure your application to send data to X-Ray. Where should you install the X-Ray daemon?","explanation":"In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. X-Ray provides an official Docker container image that you can deploy alongside your application.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html","title":"Running the X-Ray daemon on Amazon ECS"}],"answers":[{"id":"bfdfc3a1dc6179ede8ebcc926faacd94","text":"Install the X-Ray daemon on the EC2 instance where your Docker containers are running.","correct":false},{"id":"63a693207215523784e22cf504e4579b","text":"Install the X-Ray daemon on the same Docker container where the application code is running.","correct":false},{"id":"5844e2a52b5fd5bfeeab3e714e68ba83","text":"Update the Docker image to include the X-Ray daemon and provision the new version of the application.","correct":false},{"id":"981d352ae2b571b99295e2880457f235","text":"Create a separate Docker image to run the X-Ray daemon.","correct":true}]},{"id":"69332ca4-69c7-4b5c-b3fd-3dcc1b2154c6","domain":"security","question":"You would like to test the effect of a new IAM policy which you are planning to attach to a group of developers in your team. Which of the following can you use to check that the policy works as expected?","explanation":"With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies that are attached to IAM users, groups, or roles in your AWS account. You can test which actions are allowed or denied by the selected policies for specific resources.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html","title":"Testing IAM Policies with the IAM Policy Simulator"}],"answers":[{"id":"b6632fa69795d5fabc908fe75210b177","text":"IAM Policy Simulator","correct":true},{"id":"543096643aa6d28d9fac278e9257783d","text":"Amazon Inspector","correct":false},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false}]},{"id":"ac3347a8-a8e9-482f-b9e1-cf55b44e51f4","domain":"mon-trb","question":"You are hosting your website in an S3 bucket located in us-east-1, however many of your users are located in India, Africa and Europe and they are experiencing long delays. How can you improve response times for these users?","explanation":"CloudFront can speed up the delivery of your static content to users across the globe. Creating additional buckets and replicating data is not an efficient approach. Connect Direct and ElastiCache cannot be used in the ways described.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/IntroductionUseCases.html#IntroductionUseCasesStaticWebsite","title":"CloudFront Use Cases"}],"answers":[{"id":"019191e8a1a081a3818038b8cdceadd5","text":"Configure a CloudFront CDN","correct":true},{"id":"ecbdb73e217f81efac6bf02c459d6091","text":"Implement Connect Direct","correct":false},{"id":"e225c4ebf2d1a40e5aabd2ecc669e13e","text":"Use ElastiCache to cache the content in each Region","correct":false},{"id":"a4ad24886aec69baee149f06fe05ad2e","text":"Create 3 additional S3 buckets in regions local to your users and replicate the data across to the new buckets","correct":false}]},{"id":"d1b0d0fe-4931-441c-83d5-716d63424a58","domain":"mon-trb","question":"You are working on an application which shares video content to subscribed users. This morning you have received a number of complaints that users are unable to access your content and they are seeing an HTTP 504 Status Code. Which of the following could be a possible explanation?","explanation":"An HTTP 504 status code is a Gateway Timeout which indicates that when CloudFront forwarded a request to the origin, because the requested object was not in the edge cache, one of the following happened: The origin returned an HTTP 504 status code to CloudFront; or, the origin didn’t respond before the request expired. This is a server side issue, i.e. a problem or misconfiguration in your AWS infrastructure. Remember that any 5XX error indicates a server-side error, and a 4XX error indicates a client-side error.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/http-504-gateway-timeout.html","title":"HTTP 504 Gateway Timeout"}],"answers":[{"id":"76b78bdea733c0e70e13c80add3c975d","text":"There is a client side error in the user's infrastructure","correct":false},{"id":"b9434940d2516177efc8f8eef8b3de37","text":"There is a server side error within your AWS infrastructure","correct":true},{"id":"84f05e093a7f246d10fef9ea043d217a","text":"The users have a network connectivity problem","correct":false},{"id":"c8cffd0bf5d95cfc0541c7ad2d54d740","text":"The users could be attempting to access your site using an unsupported browser","correct":false}]},{"id":"cd7c8d61-2ce7-401a-956f-c01c33305ae2","domain":"deployment","question":"Which of the following statements is correct?","explanation":"EBS-backed instances can be stopped and restarted without losing the data on the volume.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/instance-store-vs-ebs/","title":"Differences between EBS and Instance Store"}],"answers":[{"id":"91fff5146ee33778a9158bd16a8ba469","text":"An Amazon VPC requires that instances be backed with EBS.","correct":false},{"id":"b335920b9f6cd5d6425a787a6183b378","text":"If you want to use auto-scaling, you must use an EBS-backed instance.","correct":false},{"id":"36e118d467e9fcf94627a5fa1bc11446","text":"Instance-store backed instances can be stopped and restarted.","correct":false},{"id":"18eb677e835b4b7214b432d085e121fb","text":"An EBS backed instance can be stopped and restarted.","correct":true}]},{"id":"418aa386-2a62-4e41-8724-c4d4adf6a318","domain":"refactoring","question":"You require a data storage solution for an application running on EC2. The data running on the application is not well-structured to fit into a defined schema. Even so, the schema would change very often as data is dependent on users. What choice of database solution would best support your application?","explanation":"Transactional data, such as e-commerce purchase transactions and financial transactions, are typically stored in relational database management systems (RDBMS) or SQL database systems. The choice of database solution depends on the use case and application characteristics. A NoSQL database is suitable when the data is not well-structured to fit into a defined schema, or when the schema changes very often. An RDBMS solution, on the other hand, is suitable when transactions happen across multiple table rows and the queries require complex joins. Amazon DynamoDB would be a better use case in this scenario versus Amazon RDS (which is a SQL-based, structured, relational database solution). AWS IoT can work with other AWS services like Lambda, Kinesis, S3, and DynamoDB to build applications that gather, process, analyze, and act on IoT data but is not a database solution. Amazon Kinesis is not a database solution, but rather a streaming data solution.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html","title":"From SQL to NoSQL"}],"answers":[{"id":"763409014035e2e7dad1fd45adc43794","text":"Use AWS IoT connected devices to interact easily and securely with AWS Lambda, Amazon Kinesis, and Amazon S3.","correct":false},{"id":"6211995f3e47e0e7423efeb101148a29","text":"Use Amazon Kinesis services to be able to collect, store, and process your users' data continuously.","correct":false},{"id":"58e198b9da277384f8be4026cb2431e1","text":"Use an RDBMS solution such as Amazon RDS to implement a SQL-based relational database solution for your application.","correct":false},{"id":"b1f7c088e6e85841d4830922aa622f9e","text":"Use a NoSQL database such as Amazon DynamoDB that can be used as an OLTP store for your application.","correct":true}]},{"id":"03a312db-34cd-4c47-9f7e-da02470c8c03","domain":"mon-trb","question":"You work in the security industry for a large consultancy. One of your customers uses Lambda extensively in their production environment and they require a log of all API calls made to and from their Lambda functions. How can you achieve this?","explanation":"Enabling CloudTrail for Lambda will allow you to log all API calls to an S3 bucket.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/logging-using-cloudtrail.html","title":"Logging Lambda API Calls Using AWS CloudTrail"}],"answers":[{"id":"2365a4464b0b2502654aeb4ddf868a14","text":"Enable CloudTrail for Lambda","correct":true},{"id":"02ef0d69dfa15f0bee0148eeee8c3936","text":"Enable Detailed Monitoring on the Lambda function","correct":false},{"id":"77bca249739dbbe611e40d1dd5f2dc7e","text":"Enable Access Logs for Lambda","correct":false},{"id":"e2cd3f939eef3b06ec217db11988c978","text":"Enable CloudWatch for Lambda","correct":false}]},{"id":"b1a8a6e6-0eb1-4c01-8d8b-4baebf067f88","domain":"development","question":"An application uses DynamoDB table as its backend data store. Each item has size of 10KB.  The application needs to perform 100 eventually consistent read operations per second, and 50 write operations per second.  What is the provisioned RCU value required to meet these requirements?","explanation":"1 RCU is equivalent to two eventually consistent reads per second of an item up to 4KB in size.  Thus, to calculate the required RCU in this scenario we need to: 1) Round up the item size to the nearest 4KB (12KB). 2) Divide by 4KB to calculate number of read units (12/4 = 3). 3) Divide by 2 to calculate the number of eventually consistent read units per item (3/2 = 1.5). 4) Multiple by operations per second to get the total RCU required (1.5*100 = 150 RCU).","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","title":"Provisioned Mode"}],"answers":[{"id":"cee631121c2ec9232f3a2f028ad5c89b","text":"500","correct":false},{"id":"7ef605fc8dba5425d6965fbd4c8fbe1f","text":"150","correct":true},{"id":"a9b7ba70783b617e9998dc4dd82eb3c5","text":"1000","correct":false},{"id":"94f6d7e04a4d452035300f18b984988c","text":"300","correct":false}]},{"id":"b170b108-ed74-40ba-a811-383fa049982d","domain":"mon-trb","question":"You receive a \"timed out\" error message when running a command using the AWS CLI. What could be a possible reason for this?","explanation":"If you see issues when running list commands on a large number of resources, the default page size of 1000 might be too high. This can cause calls to AWS services to exceed the maximum allowed time and generate a \"timed out\" error. You can use the --page-size option to specify that the AWS CLI request a smaller number of items from each call to the AWS service. The CLI still retrieves the full list, but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call. This gives the individual calls a better chance of succeeding without a timeout. Changing the page size doesn't affect the output; it affects only the number of API calls that need to be made to generate the output.","links":[{"url":"https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html","title":"AWS CLI Pagination"}],"answers":[{"id":"693a2ee98a96c9712d1720d141c29c5b","text":"You have run a command which is trying to return a large number of items and has exceeded the maximum allowed time to return results","correct":true},{"id":"9a732fb1da270d734f8c6fd13ec818f7","text":"The AWS service that you are trying to call is taking too long to respond","correct":false},{"id":"afdc0b8222f758f2d5101f7516eac7b5","text":"Your network connection is too slow","correct":false},{"id":"6671b1dd34271b2e7f713a895d4dc800","text":"The AWS service that you are trying to call is unreachable","correct":false}]},{"id":"70e8f135-19fb-4ad6-82bc-8ec0ed899d83","domain":"security","question":"You are developing a video streaming application which users can access using multiple devices, for example, laptop, tablet and cell phone. You would like to be able to track usage across the different devices and limit the number of devices from which a user can stream content. Which of the following AWS technologies could you use to achieve this?","explanation":"Cognito enables developers to remember the devices on which end-users sign in to their application. You can see the remembered devices and associated metadata through the console. In addition, you can build custom functionality using the notion of remembered devices. For example, with a content distribution application (e.g., video streaming), you can limit the number of devices from which an end-user can stream their content.","links":[{"url":"https://aws.amazon.com/blogs/mobile/tracking-and-remembering-devices-using-amazon-cognito-your-user-pools/","title":"Tracking and Remembering Devices Using Amazon Cognito"}],"answers":[{"id":"3cff09984a222738fa34702f699ab961","text":"Use a Lambda function to store session state and device type in DynamoDB","correct":false},{"id":"5348de0d4df2bd40fa189bd4e5ff1b6c","text":"Store device metadata linked to session state in ElastiCache","correct":false},{"id":"6cbb574c9488bb59fdd64fa8f509fce8","text":"Use Cognito","correct":true},{"id":"425684337e67b0a12f438c06f1c5818d","text":"Use MFA on the device","correct":false},{"id":"3a73d44e5a5b2804c4cc5dd0b6c0bfe5","text":"Use S3 to store metadata about the device and link it to session state held in DynamoDB","correct":false}]},{"id":"ac8156df-b2ea-447a-a070-346b3a5f7234","domain":"security","question":"You are working on an application which handles online credit card applications. It consists of a number of web and application servers running on EC2, customer reference data stored in S3 and transactional data stored in RDS. The security team have noticed that you have a lot of sensitive customer information stored in S3 and you have been asked to configure encryption at rest to protect the data. How can you do this?","explanation":"You can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket. When you use server-side encryption, Amazon S3 encrypts an object before saving it to disk in its data centers and decrypts it when you download the objects.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html","title":"S3 Default Encryption for S3 Buckets"}],"answers":[{"id":"46cf5e7bd245efaa706dc8411b60bf17","text":"Encrypt your local root disk before uploading the files","correct":false},{"id":"349aac318f19b866d43ee7ad35140312","text":"Use SSL to upload the files","correct":false},{"id":"f7d3a4d76ebd5e4e87874e392c6ee6da","text":"Encrypt the files locally using the AWS Encryption SDK","correct":false},{"id":"19fb0f18150f5096d8b66cea58825925","text":"Select default encryption on your S3 bucket","correct":true}]},{"id":"dbc263cc-9e31-4671-b11a-674870a5dcd3","domain":"deployment","question":"You have been asked to use Elastic Beanstalk to build a number of web servers to use in your development environment, which of the following services can you use?","explanation":"Except for Lambda, all of the services listed can be used to create a web server farm. AWS Lambda automatically runs your code without requiring you to provision or manage servers. Lambda is generally used for stateless, short-running tasks and is not suitable for long-running tasks like running a web server.","links":[{"url":"https://aws.amazon.com/lambda/","title":"What Is Lambda?"},{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html","title":"What Is AWS Elastic Beanstalk?"}],"answers":[{"id":"b578e821ab9be9669c17208a583b5899","text":"Auto Scaling Group","correct":true},{"id":"e95e5a2a3cc8625bca3d71b817367e2d","text":"Elastic Load Balancer","correct":true},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true},{"id":"81b22456f78954c460ce2f531b5e048f","text":"EC2","correct":true},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":false}]},{"id":"5b6fd3fe-de07-4eb3-a61d-f96e4b4b5f1b","domain":"refactoring","question":"Which of the following is NOT a supported event source for Lambda?","explanation":"Supported event sources which can trigger Lambda functions include: CloudWatch, DynamoDB, S3, Kinesis, CodeCommit, IoT buttons, CloudFront, Cognito, SNS, SQS, SES etc. RDS cannot trigger a function directly but you could configure RDS to send notifications to SNS and then use SNS to trigger a Lambda function. ","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html","title":"Lambda Event Sources"}],"answers":[{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":false},{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":true},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":false},{"id":"1399767aff0c1931602c17dcfb0d375e","text":"CloudWatch Events","correct":false}]},{"id":"b5ddd11e-a955-43ce-bc64-c067631c90ab","domain":"development","question":"A Developer is implementing an application that must allow users to subscribe to e-mail notifications. Which AWS service is the best option for implementing this functionality?","explanation":"Amazon Simple Notification Service (Amazon SNS) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients: publishers and subscribers, which are also referred to as producers and consumers. Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel. Subscribers (that is, web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (that is, Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic.","links":[{"url":"https://docs.aws.amazon.com/sns/latest/dg/welcome.html","title":"What is Amazon Simple Notification Service?"}],"answers":[{"id":"ef4ba2f338cdf4b615ed280ff0b2777d","text":"SQS","correct":false},{"id":"3dee8a3d0ac81b151d8a0fa87c673799","text":"WorkMail","correct":false},{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":true},{"id":"8513f757701b24dbadad3df74e817df5","text":"SES","correct":false}]},{"id":"d36289fe-be3b-4a73-8cc2-c216cfd47b05","domain":"security","question":"You're part of a developer team which is building an application that requires access to S3. Everyone on your team requires the same IAM permissions. As your team grows, how would you manage IAM policies and access to the right AWS resources in the most efficient manner?","explanation":"IAM groups are collections of IAM users in one AWS account. You can create IAM groups on a functional, organizational, or geographic basis, or by project, or on any other basis where IAM users need to access similar AWS resources to do their jobs. You can provide each IAM group with permissions to access AWS resources by assigning one or more IAM policies. All policies assigned to an IAM group are inherited by the IAM users who are members of the group. Creating IAM Users for each team member is not the most efficient manner; IAM Groups is more efficient. You cannot log into the AWS Management Console using an IAM role, nor can you do the same with Amazon Cognito. Amazon Cognito is best suited for mobile applications.","links":[{"url":"https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf?refid=em_","title":"AWS Security Best Practices"}],"answers":[{"id":"a834e2bd501ea84ca860e5213b4290ea","text":"Create one IAM role with the necessary permissions. Have all team members log into the AWS Management Console using that role. Rotate the password regularly.","correct":false},{"id":"9ef5172ef2f91f7f79640375254443b1","text":"Create an IAM Group called 'Developers'. Attach an IAM policy to the group with the appropriate permissions. Associate your IAM user and your team members' users to the Group. Add new team members to the group as appropriate.","correct":true},{"id":"d81d323b3e2be35dfba6061c653ce5d1","text":"Create an Amazon Cognito user pool for each user and a corresponding S3 bucket. Grant S3 bucket GET requests for each bucket to each Cognito user. Require users to log into the Console using their Cognito credentials.","correct":false},{"id":"7c9871a327396074304930d0dbe0fcee","text":"Create IAM Users for each team member. Attach an IAM policy to each user. Edit the IAM policy for each user adhering to the Principle of Least Privilege. Create new IAM policies for new team members as appropriate.","correct":false}]},{"id":"6f11e8ea-c824-427c-9cf9-7bebec4fe4b6","domain":"development","question":"Where should the appspec.yml be stored?","explanation":"The AppSpec file (appspec.yml) must always be in the root or your application source directory otherwise the deployment will not work. The .ebextensions folder is used to set custom environment variables in Elastic Beanstalk, not CodeDeploy.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html","title":"CodeDeploy AppSpec File"}],"answers":[{"id":"b007c30133faeba98cd0fbcedbf4fa6c","text":"In the root of your application source directory","correct":true},{"id":"57f3232fcb44408176cbdb5c8b7e4b06","text":"In /opt","correct":false},{"id":"8ee84dac450c4097f3cce035b8ede57d","text":"In the config directory in your application source directory","correct":false},{"id":"0a2d3304fa88233c14e8c9ebffba0882","text":"In the .ebextentions folder","correct":false}]},{"id":"2988d8a0-c66e-434f-948b-acee623892fc","domain":"mon-trb","question":"You work for an electric car company that has its front-end website on EC2. Company policy dictates that you must retain a history of all EC2 API calls made on your account for security analysis and operational troubleshooting purposes. Which AWS service should you use to do this?","explanation":"CloudTrail is a web service that records AWS API calls for your AWS account and delivers log files to an Amazon S3 bucket. The recorded information includes the identity of the user, the start time of the AWS API call, the source IP address, the request parameters, and the response elements returned by the service.","links":[{"url":"https://aws.amazon.com/documentation/cloudtrail/","title":"CloudTrail Overview"}],"answers":[{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":true},{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"a907a7338c1fb3821fbe8ed113c64b33","text":"CloudHSM","correct":false}]},{"id":"8b887631-86bd-436d-adee-4e2ba3b02111","domain":"security","question":"You have an application running on multiple EC2 instances, however every time an instance fails, your users complain that they lose their session. What can you do to prevent this from happening?","explanation":"There are various ways to manage user sessions including storing those sessions locally to the node responding to the HTTP request or designating a layer in your architecture which can store those sessions in a scalable and robust manner. Common approaches used include utilizing Sticky sessions or using a Distributed Cache for your session management. In order to address scalability and to provide a shared data storage for sessions that can be accessed from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution for this is to leverage an In-Memory Key/Value store such as ElastiCache.","links":[{"url":"https://aws.amazon.com/caching/session-management/","title":"Session management in AWS"}],"answers":[{"id":"76fc6bddee6b0f8d088ea5cbe4e57160","text":"Store session state in S3","correct":false},{"id":"b225818943ba4680b8e7dc9d9c376359","text":"Store session state in RDS","correct":false},{"id":"ef4fd36fa55c3c499f3fffa82a0c95e8","text":"Store session state in on the Elastic Load Balancer","correct":false},{"id":"89230492f141a4f85234c624287bb96a","text":"Store session state in ElastiCache","correct":true},{"id":"b193b1caff1bda86125cc326ca1058ac","text":"Store session state on a dedicated EC2 instance","correct":false}]},{"id":"76e6d618-2134-4a73-94dc-dc1c3af9eac3","domain":"development","question":"Which of the following are supported ways to upload and deploy your Lambda code?","explanation":"You can paste code or upload a zip file directly to the console, upload your code to S3 or use a CloudFormation template. Elastic Beanstalk is not a supported method of deploying Lambda.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html","title":"Lambda Deployment Packages"},{"url":"https://docs.aws.amazon.com/toolkit-for-eclipse/v1/user-guide/lambda-tutorial.html#lambda-tutorial-upload-code","title":"Uploading Lambda Code"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html","title":"CloudFormation and Lambda"}],"answers":[{"id":"a1d81e8594c3bbc120a7270f2475c2b1","text":"Zip your code into a zip file and upload it via the Lambda console","correct":true},{"id":"5c0aaa802c93c303836e0babb6d7e99c","text":"Zip your code into a zip file, upload it to an S3 bucket and have Lambda download it from S3","correct":true},{"id":"9a700b9b08c4e3228582eae88ca49df0","text":"Write a CloudFormation template that will deploy your environment including your code.","correct":true},{"id":"cf448e03c3d4d06aae12f839f0b9c6ec","text":"Zip your code into a zip file, upload it to Elastic Beanstalk, then deploy your environment using Elastic Beanstalk","correct":false},{"id":"185133cc98a2cbb85e317fd8493e7a26","text":"Copy and paste your code in to the integrated development environment (IDE) inside Lambda","correct":true}]},{"id":"bc69cb14-0b50-4173-925a-a9c2fdb00bb5","domain":"mon-trb","question":"You have multiple applications running on a large number of EC2 instances. You need to access the application logs from a single central location, what should you do?","explanation":"You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources. CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can create CloudWatch custom metrics for your EC2 instance statistics by creating a script through the AWS Command Line Interface and then monitor that metric by pushing it to CloudWatch. However, custom metrics are only metrics or findings reported by running a script, they are not pushing log files into CloudWatch.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html","title":"What Is Amazon CloudWatch Logs?"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-custom-metrics/","title":"Custom Metrics and CloudWatch"}],"answers":[{"id":"208fa683260008af411c8bfe394bb8bf","text":"Write a script to send the application logs to CloudWatch. Install the script on each of your application servers","correct":false},{"id":"f0d36f4f7323e9b431c82349dcc3ab89","text":"CloudTrail Logs","correct":false},{"id":"458bb7239f53c3a09ada45ce8d2eb321","text":"CloudWatch custom metrics","correct":false},{"id":"a8c600be214ced26950e704d39c3ca21","text":"CloudWatch Logs","correct":true}]},{"id":"95a7c517-4b4e-4996-b4d4-8ae11f359065","domain":"development","question":"An organization is considering performing canary deployments with their application. Which of the following statements best describes a canary deployment?","explanation":"A canary deploy allows us to gain confidence in an application update by initially just releasing it to a subsection of users. Once we are satisfied that the update is working as expected, the update is then rolled out to the remaining users.\n\nThe concept of a canary deployment is covered in the AWS Well-Architected Framework, and is a feature of API Gateway.\n\nIt can also be performed manually using Route53 Weighted Records, or via an Application Load Balancer with a Forward Action and Weighted Target Groups.","links":[{"url":"https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html","title":"Canary deployment - Well-Architected Framework"}],"answers":[{"id":"0beefb480486ea595083b6d39062c27b","text":"All instances of the original application are stopped, after which new instances of the application are started up in their place. During the transition, there is a short period of downtime.","correct":false},{"id":"03ff12365c5a8fc5fe5dfdbd2f1f00b6","text":"Each instance of the original application is taken out of service one at a time and replaced with the new version of the application. During the deployment, traffic is sent to a mix of the original and new versions.","correct":false},{"id":"d593ae8cd7b7dc0ff31e7a286abc4a60","text":"A new version of the application is deployed alongside the existing version. Once the new version is ready to handle traffic, all traffic is redirected to it.","correct":false},{"id":"18c6683d5f9bd9718e5549310c8647e2","text":"A new version of the application is deployed alongside the existing version. A proportion of application’s traffic is directed to the new application. If, after a given number of minutes, metrics demonstrate that the new version is performing correctly, the remainder of the traffic is moved to the new version.","correct":true}]},{"id":"f7a67868-ec20-40ae-a334-1bca9d02bdb7","domain":"deployment","question":"You are developing a website which allows customers to purchase tickets to popular sporting events. Your application uses S3 for static web hosting, Lambda for business logic, stores transaction data in RDS and uses DynamoDB for product and stock information. After the customer has paid for their purchase, a message is sent to an SQS queue to trigger a confirmation email to be sent out to the customer including an e-ticket for their chosen event. You want to send out the email as soon as the payment has been processed, however during testing you discover that the confirmation emails are being processed a few seconds before the stock control database has finished updating. This sometimes results in selling the same ticket twice. How can you quickly fix this without re-engineering the application?","explanation":"Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html","title":"SQS Delay Queues"}],"answers":[{"id":"953522fe58e17a4a74978232a8b50608","text":"Set the delay flag on the queue to 5 seconds, to ensure messages are not processed too quickly","correct":false},{"id":"2c519c2e315e56078976351bc313bbf9","text":"Use a FIFO queue to ensure the messages are always processed in the correct order","correct":false},{"id":"93afc9503f6f41866e3fb6bf957bc816","text":"Use Kinesis to stream the SQS messages, adding a delay of a few seconds","correct":false},{"id":"dc7f3b8fe45a2bd74a545027f1178792","text":"Use an SQS delay queue to let you postpone the delivery of SQS messages by a few seconds","correct":true}]},{"id":"0dfd155c-e9fa-4a85-baba-ba51ae43d152","domain":"deployment","question":"As a developer you have built a WordPress site. Traffic to the site has increased and you have improved the site's functionality to meet the demand of your viewers since launch. Changes are coming frequently, and you are considering using AWS CloudFormation to automate the process of building test stacks, creating a change set, and executing the change set. How would you streamline this process in AWS most efficiently?","explanation":"Continuous delivery is a release practice in which code changes are automatically built, tested, and prepared for release to production. With AWS CloudFormation and CodePipeline, you can use continuous delivery to automatically build and test changes to your AWS CloudFormation templates before promoting them to production stacks. This release process lets you rapidly and reliably make changes to your AWS infrastructure. Although you can manually interact with CloudFormation to execute the various stages, this is not the most efficient method. Amazon Inspector is an automated security assessment service which evaluates the security loopholes in deployed resources specific to EC2. Config is a monitoring and governance tool that tracks changes to your AWS environment based on rules you configure.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-basic-walkthrough.html","title":"Walkthrough: Building a Pipeline for Test and Production Stacks"}],"answers":[{"id":"6e617dd2e008968aea6984c58b17c139","text":"Use Amazon Inspector to monitor your CloudFormation environment that will send an SNS notification to Lambda when a pipeline stage is complete. Subscribe the Lambda function to the SNS topic.","correct":false},{"id":"87149b0d9dbfaac4e8ba219e908d57a1","text":"Build your test stack, create a change set, and then execute the change set by manually interacting with AWS CloudFormation.","correct":false},{"id":"4b808c56dbbec61b1137c70708faf855","text":"Create a Config rule that will look for changes within your CloudFormation stack that will trigger Lambda functions to execute actions based on the pipeline.","correct":false},{"id":"1498c0d02e9c619808313a425479642f","text":"Create a CodePipeline separated by three stages. For each stage organize actions in a pipeline. Have CodePipeline complete all actions in a stage before the stage processes new artifacts.","correct":true}]},{"id":"6d133985-53e5-48d7-a1e9-6db8bb940208","domain":"security","question":"You are working as a Developer for an online retailer. Your Security Architect has requested that any files stored in S3 must be encrypted. However some teams are continuing to upload their files without encrypting them. Which of the following will ensure that only encrypted data is uploaded?","explanation":"There are a few different ways to enforce encryption, however from the provided options, the use of a bucket policy to reject requests that do not include encryption in their header is the best answer","links":[{"url":"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/","title":"How to Prevent Uploads of Un-encrypted Objects to Amazon S3"}],"answers":[{"id":"da402b6935d7bb0b6868419dc1fcf54f","text":"Tell all team members to include the x-amz-encryption parameter in request header","correct":false},{"id":"d61960e2fc2a82679514fe6f3401150f","text":"Select the Encrypted Files Only checkbox in the S3 Permissions tab in the AWS console","correct":false},{"id":"0f6a3c5c345549f63a0da92ac93e45cf","text":"Use a bucket policy that only allows PUT operations which include the x-amz-server-side-encryption parameter in the request header","correct":true},{"id":"e7d6b8da11c238d437edeadb18bf5493","text":"Create a bucket ACL that only allows PUT operations which include the x-amz-encryption parameter in request header","correct":false}]},{"id":"a03dd6d9-131e-4ca1-8a32-ee232f5b1323","domain":"deployment","question":"Under what circumstances would you use an SQS Delay Queue?","explanation":"Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html","title":"SQS Delay Queues"}],"answers":[{"id":"a767183a27b8a253d76902f6d6b5faca","text":"To delay a message for a number of seconds while it is being polled from the queue","correct":false},{"id":"51338086c0084222e0b52b27cb080907","text":"To delay a message for a number of seconds until it has been processed","correct":false},{"id":"ec05848fbc20fc135aba127ca16850f2","text":"To hide a message for a number of seconds after it has been consumed from the queue","correct":false},{"id":"325f46d52e7f4979ed4ac9b2f986a307","text":"To postpone the delivery of new messages to a queue for a number of seconds","correct":true}]},{"id":"d33e682c-89d2-4932-9b6d-a8095809bd88","domain":"deployment","question":"You are using CloudFormation to automate the build of several application servers in your test environment. Which of the following are valid sections that can be used in your CloudFormation template?","explanation":"Parameters, Resources and Outputs are all valid. It is worth learning the CloudFormation template anatomy and understanding how each section relates.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"CloudFormation Template Anatomy"}],"answers":[{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":true},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":true},{"id":"ee611c8b9dfbbc792a9318c9837b2bcd","text":"Inputs","correct":false},{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":true},{"id":"e4ba47693cf74a797e63f4557d4b88f4","text":"Transformations","correct":false}]},{"id":"f70f0615-b415-485e-93d5-2286bc2c25cc","domain":"development","question":"What is the maximum size of an item in a DynamoDB table?","explanation":"The maximum item size in DynamoDB is 400 KB.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html","title":"Limits in Amazon DynamoDB"}],"answers":[{"id":"462e2e8040f8c2b03228a91524dd8953","text":"400 MB","correct":false},{"id":"5bdde53de5ac658c8aeeacac334b0152","text":"40 MB","correct":false},{"id":"1310fc461ca5af7082d588ba9c2a795b","text":"400 KB","correct":true},{"id":"3c0512af07779a4b40b2b3d95dd1375b","text":"40 KB","correct":false}]},{"id":"eeabb3b1-06bd-4a2f-89e9-425c07670c09","domain":"security","question":"Your application uses the STS API call AssumeRoleWithWebIdentity to enable access for users who have authenticated using a Web ID provider. Which of the following best describe what is returned by a successful call to AssumeRoleWithWebIdentity?","explanation":"AssumeRoleWithWebIdentity returns a set of temporary credentials, giving the user temporary access to AWS. It also returns an Amazon Resource Name (ARN) and the assumed role ID, which are identifiers that you can use to refer to the temporary security credentials.","links":[{"url":"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html","title":"STS: AssumeRoleWithWebIdentity"}],"answers":[{"id":"5cf1fe7b63a8a99d7d3457d8b021e2c4","text":"AssumeRoleWithWebIdentity returns an ARN of the IAM user that the user is allowed to assume temporarily","correct":false},{"id":"c4dee05a317f3836954c0577fb8df356","text":"AssumeRoleWithWebIdentity returns an assumed role ID which the user is allowed to assume temporarily","correct":false},{"id":"33401d97362173f45bb04e5a8c41b8a3","text":"AssumeRoleWithWebIdentity returns a set of temporary credentials (access key ID, secret access key and security token) which give temporary access to AWS services","correct":true},{"id":"b80ac03d28d57f31929047a4ce86a48c","text":"AssumeRoleWithWebIdentity returns an ARN of the IAM role that the user is allowed to assume temporarily","correct":false}]},{"id":"52dc8f6c-a34e-48cd-afbf-8a3b5ecd7e34","domain":"refactoring","question":"Your application is experiencing a large number of failed requests when making calls to the S3 API. Which of the following best describes the approach used by AWS SDKs for regulating flow control when retrying failed API requests?","explanation":"Numerous components on a network, such as DNS servers, switches, load balancers, and others can generate errors anywhere in the life of a given request. The usual technique for dealing with these error responses in a networked environment is to implement retries in the client application. In addition to simple retries, each AWS SDK implements exponential backoff algorithm for better flow control. The idea behind exponential backoff is to use progressively longer waits between retries for consecutive error responses.","links":[{"url":"https://docs.aws.amazon.com/general/latest/gr/api-retries.html","title":"Error Retries and Exponential Backoff in AWS"}],"answers":[{"id":"1515fbc0c63bfb67a5ec940447b2480c","text":"AWS uses bandwidth throttling to manage flow control","correct":false},{"id":"78d0dc3ec6366e5ac975d5108a27106f","text":"Feedback Based Flow Control is used to avoid contention when retrying failed requests","correct":false},{"id":"401eaf63ec9626a306617946768f9d43","text":"By default, the request is continuously retried until it is successful","correct":false},{"id":"c57cdded61f3bd3c9d54c8575f424941","text":"AWS uses Exponential Backoff to manage error retries","correct":true}]},{"id":"d8b8e087-28cf-41f5-b882-7c4d8e237e4b","domain":"refactoring","question":"You are working on a project to migrate an on-premises website to AWS, your CTO has mandated that wherever possible, Serverless technologies should be used. Which of the following services would you consider for this project?","explanation":"S3, Lambda and DynamoDB are all serverless technologies that could be used to build a website. EC2 and RDS are not considered to be serverless because they are backed by a virtual server.","links":[{"url":"https://aws.amazon.com/serverless/","title":"What is Serverless?"}],"answers":[{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":true},{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":false},{"id":"81b22456f78954c460ce2f531b5e048f","text":"EC2","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true}]},{"id":"8f888c41-8c19-4f1a-8682-7a8f8235182a","domain":"security","question":"A developer is working on a new HR application that must be able to encrypt sensitive documents, each of which is approximately 100 MB in size. The encryption needs to take place within the HR application, and each document must be encrypted using a unique key. The developer has decided to use envelope encryption, and KMS to manage their keys.\n\nWhat KMS operation should be called for each document, to most efficiently meet the requirements of the HR application?","explanation":"generate-data-key returns a plaintext data key, ready to be used to encrypt a document, and a ciphertext version of the key, encrypted using the Customer Master Key. The command should be called for each document, so a different key is used for each. Once the document is encrypted, the plaintext key is securely discarded, and the encrypted data key is stored along with the encrypted document.\n\ngenerate-data-key-without-plaintext is incorrect because it is the plaintext key that is used to encrypt the document within the application.\n\ngenerate-random is incorrect as while the response could be used as a data key; a second step would also be needed to acquire the ciphertext version of the key. It is, therefore, not the best solution.\n\nencrypt is incorrect because it can only encrypt up to 4 kilobytes of data, and because the encryption process itself would take place within KMS, directly using the Customer Master Key, not within the application. To that end, using 'encrypt' directly does not fall under AWS's definition of envelope encryption.","links":[{"url":"https://docs.aws.amazon.com/cli/latest/reference/kms/generate-data-key.html","title":"KMS CLI: generate-data-key"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/kms/encrypt.html","title":"KMS CLI: encrypt"}],"answers":[{"id":"966a36d051e3b8290953bce53c3513bf","text":"generate-random","correct":false},{"id":"f71001650370495dc4bb6bccf8fc3ac9","text":"generate-data-key-without-plaintext","correct":false},{"id":"53c82eba31f6d416f331de9162ebe997","text":"encrypt","correct":false},{"id":"0b6eb26a9e685b2edba22d0b9f8534a3","text":"generate-data-key","correct":true}]},{"id":"0d9472d0-587b-4961-9457-e37e75979b7c","domain":"development","question":"You are working on a Serverless application written in Python and running in Lambda. You have uploaded multiple versions of your code to Lambda, but would like to make sure your test environment always utilizes the latest version. How can you configure this?","explanation":"When you create a Lambda function, there is only one version: $LATEST. You can refer to the function using its Amazon Resource Name (ARN). There are two ARNs associated with this initial version, the qualified ARN which is the function ARN plus a version suffix e.g. $LATEST. Or the unqualified ARN which is the function ARN without the version suffix. The function version for an unqualified function always maps to $LATEST, so you can access the latest version using either the qualified ARN with $LATEST, or the unqualified function ARN. Lambda also supports creating aliases for each of your Lambda function versions. An alias is a pointer to a specific Lambda function version, aliases will not be updated automatically when a new version of the function becomes available.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-intro.html","title":"Introduction To Lambda Function Versioning"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda Function Versioning and Aliases"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/how-to-manage-versioning.html","title":"Lambda Versioning"}],"answers":[{"id":"cc67dedc36d230ab3574dc07b41b7443","text":"Configure the alias to automatically update to point to the latest version of the code every time it is updated","correct":false},{"id":"07c1095b47175c42406ba59e91205281","text":"Reference the function using an unqualified ARN","correct":true},{"id":"02f663ae252fea90c711171a5572d211","text":"Reference the function using a qualified ARN and the $LATEST suffix","correct":true},{"id":"d483464867986a2d830ac23858944891","text":"Create an alias to point to the correct version of your code","correct":false},{"id":"0311fc386b14ce5afd0843ecf4a6ff16","text":"Create another function to automatically update your function alias to point to the latest version of the code every time it is updated","correct":false}]},{"id":"43e8c8e5-d0e3-4502-b7e7-8c236a6625e3","domain":"mon-trb","question":"A company wants to monitor all traffic to a network interface on their bastion host. They wish to be alerted if there are more than 10 attempts to connect to the host via SSH within a one-hour time interval. What solution can the company employ to meet this requirement?","explanation":"VPC flow logs can be sent to CloudWatch Logs. A CloudWatch metric filter and alarm can be configured to send notifications when the specified criteria are satisfied. CloudTrail is not a supported destination for VPC flow logs. Amazon Inspector cannot be used to inspect network traffic in the way specified by the requirements. It performs vulnerability assessments on the host VM. Lambda functions cannot mount EBS volumes.","links":[{"url":"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html","title":"Amazon S3 Glacier Vault Lock"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-cwl.html#flow-logs-cwl-create-flow-log","title":"Creating a Flow Log That Publishes to CloudWatch Logs"}],"answers":[{"id":"cdab2e49cfa676ebe99e79d8f77f49b3","text":"Create a Lambda function that mounts the bastion host EBS volume and sends logs to CloudWatch logs. Create a CloudWatch metric filter for destination port 22. Create a CloudWatch Alarm trigger.","correct":false},{"id":"ecb0bcaac946feb8b9a9c2abcaace499","text":"Create a VPC flow log for the network interface with CloudTrail as the destination. Create a Lambda function that queries the CloudTrail logs for SSH login attempts. Trigger the Lambda function every 5 minutes with a scheduled CloudWatch event.","correct":false},{"id":"343b93587a12ff88f4e59413fbbd5d96","text":"Install the Amazon Inspector agent on the bastion host. Configure CloudWatch alerts based on Amazon Inspector findings.","correct":false},{"id":"6e51df099a849474d4791bce4aa25bb5","text":"Configure a VPC flow log with CloudWatch Logs as the destination. Create a CloudWatch metric filter for destination port 22. Create a CloudWatch Alarm trigger.","correct":true}]},{"id":"5953c122-dbdd-4d25-a8fe-3e1fd23a6c8f","domain":"mon-trb","question":"An application developer finds that performing a scan operation on a large DynamoDB table is taking a long time to execute.  What can be used to improve the performance and decrease the execution time of the scan operation?","explanation":"Parallel scans can be used by multiple worker threads in an application to perform a scan of a DynamoDB table much faster. Filter expression in a scan operation only filters the results.  Scan operation still performs the same amount of read operations. Projection expression is used to limit the attributes returned by the scan operation.  It reduces the size of the payload of the scan operation, but it does not affect the speed of the scan operation. Pagination can be used to divide the result set into multiple pages, but does not increase the performance of the scan operation.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan","title":"Parallel Scan"}],"answers":[{"id":"7eeca829a717872bbf53c3ff8a69e41a","text":"Use of pagination.","correct":false},{"id":"b68d0e7d10bb7958f9b4b5fa43f06ac3","text":"Use of parallel scans.","correct":true},{"id":"62fef50451cb13a49660c69ae13e7932","text":"Use of filter expression.","correct":false},{"id":"e262b00e3caa1a25ec14571777714e9c","text":"Use of projection expression.","correct":false}]},{"id":"54fce7fa-7d85-4dde-92e6-4e6d1e12327c","domain":"deployment","question":"You have developed an application to run on Amazon EC2. Users have increased and you've found latency issues for users from various geographic locations. You decide to create a CloudFormation template of the application's environment in order to streamline application launch in other AWS Regions to improve performance for users. When creating the CloudFormation template, what is one thing you have to ensure for the resources to launch successfully?","explanation":"AWS CloudFormation templates that declare an Amazon Elastic Compute Cloud (Amazon EC2) instance must also specify an Amazon Machine Image (AMI) ID, which includes an operating system and other software and configuration information used to launch the instance. The correct AMI ID depends on the instance type and region in which you're launching your stack. And IDs can change regularly, such as when an AMI is updated with software updates. AMIs are stored in a region and cannot be accessed in other regions. To use the AMI in another region, you must copy it to that region. IAM roles are valid across the entire account. AWS CloudFormation StackSets let you provision a common set of AWS resources across multiple accounts and regions with a single CloudFormation template. Tags are not a universal namespace and are used as metadata or labels for your resources.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-custom-resources-lambda-lookup-amiids.html","title":"Walkthrough: Looking Up Amazon Machine Image IDs"}],"answers":[{"id":"fbbfa698853564b32d5e961010c48e36","text":"Ensure the AMIs referenced in the template correspond to the AMI IDs in the desired Region.","correct":true},{"id":"5b8b4dc7a6004401e2e0fb8002dfd3c7","text":"Ensure the tags of the resources are not the same in the new Region as they are a universal namespace.","correct":false},{"id":"e2c092a149099f0d0ae393acb19afd6d","text":"This is not possible. CloudFormation templates can be launched only in a single region.","correct":false},{"id":"c2a7b7d4dab7e7ef0dc554e6054d7f77","text":"Create and validate the right IAM roles in the template in the desired Region.","correct":false}]},{"id":"313867da-7161-4083-9745-77950b6208dd","domain":"development","question":"You are using CodeBuild to create a Docker image and add the image to your Elastic Container Registry. Which of the following commands should you include in the buildspec.yml?","explanation":"Use the docker push command to add your image to your Elastic Container Registry","links":[{"url":"https://aws.amazon.com/blogs/devops/build-a-continuous-delivery-pipeline-for-your-container-images-with-amazon-ecr-as-source/","title":"Build a Continuous Delivery Pipeline for Your Container Images with Amazon ECR as Source"}],"answers":[{"id":"c1520af238ed7b988b389f732c0d6287","text":"docker build -t $REPOSITORY_URI:latest .","correct":true},{"id":"ec1f85b16af01c1d7ec994f6ba6efa32","text":"docker push $REPOSITORY_URI:latest","correct":true},{"id":"87c7975f8fe350f6f133c1f3d3b6b9f2","text":"aws codebuild docker -t $REPOSITORY_URI:latest .","correct":false},{"id":"585a787c56d351517a27b9d64d3db8ae","text":"docker add $REPOSITORY_URI:latest","correct":false},{"id":"d2d39a1ed4490154f045931eee5d18e4","text":"aws ecr push $REPOSITORY_URI:latest","correct":false}]},{"id":"86524d3b-b3e8-46ca-97a2-e12d4edfabed","domain":"development","question":"Which of the following best describes Amazon ECS?","explanation":"ECS stands for Elastic Container Service: It manages running containers on your EC2 instances. It does not act as a scheduler and it is neither serverless nor software that you manage.","links":[{"url":"http://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html","title":"About Amazon ECS"}],"answers":[{"id":"f0fbcb4f668b638f39ce41a6972d9a94","text":"The Elastic Container Service is software that you can run and manage to orchestrate many running Docker containers.","correct":false},{"id":"8180f2f15ee5ff1c554855a0352e23bf","text":"The Elastic Container Scheduler is software that you can run and manage to orchestrate many running Docker containers.","correct":false},{"id":"d3958d5a87a697631979b920c68a9ae2","text":"The Elastic Container Service is a service that manages running Docker containers on a group of your EC2 instances.","correct":true},{"id":"a966ed9a7fc5f750acbbef6754f3ad57","text":"The Elastic Container Scheduler is a serverless system to manage running many Docker containers in a flexible and cost-effective way.","correct":false}]},{"id":"53e3669b-583f-4068-9c2a-5d62e92ce51d","domain":"security","question":"You are working on a mobile phone app for an online retailer which stores customer data in DynamoDB. You would like to allow new users to sign-up using their Facebook credentials. What is the recommended approach?","explanation":"For mobile applications, using Cognito as an ID broker is the recommended approach to enabling user sign-up, sign-in and guest access using Web Identity Providers like Facebook.","links":[{"url":"https://aws.amazon.com/cognito/faqs/","title":"Cognito FAQs"}],"answers":[{"id":"220c8f0940aa1f380c7439957d6a15d0","text":"Embed encrypted AWS credentials into the application code, so that the application can access DynamoDB on the user's behalf.","correct":false},{"id":"f27fa20337c218b3cbc5c69c91585adf","text":"After the user has authenticated with Facebook, allow them to download encrypted AWS credentials to their device so that the mobile app can access DynamoDB","correct":false},{"id":"8a5ece5178d1aea29ff655967321679f","text":"After the user has successfully logged in to Facebook and received an authentication token, Cognito should be used to exchange the token for temporary access to DynamoDB","correct":true},{"id":"ff4914a8c7bb26fbd0bf9d3274f34549","text":"Write your own custom code which allows the user to log in via Facebook and receive an authentication token, then calls the AssumeRoleWithWebIdentity API and exchanges the authentication tokens for temporary access to DynamoDB","correct":false}]},{"id":"0c156bea-473c-4216-b344-51ad15046bdd","domain":"development","question":"Which of the following platforms are supported in ElasticBeanstalk","explanation":"Elastic Beanstalk provides platforms for programming languages (Java, PHP, Python, Ruby, Go), web containers (Tomcat, Passenger, Puma) and Docker containers, with multiple configurations of each.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html","title":"Elastic Beanstalk Supported Platforms"}],"answers":[{"id":"3b2819dd4c24eda2faf2052eef449551","text":"Node.js","correct":true},{"id":"10bff31c030c63d11bd1e7ab82759f6d","text":"Java with Tomcat","correct":true},{"id":"8fd82b8864d71ed7fa12b59e6e34cd1c","text":"Chef","correct":false},{"id":"01fc3141bdcacc23a2e09a5e25ea126b","text":"Single Container Docker","correct":true}]}]}}}}
