{"data":{"createNewExamAttempt":{"attempt":{"id":"5f2d4c72-342f-4f63-88bb-fca21a00f136"},"exam":{"id":"f6eada45-7fc8-4c79-aefd-f9bd482427cd","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"ffae5615-188b-4023-aae2-71270158730a","domain":"awscsapro-domain5","question":"Several teams are using an AWS VPC at the same time. The VPC has three subnets (subnet-1a, subnet-1b, subnet-1c) in three availability zones (eu-west-1a, eu-west-1b, eu-west-1c) respectively. As there are more and more AWS resources created, there is a shortage of available IP addresses in subnet-1a and subnet-1b. The subnet subnet-1c in availability zone eu-west-1c still has plenty of IP addresses. You use a CloudFormation template to create an Auto Scaling group (ASG) and an application load balancer for the ASG. You enable three availability zones for the load balancer and the Auto Scaling group also spans all the three subnets. The CloudFormation stack usually fails to launch because there are not enough IP addresses. High availability is not required for your project since it is a proof of concept. Which of the following methods is the easiest one to resolve your problem?","explanation":"The easiest way is to enable only the eu-west-1c availability zone for ELB and launch ASG instances in subnet-1c. Only the IP addresses from eu-west-1c are required to create the resources. For a VPC, the existing CIDR cannot be modified and you also cannot add another CIDR IP range to an existing subnet. For an application load balancer, there is no IP balancing feature and IP addresses from a subnet cannot be reserved by other subnets.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-subnets.html","title":"Availability Zones for Your Application Load Balancer"}],"answers":[{"id":"1f29e88227ce0771f9e43093bd53583a","text":"Add more IP addresses by extending the CIDR range for the VPC. Create a new subnet in each availability zone and reserve at least 128 IP addresses in a subnet. Modify the CloudFormation template to use the new subnets for the Auto Scaling group and ELB.","correct":false},{"id":"b8c040b89e07e5150d4a8a3899d41659","text":"Add another IPv4 CIDR to the VPC which should have at least 256 IP addresses. Add another IP CIDR block to subnet-1a and subnet-1b to increase the available IP addresses.","correct":false},{"id":"9cb12513be8578fb017ebb7d1b302f9a","text":"Enable the IP balancing feature in the application load balancer so that the IP addresses are equally distributed among subnets. When elastic load balancer is created, available IP addresses in one subnet can be reserved by other subnets.","correct":false},{"id":"94887cfd8a06ae986d5721a940c3c52b","text":"Modify your CloudFormation template to enable only the availability zone eu-west-1c for the application load balancer and launch the Auto Scaling group in subnet-1c which belongs to eu-west-1c.","correct":true}]},{"id":"1141835f-8419-4642-8777-81c354976178","domain":"awscsapro-domain2","question":"You are implementing a new eCommerce system for your organization.  It requires Red Hat Linux and uses either multicast or external cache (Redis or Memcached) to share sessions.  You need to implement SSL but do not want to manage individual certificates on each EC2 instance.  Additionally, you want to be sure all parts of the landscape are setup for high availability.  Which of the following architectures best fits the situation at the least cost?","explanation":"Because multicast is not supported in VPCs, we have to use a cache.  Redis supports more high availability configurations than Memcached.  Exclusive use of a spot fleet could leave us with no running instances, so we avoid that option.","links":[{"url":"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/SelectEngine.html","title":"Comparing Memcached and Redis - Amazon ElastiCache for Redis"}],"answers":[{"id":"f792a5026bd6de7e4734492c311889f2","text":"Use ElastiCache for Memcached as a session cache.  Use a Network Load Balancer attached to an auto scaling group of RHEL instances.  Use Certificate Manager to assign a certificate to the load balancer and terminate SSL there.","correct":false},{"id":"f046d563a942b8afa2bb70fa9daf2b13","text":"Use an Application Load Balancer attached to a spot fleet of RHEL.  Use ElastiCache for Redis as a session cache.  Use Certificate Manager to assign a certificate to the ALB and terminate SSL there.","correct":false},{"id":"38b528a48d4729c06200bead282115fb","text":"Use an Application Load Balancer attached to an auto scaling group of RHEL instances.  Enable multicast support in the VPC containing the web servers.  Use Certificate Manager to assign a certificate to the ALB and terminate SSL there.","correct":false},{"id":"55247c98d5a84134fd3e58f8b41b977f","text":"Use ElastiCache for Redis as a session cache.  Use an Application Load Balancer attached to an auto scaling group of RHEL instances.  Use Certificate Manager to assign a certificate to the ALB and terminate SSL there.","correct":true},{"id":"2cfd86b2c2bf79a4de540a7db3b9949b","text":"Use ElastiCache for Memcache as a session cache.  Use an Application Load Balancer attached to an auto scaling group of RHEL instances.  Use Certificate Manager to assign a certificate to the load balancer and terminate SSL there.","correct":false}]},{"id":"5af539b7-b132-4a3a-bc80-406c620e7325","domain":"awscsapro-domain1","question":"A food service business has begun an initiative to migrate all applications and data to the AWS cloud. Governance needs to be established before any migrations can occur. Business units such as sales, marketing, and product management have fluctuating infrastructure capacity and security requirements, while other business units like finance, operations, and human resources have more static demand. Security policies and compliance needs vary by project group within each business units. Each business unit is responsible for it's own cost center, and the finance group would like cost reporting to be as streamlined as possible. Which AWS account structure will best satisfy the company's governance needs?","explanation":"Leveraging AWS Organizations to manage an account structure with a core Organizational Unit and Organizational Units for each business unit provides flexibility for future organizational changes. Creating an account for each project group facilitates security policy differences within business units, and limits the exposure of a single security event. Managing differing security requirements by project group in a single account will require more governance maintenance. Creating billing, shared services, and log archive accounts in multiple Organizational Units will result in duplication of services, and can be done at the core level.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-laying-the-foundation/introduction.html","title":"Laying the Foundation: Setting Up Your Environment for Cost Optimization"},{"url":"https://aws.amazon.com/solutions/aws-landing-zone/?did=sl_card&trk=sl_card","title":"AWS Landing Zone"}],"answers":[{"id":"03705913700b8d76205d4203c58dc5e1","text":"Use AWS Organizations with a single Organizational Unit to consolidate costs. Create a billing account, a shared services account, and a log archive account in the Organizational Unit. Create individual accounts for each business unit. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs","correct":false},{"id":"bd400ff0d22480599228a0442d2bb8d4","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Create an Organizational Unit for each business unit that contains accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":true},{"id":"a8f7d8fbb7c6c3a1a14c91577dff42e1","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Place business units with similar security requirements in shared Organizational Units. Create accounts for each business unit in the shared Organizational Units. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"a66d8391267460b5800c5c3d07921767","text":"Use AWS Organizations to create Organizational Units for each business unit. Create a billing account, a shared services account, and a log archive account in each Organizational Unit. Create accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false}]},{"id":"9564dd14-763c-45c7-8546-a8bbe687a55e","domain":"awscsapro-domain2","question":"You are helping a company design a new data analysis system.  The company captures data in the form of small JSON files from thousands of pollution sensors across the country every 10 minutes.  Presently, they use some BASH scripts to load data into an aging IBM DB2 database but they would like to upgrade to a more scalable cloud-based option.  A key requirement is the ability to replay the time-series data to create visualizations so they have to keep and query lots of detailed data. They intend to keep their current visualization tools which are compatible with any database that provides JDBC drivers.  Which of the following architectures IS NOT suited to help them Ingest and analyze the sensor data?","explanation":"For EMR, the task nodes do not store any HDFS data.  Task node storage is considered ephemeral so this would not be a good choice.","links":[{"url":"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html","title":"Understanding Master, Core, and Task Nodes - Amazon EMR"}],"answers":[{"id":"c552ecc7ce7256e713857b27429e7753","text":"Load the files into S3 and use AWS Athena as the database layer.","correct":false},{"id":"9ee2b80a7b38c15f156422085185bf91","text":"Deploy Amazon Aurora for MySQL and download the MySQL JDBC drivers. Use AWS Glue to Ingest the raw sensor files into Aurora.","correct":false},{"id":"23ad2d64a285f603e5ca0a29c6a444e7","text":"Spin up an EMR cluster.  Ingest the data into HDFS partitions on the EMR task nodes using Flume.  Use Hive to provide JDBC access.","correct":true},{"id":"0439c9dae7a15237bd7e1bfdca41bbd2","text":"Spin up a Redshift database.  Use Kinesis Firehose to load the data directly into Redshift.","correct":false},{"id":"98678aaeaf75f088f0d57ca7b321f688","text":"Use Lambda to read the sensor files and write them to a DynamoDB table.  Use a third-party JDBC driver to provide query access.","correct":false}]},{"id":"edb30172-3f76-4423-a6bb-78a3d2fdeb42","domain":"awscsapro-domain2","question":"Your team is managing hundreds of Linux and Windows EC2 instances in different environments such as development, QA, staging and production. You need a tool to help you automate the process of patching instances so that the operating systems have latest patches and meet the compliance policies. You want to manage the patching in different groups depending on the environment. For example, patches should be deployed and tested in the QA environment first before the production environment. How would you achieve this requirement through an AWS service?","explanation":"AWS Systems Manager Patch Manager is the most appropriate tool to manage the patching for large groups of EC2 or on-premises instances. For different environments, users can configure patch groups using the \"Patch Group\" tag and then establish a patch baseline for each patch group. It is better to manage instances with the \"Patch Group\" tag rather than other customized tags. AWS SSM Session Manager and AWS SSM Run Command are not suitable to deploy patches across a large number of instances. The AWS-RunRemoteScript command is also incorrect as it is used to execute scripts stored in a remote location.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-patchgroups.html","title":"Patch Manager Patch Groups"}],"answers":[{"id":"08b9feba2a12ba67141a0ffe02938798","text":"Add patch group tags in the EC2 instances. Perform the patching using the command AWS-RunRemoteScript in AWS SSM Run Command. Patch on the QA environment first by selecting the QA patch group tag.","correct":false},{"id":"b99f4271e073e1e030f3c26c383c5959","text":"In AWS Systems Manager Patch Manager, create different patch groups using the tag key \"Patch Group\" and configure a patch baseline for each patch group. Schedule the patching in a maintenance window by selecting a patch group.","correct":true},{"id":"32ac8d27221ec8e699eb4e872d3f6ed0","text":"Centrally manage the instances in AWS SSM Managed Instances and divide them into different categories. Perform the patching activities from AWS SSM Session Manager in a maintenance window.","correct":false},{"id":"6c4763d45c6cb24622b0db9533f95e0c","text":"Create environmental tags in EC2 instances such as a tag key named \"env\". In AWS SSM Patch Manager, configure patching activities by selecting the instances using the tag. Patch on the QA environment first and perform the necessary testing.","correct":false}]},{"id":"66d28221-31ce-4cf0-aca3-2b5a69535bb5","domain":"awscsapro-domain3","question":"You are consulting with a small Engineering firm that wants to move to a Bring-Your-Own-Device policy where employees are given some money to buy whatever computer they want (within certain standards).  Because of device management and security concerns, along with this policy is the need to create a virtualized desktop concept.  The only problem is that the specialized engineering applications used by the employees only run on Linux.  Considering current platform limitations, what is the best way to deliver a desktop-as-a-service for this client?","explanation":"AWS Workspaces added support for Linux desktops the middle of 2018.  BYOD scenarios work together well with a DaaS concept to provide security, manageability and cost-effectiveness.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/create-custom-bundle.html","title":"Create a Custom WorkSpaces Bundle - Amazon WorkSpaces"}],"answers":[{"id":"bf3d66705af4677c9ade8605aa6bd89a","text":"Given current limitations, running Linux GUI applications remotely on AWS is not feasible.  They should reconsider their BYOD policy decision.","correct":false},{"id":"3480305b106307937ed81bba73d294ab","text":"Package the required apps as WAM packages.  When launching new Windows Workspaces, instruct users to allow WAM to auto-install the suite of applications prior to using the Workspace.","correct":false},{"id":"4018dbf7b4646b285f5ceaa7b49a5934","text":"Launch a Linux Workspace in AWS WorkSpaces and customized it with the required software.  Then, create a custom bundle from that image and use that bundle when you launch subsequent Workspaces.","correct":true},{"id":"e8a33593afbd82697d0ab168304265ed","text":"Launch an EC2 Linux instance and install XWindows and Gnome as the GUI.  Configure VNC to allow remote login via GUI and load the required software.  Create an AMI and use that to launch subsequent desktops.","correct":false},{"id":"0cbf457eccae17779144d4e64e92a43e","text":"Launch a Windows Workspace and install VirtualBox along with a minimal Linux image.  Within that Linux image, install the required software.  Create an image of the Windows Workspace and create a custom bundle from that image.  Use that bundle when launching subsequent Workspaces.","correct":false}]},{"id":"3d98c88b-ccae-4d7a-804a-b329b8afbdb5","domain":"awscsapro-domain1","question":"You work for a freight truck operating company that operates a website for tracking the realtime location of trucks. The website has a 3-tier architecture with web and application tiers communicating with a PostgreSQL database. Under average load, the website requires 8 web servers and 3 application servers, with average CPU consumption at 85%. Though the experience will suffer, the application can still operate with 6 web servers and 2 application servers; however, in that case, the CPU usage is close to 100%. You are deploying this application in us-west-2 region which has four AZs (Availability Zones). Select the architecture that provisions maximum availability and the ability to withstand the loss of up to two Availability Zones at the same time, being cost-efficient as well.","explanation":"The key to answering this kind of question is using a simple mathematical formula. If the requirement states that the least number of Availability Zones which will still be functioning after a catastrophic loss is X, then the servers I need per AZ is n such that n * X = minimum number of EC2 Instances required by the application.\nLet us apply this formula to the web-tier. The minimum number needed for the application to function is 6. If I am spreading my instances across 4 AZ-s, then X = 2 because I must be able to withstand the loss of two AZ-s. Therefore, n * 2 = 6. Thus, n = 3. Which means I need 3 web servers in each of my AZ-s. Let us reverse calculate with that number to be sure. If I have 3 web servers per AZ, and I have 4 AZ-s, I have 12 web servers running. Now if 2 AZ-s fail, I will still have 6 web servers running. Thus, my application will still perform, which is the requirement.\nThere are a few additional interesting aspects to this kind of problems. First, note that some of the answer choices use 3 AZ-s instead of all 4. This is because the exam wants to penalize inattentive reading, in case you miss that detail. Second, using the above formula sometimes results in multiple correct answers. In that case, we should choose the one with the lower total number of EC2 instances because that will be the lower-cost option. Thirdly, some of the answer choices would be correct if you are trying to make the architecture withstand the failure of 1 AZ (and not 2). This again is trying to penalize inattentive reading in case you are in a hurry, read the question partially, and try to find the correct answer based on 1 AZ going down instead of two, which is the stated requirement.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html","title":"AWS Regions and Availability Zones"},{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html#arch-AutoScalingMultiAZ","title":"Distributing Instances Across Availability Zones"}],"answers":[{"id":"2a80c20c82cfb8e8bd358b60ee1952a5","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning all 4 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 3 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":true},{"id":"0d286ae78f17320b7244d3c53be500ba","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning 3 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 3 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":false},{"id":"3bff2d20d9b822da03cdfc315586449e","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning all 4 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 2 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":false},{"id":"698838baa59961738cfcdc08cbdcd653","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning 3 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 6 EC2 Instances in each Availability Zone for the web-tier. Deploy 2 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS with Read Replicas.","correct":false}]},{"id":"65288d1e-af34-43b4-9be7-0c1696c649fc","domain":"awscsapro-domain2","question":"What backup and restore options are available to you when using RDS for Oracle?","explanation":"Amazon RDS for Oracle can use the standard backup methods for RDS which is Snapshot and Point In Time Recovery.  You can also use Data Pump to export logical data to binary files, which you can later import into the database as well as the standard 'exp' and 'imp' utilities.  RMAN is not supported in RDS as a backup mechanism, although you can run certain RMAN commands against the database using the rdsadmin.rdsadmin_rman_util package.  Replication Backups is not a valid function within RDS for Oracle.","links":[{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"Amazon RDS for Oracle FAQs"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Oracle.html","title":"Oracle on Amazon RDS"},{"url":"https://aws.amazon.com/rds/oracle/features/","title":"Amazon RDS for Oracle features"}],"answers":[{"id":"4cff81b1f97c64130a64b1ea818c7f24","text":"Oracle Export/Import Utilities","correct":true},{"id":"5cd4a59bc9ce2cd88904b1edee24be49","text":"Oracle Recovery Manager (RMAN)","correct":false},{"id":"bf7eb3e57b2ec9b7b52b41ebe6c155ce","text":"RDS Snapshot and Point In Time Recovery","correct":true},{"id":"d93f3c575a1b1947fc462dc84cd425bc","text":"Replication Backups","correct":false},{"id":"d7b065bec7ec0dbd1d1985543b28aec5","text":"Oracle Data Pump Export and Import","correct":true}]},{"id":"1aa1b2b9-bb81-44d6-bcee-ec9408859b70","domain":"awscsapro-domain4","question":"A small business owner owns two businesses - a restaurant and a pet grooming facility. Each of her businesses has its own website, respectively www.xyz-restaurant.com and www.pqr-pet-grooming.com, both supporting only HTTP (no HTTPS). One single EC2 instance on AWS serves both the websites. This Ec2 instance has an Elastic IP Address attached to it, with two Route 53 A records pointing at it. The owner wants to add credit card payment processing to both her websites. As a result, she wants to ensure that both sites are served on HTTPS. Also, due to the popularity of her businesses, she needs to run a second EC2 instance which will be an exact copy of the existing one. She plans to create the second instance from the AMI snapshot of the first one, hoping to not needing to make any website configuration or code changes on any of the instances. She wants the traffic for each site to be equally or randomly split between the two servers though all requests from the same user-session must reach the same server.\nAs an AWS architect, what approach would you recommend for functionality as well as cost-effectiveness?","explanation":"This question tests several aspects of Load Balancers, knowledge of SNI and SAN, Sticky Sessions and Route 53.\nApplication Load Balancer supports SNI - hence it can deal with multiple SSL certificates per Listener. Thus, the option that says that a single ALB can handle a single SSL certificate only is incorrect. Also, as the question states that no code or website configuration changes can be done, we cannot terminate SSL at the EC2 instances - as doing so would surely need changes to the website configuration if not both code and website configuration. This eliminates the options that propose the Classic Load Balancer or a completely Load-Balancer-less solution.\nThe Classic Load Balancer option would technically work as long as the website configuration can be updated to terminate SSL at the EC2 instance. In fact, that would be the lowest cost solution from a Load Balancer perspective as CLB costs less than ALB - however, terminating SSL at the EC2 instances usually requires greater compute capacity, so the cost of EC2 instance could go up slightly. Note that the CLB option correctly states that SAN must be used with CLB, as CLB does not support SNI. Also, when SAN is used with CLB, it correctly states that the listener must be configured as TCP instead of HTTPS as the SSL termination must occur at the EC2 instance in that case. However, the only reason this option cannot be selected as correct is the restriction imposed on not changing the AMI at all. We cannot terminate SSL on the EC2 instance without changing the AMI or configuring the website.\nThe Load-Balancer-less solution that achieves routing via Route 53 has two problems - one is terminating SSL at the EC2 instance. The other is the fact that Route 53 does not provide Sticky Sessions.\nHence, the right answer is to use SNI with both SSL certificates bound to the same ALB listener, use Sticky Sessions, and use Route 53 to CNAME the website domains to the ALB","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/","title":"SNI and ALB"},{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-https-load-balancers.html","title":"Classic Load Balancer Listener Configuration for SAN"}],"answers":[{"id":"d1e651efb4dab3eecdbcda26841cba22","text":"Procure an SSL certificate for one of the domains and add a Subject Alternative Name (SAN) for the other domain to the same certificate. Deploy a Classic Load Balancer with a single TCP Listener. Deploy the SAN certificate to the Classic Load Balancer. Add both the instances as Targets to the CLB. Turn on Session Affinity on the CLB. Terminate SSL on the EC2 instances. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the CLB","correct":false},{"id":"676aa985e464f22918d04ea588f87ae4","text":"Procure SSL certificates for both the domains. Attach a second Elastic IP Address to the second EC2 instance. Add two Multivalue-answer records to Route53, one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, each of type A Record, and each with both the Elastic IP addresses - one for each domain name, one domain per hosted zone. This will ensure traffic being equally split between the two instances. Terminate SSL at the EC2 instances. Turn on Session Affinity for both the Route 53 Hosted Zones","correct":false},{"id":"5344d22fe688e9bd0ec7d0ff5a2ca246","text":"Procure SSL certificates for both the domains. Deploy an Application Load Balancer. As a single ALB can only handle a single SSL certificate, SSL termination must be now done at the EC2 instance. Hence, create a TCP/443 Listener on the ALB. Add both the instances as Targets belonging to the same Target Group to the ALB, assigning the Target Group to the Listener. Turn on Sticky Sessions for this Target Group. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the ALB","correct":false},{"id":"8b618d3847d47090857df2e1dd20b574","text":"Procure SSL certificates for both the domains. Deploy an Application Load Balancer with a single HTTPS Listener. Bind both certificates to the Listener on the Load Balancer. Add both the instances as Targets belonging to the same Target Group to the ALB, assigning the Target Group to the Listener. Turn on Sticky Sessions for this Target Group. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the ALB","correct":true}]},{"id":"bdd6d6a9-48a5-45f8-bb74-873f266d85df","domain":"awscsapro-domain2","question":"A hospital would like to reduce the number of readmissions for high risk patients by implementing an interactive voice response system to provide reminders about follow up visit requirements after patients are discharged. The hospital has the capability to automatically send HL7 messages that include the patient's phone number and follow up visit information from its medical records application via Apache Camel. They've chosen to deploy the solution on AWS. They already have a VPN connection to AWS, and all aspects of the application need to be HIPAA eligible. Which architecture will provide the most resilient and cost effective solution for the automated call system?","explanation":"S3 provides a low cost repository for the HL7 messages received. Having Lambda write the object keys to SQS, and having another Lambda function retrieve and parse the messages gives the architecture asynchronous workflow. Amazon Connect provides the capability to define call flows and perform IVR functions. Each of these services is HIPAA eligible. DynamoDB is also a good option for storing message information, but will be more expensive than S3. Amazon Pinpoint can place outbound calls, but is not able to perform interactive voice response functions. Amazon Comprehend Medical doesn't create call flow sequences.","links":[{"url":"https://aws.amazon.com/compliance/hipaa-eligible-services-reference/","title":"HIPAA Eligible Services Reference"},{"url":"https://aws.amazon.com/connect/","title":"Amazon Connect"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/reduce-hospital-readmissions-ra.pdf?did=wp_card&trk=wp_card","title":"Reducing Hospital Readmissions"}],"answers":[{"id":"c9bee4c5e4463b67e2726d3bed2ceed1","text":"Configure Apache Camel to write the HL7 messages to Amazon Kineses Data Firehose, which stores the patient information in Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"9585e2e5994ba5cdc2db33c22d9230cf","text":"Set up Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Connect to place the call to the patient.","correct":false},{"id":"8121c4e6e285161311cacf7b8031d5af","text":"Configure Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to write each HL7 message object key to Amazon Simple Queue Service FIFO. Have another Lambda function read messages in sequence from the SQS queue and use the object key to retrieve and parse the HL7 messages. Use that same Lambda function to write patient information to Amazon Connect to place the call using an established call flow.","correct":true},{"id":"a8c26f088ade23063b2bf5f202a74cce","text":"Have Apache Camel write the HL7 messages to Amazon Kineses Data Streams. Configure a Lambda function as a consumer of the stream to parse the HL7 message and write the information to Amazon DynamoDB. Trigger another Lambda function to pull the patient data from DynamoDB and send it to Amazon Pinpoint to place the outbound call.","correct":false}]},{"id":"760cbc05-d8ba-4d58-a555-250f26815963","domain":"awscsapro-domain4","question":"Your team is working on a long-term government project. You have purchased several reserved instances (c5.18xlarge) to host the applications to reduce costs. There is a requirement to track the utilization status of these instances and the actual utilization should be always over 80% of the reservation. When the utilization falls below 80%, the team members need to receive email and SMS alerts immediately. Which AWS services would you use to achieve this requirement?","explanation":"The AWS Budgets service is able to track the reserved instances utilization. Several optional budget parameters can be configured such as linked account, region and instance type. You can use an SNS notification to receive alerts when the actual usage is less than a defined threshold. You do not need to maintain a new Lambda function to get the usage data and send alerts. Besides, either CloudWatch alarm or Trusted Advisor cannot provide alerts based on the reserved instances utilization.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-create.html#create-reservation-budget","title":"Create a reservation budget"}],"answers":[{"id":"ba6dc1f65ab53b3ac3bb10944df422b4","text":"Create a Lambda function using Python boto3 to get the utilization status of the reserved instances. Execute the Lambda function at a regular basis such as every day and notify the team by an SNS topic if the utilization drops below 80%.","correct":false},{"id":"a4024e0dbac3a9e9e371ebfe3a511ba7","text":"In the Cost Optimization category of AWS Trusted Advisor, monitor the usage of the EC2 reserved instances. If the usage is less than 80% of the reservation, Trusted Advisor raises an action recommended alarm and sends a message to an SNS topic which notifies the team via email and SMS.","correct":false},{"id":"c13e13d0cd86acac35a5358fff11066a","text":"Configure the reserved EC2 instances to send the utilization metrics to CloudWatch. Create a CloudWatch dashboard based on the metrics and set up a CloudWatch alarm. Use an SNS topic to receive the alert when the alarm is on.","correct":false},{"id":"9cb24caa236c81ae714987bb14b0faee","text":"Create a new reservation budget in AWS Budgets service. Set the reservation budget type to be RI Utilization and configure the utilization threshold to be 80%. Use an SNS topic to notify the team when the utilization drops below 80%.","correct":true}]},{"id":"6dc7fe81-03aa-45d6-b8e1-6dc3b70914e0","domain":"awscsapro-domain1","question":"A company owns multiple AWS accounts managed in an AWS Organization. You need to generate daily cost and usage reports that include the activities of all the member accounts. The reports should track the AWS usage for each resource type and provide estimated charges. The report files also need to be delivered to an Amazon S3 bucket for storage. How would you create the required reports?","explanation":"The consolidated billing feature in AWS Organization does not generate billing reports automatically. You need to configure the AWS Cost and Usage Reports in the master account and use an S3 bucket to store the reports. The generated reports include activities for all the member accounts and it is not required to create a report in each member's account. The option of CloudWatch Event rule and Lambda function may work however it is not a straightforward solution.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-reports-costusage.html","title":"AWS Cost and Usage Report"}],"answers":[{"id":"b66bf1016802f0edb247437b5fda31cb","text":"Login in each AWS account using the root IAM user, configure the daily Cost and Usage Report and set up a central S3 bucket to save the reports from all AWS accounts. Store the reports in different folders in the S3 bucket.","correct":false},{"id":"967b7874a080c033470777ce955a4550","text":"In the master account of the AWS Organization, generate the AWS Cost and Usage Reports and save the reports in an S3 bucket. Modify the bucket policy to allow the billing reports service to put objects.","correct":true},{"id":"033f176fcb8f66b1ee9fb950c8741cda","text":"Enable the consolidated billing feature in the AWS Organization which automatically generates a daily billing report. Predefine an S3 bucket to store the reports. Make sure the S3 bucket has a bucket policy to allow the AWS Organization service to write files.","correct":false},{"id":"149237c5674e21794204a4a0ca00bee2","text":"Create a CloudWatch Event rule that runs every day. Register a Lambda function target which calls the PutReportDefinition API to get cost reports of all AWS accounts and store them in an S3 bucket.","correct":false}]},{"id":"55b78611-5890-4835-8371-66af208d25a2","domain":"awscsapro-domain2","question":"ACME Company has decided to migrate their on-premises 800TB data warehouse and their 200TB Hadoop cluster to Amazon Redshift. The migration plan calls for staging all of the data in Amazon S3 before loading it into Redshift in order to accomplish the desired distribution across the compute nodes in the cluster. ACME has an AWS Direct Connect 500 Mbps connection to AWS. However, calculations are showing the effective transfer rate won't allow them to complete the migration during the three month time frame they have to complete the project. What migration approach should they implement to complete the project on-time and with the least amount of effort for the migration team?","explanation":"Loading multiple Snowball Edge devices concurrently over a number of waves can be accomplished in less than a month with reasonable internal network performance. Snowball Edge's onboard S3-Compatible Endpoint makes for seamless transfer from a data warehouse S3 interface, which most of the major data warehouse vendors support. Hadoop data can be copied to the S3 endpoint after mounting it on a staging workstation. Transferring 1PB over a 500Mbps Direct Connect will take approximately 8 months, so increasing it to a 1Gbps connection will complete in a minimum of 4 months. Loading concurrent Snowball devices will work, but the staging workstation won't be able to mount the data warehouse directly. The data will most likely need to be unloaded into delimited flat files on the workstation's filesystem. Using Database Migration Service won't overcome the network bandwidth issues.","links":[{"url":"https://aws.amazon.com/snowball-edge/","title":"AWS Snowball Edge"},{"url":"https://aws.amazon.com/blogs/storage/data-migration-best-practices-with-snowball-edge/","title":"Data Migration Best Practices with AWS Snowball Edge"}],"answers":[{"id":"2b70a067963b17ed73d482ffd2fc810a","text":"Attach multiple Snowball Edge devices to the on-premises network. Load the data warehouse data with an S3 interface supported by the data warehouse platform. Mount the Hadoop filesystem from a staging workstation using native connectors and transfer the data through the AWS CLI","correct":true},{"id":"e705d456884fba2f673d22a9e8e9b654","text":"Increase the Direct Connect capacity to a 1Gbps connection and copy the data from the data warehouse and the Hadoop cluster directly to S3 using native tools","correct":false},{"id":"8dfc8bb936df7dbaf648fa2d5760508e","text":"Attach multiple AWS Snowball devices to the on-premises network. Use a staging workstation to mount the data warehouse and the Hadoop filesystem as data sources. Use the AWS Snowball client to load the data onto the Snowball device","correct":false},{"id":"06696779bc2515fee9a6f91b4eb57c6b","text":"Reduce the project time frame by leveraging AWS Database Migration Service to load both the data warehouse and Hadoop cluster data directly from the on-premises data sources to Redshift, eliminating the S3 intermediate stage. Then, re-organize the data based on the distribution scheme afterwards","correct":false}]},{"id":"08a68d51-48ba-43b7-b0c3-c24e04bb33a8","domain":"awscsapro-domain3","question":"You have just completed the move of a Microsoft SQL Server database over to a Windows Server EC2 instance.  Rather than logging in periodically to check for patches, you want something more proactive.  Which of the following would be the most appropriate for this?","explanation":"The default predefined patch baseline for Windows servers in Patch Manager is AWS-DefaultPatchBaseline.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html","title":"Default and Custom Patch Baselines - AWS Systems Manager"}],"answers":[{"id":"b40084bb5d5139b353505e5f942afc34","text":"Make use of Patch Manager to apply patches as you have defined in the Patch Groups","correct":false},{"id":"5ee836bac8680ff00d457a8d7f90fad6","text":"Make use of Patch Manager and the AWS-WindowsDefaultPatchBaseline pre-defined baseline","correct":false},{"id":"78beead90b3e8deb4bf1ee7e3544a309","text":"Make use of AWS Batch to apply patches as they appear on the RSS feed from Microsoft","correct":false},{"id":"4b5715f588231d7445bb512181ea13a2","text":"Make use of Server Manager and the AWS-LinuxWindowsDefaultPatchBaseline pre-defined baseline","correct":false},{"id":"42c46b2fcd3034cf79b83f0d5dc37f7d","text":"Make use of Patch Manager and the AWS-DefaultPatchBaseline pre-defined baseline","correct":true}]},{"id":"5d35c6d3-3eaf-49d0-b64e-611d74d40af0","domain":"awscsapro-domain2","question":"You need to design a new CloudFormation template for several security groups. The security groups are required in different environments such as QA, Dev and Production. The allowed CIDR range in the security group ingress rule depends on environments. For example, the allowed inbound address range is 10.0.0.0/16 for non-production and 10.1.0.0/16 for production. You prefer to maintain a single template for all the environments. What is the best method for you to choose?","explanation":"CloudFormation has an optional Conditions section that contains statements to determine whether or not entities should be created or configured. Then you can use the \"Fn::If\" function to check the condition and return different values. You do not need to use Jenkins to pre-process the template and there are no \"Fn::Case\" and \"Fn::Switch\" intrinsic functions.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html","title":"CloudFormation conditions"}],"answers":[{"id":"d273c3216496e7d41cb97094f83d3e33","text":"Create an environment variable and pass it to the CloudFormation template. In the security group resources, use Fn::Case and Fn::Switch to check the variable value and return different CIDR IP range.","correct":false},{"id":"d2661ed9bcd0f8b132885660604a665a","text":"Create a Jenkins pipeline with an environment variable. Depending on the variable value, modify the template script to use the correct CIDR IP range. Deploy the CloudFormation stack with the updated template.","correct":false},{"id":"c4a393b1cce09df81069eb93f2d38054","text":"Use the environment type as an input parameter and create a condition based on the parameter. In the AWS::EC2::SecurityGroupIngress resource, use Fn::If to check the condition and return related source CIDR range.","correct":true},{"id":"7afa8c2f0fa2ef8966917c74044516dd","text":"CloudFormation does not provide functions for conditions. Use Terraform instead. Create a variable file to manage different CIDR IP ranges. Pass in the environment variable value and use \"Terraform apply\" to deploy all the security group resources at one time.","correct":false}]},{"id":"6a9b273b-601e-4f90-ae40-9b87c2313945","domain":"awscsapro-domain4","question":"You are consulting for a media company that sells archival footage of old Hollywood movies.  Their customer base is largely other production companies who like to include these old clips in modern projects and pay well for the licenses.  Unfortunately, as digitization as increased, their business has decreased since companies can get clips, often illegally, from the internet.  As a result, the company has had to shift significant resources and budget to the Legal department and away from the IT department.  You had been leasing a large SAN for storage of the media but its lease is ending and you need to find a new home for the collection.  The company prides itself on fulfilling customer requests quickly and has a target of 10-15 minutes from the time a customer requests a clip until that customer is able to download that clip.  The company gets about 4-5 requests per month but licensing fees for those requests are usually tens of thousands of dollars.  You have been challenged to come up with the most cost-effective way to store the 20TB of digitized media but still meet customer needs.  Which of the following would you choose? ","explanation":"Due to the relatively infrequent requests and the SLA for retrieving the requests, Glacier and Expedited Retrieval provide the best combination of meeting the requirements and minimizing cost.","links":[{"url":"https://aws.amazon.com/s3/pricing/","title":"Cloud Storage Pricing | S3 Pricing by Region | Amazon Simple Storage Service"}],"answers":[{"id":"354579a5bfb828ebee6304cac6ea1406","text":"Use Storage Gateway to migrate the media files to S3.  Create a Lifecycle policy to move the files to Reduced Redundancy storage after 1 day. Upon a customer request, create a pre-signed URL for the resource for customer downloading.","correct":false},{"id":"a0f489ddd5ea014aed1b848b3027c8eb","text":"Use DMS to import the data into S3.  Create a Lifecycle policy to move the data to One-Zone Infrequent Access state.  Upon customer request, create a pre-signed URL with an expiration of 1 day and email to the customer for downloading.","correct":false},{"id":"29c0c01d440c4ac095577d1f09b935f2","text":"Use Snowball to transfer the data to S3.  Use Elastic Transcoder to compress the media files into a smaller format.  Store the media files in small groups on EFS volumes.  Upon a customer request, mount the EFS volume from a T3.micro spot instance web server and provide the customer with a download URL.","correct":false},{"id":"da98bfd22729a645118d35c438df8bfd","text":"Use Snowball to transfer the data to AWS S3.  Create a Lifecycle policy that will archive all the media clips older than 1 day to Glacier.  Upon a customer request, initiate an Expedited retrieval of the media file.  Once restored, email the customer a pre-signed URL for downloading.","correct":true}]},{"id":"9fc0785a-d5cb-47e3-bc2f-829b5a36ba26","domain":"awscsapro-domain3","question":"You work for a Genomics company which has decided to migrate its DNA Sequencing application to the AWS Cloud. The application is containerized. Currently, container image A works on genomics data residing on an on-premises file server, validating the data and updating the metadata in a local database. When it is done, engineers manually trigger 100 or more instances of container image B that process this data in parallel by reading the metadata, creating output files. When all these container instances have done their job, engineers manually trigger container image C that validates the results, cleans up and sends notifications.\nThe CTO has decided to use S3 for storing the input and output data files. She has also mandated that the parallel processing phase should run on a fleet of Spot EC2 instances to reduce compute costs. She also wants to automate the workflow, so that engineers do not have to manually trigger the next set of actions. The requirement is to minimize administrative overhead and custom development for the migration.\nAs the AWS Architect, which of the following approaches should you recommend?","explanation":"AWS ECS does not natively provide workflow management. In an ECS service definition file, you cannot specify a sequence of tasks with execution dependencies such that one will be run only after the previous one completes. Hence, the two ECS choices are ruled out.\nDistraction warning - Fargate does not allow you to specify Spot instances as it is serverless in nature (it absolves you from specifying server details). This effectively creates a distraction - when the candidate rules out ECS Fargate due to this reason, they may be relieved to see the ECS EC2 choice and jump to a conclusion because it is relatively easy to remember that EC2 launch type actually lets you select Spot instances. However, this distraction is designed to take focus away from the fact that neither of these two choices is correct. Both of the choices require service definition files to set up execution workflows. Task instances mentioned in an ECS service definition file are executed in parallel - ECS does not control the sequence of tasks.\nAWS SWF does not let you specify Spot instances either. Also, SWF is usually used in cases where human intervention is needed in the workflow.\nThis leaves AWS Batch as the correct answer. AWS Batch is indeed the most suitable AWS service for this scenario as it meets all requirements.","links":[{"url":"https://docs.aws.amazon.com/batch/latest/userguide/create-compute-environment.html","title":"How to create a compute environment for AWS Batch"},{"url":"https://docs.aws.amazon.com/batch/latest/userguide/example_array_job.html","title":"Example AWS Batch Array Job Workflow"},{"url":"https://aws.amazon.com/ec2/spot/containers-for-less/get-started/","title":"How to run ECS clusters in EC2 Spot Instances"}],"answers":[{"id":"757ddde350053553e44844d066c91386","text":"Use AWS ECS with Fargate Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"e46ada36d33a9e5b23aa37ee94c4c5d6","text":"Use AWS ECS with EC2 Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"ceb4c03a526e8ddb01ada7a40bb60001","text":"Use AWS Batch, setting up an array job with 100 or more copies preceded by pre-requisite and follow-up jobs where the workflow is controlled by dependencies between jobs. Also, use Spot as the Provisioning Model for compute environment","correct":true},{"id":"49755d6c34da495b8c91964f52946d29","text":"Use AWS SWF workers and deciders to manage the workflow. Configure the workers to use EC2 Spot Instances","correct":false}]},{"id":"bc9f1b35-bf56-4bf1-b563-9bd2d864e4bc","domain":"awscsapro-domain2","question":"A global digital automotive marketplace is using Lambda@Edge function with CloudFront to redirect incoming HTTP traffic to custom origins based on matching custom headers or client IP addresses with a list of redirection rules. The Lambda@Edge function reads these rules from a file, rules.json, which it fetches from an S3 bucket. The file changes every day because several teams in the company uses the file for different purposes, including but not limited to, (a) the security team uses the file to honeypot potential malicious traffic (b) the engineering team uses the file to do A-B testing on new features, (c) the product team experiments with new mobile platforms by redirecting traffic from a specific kind of mobile device to a specific set of server farms, etc.. As a result, the file can be as big as 200 KB. Recently, the response time of the website has degraded. On investigation, you have found that this Lambda@Edge function is taking too long to fetch the rules.json file from the S3 bucket. The existing CI-CD pipeline deploys the file to a versioning-enabled S3 bucket when any change is committed to source control. Any change in rules.json must reflect within 1 hour at all Cloudfront Edge locations. Select two options from the ones below that will not work in improving the latency of fetching this file?","explanation":"A key to answering this question is to not miss the fact that it asks which two of the answers will not help. AWS SA-P exam can occasionally frame the question with a not. Also, knowledge of how Lambda@Edge functions work with CloudFront is important for the exam.\nThere will be no improvement in the fetching time if we reconfigure the S3 bucket as a static website. In fact, doing so might add a layer of redirection during routing.\nLambda@Edge does not guarantee the persistence of global variables in memory between invocations. While it might be possible to use global variables for a short time as cache, provided the code does not make any assumptions about the guarantee of persistence, it is a bad idea to solely depend on Lambda@Edge memory between invocations. AWS does not guarantee using the same container instance for any number of requests, though it will try to re-use a warmed up instance for the same function invocation landing on the same edge node. If it is re-using the same container instance from the one used by the last Lambda@Edge function, the global variable trick will work. However, as the option clearly says that such usage is guaranteed (which is false and will not work), it is one of the answer choices to select in this case.","links":[{"url":"https://aws.amazon.com/blogs/networking-and-content-delivery/leveraging-external-data-in-lambdaedge/","title":"Leveraging external data in Lambda@Edge"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-limits.html#limits-lambda-at-edge","title":"Limits on Lambda@Edge"}],"answers":[{"id":"5a14a6d0367c2a6daf2d4faccf1fbdb3","text":"Change the Lambda@Edge code to save the contents of the rules.json file in a global variable so that it is cached in Lambda@Edge memory, with a TTL of 55 minutes, persisted between invocations. Lambda@Edge guarantees persistence of variables in memory between invocations.","correct":true},{"id":"058638ecef0dd62ee83782f5cbbba63b","text":"Reconfigure the S3 bucket as a static website. Use the website endpoint to download the file instead of directly accessing the bucket from the Lambda@Edge function. This will cause HTTP GET requests to be cached by S3, thus improving the latency of fetching the file","correct":true},{"id":"fd6d6c32ad84f6d5856477cd8f27d361","text":"Define a separate cache behaviour for *.json in your Cloudfront web distribution, setting the origin as the S3 bucket. Change the Lambda@Edge function code to use the Cloudfront download URL instead of downloading the file directly from S3. This way, the file will be cached by Cloudfront avoiding expensive round trip time to S3 each time. Set the Cloudfront TTL to 45 minutes.","correct":false},{"id":"ffa2541e9e07a612a02728b1135014f9","text":"Include the rules.json file in the Lambda@Edge deployment package. Change the CI-CD pipeline to deploy a new Lambda@Edge version every time the file changes. Change the Lambda@Edge function code to read the file locally instead of reading it from S3. This will improve the latency of fetching the file.","correct":false}]},{"id":"f02ba751-479b-4ff0-a09b-8f18a63177b5","domain":"awscsapro-domain3","question":"An automotive supply company has decided to migrate their online ordering application to AWS. The application leverages a Model-View-Controller architecture with the user interface handled by a Tomcat server and twenty thousand lines of Java Servlet code. Business logic also resides in two thousand lines of PL/SQL stored procedure code in an Oracle database. The company's technology leadership has directed your team to move the database to a more cost-effective offering, and to adopt a more cloud-native architecture. Business objectives dictate that the application must be live in the AWS cloud in sixty days. Which migration approach will provide the most scalable architecture and meet the schedule objectives?","explanation":"This solution will require trade-offs between schedule requirements and architectural desires. Converting twenty thousand lines of Model-View-Controller code to a serverless architecture in sixty days is unreasonable, so moving the Tomcat MVC as-is to EC2 for the initial migration is the best approach. We can migrate to a serverless user interface in a later phase. Database Migration Service will suit our needs well for moving the application data to Aurora, but the most scalable architecture strategy is to migrate the stored procedure code out of the database so that database nodes won't need to be resized when the business logic needs more compute resources. Under normal circumstances, recoding two thousand lines of PL/SQL code to Python Lambda functions within a sixty day time frame will not be a problem.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/blogs/database/migrate-your-procedural-sql-code-with-the-aws-schema-conversion-tool/","title":"Migrate Your Procedural SQL Code with the AWS Schema Conversion Tool"},{"url":"https://aws.amazon.com/lambda/","title":"AWS Lambda"}],"answers":[{"id":"db222d8a15bda541fc4147908131cfd6","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service to move the application data into Amazon Aurora. Convert the stored procedure code to AWS Lambda Python functions, and modify the Servlet code to invoke them","correct":true},{"id":"5ab40b2a54c4e82a7aafa05c8fc9a458","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service to migrate the application data and stored procedures to an Amazon RDS Oracle instance","correct":false},{"id":"2cf84f7f8daee7548143ad181423c7cb","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"97a348ed01424c357958b49bcc030935","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false}]},{"id":"2afb0db9-a43f-4e97-8272-5ce423ded162","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They notice on their RDS console that one of their mission-critical production databases has an \"Available\" listed under the Maintenance column.  They are extremely concerned that any sort of updates to the database will negatively impact their DB-intensive mission-critical application.  They at least want to review the update before it gets applied, but they are not sure when they will get around to that.  What do you suggest they do?","explanation":"For RDS, certain OS updates are marked as Required. If you defer a required update, you receive a notice from Amazon RDS indicating when the update will be performed. Other updates are marked as Available, and these you can defer indefinitely.  You can also apply the maintenance items immediately or schedule the maintenance for your next maintenance window.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html","title":"Maintaining a DB Instance - Amazon Relational Database Service"}],"answers":[{"id":"35c3b3d1cb8d9866e1abc96dec8bfa3f","text":"Apply the maintenance items immediately.  AWS validates each update with each customer's RDS instances using a shadow image so there is little risk here.","correct":false},{"id":"66ed278812c2a52913954afa52952b97","text":"The maintenance will be automatically performed during the next maintenance window.  They have no choice in the matter.","correct":false},{"id":"b9f81cfac73d5c4d2871a7f969ecb9f3","text":"Disable the Maintenance Window so the updates will not be applied.","correct":false},{"id":"93df9eb71cb48a0c821fe555e35f5b62","text":"Defer the updates indefinitely until they are comfortable.","correct":true},{"id":"6aeada3a9046319bc858239b15031f66","text":"Backup the database immediate because the updates could come at any time.  If possible, create a Read Replica to act as a standby in case problems are introduced with the update.","correct":false}]},{"id":"42a413d2-b7c0-4f63-ab1c-37b8ec9b724a","domain":"awscsapro-domain2","question":"You have been contracted by a small start-up to help them get ready for their new product release--a web-based application that lets users browse through detailed photographs of the world's most famous paintings.  The company is expecting a huge debut with very heavy traffic so the solution should be robust and scalable with the least amount of hands-on management.  A key feature of their app is that they have created separate web sites specifically optimized for three different form factors: mobile phone, tablet and desktop.  As such, they need the ability to detect the device and direct the requester to the proper version of the site.  Which architecture will do this and meet the requirements? ","explanation":"A common use case for Lambda@Edge is to implement some front-end logic for incoming requests as close to the requester as possible.  In this case, we can use a custom Lambda function to attempt to determine the device type in the HTTP request.  We can then direct them to the CloudFront distribution that is optimized for their form-factor.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/11/lambda-at-edge-now-supports-content-based-dynamic-origin-selection-network-calls-from-viewer-events-and-advanced-response-generation/","title":"Lambda@Edge Now Supports Content-Based Dynamic Origin Selection, Network  Calls from Viewer Events, and Advanced Response Generation"}],"answers":[{"id":"2e04372e48d55855d33ef7c797110044","text":"Use Amazon AppSync to detect the type of device issuing the inbound request.  Use a Lambda function to redirect to the proper CloudFront distribution based on device type returned from AppSync.","correct":false},{"id":"ea4ee05f24a93c3b1188072a0f4777cc","text":"Configure a Network Load Balancer to use SNI to direct the request to different EC2 web servers based on device type.  Configure multiple auto scaling groups to maintain a minimum number of servers for each device type.","correct":false},{"id":"f28bf27096a077030c408fe2417a1d0e","text":"Create multiple distributions in CloudFront for each needed origin.  Use Route 53 to dynamically direct the requester to the appropriate alias based on device type.","correct":false},{"id":"dbb8c65d7e02773f600d4e87fecc45b5","text":"Build a custom Lambda function to dynamically redirect the requester to the proper S3 origin based on device type.  Associate a CloudFront distribution with a Lambda@Edge function.","correct":true},{"id":"d6b70dc60b5f361f7b92d61ac7964558","text":"Configure the Requester Interrogation feature on CloudFront to identify the device used by the requester.  Redirect the requester to the desired S3 origin based on the device type.  ","correct":false}]},{"id":"a4822e4f-9c97-4726-9e05-df333bf77889","domain":"awscsapro-domain2","question":"A new AWS cloud-native social-networking web application is being developed in your company. As the AWS Architect, you are guiding the technical team in making architectural and design decisions. Though the web application will enable end-users to interact with the server-side using long-running sessions, the application itself is being designed to be completely stateless so that any HTTP request made from any browser can be handled by any instance of the web application. The application must scale to handle tens of thousands of peak concurrent users, but the steady-state is expected to be less than 100 users. The incoming requests are not idempotent. In other words, if an error happens while processing a given request, it cannot be re-processed by another instance. The application processes a large amount of data in every request, and needs around 1 GB of temporary disk space. It also needs to store and retrieve information from a back-end database that must be optimized to store and navigate two-way relationships. It must also be protected from advanced DDoS attacks.\nWhich architecture is the most suitable for hosting this web application on AWS?","explanation":"This question tests the knowledge of quite a few different areas related to developing and deploying enterprise web applications on AWS. Firstly, the AWS managed database service optimized for storing and navigating social-networking style relationships is AWS Neptune. Therefore, the option that picks DynamoDB instead of Neptune is eliminated. Secondly, the same option also picks Lambda, which, again, is not the suitable technology because of the temporary disk space requirements, which is 1 GB. Lambda only provides 512 MB of temporary hard disk space and this is a hard limit. Thirdly, Spot Instances cannot be used because of the non-idempotent nature of the requests. The question clearly states that if there is an error processing an incoming request, it cannot be re-tried. Using Spot EC2 Instances is only justified when another EC2 instance can re-process a given request that could be interrupted in case a Spot EC2 instance disappears suddenly. This eliminates two out of the remaining three choices. Fourthly, ALB Sticky Sessions should not be used for session management, as the design goal is very clear - any server instance should be able to process any incoming HTTP request in spite of lengthy user sessions. This suggests that the sessions must be saved externally to some database, which any server instance can read and resume from where the last server instance left off. This is not possible with Sticky Sessions in the ALB. Thus, the option suggesting the usage of Sticky Sessions is eliminated. Instead, the session data should be saved in Elasticache cluster. After eliminating all these choices, only one choice remains.","links":[{"url":"https://aws.amazon.com/caching/session-management/","title":"Session Management in web applications on AWS"},{"url":"https://aws.amazon.com/neptune/","title":"Amazon Neptune"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/limits.html","title":"AWS Lambda Limits"}],"answers":[{"id":"6d0972c01bd5270a10a1cf8eb72d18c2","text":"Run the entire web application using Lambda functions behind an API Gateway API. Request an increase to the soft limit of 1000 concurrent Lambda functions per region to accommodate the peak. Store user session data in Elasticache Redis cluster. Use AWS DynamoDB as the managed database layer. Use and configure AWS Shield Advanced for DDoS protection.","correct":false},{"id":"a052133db9f20c8e24a3ed1c8c1d4e90","text":"Run the web application on EC2 instances added as targets to an Application Load Balancer. Store user session data in Elasticache Redis cluster. Use an Auto Scaling Group consisting of EC2 instances using a mix of Reserved and On-demand instances for handling steady-state and peak traffic respectively. Use AWS Neptune as the managed database layer. Use and configure AWS Shield Advanced for DDoS protection.","correct":true},{"id":"84fe8968d1e32cd0473785de13e7e230","text":"Run the web application on EC2 instances added as targets to an Application Load Balancer. Turn on Sticky Session for the ALB for user session management. Use an Auto Scaling Group consisting of Spot EC2 instances for handling peak traffic and Reserved Instances for handling steady-state traffic. Use AWS Neptune as the managed database layer. Use and configure AWS Shield Advanced for DDoS protection.","correct":false},{"id":"cc351cceb99405f26e33b0c2d2905144","text":"Run the web application on EC2 instances added as targets to an Application Load Balancer. Store user session data in Elasticache Redis cluster. Use an Auto Scaling Group consisting of EC2 instances using a mix of Reserved and Spot instances for handling steady-state and peak traffic respectively. Use AWS Neptune as the managed database layer. Use and configure AWS Shield Advanced for DDoS protection.","correct":false}]},{"id":"e720cd54-de67-42de-ba10-593dee0582e6","domain":"awscsapro-domain3","question":"You are the Enterprise Architect in a Risk Quantification firm. The firm has a website which end-users can use to apply for loans and also track the status of their loan application if they log in. When a loan application comes in, several downstream systems need to independently process the application. Right now, the website server-side code invokes these systems one after the other, synchronously, in a tight loop. If one of these downstream systems times out or throws an exception, the entire loan application processing errors out. Even if none of these downstream systems fail, the time it takes to process a loan application is very high due to the serial nature of these systems being invoked. Your CTO wants only the loan-processing application moved to the AWS cloud and re-architected at the same time.\nThe downstream systems are all hosted on-premises and will continue to remain on-premises. They expose REST endpoints that accept POST HTTPS requests, use self-signed certificates and respond synchronously only when they are done processing an application. After re-architecture, all downstream systems must independently start processing an incoming loan application simultaneously.\nYour CTO wants to know how the loan-processing website application can be architected in the AWS Cloud, and what supporting changes will be needed in the downstream systems on-premises. He wants to minimize code changes to the downstream on-premises systems. Choose the best option","explanation":"This is an example of a verbose question with verbose answer choices. You can expect a few such questions in the exam, testing your time management skills. Try to vertically scan the answers to see which parts differ between them. Sometimes, though the answers seem big, a large part of each is identical. You can ignore those parts, as there is nothing to choose between the.\nAmong the four choices, two use SQS and two use SNS to feed the incoming loan applications to the downstream systems. You cannot automatically eliminate either SQS or SNS, as a working solution can be designed with either.\nLet us see how we can achieve this using SNS first. The basic requirement here is fan-out - a single loan application must be processed by several downstream systems, so there are multiple consumers. Hence, SNS is a natural fit. SNS supports multiple subscribers for a topic. SNS also supports HTTP/HTTPS subscribers. SNS makes POST REST API call to as many HTTP/HTTPS subscribers exist on the topic, so it fits the bill. However, there is a small problem - the requirement states that the downstream systems must be changed as little as possible. If we follow this design, we must change the HTTP Listening part of the downstream systems significantly. Because SNS is directly calling them now, SNS will use its own headers and body format. In fact, SNS POST-s two kinds of messages - one is Subscription Confirmation and one is Notification. A special HTTP header (x-amz-sns-message-type) has the right type in its value. The server side now must parse this header out and look for only the Notification type of message. The body itself will then be JSON formatted with the payload. While the server is probably used to process just the core payload (loan application data) as the HTTP body, the same will now be hidden inside a JSON field called Message inside the request body. Additionally, the downstream systems will have to deal with SNS retries, thus the loan application part must be made idempotent (if the same loan application lands twice, it will ignore the duplicates). Thus, though it is technically possible to design the solution using SNS, it will result in a lot of changes in the downstream systems. Hence, though the SNS option will work, it is not the correct answer because of this reason.\nNow, let us see how we can design this using SQS. While SQS does not support fan-out (multiple consumers for the same message), the proposed solution uses a Lambda function to achieve fan-out. The Lambda function will pick up the message, and then call the downstream systems one by one. The key to making this work is, of course, to modify the downstream systems from synchronous monolithic beasts to asynchronous servers so that they can instantly respond to the Lambda function and then continue to process the application. We will then have to provide a callback for when it is done. The solution uses an API Gateway for that purpose. Overall, the solution is elegant, and changes to the downstream systems are less than what SNS requires. Hence, SQS is the correct answer.\nNote that one version of the SNS design proposes to retain the synchronous nature of the downstream systems. That will not work as SNS will not wait more than 15 seconds for a response. The response will then be lost and the main website app will never know the results from the downstream systems.\nAlso, note that though SNS requires the HTTPS subscriber to present a trusted CA-signed certificate, there is no such requirement for Lambda because Lambda is basically your code, you can decide to trust anyone.","links":[{"url":"https://docs.aws.amazon.com/sns/latest/dg/sns-http-https-endpoint-as-subscriber.html","title":"Using Amazon SNS for System-to-System Messaging with an HTTP/S Endpoint as a Subscriber"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","title":"Using AWS Lambda with Amazon SQS"}],"answers":[{"id":"48d42d3290142cbfc8207e042690b35f","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting so that SNS does not retry, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (c) Make them idempotent for the same loan application as SNS may retry in case of lost messages or timeouts (d) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as SNS will not be able to POST to a server with self-signed certificate","correct":false},{"id":"eec2740374df8038093d636a17252168","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Override the default delivery policy on the subscriber endpoint to remove retries so that downstream systems do not have to worry about synchronous responses taking time or idempotency of retries. Make the following changes in the downstream systems - (a) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (b) Procure server certificates from a trusted Certificate Authority (CA) instead of using the self-signed certificate as SNS will not be able to POST to a server with a self-signed certificate","correct":false},{"id":"3d032e65493c0733ebe65683fb66a562","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems and (c) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as your Lambda function will not be able to POST to a server with self-signed certificate","correct":false},{"id":"3fb725a8e71fa96168f18e50a146b4f0","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed and (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems","correct":true}]},{"id":"de88bc69-44a8-4a12-b28f-0a5e86db3939","domain":"awscsapro-domain1","question":"You have been entrusted to act as the interim AWS Administrator following the departure of the erstwhile Administrator in your company. You notice that there are several existing roles called role-engineer, role-manager, role-qa, role-dba, role-data-scientist, etc. When a new person joins the company, the new IAM user simply assumes the right role while using AWS - this allows central management of permissions and eliminates the need to manage permissions on a per-user basis.\nA new QA hire joins the company a few days later. You create an IAM User for her. You attach a Policy to the new IAM User that allows Action STS AssumeRole on any Resource. However, when this employee logs in the same day and tries to switch roles to role-qa, she is denied and is unable to assume the role-qa Role.\nWhat could be one reason why this is happening and how can it be best fixed?","explanation":"In order to allow an IAM User to successfully assume an IAM Role, two things must happen. First, the Policy attached to the User must allow the action STS AssumeRole. This is already true according to the question. Second, the Trust Policy of the Role itself must allow the User in question to assume the Role. This second condition can be met if we specify the arn of the User in the Principal element of the Trust Policy. In general, this question can be answered if the candidate is familiar with the concept of Principal in a Role, see link - A Principal within an Amazon IAM Role specifies the user (IAM user, federated user, or assumed-role user), AWS account, AWS service, or other principal entity that is allowed or denied to assume or impersonate that Role. Trust Policy is different than the Policy permissions - think of Policy Permissions as [what can be accessed] and Trust Policy as [who can access].\nTrust Policy cannot belong to an IAM User, hence the choice that claims the problem to be an unmodified User Trust Policy is incorrect. IAM changes are instantly effective, so the choice that points at the need of a time delay is also incorrect. Among the other two choices, the knowledge needed to pick the right one is an awareness of the Principal element.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html","title":"AWS JSON Policy Elements - Principal"},{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html","title":"IAM Roles"}],"answers":[{"id":"765f20e64b35dbc08c2bc319bcbe7e1a","text":"Sufficient time has not passed since you made the changes. It takes up to 12 hours to propagate IAM role changes. To fix this, ask her to try again the next day.","correct":false},{"id":"dbb48c05e238c18bdb9c17ee265e387b","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Condition element of the Trust Policy of the Role","correct":false},{"id":"e6dacaf19a289e1f73855c5e904b21fb","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Principal element of the Trust Policy of the Role","correct":true},{"id":"b3bf261fca3a734ad312d3ac0e5d0589","text":"You have not modified the Trust Policy of the IAM User to trust the Role role-qa. To fix this, add a Condition to the IAM Policy attached to the new user that filters on the role and specify the arn of role-qa","correct":false}]},{"id":"4eb7884e-f6fd-4803-83bf-8ab6faca4dd5","domain":"awscsapro-domain1","question":"You have been asked to give employees the simplest way of accessing the corporate intranet and other internal resources, from their iPhone or iPad.  The solution should allow access via a Web browser, authentication via SAML integration and you need to ensure that no corporate data is cached on their device. Which option would meet all of these requirements?","explanation":"Amazon WorkLink is a fully managed, cloud-based service that enables secure access to internal websites and apps from mobile devices. It provides single URL access to the applications and also links to existing SAML-based identity providers.  Amazon WorkLink does not store or cache data on user devices as the web content is rendered in AWS and sent to user devices as encrypted Scalable Vector Graphics (SVG).  WorkLink meets all of the requirements in the question and is therefore the only correct answer.","links":[{"url":"https://aws.amazon.com/worklink/faqs/","title":"Amazon WorkLink FAQs"}],"answers":[{"id":"8b3531ac066ba672af41cfd6c438fdb9","text":"Place all internal servers in a public subnet and lock down access via Security Groups to the IP address of each mobile user","correct":false},{"id":"668cfaeb2878db8e709660735f0ff009","text":"Configure Amazon WorkLink and connect to the servers using a Web Browser with the link provided","correct":true},{"id":"0d0cb2140013a20863f643412ebd4698","text":"Tunnel through a Bastion Host into your VPC and view all internal servers via a Web Browser","correct":false},{"id":"2ca6fccac916b71e240a465c8caf457e","text":"Connect into the VPC where the internal servers are located using Amazon Client VPN and view the sites using a Web Browser","correct":false}]},{"id":"b5f80e7e-7b92-402f-b74d-7397f0778eaf","domain":"awscsapro-domain4","question":"You are an AWS administrator and you want to enforce a company policy to reduce the cost of AWS usage. For the AWS accounts of testing environments, all instances in Auto Scaling groups should be terminated at 10:00PM every night and one instance in ASG should be launched at 6:00AM every morning so that the team can resume their work. At 10:00PM and 6:00AM, the team should get an email alert for the scaling activities. Which of the following methods would you use to implement this?","explanation":"A Lambda function can be used to modify the desired capacity at 10:00PM and 6:00AM. Only the desired capacity should be changed and the maximum capacity should be kept. If the maximum capacity is 1 during the day, the number of instances in ASG cannot be over 1. AWS CLI \"terminate-instances\" is inappropriate to terminate all instances in an ASG because ASG will automatically create new ones to maintain the desired capacity.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-manual-scaling.html","title":"Manual Scaling for Amazon EC2 Auto Scaling"}],"answers":[{"id":"5e1b982a6d6e29c160572d2430d8e3bb","text":"Use AWS CLI \"aws autoscaling put-scheduled-update-group-action\" to modify the desired capacity and maximum capacity to be 0 at 10:00PM and 1 at 6:00AM. Notify the team by configuring a CloudWatch Event rule.","correct":false},{"id":"0344a7b1df36d0f6e904c191acd31686","text":"Create a Lambda function that runs at 10:00PM and 6:00AM every day. The function terminates all EC2 instances using AWS CLI \"terminate-instances\" at 10:00PM and launches a new instance using AWS CLI \"run-instances\" at 6:00AM. Notify the team by publishing a message to an SNS topic.","correct":false},{"id":"8bbb4a7f1fc80cbb616df074b2984bd7","text":"Create a Lambda function that loops through all Auto Scaling groups, modifies the desired capacity to be 0 every 10:00PM and increases the desired capacity to be 1 every 6:00AM. Notify the team through an SNS notification in the Lambda function.","correct":true},{"id":"32fa65e299613094d47b0e1a67a22636","text":"Create a cron job running in an EC2 instance. The cron job modifies the minimum, desired and maximum capacities to 0 at 10:00PM and changes the minimum, desired and maximum capacities back to 1 at 6:00AM. Configure a CloudWatch Event rule to notify the team.","correct":false}]},{"id":"2c034786-9b7e-4933-aad2-d0c4b1d89ca8","domain":"awscsapro-domain2","question":"A beach apparel company has begun an initiative to improve their sales analytics capabilities using AWS services. They'll need to be able to visualize summary sales data by product line, territory, and sales channel for each day, month, and year, and they'll need to be able to drill-down with ad-hoc queries on individual sales records. There are multiple data sources that provide transactional information in different formats. The company has chosen Amazon QuickSight as their visualization tool for the summary information. Visualizations and drill-down queries will require three years of rolling sales history, which estimates to seven petabytes of data. Which architecture will provide the best performance and cost efficiency?","explanation":"Using S3 to store the detailed sales transaction data and using Lambda to standardize data formats is the most cost effective option. Storing the summary data in Redshift provides a high performance option for reads from QuickSight, and keeping the detailed transaction data out of Redshift allows for smaller node sizes and lower cost. Amazon Redshift Spectrum can be used for drill-down queries that join tables from both Redshift and S3. For answer number two, Redshift will be a better option than Aurora for OLAP query performance due to it's columnar organization. Answer number four provides no simple way to perform ad-hoc drill down queries.","links":[{"url":"https://aws.amazon.com/redshift/","title":"Amazon Redshift"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"ebf4a6c248510d05a046c8f0ea4298b7","text":"Use Amazon Kinesis Data Analytics to format the data source transactions in a standard way and load it into Amazon Aurora. Invoke Lambda functions to aggregate the data and write it into summary tables in Aurora","correct":false},{"id":"b3deac265c2195cb988c345d096800fd","text":"Ingest individual sales transactions from each data source into Amazon S3 with Amazon Kineses Data Firehose. Trigger an AWS Lambda function to format the transaction data in a standard way and redeposit the results in S3. Run AWS Glue jobs to aggregate the summary data into Amazon Redshift","correct":true},{"id":"4740b70569f15040edf0916e47386757","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Streams and write them to Amazon Elastic Block Store on EC2 instances in Auto Scaling Groups. Perform data format standardization and summary aggregation on EC2, and write the summary results to Amazon Redshift tables","correct":false},{"id":"a08e38f138d23c0f1759ab2d1801f67e","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Firehose and load them into Amazon Redshift. Run AWS Glue jobs to format the transaction data in a standard way and perform aggregate functions to write the data into summary tables in Redshift","correct":false}]},{"id":"dc82c397-347d-4f69-bb06-03822238c7a0","domain":"awscsapro-domain1","question":"You are consulting for a large multi-national company that is designing their AWS account structure.  The company policy says that they must maintain a centralized logging repository but localized security management.  For economic efficiency, they also require all sub-account charges to roll up under one invoice.  Which of the following solutions most efficiently addresses these requirements?","explanation":"Service Control Policies are an effective way to broadly restrict access to certain features of sub-accounts.  Use of a single separate logging account is an effective way to create a secure logging repository.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"74a6c6df518100b16da3f16e870b5d5c","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Create localized IAM policies to restrict modification of CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create a centralized security account and establish trust relationships between each sub-account.","correct":false},{"id":"871870fa49beedfb95106595e4a1c9f4","text":"Configure billing for each account to load into a consolidated RedShift instance.  Create a centralized security account and establish trust relationships between each sub-account.  Configure admin roles within IAM of each sub-account for local administrators.  Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  ","correct":false},{"id":"cbec34b5388f7f183659e82c20fb3abf","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use an SCP to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.","correct":true},{"id":"0c9b5a803a99a3d2ef53869b6857c0e0","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use ACLs to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.  Establish trust relationships between the Consolidated Billing account and all sub-accounts.","correct":false}]},{"id":"338ce579-a236-4282-9670-8da3b0baf2e9","domain":"awscsapro-domain3","question":"You are helping a Retail client migrate some of their assets over to AWS.  Presently, they are in the process of moving their Enterprise Data Warehouse.  They are planning to re-host their very large Oracle data warehouse on EC2 in a high availability configuration across AZs.  They presently have several Scala scripts that process some detailed Point of Sale data that is collected each day.  The scripts perform some aggregation on the data and import the aggregate into their Oracle database.  They want to move this process to AWS as well.  Which option would be the most cost-effective way for them to do this?","explanation":"AWS Glue is a fully managed extract, translate and loading service and is compatible with Scala.  EMR could do this but represents more overhead than necessary.  Lambda is not compatible with Scala and migrating to Redshift does not bring anything in this case if the customer wants to retain their Oracle database.","links":[{"url":"https://aws.amazon.com/glue/faqs/","title":"AWS Glue Features - Amazon Web Services"}],"answers":[{"id":"4f1ff8b853c3ba363bdd2bda53538ab4","text":"Migrate from Oracle to Redshift and use Kinesis Firehose.","correct":false},{"id":"a445a1a877009cd7c31858687a818116","text":"Import your Scala scripts into AWS SCT for processing.","correct":false},{"id":"04578ae8419780f9dc441d01fe11582d","text":"Create Lambda functions using your Scala scripts.","correct":false},{"id":"8b01d948d5ad2f4b1c8e817c2d98e7c2","text":"Migrate the processing to AWS Glue.","correct":true},{"id":"1a4de6676c8c078310e08aad71d9dce6","text":"Migrate the processing to AWS EMR.","correct":false}]},{"id":"d2b0a9d5-1875-4a55-968d-3a2858601296","domain":"awscsapro-domain2","question":"You currently are using several CloudFormation templates. They are used to create stacks that include the resources of VPC subnets, Elastic Load Balancers, Auto Scaling groups, etc. You want to deploy all the stacks with a root stack so that all the resources can be configured at one time. Meanwhile, you need to isolate information sharing to within this stack group, which means other stacks outside of the stack group can not import its resources. For example, one stack creates a VPC subnet resource and this subnet can only be referenced by the stack group. What is the best way to implement this?","explanation":"As the stack outputs should be limited within the stack group, nested stacks should be chosen. The export stack outputs cannot prevent other stacks to use them. The AWS::CloudFormation::Stack resource type is used in nested stacks to provide dependencies. The DependsOn attribute is not used for configuring nested stacks.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html","title":"Exporting stack output values"}],"answers":[{"id":"e208251ba3e5646edab96df0da85794c","text":"Upload stack templates to an S3 bucket. Create a root CloudFormation stack to use the uploaded templates with the resource type of \"AWS::CloudFormation::Template\". Configure the \"TemplateURL\" field with the template location in S3.","correct":false},{"id":"fe3bf9eee211c59e9c64ad17a2d32e27","text":"Export output values for each child stack if needed. Create a parent stack to use the exported values from child stacks to deploy and manage all resources at one time.","correct":false},{"id":"e44006dac54e63b93a8804a4e632eeb5","text":"Upload the root and all child stack templates to an S3 bucket under the same directory. Use the \"DependsOn\" attribute in the root template to add dependencies. When the root stack is created, all the child stacks are created first. ","correct":false},{"id":"aca679ff150a8a8176faf99cc057e825","text":"Create nested stacks with the \"AWS::CloudFormation::Stack\" resources. Use the outputs from one stack in the nested stack group as inputs to another stack in the group if needed.","correct":true}]},{"id":"1080e29c-045b-4714-8f41-6be669f865a5","domain":"awscsapro-domain4","question":"Your company stores financial transactions in a write-heavy MySQL 5.6 RDS database instance. The innovation team is developing a simulation algorithm. They have made a point-in-time copy of the production database for running tests. Each such test runs for a few minutes and changes the data. They need to run thousands of such tests and compare the results, where each test is run on the same test data. Therefore, after each test is completed, they want the test database back in its original state with the initial data, so that they can run the next test quickly. Currently, they use a DB Snapshot that contains the initial state. They restore the database after every test run. The restoration process takes a couple of hours, slowing them down. They want to automate the process of running tests and ideally run these tests every few minutes to be more productive. Suggest a suitable mechanism for quickly achieving this with least effort and lowest cost.","explanation":"This question tests the knowledge of Aurora Backtracking feature, which is the best answer. This question also tests the knowledge of EBS and RDS MySQL. An EBS restore will not necessarily be any quicker than a DB Snapshot restore. Also, spinning up thousands of instances of RDS will firstly be met with a soft-limit increasing resistance by AWS (the current per account per region limit is 40), not to mention the drastic cost implications of so many instances running. Also, MySQLDump is not necessarily a whole lot quicker, not like Aurora backtracking anyway.\nIt is important to remember the fundamental difference between backtracking and point-in-time restore from DB Snapshots. Backtracking rewinds the same database. Point-in-time restoration from snapshots creates a new database. Hence, the latter is more time-consuming. Backtracking is much faster, but it is only available in Aurora, that too, only for certain database engines and versions as of 2019. Remember that if you turn on backtracking, you are paying for the extra storage, as Aurora must keep change records in order to be able to backtrack. However, that extra cost for one instance is not much compared to running hundreds or thousands of RDS instances. Hence, the Aurora solution is also the most cost-effective.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","title":"Backtracking an Aurora DB Cluster"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Import.html","title":"Migrating an RDS MySQL Snapshot to Aurora"}],"answers":[{"id":"8ff1b66ec30e68f3b0af77ca834fa8b4","text":"Restore the database in its initial state on an EC2 instance with Provisioned IOPS EBS disks. Take an EBS Snapshot. After each test, restore an EBS Volume from the EBS Snapshot. EBS Snapshots are much quicker to restore. Run the test on the EC2 Instance with the restored EBS Volume","correct":false},{"id":"b29796f55566af78fb926244627009f8","text":"Migrate the initial RDS MySQL Snapshot to an AWS Aurora DB Cluster. Turn on backtracking while creating the test Aurora MySQL DB Cluster. After each test is completed, backtrack to the point in time before the test.","correct":true},{"id":"093d4a2bd5748fd50abe9ea0a1adc3b7","text":"Spin up as many RDS MySQL Instances from the initial snapshot as required to run all the tests in parallel. That way, the innovation team can save time and not have to wait for one test to complete to run the next","correct":false},{"id":"9d29133ab1fe457147ed07251f102d5b","text":"Use MySQLDump to make a dump of the database in its initial state. Instead of restoring the database from its RDS DB Snapshot each time, restore it from the dump. This will make the process much quicker","correct":false}]},{"id":"0bfd631e-c08c-406a-acf5-a07416aab129","domain":"awscsapro-domain5","question":"Your team uses a CloudFormation stack to manage AWS infrastructure resources in production. As the AWS resources are used by a large number of customers, the update to the CloudFormation stack should be very cautious. Your manager asks for additional insight into the changes that CloudFormation is planning to perform when it updates the stack with a new template. The change needs to be reviewed before being applied by a DevOps engineer. What is the best method to achieve this requirement?","explanation":"CloudFormation Change Sets are able to provide the information on how the running resources are affected by a stack update. The outputs can be reviewed before being executed. Users can view the Change Set through AWS Console or CLI. The Retain option in the DeletionPolicy, CloudFormation stack policy or termination protection helps on protecting the stack resources. However, they cannot provide a summary of  changes in a stack update.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"935df51ce2c7f07994c2b8a257489e00","text":"Create a CloudFormation Change Set using AWS Management Console or CLI, review the changes to see if the modifications are as expected and execute the changes to update the stack.","correct":true},{"id":"700aa7cb0f0e8cfb417b67ae5d49e962","text":"Add a CloudFormation stack policy to prevent updates to stack resources. Only after the changes are reviewed and approved, change the stack policy to allow the stack update. Revert the stack policy after the change.","correct":false},{"id":"4657e544cc1daf4315865e230d92dd00","text":"For key AWS resources in the CloudFormation stack, add a Retain option in the DeletionPolicy attribute, which prevents the resources from being accidentally deleted by a stack update. Add a Delete option for the resources that you want to delete along with the stack.","correct":false},{"id":"550c80eaf15eeb89d49aa2a86eb747a6","text":"Enable termination protection in the CloudFormation stack so that the AWS resources cannot be accidentally deleted or modified. Disable the protection only if the changes are approved. Execute the changes in a maintenance window.","correct":false}]},{"id":"254eeafd-1183-4117-936e-f6f7ffa9d88a","domain":"awscsapro-domain2","question":"A client is having a challenge with performance of a custom data collection application.  The application collects data from machines on their factory floor at up to 1000 records per second.  It uses a Python script to collect data from the machines and write records to a DynamoDB table.  Unfortunately, under times of peak data generation, which only last 1-2 minutes at a time, the Python application has timeouts when trying to write to DynamoDB.  They don't do any analytics on the data but only have to keep it for potential warranty issues.  They are willing to re-architect the whole solution if it will mean a more reliable process.  Which of the following options would you recommend to give them the most scalable and cost-efficient solution?","explanation":"The application is likely running into throttling when writing to DynamoDB.  Kinesis Firehose makes for a good option in this case to accommodate streaming records.  Since we do not have to perform any analytics, we can simply store it on S3 using Firehose.","links":[{"url":"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html","title":"What Is Amazon Kinesis Data Firehose? - Amazon Kinesis Data Firehose"}],"answers":[{"id":"61f03e55a88efffb8060d5d3e07f247d","text":"Turn on DynamoDB Auto Scaling and configure appropriate upper and lower limits.","correct":false},{"id":"6ac1d0de8c41824a325523f050a3784e","text":"Change the application design to write the data records to EMR.  Use a Pig script to transfer the data from EMR to DynamoDB periodically.","correct":false},{"id":"7e2676bbdbc658bd6a68274495e2376f","text":"Change the application design to use Kinesis to take in the data.  Use Kinesis Firehose to spool the data files out to S3.  Use S3 Lifecycle to transition the files to Glacier after a few days.","correct":true},{"id":"0bd59f25e091570f7b171c3dd76b0206","text":"Change the application design to use SQS and a custom process on an EC2 spot fleet to throttle inbound messages into DynamoDB.","correct":false},{"id":"80999a665ea6597ad65e91f9d4a84b3f","text":"Change the application design to use Kinesis Streams to take in the data.  Provision at least 5 shards to ensure enough peak capacity.  Configure the Kinesis Streams to load the data into DynamoDB.  Increase the RCU and WCU for the DynamoDB table to match peak needs.","correct":false},{"id":"b30cd1d2aae071493c960798a422ade4","text":"Change the application design to use SWF to take in the data.  Use Amazon Elasticache in front of the DynamoDB database as a buffer to throttle the writes.","correct":false}]},{"id":"6b6689f4-b150-482a-aa96-eab1674cb232","domain":"awscsapro-domain5","question":"Quality Auto Parts, Inc. has installed IoT sensors across all of their manufacturing lines. The devices send data to both AWS IoT Core and Amazon Kinesis Data Streams. Kinesis Data Streams triggers a Lambda function to format the data, and then forwards it to AWS IoT Analytics to perform monitoring and time-series analyses, and to take actions based on business processes. After an equipment failure on one of the manufacturing lines causes tens of thousands of dollars in revenue losses, it's determined that alarms for a specific piece of equipment where received seventy-five seconds after the issue originated, and that automated corrective action within a few seconds of the problem could have avoided the financial losses altogether. What changes should be made to the architecture to improve the latency of device alerts?","explanation":"AWS IoT Analytics is useful for understanding long-term device performance, performing business reporting, and identifying predictive fleet maintenance needs, but common latencies run from seconds to minutes. If you need to analyze IoT data in real-time for device monitoring, use Kinesis Data Analytics, which provides latencies in the millisecond to seconds range. A Lambda function can be used as the destination for Kinesis Data Analytics to perform corrective actions. IoT Core rules can write messages to a Kinesis stream, but not directly to Kinesis Data Analytics. Having a Lambda function perform anomaly detection will work, but will require more logic to be written for query setup and execution than using a specialized service like Kinesis Data Analytics. With Amazon CloudWatch Alarms, an alarm will watch a single metric over a period time, but will not provide the capabilities of SQL to detect complex anomaly conditions.","links":[{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/aws-reference-architecture-time-series-processing.pdf?did=wp_card&trk=wp_card","title":"Processing IoT Time Series Data on AWS"},{"url":"https://aws.amazon.com/iot-analytics/faq/","title":"AWS IoT Analytics FAQs"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/05/introducing-real-time-iot-device-monitoring-with-kinesis-data-analytics/","title":"Introducing Real-Time IoT Device Monitoring with Kinesis Data Analytics"}],"answers":[{"id":"522af3cd7d520d3e94e97c02d19c0672","text":"Create an AWS IoT Core rule to write the message to Amazon CloudWatch Alarms to detect anomalies in the data. Invoke another AWS Lambda function from CloudWatch Alarms to perform device corrective action when needed.","correct":false},{"id":"7e2eb8f3a96a390aab88e66000821c26","text":"Create an AWS IoT Core rule to write the message to Amazon Kinesis Data Analytics to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":false},{"id":"71ade679994c0c1e6e55b3853194e4c5","text":"Add another AWS Lambda function as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Have the Lambda function write the anomalies to Amazon DynamoDB and perform device corrective action when needed.","correct":false},{"id":"fe8ff20697982ca413f81ea14472e603","text":"Add Amazon Kinesis Data Analytics as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":true}]},{"id":"e70b0410-8628-4150-845e-a4bba1f66ec6","domain":"awscsapro-domain2","question":"You work for a credit union which has two VPC-s in us-west-1 - VPC A and VPC B. The CIDR of VPC A is 172.16.0.128/25, and 50% of its available private IP addresses are already used up. The CIDR of VPC B is 172.16.0.0/25, and 5% of its available private IP addresses are already used up.\nThe development team is creating a new service that will deploy an API Gateway and use a Lambda function as its back-end. The Lambda function must read an S3 bucket using an S3 VPC Endpoint deployed in VPC A. Then, it must write to a Cassandra database hosted on an EC2 instance in VPC B.\nThe Lambda function must be deployed in either VPC A or VPC B. Also, the expected number of peak concurrent Lambda function instances is 300, with each instance needing 1 GB of memory. The development team expects that at peak, 100 free IP addresses will be needed to accommodate all the Lambda function instances.\nAs the AWS Architect, you need to advise the development team of the right AWS Architecture to make this possible. What should you suggest?","explanation":"This question requires the knowledge of the following:\n(a)Counting the number of the available private IP address in a CIDR. A CIDR ending with /25 has 128 IP addresses available, minus whatever number AWS reserves for its own (which is 5 per subnet, so we cannot really tell that number here as we do not know how many subnets are there in VPC A or B). For VPC A, half of these are already used, so around 64 are remaining. For VPC B, 5% is used, so 6 or 7 are used, leaving more than 120 available. Thus, VPC A clearly does not have enough space left for 100 Lambda functions, while VPC B surely does. This eliminates the choice that wants to deploy the Lambda function in VPC A without adding a secondary CIDR to it. Though we can eliminate the choice because of this reason, it can be worthwhile to note that it does state something important and true - S3 VPC Endpoint cannot be accessed from a different VPC using a peering connection.\n(b)Determining if CIDR-s are overlapping. 172.16.0.0/25 and 172.16.0.128/25 are not overlapping. Hence, the choice that uses the overlapping argument against peering is eliminated.\n(c)Whether an S3 VPC Endpoint in one VPC can be accessed via a VPC-peered network from a second VPC. The answer is no. Though most VPC Endpoints (that are of the type Interface Endpoint, also called PrivateLink) do not have this limitation, S3 and DynamoDB VPC Endpoints are not of this type. They are of type Gateway Endpoint. Gateway Endpoints cannot be accessed from another VPC if the VPC-s are peered. This eliminates VPC B as the hosting choice for the Lambda function. Remember that the Cassandra Database Instance can be accessed easily over a VPC Peering Connection, so VPC A and B needs to be peered, and VPC A must house the Lambda function, provided we make enough space in VPC A.\nThus, the only correct answer is to host the Lambda function in VPC A after adding a secondary CIDR to make space for the 100 ENI-s needed by the Lambda function at its peak","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html","title":"Gateway VPC Endpoints"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html","title":"Interface VPC Endpoints (AWS PrivateLink)"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html","title":"Search for VPC and Subnet Sizing"}],"answers":[{"id":"d49117fbb6670c3f47211d6c4410e9e8","text":"Peer VPC A and VPC B. Deploy the Lambda function in VPC B because only VPC B has enough private IP addresses left to accommodate the Lambda function's peak usage demands. The Lambda function can then access the Cassandra Database Instance in the same VPC, and also access the S3 VPC Endpoint over the Peering connection using appropriate routes added to the Routing Table","correct":false},{"id":"9d99afaedfe56a6a5a3e43972f59eb43","text":"VPC A and VPC B cannot be peered owing to their overlapping IP Address ranges. Therefore, it is impossible to deploy the Lambda Function in either VPC because it needs to access some resources in both the VPC-s, unless a new S3 VPC Endpoint is deployed in VPC B. In that case, VPC B will house both the new S3 Endpoint and the Cassandra Database Instance, and therefore, the Lambda function can be deployed in VPC B","correct":false},{"id":"b281cc03c30d338041be0e55efeba980","text":"Peer VPC A and VPC B. Deploy the Lambda function in VPC A because S3 VPC Endpoint cannot be accessed from a different VPC using a peering connection. The Lambda function can then access the S3 VPC Endpoint in the same VPC, and also access the Cassandra Database Instance over the Peering connection using appropriate routes added to the Routing Table","correct":false},{"id":"45c82d2dc99061d7e4f27c276f170b4b","text":"Peer VPC A and VPC B. Deploy the Lambda function in VPC A after adding a secondary CIDR Range to VPC A so that the available number of free IP addresses in VPC A expands to accommodate the Lambda function's peak usage demands. The Lambda function can then access the S3 VPC Endpoint in the same VPC, and also access the Cassandra Database Instance over the Peering connection using appropriate routes added to the Routing Table","correct":true}]},{"id":"b533b3c1-222f-4f33-99da-2c828e98ff91","domain":"awscsapro-domain5","question":"You have run out of root disk space on your Windows EC2 instance.  What is the most efficient way to solve this?","explanation":"We can easily increase the size of an EBS from the console or the CLI (using modify-volume) but then we also need to allow the OS to expand the resized volume so we can use it.  For Windows Server, we could use Disk Manager.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/expand-ebs-root-volume-windows/","title":"Expand the EBS Root Volume of Your EC2 Windows Instance"}],"answers":[{"id":"38862a719074689f75df6a20a42f7df7","text":"Use AWS System Manager Run service to remotely execute a PowerShell script using AWS Tools for PowerShell to expand the volume using the ModifyInstance command.","correct":false},{"id":"229b013eff2d5f53df7b9c3a60bd2418","text":"From the AWS Console, select Modify Volume for the EBS volume.  Enter the new size and confirm the change.  Connect to your Windows instance and use Disk Manager to extend the newly resized volume.","correct":true},{"id":"346ac05b4353d34c630eb6d233f8a35d","text":"Compress all files on the root volume using the built-in zip utility.  Modern versions of Windows will automatically unzip the files when they are accessed.","correct":false},{"id":"b893490015da5047b17b1220d43f4a1c","text":"From the AWS CLI, use the \"modify-instance\" command for EC2 to resize the volume to a larger size.  Using RDP, connect to the Windows instances and use Disk Manager to expand the volume.","correct":false}]},{"id":"6a0e9756-1b9e-495c-965b-a8c715843d4f","domain":"awscsapro-domain1","question":"A client has asked you to help troubleshoot a Service Control Policy.  Upon reviewing the policy, you notice that they have used multiple \"Statement\" elements for each Effect/Action/Resource object but the policy is not working. What would you suggest next?  ","explanation":"The syntax for an SCP requires only one Statement element.  You can have multiple objects within a single Statement element though. ","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/org_troubleshoot_policies.html","title":"Troubleshooting AWS Organizations Policies - AWS Organizations"}],"answers":[{"id":"c88df8df089061b0ce17c8aba3a63305","text":"Have them apply the same policy on another OU to eliminate any localized conflicts.","correct":false},{"id":"6d5becfe4962c177196b4dce486c4e58","text":"Look elsewhere as multiple Statement elements are used when multiple conditions are specified in SCPs.","correct":false},{"id":"3410d9f58eea1a807fdc3d0f6194c3ce","text":"Change the policy to combine the multiple Statement elements into one element with an object array.","correct":true},{"id":"4b06a8d83f7ea11c2700b91cae578fbb","text":"Split the SCP out into multiple policies and apply in a cascading manner to higher level OUs.","correct":false}]},{"id":"8765bd56-057b-488c-9a0a-f5bd413dd240","domain":"awscsapro-domain5","question":"Due to new corporate policies on data security, you are now required to use encryption at rest for all data.  You have some EC2 Linux instances on AWS that were created without encryption for the root EBS volume.  What can you do that meet the requirement and reduce administrative overhead?","explanation":"AWS does support encrypted root volumes but conversion from unencrypted root to an encrypted root requires a bit of a process. You must first create an AMI then copy that newly created AMI to the same region, specifying that you want to encrypt the EBS volumes during the copy.  You can then create a new instance with an encrypted root volume from the copied AMI.  You can use either a generated key from KMS or your own CMK imported into KMS.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html","title":"AMIs with Encrypted Snapshots - Amazon Elastic Compute Cloud"},{"url":"https://aws.amazon.com/blogs/aws/new-encrypted-ebs-boot-volumes/","title":"New – Encrypted EBS Boot Volumes | AWS News Blog"}],"answers":[{"id":"1e30ccf0b75e9e70fd76c6e041510c75","text":"At present, EC2 does not support encrypted root volumes.  Create new encrypted EBS data volumes and attach the new volumes to the existing instances.  Use RSYNC to migrate all the non-OS data over to the encrypted data volumes.","correct":false},{"id":"180e9aecdb74f204b1df00ffe6fa8b56","text":"Stop the instances and create AMIs from the instances.  Copy the AMIs to the same region and select the \"Encrypt target EBS snapshots\".  Redeploy the instances using the AMI copies you made with encrypted root volumes.","correct":true},{"id":"9fbbb2e71386cb7f7a7ed77129a1a960","text":"Create an encrypted EFS instance and mount-points in the respective subnets.  Log into the instance and mount an encrypted EFS mount-point.  Copy all the root files over to the EFS mount point.  Edit the FSTAB file to mount the EFS mount point as the root volume instead of the root EBS device and reboot.","correct":false},{"id":"4430b7492a6c058c3574ce4e8ea43955","text":"Stop the instances and temporarily detach the EBS volumes.  Attach the root volumes to another EC2 instance and mount them a data volume.  Use a encryption tool like GPG or OpenPGP to recursively encrypt all the files on the mounted root volumes.  Detach and reattach the encrypted EBS volumes to the original instances and restart.  Import the encryption keys in KMS as a CMK.","correct":false},{"id":"50c17b27f0bd0390ae321943f7db5c3d","text":"Create a certificate in CMS for the encryption key.  Stop the instances and temporarily detach the root volumes.  Via the AWS CLI, enable encryption on the root volumes using the \"ebs modify-volume\" argument with the flag of \"encryption=<CMS ARN>\" to specify the certificate.","correct":false}]},{"id":"f7e5105f-7ae9-4c04-b1e7-2d6165db236c","domain":"awscsapro-domain2","question":"You work for a specialty retail organization. They are building out their AWS VPC for running a few applications. They store sensitive customer information in two different encrypted S3 buckets. The applications running in the VPC access, store and process sensitive customer information by reading from and writing to both the S3 buckets. The company is also using a hybrid approach and has several workloads running on-premises. The on-premises datacenter is connected to their AWS VPC using Direct Connect.\nYou have proposed that an S3 VPC Endpoint be created to access the two S3 buckets from the VPC so that sensitive customer data is not exposed to the internet.\nSelect two correct statements from the following that relate to designing this solution using VPC Endpoint.","explanation":"S3 VPC Endpoint is a common topic tested in the SA-P Exam, as it enables S3 access over a private network, which is a common security requirement in many organizations. It is also a cost-effective way to establish outbound connection to S3, as the alternative is to use NAT Gateways, which are charged by the hour even if there is no traffic using them.\nOn vertical scanning of the answer choices, it should be obvious that one of the two closely worded choices is correct, and one of the other two choices is correct as well. That is because if there are 2 or 3 or 4 closely worded choices, only one (or in some rare cases, two) is correct - this is a common pattern in the SA-P test.\nFor the closely worded pair - the bucket policy of an S3 bucket will always specify who can or cannot access the bucket. It will not dictate how a VPC Endpoint behaves. Hence, the choice that suggests that a bucket policy can control a VPC Endpoint is incorrect.\nBetween the other two choices, remember that a VPC Endpoint can connect to any number of S3 buckets by default. One Endpoint for each bucket is simply not scalable, and should stand out as incorrect.\nThe remaining choice is correct because the S3 VPC Endpoint is of type Gateway Endpoint as opposed to Interface Endpoint, and a subnet needs Routes in the Routing Table for sources in the subnet to be able to connect to it. Read the links provided to understand the differences","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies-vpc-endpoint.html","title":"Example bucket policies including ones that discuss use of sourceVpce attribute"},{"url":"https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/","title":"How to set up VPC Endpoint for S3 access including Route Table for subnets accessing S3"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html","title":"VPC Endpoint documentation discussing differences between gateway endpoints and interface endpoints"}],"answers":[{"id":"96bd9d9981d98c1d98550618acedc673","text":"Bucket policies on the two S3 buckets can specify the id of each VPC Endpoint using AWS attribute sourceVpce to further restrict which S3 buckets can be accessed by the VPC Endpoint","correct":false},{"id":"19bc72413543fc85eb58b5edf43fb2db","text":"Each VPC Endpoint is a Gateway Endpoint that also requires correct routes in the Route Table associated with each subnet that wants to access the endpoint","correct":true},{"id":"28b96a455027a61e12e57f73ae7956fc","text":"You need two VPC Endpoints, one for each S3 bucket, as a single VPC Endpoint can only access a single S3 bucket","correct":false},{"id":"55b07e707272bf499484e3f28a8fac88","text":"Bucket policies on the two S3 buckets can specify the id of each VPC Endpoint using AWS attribute sourceVpce to further restrict which VPC Endpoints can access each bucket","correct":true}]},{"id":"a2fb4f91-4c73-4080-bbf3-6d07a1b2ce03","domain":"awscsapro-domain2","question":"You have been asked to investigate creating a production Oracle server in RDS.  You need to choose the correct options that will allow you to run the latest version of Oracle 12c with High Availability.  You do not currently have any Oracle licenses. Which of the below are valid options?","explanation":"To get to the correct answer, you must first disregard any option with Oracle Data Guard as this is not available in RDS, then remove any answer containing the editions SE or SE1 as they only allow version 11g to be deployed, not 12c.  The remaining two options are correct as they allow High Availability, 12c and either a Bring-You-Own or licence included option, so you can ensure you get the best value.","links":[{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"Amazon RDS for Oracle FAQs"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Oracle.html","title":"Oracle on Amazon RDS"}],"answers":[{"id":"0d9a63f8cb9a74845f363b450fa5cf11","text":"Purchase your own licenses from Oracle, Choose either the Enterprise or SE2 editions, Bring-Your-Own-Licence and enable Multi-AZ","correct":true},{"id":"1ed8c7e073924060dbe648fbdf8a9c17","text":"Choose the SE2 edition with licence included and enable Multi-AZ","correct":true},{"id":"5d1af18d1bfbdb6d990f138d844f75dc","text":"Choose either the SE or SE1 editions, Bring-Your-Own-Licence and Oracle Data Guard","correct":false},{"id":"0cb21536be9ea582178c7e54e365bd33","text":"Choose either the SE or SE1 editions, purchase your own licenses from Oracle and enable Multi-AZ","correct":false}]},{"id":"1520156f-0918-4ab4-a759-ce33a931c744","domain":"awscsapro-domain5","question":"Your company has an online shopping web application. It has adopted a microservices architecture approach and a standard SQS queue is used to receive the orders placed by the customers. A Lambda function sends orders to the queue and another Lambda function fetches messages from the queue and processes them. On some occasions the message in the queue cannot be handled properly. For example, when an order has a deleted production ID, the message cannot be consumed successfully and is returned to the queue. The problematic messages in the queue keep growing and the ability to process normal messages is affected. You need a mechanism to handle the message failure and isolate error messages for further analysis. Which method would you choose?","explanation":"It is not a good idea to adjust the retention period or simply delete the messages that fail to be processed as the question asks for a mechanism to isolate the messages for further troubleshooting. A redrive policy should be used to auto-forward error message to a dead letter queue. Then you can analyze the contents of messages to diagnose the producer’s or consumer’s issues. One thing to note is that a standard queue can only have another standard queue as the dead letter queue. Therefore a FIFO dead letter queue is incorrect as this scenario uses a standard SQS queue and requires a standard dead letter queue.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","title":"Amazon SQS dead-letter queues"}],"answers":[{"id":"64fd54fe78b534f0eac9222a6e32747d","text":"Create a FIFO (First-In-First-Out) queue as the dead letter queue and use a redrive policy to forward problematic messages to this new queue. Create a Lambda function to read the message contents in the FIFO queue for further analysis.","correct":false},{"id":"3c8453a6c57faf61e761f771bab6f1af","text":"Create a standard queue as the dead letter queue and configure a redrive policy to put error messages to the dead letter queue. Analyze the contents of messages in the dead letter queue to diagnose the issues.","correct":true},{"id":"f7b3898cfcb4851b120c9b14d044ab90","text":"Decrease the message retention period of the queue to 1 day. When the messages are not processed properly and put back in the queue, they can be quickly deleted when the retention period expires.","correct":false},{"id":"a7eb53a7df09334677590165f666c58f","text":"Modify the error handling logic of the Lambda function to delete the messages whenever the processing is unsuccessful with an error or exception. The error messages do not return to the queue and the normal message handling is not blocked.","correct":false}]},{"id":"6da286f8-23a6-4e8a-a3a4-c7b496a06523","domain":"awscsapro-domain5","question":"An online health foods retailer stores its product catalog in an Amazon Aurora database. The catalog contains over 6,000 products. They'd like to offer a product search engine on the website using Amazon Elasticsearch Service. They'll use AWS Database Migration Service (DMS) to perform the initial load of the Elasticsearch indexes, and to handle change data capture (CDC) going forward. During the initial load of the indexes, the DMS job terminates with an Elasticsearch return code of 429 and a message stating 'Too many requests'. What must be done to load the Elasticsearch indexes successfully?","explanation":"When the ElasticSearch indexing queue is full, a 429 response code is returned and an es_rejected_execution_exception is thrown. The DMS load task then terminates. Throttling the DMS input stream based on the number of Elasticsearch indexes, shards, and replicas to be loaded will result in a successfully completed job. The DMS MaxFullLoadSubTasks parameter indicates how many source tables to load in parallel, and the ParallelLoadThreads parameter determines the number of threads that can be allocated for a given table. Increasing Elasticsearch shards without modifying DMS subtask and thread parameters could still overrun the request queue. Changing the DMS stream buffer count won't help with this issue. Amazon Elasticsearch currently doesn't provide support for AWS Glue as a source, so integration would require significant effort. Increasing Elasticsearch EBS volume IOPS won't solve an ingress queue overrun problem. The DMS batch split size parameter sets the maximum number of changes applied in a single batch, but doesn't reduce the total number of requests.","links":[{"url":"https://aws.amazon.com/dms/","title":"Amazon Database Migration Service"},{"url":"https://aws.amazon.com/elasticsearch-service/","title":"Amazon Elasticsearch Service"},{"url":"https://aws.amazon.com/blogs/database/scale-amazon-elasticsearch-service-for-aws-database-migration-service-migrations/","title":"Scale Amazon Elasticsearch Service for AWS Database Migration Service migrations"}],"answers":[{"id":"62ac117860ebe5397e04bad8ea29a5fb","text":"Replace DMS with AWS Glue for the initial index load and ongoing change data capture. Enable parallel reads when the ETL methods are called in the Glue jobs","correct":false},{"id":"c1e1a85b26a30433a6af5d30c8bb8d76","text":"Calculate the number of queue slots required for the Elasticsearch bulk request as a product of the number of indexes, shards, and replicas. Adjust DMS subtask and thread parameters accordingly","correct":true},{"id":"157e8733386382289486b3592774442f","text":"Increase the number of Elasticsearch shards for each index to increase distribution of the load. Change the DMS stream buffer count parameter to match the number of Elasticsearch shards","correct":false},{"id":"ee750f72f1e70b83f6d83819f2d504f5","text":"Raise the baseline IOPS performance of the Elasticsearch cluster EBS volumes to enable more throughput. Increase the DMS batch split size parameter to send more data in each request and reduce the number of total requests","correct":false}]},{"id":"edf9ffa9-02ec-4341-a179-577cd590543e","domain":"awscsapro-domain1","question":"You work for a technology product company that owns two AWS Accounts - Prod and DevTest, both belonging to the same Organizational Unit (OU) under AWS Organizations Root. There are three different teams in your company - Dev Team, Testing Team and Ops Team. While Dev and Testing Team members have IAM Users created in the DevTest account, the Ops Team members have IAM Users created in the Prod account. There is an S3 bucket created in the Prod account that Testing Team members need access to - they need both read and write access. What is the best way to give the Testing Team members access to the Prod account S3 bucket?","explanation":"Cross-Account Access is best achieved using IAM Cross-Account Roles. The solution that suggests that the Testing Team members have to sign in to a different AWS account every time they need to access the S3 bucket is not correct as it is inefficient and unproductive.\nThere is no such AWS Organization feature that can directly enable Cross-Account Access from the console. There no way to select or deselect groups or users in this manner. Hence, the option that suggests using these is eliminated.\nThe remaining two options are a play in words. Carefully read both options. The Role must be created in the Trusting Account, in this case Prod Account because it has the Resource (S3 bucket) that needs to be accessed by the someone from another AWS Account, i.e., the Trusted Account.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html","title":"Tutorial - Delegate Access Across AWS Accounts Using IAM Roles"},{"url":"https://aws.amazon.com/organizations/","title":"AWS Organizations"},{"url":"https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/","title":"How to Enable Cross-Account Access to the AWS Management Console"}],"answers":[{"id":"87953f71b8915388e4ecb3f7b2378374","text":"Create an IAM Role in the DevTest Account that allows access to the Prod Account, thus establishing trust between the Accounts. Attach a Trust Policy to the Role that grants the Testing Team members permission to assume the Role. Attach an Access Policy to the Testing Team's IAM Users in the DevTest Account that allows them to call STS AssumeRole for the specific Resource whose value is the ARN of the Role in Prod Account. Attach a Bucket Policy to the S3 Bucket that specifies the IAM Users of Testing Team members as Principal.","correct":false},{"id":"56e4fe89e8a8ffb36bfcd67126afa0d5","text":"Create an IAM Role in the Prod Account that allows the DevTest Account to assume it, thus establishing trust between the Accounts. Attach an Access Policy to the Role that allows access to the S3 bucket. Attach a Bucket Policy to the S3 Bucket that specifies the ARN of the Role as Principal. Attach an Access Policy to the Testing Team's IAM Users in the DevTest Account that allows them to call STS AssumeRole for the specific Resource whose value is the ARN of the Role in Prod Account.","correct":true},{"id":"e661ebc45484fb5a715bc96c884b4024","text":"Enable Cross-Account Access at the AWS Organizational Unit (OU) level from the console. Deselect the Dev Team UAM Users from Cross-Account Access Setup Wizard. This will allow only the Testing Team members to be able to access the Prod account. Write a bucket policy for the S3 bucket that lists the Testing Team members as Principals who are allowed to access the bucket.","correct":false},{"id":"e91ff3381d499ccd06e7891c37a77891","text":"Create IAM Users for the Testing Team members in the Prod account. Create a Testing IAM Group in the Prod account and add the IAM Users of the Testing team members to the Group. Assign an access policy to the Testing Group in the Prod account that grants Read and Write access to the correct S3 bucket. Testing team members will sign into the Prod account to access the S3 bucket.","correct":false}]},{"id":"95f1d7a8-c3d4-4fec-952a-72385aa8b4c8","domain":"awscsapro-domain5","question":"You are consulting for a company that performs specialized customer data analytics.  Their customers can upload raw customer data to a website and receive back demographic statistics.  Their application consists of a REST API created using PHP and Apache.  The application is self-contained and works in real-time to return results as a JSON response to the REST API call.  Because there is customer data involved, company policy states that data must be encrypted in transit and at rest.  Sometimes, there are data quality issues and the PHP application will throw an error.  The company wants to be notified immediately when this occurs so they can proactively reach out to the customer.  Additionally, many of the company's customers use very old mainframe systems that can only access internet resources using IP address rather than a FQDN.  Which architecture will meet these requirements fully?","explanation":"The requirement of a static IP leads us to a Network Load Balancer with an EIP.","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/","title":"Elastic Load Balancing Features"}],"answers":[{"id":"5e4c5230c7e08202a0ea0575d5412d57","text":"Provision a Network Load Balancer with an EIP in front of your EC2 target group.  Install the CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.  Terminate SSL on the EC2 instances.","correct":true},{"id":"0c8e5c32f081b0484e86b71651ae3642","text":"Provision a Network Load Balancer in front of your EC2 target group and terminate SSL at the load balancer using Certificate Manager.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"a9ed133e35b8332aea2bf603521b891a","text":"Provision an Application Load Balancer in front of your EC2 target group and offload SSL to CloudHSM.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch and configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from CloudHSM.","correct":false},{"id":"569eec0061a1a97be77e3bdab43a1756","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Offload SSL termination using AWS KMS.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"434ce04b2c3a4d2e679d37df43de2585","text":"Provision an Application Load Balancer with an EIP in front of your EC2 target group and terminate SSL at the ALB.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"e53df806e37b325d7f61be27772875f1","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Enabled SSL termination on the API Gateway using Certificate Manager.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false}]},{"id":"230f422f-7118-4096-8dce-59c642fb55c8","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a new Direct Connect connection.  The connection is up and you can ping the AWS peer IP address, but the BGP peering session cannot be established.  What should be your next logical troubleshooting steps?","explanation":"Because the connection is up and we can ping the AWS peer, the problem must be at a higher level on the OSI model than the Physical or Data layers.  BGP uses TCP port 179 to communicate routes so we should check that no NACL or SG is blocking it.  Additionally, we should make sure the ASNs are properly configured in the proper ranges.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/Troubleshooting.html","title":"Troubleshooting AWS Direct Connect - AWS Direct Connect"}],"answers":[{"id":"81977d7a1eb5714746851077b93f44d6","text":"Power cycle all the equipment to clear ARP table cache.","correct":false},{"id":"edd3f9408cecbbf9182678ccc51d7981","text":"Ask your network provider to provide you with a cross connect completion notice and compare the ports with those listed on your LOA-CFA","correct":false},{"id":"16e5aea88df69cc18f99e3f066ec99c1","text":"Ensure that the local ASNs and AWS-side ASNs are properly configured.","correct":true},{"id":"45d4c1753395277878b9a17343628c52","text":"Ensure that the VLAN is configured properly between your on-prem router the provider. ","correct":false},{"id":"3d2a55832b90f19a2137e8715525d717","text":"Make sure no firewalls or ACLs are blocking TCP port 179 or any high-numbered ephemeral ports.","correct":true},{"id":"8fc27418eee2ce07b64bc672007d2c1b","text":"Contact the co-location provider and request a written report for the Tx/Rx optical signal across the cross connect.","correct":false}]},{"id":"05e085a9-4de3-46fe-9470-10c7f2faba57","domain":"awscsapro-domain5","question":"You are consulting with a client who is in the process of migrating over to AWS.  Their current on-prem Linux servers use RAID1 to provide redundancy.  One of the big benefits they are looking forward to with moving to AWS is the ability to create snapshots of EBS volumes without downtime.  Right now, they intend on migrating the servers over to AWS and retaining the same disk configuration.  What is your advice for them?","explanation":"Because RAID is based upon multiple volumes being in sync, taking snapshots of an individual volume that's part of a active and mounted RAID array would not create a proper backup.  You must first unmount the RAID volume and then create the snapshots of the component volumes.  This of course means any data on the RAID volume would be unavailable.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html","title":"RAID Configuration on Linux - Amazon Elastic Compute Cloud"}],"answers":[{"id":"bee3a756d6bfddce4e9917e171a4b0e2","text":"Consider using RAID0 when on AWS for performance reasons.","correct":false},{"id":"73b207bc7a947de1eed26bc058b4b67b","text":"If snapshots without downtime are the priority, do not use RAID.","correct":true},{"id":"ead9938f5d4fc4d2df30763406b6a8e5","text":"Consider using RAID10 when on AWS because it offers the best of both RAID0 and RAID1.","correct":false},{"id":"21a6a5218cf9778e0184eed7897c54ce","text":"Consider using RAID6 rather than RAID1 on AWS for performance reasons.","correct":false},{"id":"99a9d29ef15a0ced996c1510ff6d8f6a","text":"EC2 does not support RAID configurations.","correct":false}]},{"id":"c01346c8-d230-4b52-b53d-78cdbfbc7794","domain":"awscsapro-domain4","question":"You work for an Insurance Company as an IT Architect. The development team for a microservices-based claim-processing system has created containerized applications to run on ECS. They have spun up a Fargate ECS cluster in their development VPC inside a private subnet. The containerized application uses awslogs driver to send logs to Cloudwatch. The ECS task definition files use private ECR images that are pulled down to ensure that the latest image is running always. The cluster is having connectivity problems as it cannot seem to connect with ECR to pull the latest images and neither can it connect with Cloudwatch to log. The development team has approached you to help troubleshoot the issue. What is a possible reason for this and what is the best way to fix it?","explanation":"ECS Fargate clusters can be deployed in a private subnet. Hence, we can safely eliminate the choice that says that ECS Fargate clusters must be deployed in a public subnet only.\nECS Fargate clusters do not need the user to control the ECS Agent Version on the nodes, as Fargate is serverless by design. Hence, we can safely eliminate the choice that deals with ECS Agent Version.\nThis leaves two options. One proposes using NAT Gateway. The other proposes using ECS Interface VPC Endpoint. Both are working solutions. However, one of them makes a false claim - it states that ECS Fargate clusters connect to ECR or Cloudwatch only over the internet. That is not true, as it can connect either using a public or a private network. Hence, the only fully correct choice is the one that uses ECS Interface VPC Endpoint","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/vpc-endpoints.html","title":"Amazon ECS Interface VPC Endpoints (AWS PrivateLink)"},{"url":"https://aws.amazon.com/blogs/compute/setting-up-aws-privatelink-for-amazon-ecs-and-amazon-ecr/","title":"Setting up AWS PrivateLink for Amazon ECR"}],"answers":[{"id":"d73a8a48aaef0ddbe725f06264c3c6a3","text":"ECS Fargate clusters can connect to ECR (to pull down latest private images) or Cloudwatch (to log) only over the internet. Hence, if it is deployed in a private subnet, it needs a route to a NAT Gateway which must be connected to an Internet Gateway. To fix this issue, deploy a NAT Gateway in a public subnet of the VPC and add appropriate routes to the Routing Table","correct":false},{"id":"43901acf8308ae39f9cf7006afa72751","text":"ECS Fargate clusters can connect to ECR (to pull down latest private images) or Cloudwatch (to log) using either private or public network. Hence, if it is deployed in a private subnet, deploy an ECS Interface VPC Endpoint in the same subnet for connecting to internal services. Add appropriate routes to the Routing Table","correct":true},{"id":"f54cb63446f49ab92455024f17ae67b8","text":"ECS Fargate clusters must be deployed in a public subnet so that it can use the Internet Gateway to communicate with ECR or Cloudwatch. To fix this, redeploy the cluster in a public subnet","correct":false},{"id":"d103c9eb11e007e2a9410cfb2e1bc1ba","text":"The version of the ECS agent may be too old. To fix this, upgrade the ECS Agent version in the cluster nodes to be compatible with connectivity requirements","correct":false}]},{"id":"2eee6f1c-96d7-4d2b-821f-4ce8acaf3de3","domain":"awscsapro-domain5","question":"You've deployed a mobile app for a dance competition television show's viewers to vote on performances. The app's backend leverages Amazon API Gateway, AWS Lambda, and Amazon RDS Oracle, with voting activity going from devices directly to API Gateway. In the middle of the broadcast, you begin receiving errors in CloudWatch indicating that the database connection pool has been exhausted. You also see log entries in CloudWatch with a 429 status code. After the show concludes, ratings for the app indicate a very poor user experience, with multiple retries needed to cast a vote. What would be the best way to increase the scalability of the app going forward?","explanation":"Placing Kineses between API Gateway and Lambda decouples the architecture, making use of an intermediary service to buffer incoming requests. The 429 status code indicates a Lambda concurrency throttling error, which you can resolve by controlling the Kinesis batch size per batch delivery. Database sharding will increase scalability, but will still have an upper limit of capacity. Increasing available Lambda memory will have no effect. Inserting a Lambda traffic manager doesn't address the database scalability issues, nor does increasing the regional Lambda concurrency limit. Modifying RDS DB Parameter Group values will require a database restart to take effect, which won't be feasible during live voting activity.","links":[{"url":"https://aws.amazon.com/blogs/architecture/how-to-design-your-serverless-apps-for-massive-scale/","title":"How to Design Your Serverless Apps for Massive Scale"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/scaling.html","title":"AWS Lambda Function Scaling"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","title":"Using AWS Lambda with Amazon Kinesis"}],"answers":[{"id":"a4051078cf0289e689e79fe70eab1f14","text":"Create a separate Lambda function to increase the maximum DB connection value in the RDS DB Parameter Group when a CloudWatch Metrics DB connection threshold is exceeded. Invoke Lambda functions with an 'event' invocation type to retry failed events automatically","correct":false},{"id":"6b6e6fb8b54db42df4d343376ccd8c60","text":"Insert Amazon Kinesis between API Gateway and Lambda, and configure Kinesis as an event source for Lambda. Set the number of records to be read from a Kinesis shard to an optimal value based on volume projections","correct":true},{"id":"2f66b519925956d6a91d0026056904d9","text":"Have API Gateway route requests to a new Lambda function that manages traffic and retries for the voting logic Lambda function. Request that the regional function concurrency limit be increased based on volume projections","correct":false},{"id":"8f962935878f87ebc9523dc54f505886","text":"Scale the database horizontally by creating additional instances and use sharding to distribute the data across them. Provide the Lambda function with a mapping of the sharding scheme in DynamoDB. Increase the amount of memory available to the Lambda function during execution","correct":false}]},{"id":"663fbd6a-87bd-4fa6-a0ea-428ba2de5b51","domain":"awscsapro-domain5","question":"You manage a relatively complex landscape across multiple AZs.  You notice that the incoming requests vary mostly depending on the time of day but also there is a more unpredictable component resulting in smaller spikes and valleys for your resources.  Fortunately, you manage this landscape via OpsWorks Stacks.  What options, if any, are available to you as part of the OpsWorks featureset.","explanation":"OpsWorks Stacks offers three types of scaling: 24/7 for instances that remain on all the time; time-based for instances that can be scheduled for a certain time of day and on certain days of the week; and load-based scaling which will add instances based on metrics.  All this can be configured from within the OpsWorks Stack console.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/best-practices-autoscale.html","title":"Best Practices: Optimizing the Number of Application Servers - AWS OpsWorks"}],"answers":[{"id":"b7ff5b06f51facca179494cb2bb00e55","text":"You can enabled CloudFormation Anticipated Scaling that uses past CloudWatch metrics and machine learning to automatically design a scaling policy optimized for the incoming request patterns.","correct":false},{"id":"75ab4de4ea42c1971b0ee09ae04ca591","text":"You would define a baseline level of resources and configure them for 24/7 instances.  Then you could define a time-based instances to cover certain times of day.  Finally, you could cover the volatile spikes with a load-based instances.  All this can be done within OpsWorks Stacks.","correct":true},{"id":"3622d494dceb973760a46dea038d1dc2","text":"If you need the ability to dynamically scale, you will need to use OpsWorks for Chef Automate.  OpsWorks Stacks does not support scaling.","correct":false},{"id":"216c997091da6e24174ad1b83d0be8b9","text":"You would define a baseline level of resources within the OpsWorks Stack Console to cover the average load.  But for the periodic load, that requires a scheduled auto-scaling policy.  Similarly, for the volatile spikes, you must use a stepped auto-scaling policy defined in an auto scaling group. ","correct":false}]},{"id":"abd5a1c8-719f-4025-9804-8876b93d5633","domain":"awscsapro-domain2","question":"You are troubleshooting a CloudFront setup for a client.  The client has an Apache web server that is configured for both HTTP and HTTPS.  It has a valid TLS certificate acquired from LetsEncrypt.org.  They have also configured the Apache server to redirect HTTP to HTTPS to ensure a safe connection.  In front of that web server, they have created a CloudFront distribution with the web server as the origin.  The distribution is set for GET and HEAD HTTP methods, an Origin Protocol Policy of HTTP only, Minimum TTL of zero and Default TTL of 86400 seconds.  When a web browser tries to connect to the CloudFront URL, the browser just spins and never reaches the web server.  However, when a web browser points to the web server itself, we get the page properly. Which of the following if done by themselves would most likely fix the problem?","explanation":"With CloudFront only configured for HTTP Only, we have a loop when the web server redirects HTTP to HTTPS.  We can either enable HTTPS on CloudFront or disable the redirection policy on the Apache server.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/custom-origin-cloudfront-fails/","title":"Troubleshoot Failing Custom Origins in CloudFront"}],"answers":[{"id":"a24567748b0a6f2b185b2d69a0ced579","text":"Add the OPTION HTTP methods on the CloudFront distribution.","correct":false},{"id":"c7717815f23591b0fdaf1c616725535b","text":"Remove the redirection policy on the origin server and allow it to accept HTTP.","correct":true},{"id":"961c2031ccad89e399f2ffee45459704","text":"Enable CloudFront to forward cookies and enable query string forwarding.","correct":false},{"id":"caa85ea467e37e906dca5499b196b327","text":"Use an ALB instead of CloudFront to provide content caching.","correct":false},{"id":"f393ea6e3b2e4d003e821f57c52df1d2","text":"Change the CloudFront distribution origin protocol policy to use only HTTPS.","correct":true},{"id":"2970fc3adfc086f41b20364111c10e11","text":"Add POST and PUT HTTP methods on the CloudFront distribution.","correct":false}]},{"id":"f7d7767b-9159-4e53-8e37-ff9bf41ace17","domain":"awscsapro-domain5","question":"You are working with a client to help them design a future AWS architecture for their web environment.  They are open with regard to the specific services and tools used but it needs to consist of a presentation layer and a data store layer.  In a brainstorming session, these options were conceived.  As the consulting architect, which of these would you consider feasible?","explanation":"The only two options which contain feasible options are the Beanstalk and S3/Dynamo methods.  One would not create a new K8s deployment for for a new web update.  CodeBuild and AWS Config are not the correct tools for how they are being suggested.","links":[{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild – Fully Managed Build Service"}],"answers":[{"id":"f5c7f9be39386b15747c0fe57d5040ba","text":"Deploy Kubernetes on an auto-scaled group of EC2 instances.  Define pods to represent the multiple tiers of the landscape.  Use ElastiCache for Memcached to offload queries from a Multi-AZ RDS instance.  To deploy changes to the landscape, create a new EKS deployment containing all the updated service containers and deploy them to replace all the previous existing tiers.  Ensure the DevOps team understands the rollback procedures.","correct":false},{"id":"d6a73928290b05c25d87e26ece9e94a6","text":"Create a monolithic architecture using Elastic Beanstalk configured in the console.  Create an RDS instance outside the Beanstalk environment and configure it for multi-AZ availability.  When a new landscape change is required, use a command line script to implement the change.","correct":true},{"id":"c27a151a7715575fb1ebf0225c6aee09","text":"Use the AngularJS framework to create a single-page application.  Use the API Gateway to provide public access to DynamoDB to serve as the data layer.  Store the web page on S3 and deploy it using CloudFront.  When changes are required, upload the new web page to S3.  Use S3 Events to trigger a Lambda function which expires the cache on CloudFront.","correct":true},{"id":"01a7f4c09f65e41884b0a72843a8d55b","text":"Deploy an auto scaling group of EC2 instances behind an Application Load Balancer.  Provision a Mulit-AZ RDS instance to act as the data store, configuring a caching layer to offload queries from the database.  Use a User Script in the AMI definition to download the latest web assets from S3 upon boot-up.  When changes are required, use AWS Config to automatically fetch a new version of web content from S3 when a new version is created.","correct":false},{"id":"ca4cc92538f3b73d221c9b5d4378e1f8","text":"Setup a traditional three tier architecture with a CloudFormation template per tier and one master template to link in the others.  Configure a CodeBuild stack and set this stack to perform automated Blue Green deployments whenever any code change is made.","correct":false}]},{"id":"4ebf4191-8562-4f67-945d-ea5aae2f9f26","domain":"awscsapro-domain3","question":"You are consulting for a client who is trying to define a comprehensive cloud migration roadmap.  They have a legacy custom ERP system written in RPG running on an AS400 system.  RPG programmers are becoming rare so support is an issue.  They run Lotus Notes email which has not been upgraded in years and thus out of support.  They do have a web application that serves as their CRM created several years ago by a consulting group.  It is a Java and JSP-based application running on Tomcat with MySQL as the data layer hosted on a Red Hat Linux server. The company is in a real growth cycle and realizes their current platforms cannot sustain them.  So, they are about to launch a project to implement SAP as a replacement for their legacy ERP system over the next year.  What migration strategy would you recommend for their landscape that would allow them to modernize as soon as possible?","explanation":"In this case, retiring Lotus Notes is the better move because it would just prolong the inevitable by simply migrating to EC2.  The CRM system is fairly new and can be re-platformed on Elastic Beanstalk.  Due to the impending ERP upgrade, it makes no sense to do anything with the legacy ERP.  It would take lots of work to port over an RPG application to run on AWS--if it's even possible.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud | AWS Cloud Enterprise  Strategy Blog"}],"answers":[{"id":"c7be9ce93a4a7a74b44e281cb697dcc4","text":"Begin a product search for a new CRM system that is cloud-ready.  Once identified, migrate the existing CRM into the new CRM system.  Migrate Lotus Notes to Workmail using the AWS Migration Hub.  Invest in training the IT staff about AWS through a Certified AWS Training Partner.  Provision and run the various SAP environments from scratch using EKS.  Do nothing to the legacy ERP until the SAP implementation is complete.","correct":false},{"id":"480496ebe4100958e2f466291752ae2d","text":"Retire the CRM application and migrate the MySQL data over to Aurora.  Use QuickSight to provide access to the application for users.  Pay back support agreements to bring Lotus Notes back into support so it can be upgraded.  Migrate Notes email to EC2 instances.  Invest time in training Operations staff CloudFormation.  Create the complete SAP landscape as scriptable elements.  Do nothing to the legacy ERP platform until the SAP implementation is complete.","correct":false},{"id":"7579c185856bdbbcffbe49a649866488","text":"Rehost Lotus Notes mail on EC2 instances.  Refactor the CRM application to make use of Lambda and DynamoDB.  Use a third-party RPG to Java conversion tool to create Java versions of the legacy ERP to make it more supportable. Invest time in training developers continuous integration and continuous deployment concepts.  Because SAP implementations always take longer than estimated, rehost the legacy ERP system on EC2 instances so the AS400 can be retired.","correct":false},{"id":"20ef21af4ab4ecda70e90e90861b154c","text":"Retire the Lotus Notes email and implement AWS Workmail.  Replatform the CRM application Tomcat portion to Elastic Beanstalk and the data store to MySQL RDS.  Invest time in training Operations staff CloudFormation and spend time architecting the landscape for the new SAP platform.  Do nothing to the legacy ERP platform until the SAP implementation is complete.  ","correct":true}]},{"id":"482e75c9-071e-4a10-83f4-575f9c15b885","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They have just accidentally deleted the private key portion of their EC2 key pair.  Now, they are unable to SSH into their Amazon Linux servers.  Unfortunately the keys were not backed up and are considered gone for good.  What can this customer do to regain access to their instances?","explanation":"The two methods that AWS recommends if you lose a private key for an EC2 key pair are using Systems Manager Automation or using a secondary instance to edit the authorized_keys file.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2reset.html","title":"Reset Passwords and SSH Keys on Amazon EC2 Instances - AWS Systems Manager"}],"answers":[{"id":"79457a1b908d4a36cfeba625be909d40","text":"Use AWS Systems Manager Automation with the AWSSupport-ResetAccess document to create a new SSH key for your current instance.","correct":true},{"id":"bec3d01a56c851f61a1a09c852635db7","text":"Generate and upload a new key pair.  Stop the instances and select the new key pair from the dropdown on the Instance Settings sub-menu in the Console.","correct":false},{"id":"f17aa014620843abd81fa849982566b0","text":"Create a new key pair in KMS then assign the new public key to the required EC2 instance.","correct":false},{"id":"492c38d2fe2c3b96608bb8436592fe26","text":"Use the AWS CLI with the EC2 ModifyInstance action to enable SSH password-only access for the ec2-user account.  Attach using a password rather than an SSH key.  Modify the authorized_key file for the new public key.","correct":false},{"id":"be45655cb1d64dff71a97aa729bc4e4a","text":"Open the TELNET port (port 23) on the Security Group for the server.  Use a TELNET client to attach to the instances using the root account and password.  Modify the authorized_key file with the new public key.","correct":false},{"id":"509a77ae9827c5fcb60ccecc62fc9853","text":"Stop the instances, detach its root volume and attach it as a data volume to another instances.  Modify the authorized_keys file, move the volume back to the original instance and restart the instances.","correct":true}]},{"id":"7c5f884f-c0f9-4028-a725-50819d704324","domain":"awscsapro-domain5","question":"You deploy an application load balancer and an Auto Scaling group (ASG) in production for a new project. When instances in the ASG have a high CPU utilization, a new instance is launched. However, the new instance fails the health check from the ASG and has been terminated after some time. You check the logs in the instance and find that the startup script does not finish yet before the instance is terminated. How would you resolve the problem?","explanation":"Amazon EC2 Auto Scaling waits until the health check grace period ends before checking the health status of the instance. The grace period timer should be increased to give the instance more time to finish the startup script. Increasing the healthy threshold makes the instance more difficult to become healthy. Decreasing the timeout value also does not help as the instance may become unhealthy very quickly. Modifying the health check type from ELB to EC2 is unsuitable as the ASG cannot get the instance status from the application level. Even if the instance shows as healthy in ASG, the application may not be ready yet.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Check Grace Period"}],"answers":[{"id":"1d8964ced10da94d49d1a1efe02bbbca","text":"Modify the health check type from ELB to EC2 in the Auto Scaling group. Configure ASG to check the EC2 instance status. As long as the instance does not have a system level issue, it will not fail the health check in the ASG even when the startup script is still running.","correct":false},{"id":"e322fd6a55a044826665dfce4ae7020b","text":"Decrease the timeout value in the ELB health check from 5 seconds to 1 second so that when the ELB performs the health check on the backend instances, the instances are able to respond in time before a timeout occurs.","correct":false},{"id":"b5ca84726bf6fcfd6522ab188c8224ea","text":"Increase the default healthy threshold in the health check of elastic load balancer from 5 to 10 so that the instance will become healthy more quickly once the startup script finishes in the new instance.","correct":false},{"id":"e7ad9ea949ae87c6a6001a94f5a9bf48","text":"Increase the health check grace period in the Auto Scaling group configurations. When a new instance boots up, it is given more time to execute the startup scripts and run applications before the health check from ASG.","correct":true}]},{"id":"f3fef147-7b9d-45b6-8b2b-d943c90e8920","domain":"awscsapro-domain5","question":"You are assisting a company in the migration of their container-based web landscape over to Amazon.  They have a total of 21 containers which comprise their DEV, QA and Production environments.  All environment are identical in design and size.  Each environment consists of 3 web servers, 3 app servers and 1 datastore server.  Given the landscape, which of the provided options would be best for them to minimize maintenance?","explanation":"Deploying containers via ECS is a good option but we would want to use the EC2 hosted path.  Fargate is generally used for transient workloads and our datastore would be something we'd want to persist.  We might be able to deploy the data store with RDS, but the question does not make it clear if the data store is an RDS-supported database.  It could be a NoSQL data store or some other database unsupported by RDS.  Similarly, a MEAN stack under Elastic Beanstalk might not be compatible with our landscape either.","links":[{"url":"https://aws.amazon.com/ecs/resources/","title":"Resources for Amazon ECS - run containers in production"}],"answers":[{"id":"619957021a43a829fbb6228467323ca1","text":"Deploy the web, app and database servers using ECS on EC2.  Purchase 1-year reserved instance contracts for the required EC2 instances.","correct":true},{"id":"f05469f4c7578263f4271e7514c338ef","text":"Deploy the web and app servers in each environment using ECS.  Provision an RDS instance for each environment.  Use AWS Systems Manager to provide a common management console.","correct":false},{"id":"e0ea997f77cb156d35ec716cf772c49c","text":"Deploy the web, app and database containers using ECS.  Make use of Fargate for the underlying ECS infrastructure.","correct":false},{"id":"c1354e6d48fedccbf7b4e9c18854d980","text":"Redeploy the web landscape on a MEAN stack under Elastic Beanstalk, making use of auto-scaling groups to right-size the respective environments.  ","correct":false}]},{"id":"63da01c2-9c4d-4abd-a482-06ede2baf728","domain":"awscsapro-domain2","question":"A popular royalty free photography website has decided to run their business on AWS. They receive hundreds of images from photographers each week to be included in their catalog. Amazon S3 has been selected as the image repository. As the business has grown, the task of creating catalog entries manually has become unsustainable. They'd like to automate the process and store the catalog information in Amazon DynamoDB. Which architecture will provide the most scalable solution for automatically adding content to their image catalog going forward?","explanation":"Calling the S3 API to upload the images will suffice for this use case. Streaming ingest is not needed for this volume of data. AWS Step Functions will orchestrate the process of discovering both the image metadata with a Lambda function and the image object data with Rekognition. Rekognition will not return the image metadata. AWS Elemental MediaStore is used for originating and storing video assets for live or on-demand media workflows, not image recognition. Kinesis Video Analytics is not a currently supported service.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://github.com/aws-samples/lambda-refarch-imagerecognition","title":"Serverless Reference Architecture: Image Recognition and Processing Backend"}],"answers":[{"id":"0710b6a1f2a807da2cfc1a940e7014e9","text":"Deploy Amazon Kinesis Data Streams to ingest the images with two consumers. Setup Amazon Kinesis Firehose as the first consumer to deposit the images into S3. Configure Amazon Kinesis Video Analytics as the second consumer to extract the image's metadata and object information. Invoke a Lambda function to store the discovered information in DynamoDB.","correct":false},{"id":"dd8565c85d19c2867d3a6768f512b404","text":"Programmatically call the S3 API to upload the images. Trigger an AWS Lambda function to kick off execution of a state machine in AWS Step Functions. Create state machine sub-steps to invoke Lambda functions which extract image metadata, detect objects in the image with Amazon Rekognition, and store the discovered data in DynamoDB.","correct":true},{"id":"42367a003a702450700b58798073122b","text":"Deploy Amazon Kinesis Data Firehose to ingest images into S3. Invoke a Lambda function to pass the image's S3 key to Amazon Rekognition, which will extract the image metadata and detect objects in the image. Invoke a Lambda function to store the discovered data in DynamoDB.","correct":false},{"id":"f98112f4f94f4a8d66edee560976acc2","text":"Programmatically call the S3 API to upload the images. Trigger an AWS Lambda function to send the image's S3 key to AWS Elemental MediaStore, which will extract the image's metadata, discover image patterns through machine learning, and deposit artifacts back into S3. Invoke a Lambda function to write the artifact data to DynamoDB.","correct":false}]},{"id":"0abe2292-3f6e-47e1-93d9-6af24d5ea4c2","domain":"awscsapro-domain4","question":"A graphic design company has purchased eighteen m5.xlarge regional Reserved Instances and sixteen c5.xlarge zonal Reserved Instances. They receive their monthly AWS bill and find the invoice amount to be significantly higher than expected. Upon investigation, they discover RI discounted and non-discounted charges for nine m5.xlarge instances, nine m5.2xlarge instances, eight c5.xlarge instances, and eight c5.2xlarge instances. The business will need all of this capacity for at least the next twelve months. As their consultant, what would you advise them to do to maximize and monitor their RI discounts?","explanation":"Regional Reserved Instances allow for application of RI discounts within instance families, so all of the m5 instances are covered. Zonal Reserved Instances only provide discounts for specific instance types and sizes. So purchase of additional RIs would lower costs on the eight c5.2xlarge instances. Unused Reserved Instances are contractual and cannot be cancelled, so looking for another place to use them is the right approach. They could possibly be sold on the Reserved Instance Marketplace. AWS Budgets reservation budgets provide visibility and alerting on RI coverage specifically. Cost budgets and usage budgets may be useful, but they won't target RI coverage specifically.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html","title":"Regional and Zonal Reserved Instances (Scope)"},{"url":"https://aws.amazon.com/blogs/aws-cost-management/launch-instance-family-coverage-budgets/","title":"Launch: Instance Family Coverage Budgets"}],"answers":[{"id":"4b6d77d8423cd16b8e711b482dbcafbf","text":"Purchase an additional nine c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":true},{"id":"1c87a2204ad8da355c8ca0890aa06785","text":"Purchase an additional nine m5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances. Create an AWS Budgets usage budget that sends notification whenever RI coverage drops below 60%","correct":false},{"id":"8c1f6db4cd04a86fd43efa1bc97640ea","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances and eight c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":false},{"id":"8e0e716570fad7bcaf6fe8bebe4f9d49","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Cancel the Reserved Instances for the nine unused m5.xlarge instances and eight unused c5.xlarge instances. Create an AWS Budgets cost budget that sends notification whenever costs exceed 80% of usage expectation","correct":false}]},{"id":"dac73d1f-8c64-48b1-90be-3432e789933d","domain":"awscsapro-domain2","question":"Your company is bringing to market a new Windows-based application for Computer Aided Manufacturing.  As part of the promotion campaign, you want to allow users an opportunity to try the software without having to purchase it.  The software is quite complex and requires specialized drivers so it's not conducive to allowing the public to download and install in their own systems.  Rather you want to control the installation and configuration.  Therefore, you want something such as a VDI concept.  You'll also need to have a landing page as well as a custom subdomain (demo.company.com) and limit users to 1 hour of use at a time to contain costs.  Which of the following would you recommend to minimize cost and complexity?","explanation":"AppStream is a way to deploy an application on a virtual desktop and allow anyone with a browser to use the application.  This is the most efficient and simplest option given the other choices.","links":[{"url":"https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html","title":"What Is Amazon AppStream 2.0? - Amazon AppStream 2.0"}],"answers":[{"id":"129d13b81f52a8ab01ce9e8ea7009289","text":"Create a landing page in HTML and deploy to an S3 bucket configured as a Static Web Host.  Use Route 53 to create a DNS record for the \"demo\" subdomain as alias record for the S3 bucket.  Deploy your application using Amazon AppStream.  Set Maximum Session Duration for 1 hour.","correct":true},{"id":"9b85d56999fa276cfa4a01df8195700c","text":"Create a landing page in HTML and deploy to an S3 bucket configured as a Static Web Host.  Embed in the HTML a Javascript-based RDP client that is downloaded with the webpage.  Create a CloudFront distribution with the S3 bucket as the origin.  Use an S3 Event to launch a Lambda function which starts up an EC2 instance with your golden AMI.  Once the instance is up and running, use a web socket call from the Lambda function to initiate the RDP client and log the user in.  After 1 hour, have the Lambda function issue a shutdown command to the EC2 instance.","correct":false},{"id":"a1c67181ea5765383a7b477e821391b4","text":"Configure an EC2 auto scaling fleet of spot instances with your golden AMI.  Create security groups to allow inbound RDP for the auto scaling group.  Deploy Apache Guacamole on an EC2 instance and place your landing page in its web server directory.  Use Guacamole to provide an RDP session into one of the EC2 instances directly in the users browser.  Use AWS Batch to reboot the EC2 instances after 1 hour of runtime.  ","correct":false},{"id":"bdc9c29dcecb6611cd71c4660fc235ca","text":"Deploy your application as an app in the Workspaces Application Manager.  Spin up several Workspaces and configure them to automatically install your application via WAM. Create the landing page such that it redirects to the web client for Workspaces and deploy the landing page via S3 configured as a web host.  Use Route 53 to create a DNS record for the demo subdomain as an alias record for the S3 bucket.  Configure the Workspaces for a 1 hour timeout.  ","correct":false}]},{"id":"f43ec458-0ff5-4633-a57b-6bf82f60bd14","domain":"awscsapro-domain5","question":"You have a target group in an elastic load balancer (ELB) and its target type is \"instance\". You attach an Auto Scaling group (ASG) in the target group. All the instances pass the health check and have a healthy state in the target group. Due to a new requirement, the ELB target group needs to forward the incoming traffic to an IP address that belongs to an on-premise server. The ASG is no longer needed. There is already a VPN connection between the on-premise server and AWS VPC. How would you configure the target in the ELB target group?","explanation":"The target type of existing target groups cannot be changed from \"instance\" to \"IP\". Because of this, users have to create a new target group and set the target type to be \"IP\". After that, the on-premise IP address can be registered as a target. A domain name cannot be registered as a target in the target group. You also do not need to create a new elastic load balancer since you only need a new target group to register the IP address.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#target-type","title":"Target type in ELB target group"}],"answers":[{"id":"265de6fdcab5323b156774dcb949d309","text":"Create a new network load balancer with a new listener and target group. Configure the target type to be \"IP\" in the target group and attach the on-premise IP address to it. Set up the health check using the HTTP protocol.","correct":false},{"id":"99f63c1cf5bcfbcb188328714abb8ed3","text":"Register a record set in AWS Route 53 to forward a domain name to the on-premise IP address. Modify the target group to register the domain name as its target. Remove the previous Auto Scaling group from the target group.","correct":false},{"id":"e0b619a421d68626ffb82b1a6e1d22d5","text":"Remove the Auto Scaling group from the target group and modify the target type to be \"IP\". Attach the IP address to the target and set up the IP address and port in the health check configurations.","correct":false},{"id":"49646fca04901301d145a2a814a7e481","text":"In the elastic load balancer, create a new target group with an \"IP\" target type. Register the on-premise IP address as its target. Monitor if the target becomes healthy after some time. Remove the old target group.","correct":true}]},{"id":"49107f33-5b31-4d7e-a2cb-95f3ce8a2d75","domain":"awscsapro-domain1","question":"Your customer has setup AWS Organizations to help manage a collection of AWS Accounts.  They are running into a problem though and need your help.  They have created accounts for each business unit and applied SCPs to those OUs. However, they notice that root accounts in in those sub-accounts can still change root access keys and disable MFA.  How do you instruct your customer?","explanation":"Service Control Policies can control many aspects but they cannot restrict root account actions of changing root access keys or disabling MFA.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"194cc2d07b5c378b62b1e090f0aea956","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the entire OU in the root account.","correct":false},{"id":"77df34553819fdc2e31fb79762948993","text":"You can establish a trust with the top-level account and use the \"organizations:ServicePrinciple\" condition key to restrict root access at the sub-account level.","correct":false},{"id":"94a2f948d3d2f9c317a6ebb1f5a24ea5","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the specific sub-accounts.","correct":false},{"id":"3d722952f024bcec9174a311c17dcc14","text":"You can not use SCPs to restrict root account activities of changing the root password or managing MFA settings.","correct":true}]},{"id":"1eb605a0-e0bc-4666-9fe9-aa249901bcb5","domain":"awscsapro-domain3","question":"Your company currently runs SharePoint as it's internal collaboration platform. It's hosted in the corporate data center on VMware ESXi virtual machines. To reduce costs, IT leadership has decided not to renew its VMware license agreement for the coming year. They've also decided on an AWS cloud-first approach going forward, and have ordered an AWS Direct Connect for connectivity back to the corporate network. On-premises Active Directory handles SharePoint authentication, and will continue to do so in the future. You've been tasked with determining the best way to deliver SharePoint to the company's users after the VMware agreement expires. How will you architect the solution in a cost effective and operationally efficient way?","explanation":"Deploying SharePoint Web Front Ends in separate Availability Zones behind a Network Load Balancer, with SharePoint App Servers in those same subnets, provides a highly reliable solution. RDS SQL Server supports Always On Availability Groups. Since RDS is a managed service, operational efficiency is achieved. Amazon Workspaces also provides managed service benefits for remote desktops, and gives the company the opportunity to have users use lower cost hardware. It can all be authenticated through an AWS Managed AD trust relationship with the on-premises Active Directory. The Managed AD managed service provides better operational efficiency than creating Domain Controllers on EC2. Introducing VMware Cloud on AWS for the database layer results in more networking complexity, and is not necessary since RDS supports Always On clusters. Remote Desktop Gateways will require higher cost end-user hardware.","links":[{"url":"https://d1.awsstatic.com/VMwareCloudonAWS/SharePoint-Hybrid_Reference-Architecture.pdf?did=wp_card&trk=wp_card","title":"SharePoint Reference Architecture - AWS and VMware Cloud on AWS"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html","title":"Multi-AZ Deployments for Microsoft SQL Server"},{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_microsoft_ad.html","title":"AWS Managed Microsoft AD"}],"answers":[{"id":"1ff58ba530d63513506d1a8a58cb91b9","text":"Deploy a Network Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Run an Amazon RDS SQL Server Always On cluster. Use AWS Directory Service Managed AD for authentication in a trust relationship with the on-premises Active Directory. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":true},{"id":"c2b436befa546d5de6bf07d1d3eb3766","text":"Configure an Application Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Run SQL Server Always On clusters on VMware Cloud on AWS. Use AWS Directory Service AD Connector for authentication from the on-premises Active Directory. Implement Remote Desktop Gateways in each subnet to provide connectivity for Windows desktops.","correct":false},{"id":"140b5814920d29ba818858f73c97577b","text":"Implement a Network Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers, SQL Server instances, and Active Directory Domain Controllers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Configure the SQL Server instances as Always On clusters. Join the Domain Controllers to the on-premises AD forest. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":false},{"id":"56ace4242f33e5150f0a40b3627f9568","text":"Use an Application Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers and SQL Server instances on EC2 instances in the same subnets as the SharePoint Web Front Ends. Configure the SQL Server instances as Always On clusters. Use AWS Directory Service AD Connector for authentication from the on-premises Active Directory. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":false}]},{"id":"bbcb9a8c-f84d-4424-b199-9047a4625e15","domain":"awscsapro-domain2","question":"Your company's DevOps manager has asked you to implement a CI/CD methodology and tool chain for a new financial analysis application that will run on AWS. Code will be written by multiple teams, each team owning a separate AWS account. Each team will also be responsible for a Docker image for their piece of the application. Each team's Docker image will need to include code from other teams. Which approach will provide the most operationally efficient solution?","explanation":"AWS CodePipeline, AWS CodeCommit, and AWS CodeBuild all allow cross-account access once the appropriate resource-level permissions have been granted. Orchestrating deployments from a single DevOps account will provide the most operationally efficient solution, resulting in less need for coordination of services and configurations across development team accounts.","links":[{"url":"https://aws.amazon.com/products/developer-tools/","title":"Developer Tools on AWS"},{"url":"https://aws.amazon.com/blogs/devops/how-to-use-cross-account-ecr-images-in-aws-codebuild-for-your-build-environment/","title":"How to Use Cross-Account ECR Images in AWS CodeBuild for Your Build Environment"}],"answers":[{"id":"c32df9c97fbc40e07515d4ca41de63e2","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in the team accounts","correct":false},{"id":"4a4bd59860afb498d97f0d01cff52b7a","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in a single DevOps account","correct":false},{"id":"b9ae951b67f2fed4a145bd7f591c8631","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the DevOps account to AWS CodeCommit in the team accounts to get the latest code. Perform cross-account access from AWS CodeBuild in the DevOps account to AWS CodeBuild in the team accounts to get the Docker images. Perform deployments from AWS CodeDeploy in the DevOps account","correct":true},{"id":"83443cdcabe51198b91ed96d84eed4a6","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform all deployments from AWS CodeDeploy in the DevOps account","correct":false}]},{"id":"7eebbdef-e751-4d76-be2a-1e3a746b87f6","domain":"awscsapro-domain5","question":"You are a database administrator for a company in the process of changing over from RDS MySQL to Amazon Aurora for MySQL.  You setup the new Aurora database in a similar fashion to how your pre-existing RDS MySQL landscape was setup:  Multi-AZ with Read Replica in a backup region.  You have just completed the migration of data and verified that the new Aurora landscape is performing like it should.  You are now in the process of decommissioning the old RDS MySQL landscape.  First, you decide to disable automatic backups.  Via the console, you try to set the Retention Period to 0 but receive an error saying \"Cannot Set Backup Retention Period to 0\".  How can you disable automatic backups?","explanation":" For RDS, Read Replicas require backups for managing read replica logs and thus you cannot set the retention period to 0.  You must first remove the read replicas and then you can disable backups.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Troubleshooting.html#CHAP_Troubleshooting.Backup.Retention","title":"Troubleshooting - Amazon Relational Database Service"}],"answers":[{"id":"7086f25af0d0410de9d8826003760752","text":"You cannot disable automatic backups on RDS instances.  This feature is built into the platform as a failsafe.","correct":false},{"id":"898042b8eeafc1bbb4dd146c98dbb919","text":"Remove the Read Replicas first.","correct":true},{"id":"fb2c22782c91868676981ff65332f1d5","text":"You cannot disable backups via the console.  You must do this via the AWS CLI or SDK.","correct":false},{"id":"c61323c176e6d23809f0d467770ed205","text":"Automatic backups are enabled and disabled at the database engine level.  You need to login using a MySQL client to turn off automatic backups.","correct":false},{"id":"5698f7c6bb0530fc1c0ad18e6911f528","text":"You must first reprovision the database as a single AZ instances.  Multi-AZ replication requires backups to be enabled.","correct":false}]},{"id":"3ca7c5e0-432a-4d40-afee-ab996819b429","domain":"awscsapro-domain3","question":"You are consulting with a client to guide them on migration of an in-house data center to AWS.  The client has stipulated in the contract that the migration cannot require any more than 1 hour downtime at a time and that there is always a fallback path.  Additionally, they want an overall increase in business continuity capabilities when the migration is done.  Their landscape is as follows:  (1) Several databases with about 1TB of data combined which are heavily used 24x7 and considered mission critical; (2) About 40TB of historic files which are read sometimes but almost never updated; (3) About 150 web servers on VMware in various states of customization of which there is a current project underway to standardize them.  The client's team has suggested some next steps but because they aren't yet familiar with AWS, they are not using equivalent AWS terms.  Translating their suggestions, which of the following activities would you choose to meet the requirements, reducing costs and management where possible?","explanation":"The database migration suggestion aligns well with DMS as it can keep the databases in sync until cutover.  SAN replication sounds a lot like Storage Gateway which is a reasonable way to migrate data to AWS.  However, simply using K8s does not convert your VMs into containers or make them serverless.  We can't restore tapes to AWS.  Creating the same VM landscape on AWS just adds an additional layer of complexity that's not needed.","links":[{"url":"https://aws.amazon.com/dms/faqs/","title":"AWS Database Migration Service FAQs - Amazon Web Services"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"}],"answers":[{"id":"31ea0eccdcea45ea4fce3b9459de52d4","text":"Use some block-level SAN replication tool to gradually migrate the on-prem historic files to AWS.","correct":true},{"id":"3a02ebcd33fe18255e4ce43e8babb730","text":"Over several months, at end of business on Friday, backup all the servers and data to tape and restore to new instances in AWS to prove out AWS capabilities and reliability.","correct":false},{"id":"75d528d2ec243c60d1478ae605c89f40","text":"Build a matching VMware environment on AWS and use third-party tools to backup and restore the VMs there.","correct":false},{"id":"d2dde578790a34d9e740015474ea23e4","text":"Migrate the majority of the 150 web servers to a serverless concept by moving the VMs to a Kubernetes cluster.","correct":false},{"id":"801ce55cfc2a125e7d17c729ca3e2e93","text":"Create new high powered stand-alone database instances in AWS and migrate data from on-prem database.  Use log shipping to keep the databases in sync.  Once we better understand AWS, we'll rebuild the servers and repartition the tables. ","correct":true}]},{"id":"5f6d53a1-1b9c-46fe-9a8e-a5706e72914d","domain":"awscsapro-domain4","question":"Which of the following is an example of buffer-based approach to controlling costs?","explanation":"The buffer-based approach to controlling costs is discussed in the Cost Optimization Pillar of the AWS Well-Architected Framework.  A buffer is a mechanism to ensure that applications can communicate with each other when they are running at different rates over time.  By decoupling the throughput rate of a process, you can better govern and smooth demand--creating a less volatile and reactionary landscape.  As a result, costs can be reduced by optimizing for the steady state. ","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"26437f21964d56c3e373f55d997101ed","text":"A mobile image upload and processing service makes use of SQS to smooth an erratic demand curve.","correct":true},{"id":"878d4fde3965b2e5f84c543b2cca1dfc","text":"A production ERP landscape is scaled up during the month-end financial close period to provide some padding for the additional processing and reports so they do not impact the normal business processes.","correct":false},{"id":"f867e23e60a3917c1ebe5e2c4ced818c","text":"An auto-scaling fleet is created to dynamically adjust available compute resources based network connection events as reported by CloudWatch.","correct":false},{"id":"5bc62ebd024ca5594793ca76f08cd960","text":"A public-facing API is created using API Gateway and Lambda.  As a serverless architecture, it scales seamlessly in step with demand.","correct":false}]},{"id":"447b50dd-cf00-4688-8181-2d87c302c538","domain":"awscsapro-domain2","question":"An application that collects time-series data uses DynamoDB as its data store and has amassed quite a collection of data--1TB in all.  Over time, you have noticed a regular query has slowed down to the point where it is causing issues.  You have verified that the query is optimized to use the partition key so you need to look elsewhere for performance improvements.  Which of the following when done together could you do to improve performance without increasing AWS costs?","explanation":"As a DynamoDB table grows, it will spit into more partitions.  If the RCU and WCU remain constant, they are divided equally across the partitions.  When the allocation of that partition is used up, you risk throttling.  AWS will permit burst RCU and WCU at times but it is not assured.  You could increase the RCU and WCU but this would increase cost. Therefore, we can archive off as much data as possible but also need to shrink the partitions down to something more reasonable.  We can do this by backing up and recreating/restoring the table.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html","title":"Best Practices for DynamoDB - Amazon DynamoDB"}],"answers":[{"id":"28004d3db4c1cf8f98d735e3893f14d0","text":"Export the data then import it into a newly created table.","correct":true},{"id":"21bb1c71dec739e9fc76b69fb51ee515","text":"Create a secondary global index on the most common fields to increase performance. ","correct":false},{"id":"59b19c32945103a6f4da6ef17acd7f75","text":"Change the query to use a secondary local index instead of the partition key.","correct":false},{"id":"e132b6523705a41bc5ed88930a62a21c","text":"Increase the provisioned read and write capacity units.","correct":false},{"id":"be0c8b120e1730a1fc73ec1b0ae38d50","text":"Archive off as much old data as possible to reduce the size of the table.","correct":true}]},{"id":"d58acd29-561d-4017-a55d-bcff8eb40f90","domain":"awscsapro-domain4","question":"Last month's AWS service cost is much higher than the previous months. You check the billing information and find that the used hours of Elastic Load Balancer (ELB) increases dramatically. You manager asks you to plan and control the ELB usage. When the ELB service has been used for over 5000 hours in a month, the team should get an email notification immediately and further actions will be taken accordingly. Which of the following options is the easiest one for you to choose?","explanation":"AWS Budgets include the types of cost budget, usage budget, reservation budget and savings plan budget. The usage budget enables you to plan the usage of ELB service and receive budget alerts when the actual usage becomes more than a threshold (5000 hours in this scenario). The cost budget type is incorrect as it evaluates the cost instead of usage and you cannot receive budget alerts from either Cost Explorer or AWS Config.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing your costs with Budgets"}],"answers":[{"id":"39f1073c8a5f51031cb743e7cf45dce2","text":"Calculate the estimated ELB cost when the total ELB usage is 5000 hours in a month. Configure a cost budget in AWS Budgets for the EC2 ELB service and set the number as the threshold. When the cost is greater than the user-defined threshold, send an email alert to the team.","correct":false},{"id":"15fe48e7010a90e06774d9aa9668704a","text":"Create a usage budget in AWS Budgets. Check the ELB running hours for every month and set the budgeted amount to be 5000 hours. Configure an email alert or an Amazon Simple Notification Service (SNS) notification when the actual usage is more than the threshold.","correct":true},{"id":"baf09736ecf556311eb6e77579877ccd","text":"In AWS Config, monitor the usage of all ELB resources within the AWS account. Create a custom Config rule via a Lambda function that calculates the ELB usage and sends an alert message to an SNS topic when the usage is over 5000 hours.","correct":false},{"id":"0f88bd3ee9fd3724922aa89baa9f656d","text":"Launch the Cost Explorer in AWS billing dashboard, filter the EC2 ELB service and configure a CloudWatch alert to track its actual monthly usage. When the monthly ELB usage grows more than 5000 hours, raise the CloudWatch alert and notify an SNS topic.","correct":false}]},{"id":"7b8f1346-92b4-4a87-86d2-c8e65935589b","domain":"awscsapro-domain2","question":"A non-profit organization is putting on a comedy night fundraiser. Tickets are sold online, and attendees are asked to upload a facial photo to check-in at the venue the night of the event. The organization would like to provide the most streamlined entry process possible. A non-profit representative will take a photo at the door to match the photo submitted by the attendee at the time of ticket purchase. Which architecture will provide the highest level of automation to meet their needs?","explanation":"Amazon Rekognition can extract facial features into a feature vector and store that vector in a face collection via the IndexFaces API. Storing the face_id of the vector in DynamoDB, along with the attendee's name, provides the capability to validate the ticket sale at the door. At the event, the picture that's taken can be sent to Rekognition's SearchFaceByImage API to look for a match in the face collection. When a match is found, the face_id can be used to retrieve the attendee's name from DynamoDB to validate the event check-in. AWS DeepLens doesn't provide a SearchFaceByImage API to look for matching images in S3. Having Lambda call the Rekognition CompareFaces API will work, but will require much more coding to compare all the images one-by-one rather than using the automated face collection functionality. Rekognition can not be configured as a target for a Kinesis stream.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition"},{"url":"https://aws.amazon.com/solutions/auto-check-in-app/?did=sl_card&trk=sl_card#","title":"Auto Check-In App"}],"answers":[{"id":"eea359cb33a2ae4f87ccbe358a59201d","text":"Store the attendee's facial photo in Amazon S3 when the ticket is purchased. Trigger a Lambda function to write the attendee's name and photo S3 object key to Amazon DynamoDB. At the event, have a Python-based UI use a camera to take the attendee's picture and send it to Amazon API Gateway, which triggers another Lambda function. Have this Lambda function store the image in S3, and call the Rekognition CompareFaces API to search S3 images for a match. Retrieve the matched attendee's name from the DynamoDB.","correct":false},{"id":"8f60e8fc429b98a1dc7a37e6c71ca61b","text":"Upload the attendee's facial photo to Amazon S3 when the ticket is purchased. Trigger a Lambda function, which calls Amazon Rekognition to create a feature vector and store it in a face collection. Store the attendee's name and vector id in Amazon DynamoDB. At the event, have a Python-based UI use a camera to take the attendee's picture and send it to Amazon API Gateway, which triggers another Lambda function. Have this Lambda function call Rekognition to search the face collection and return an id if a match is found. Retrieve the attendee's name from the DynamoDB.","correct":true},{"id":"2caca948e6ce72046a513b4b48b36d19","text":"Send the attendee's facial photo to Amazon S3 when the ticket is purchased. Trigger a Lambda function to store the attendee's name and photo S3 object key in Amazon DynamoDB. At the event, have an AWS DeepLens take the attendee's picture and look to match it with an image in S3 via the SearchFaceByImage API. If a match is found, trigger an AWS Lambda function use the S3 object key to retrieve the attendee's name from the DynamoDB.","correct":false},{"id":"9369a3a40d7028e2ff2c87f02c0339a9","text":"Write the attendee's facial photo to an Amazon Kinesis stream when the ticket is purchased. Configure Amazon Rekognition as the target of the stream to create a facial feature vector and store it in Amazon S3. At the event, have a Python-based UI use a camera to take the attendee's picture and send it to another Amazon Kinesis stream with Rekognition as the target. Have Rekognition compare the attendee's image with those in S3 for a match.","correct":false}]},{"id":"1239c235-107c-4f5e-8bac-9dc824c00680","domain":"awscsapro-domain5","question":"You are helping a client with some process automation.  They have managed to get their website landscape and deployment process encapsulated in a large CloudFormation template.  They have recently contracted with a third-party service to provide some automated UI testing.  To initiate the test scripts, they need to make a call out to an external REST API.  They would like to integrate this into their existing CloudFormation template but not quite sure of the best way to do that.  Help them decide which of the following ideas is feasible and incurs the least extra cost.","explanation":"To integrate external services into a CloudFormation template, we can use a custom resource.  Lambda makes a very good choice for this scenario because it can handle some logic if needed and make a call out to an external API.  Using an EC2 instances to make this call is excessive and we likely would not have the ability to configure the third-party API to poll an SQS queue.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html","title":"AWS Lambda-backed Custom Resources - AWS CloudFormation"}],"answers":[{"id":"f761e84ee0cd0f689465458a41b69fae","text":"Include an SQS queue definition in the CloudFormation template.  Define a User Script on the deployed EC2 instance which will insert a message into the SQS queue only once it has fully booted.  Configure the external REST API to use long polling to check the queue for new messages in order to initiate the testing process.","correct":false},{"id":"43569df3ec4b7db0265dea4051c04644","text":"Add a small EC2 instance definition to the CloudFormation template.  Define a User Script for that instance which will install a custom application from S3 to call out to the external REST API endpoint using the POST method to trigger the testing process.  Add a CleanUp parameter to the EC2 instance definition that will shut down the instance once the activity has completed.","correct":false},{"id":"6b3b26e17f2323a91f04f792f0c2d20c","text":"Create a Lambda function which issues a call out to the external REST API using the POST method.  Define a custom resources in the CloudFormation template and associate the Lambda function and execution role with the custom resource.  Include DependsOn to ensure that the function is only called after the other instances are ready.","correct":true},{"id":"97a123d11bbf3be0e3e1788e2f0874ac","text":"Add an API Gateway deployment to the CloudFormation template.  Add the DependsOn parameter to the API Gateway resource to ensure that the call to the external API only happens after all the other resources have been created.  Create a POST method and define it as a proxy for the external REST API endpoint.  Using SWF, call the API Gateway endpoint to trigger the testing process.","correct":false}]},{"id":"79f2f5be-b591-44e6-957b-eb0383640d7d","domain":"awscsapro-domain5","question":"You have a standard SQS queue to receive messages from the frontend application. The backend application is JAVA based and the AWS SDK is used to get the messages from the queue for processing. The SQS queue is not busy most of the time. According to the backend application logs, there is a high number of empty ReceiveMessageResponse instances returned. You want to adjust the settings to minimize the number of empty responses and reduce the cost. How would you implement this? ","explanation":"Amazon SQS long polling is preferable to short polling in most of the cases. Long polling requests let the consumers receive messages as soon as they arrive in the queue. It can help to reduce the number of empty responses. In order to enable long polling, the attribute ReceiveMessageWaitTimeSeconds should be more than 0. Short polling is incorrect. Visibility timeout and delivery delay do not address the problem of empty responses.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html","title":"Amazon SQS short and long polling"}],"answers":[{"id":"203fa6faf2e6bf53939b43300ec6dac2","text":"Increase the default visibility timeout of the queue to reduce the possibilities that the messages become visible to consumers again. The application can also use the ChangeMessageVisibility API to specify a suitable timeout value.","correct":false},{"id":"c3cbf51c591cb7fa17bd023ab814f95c","text":"Consume the messages in the SQS queue using long polling. Set the queue attribute ReceiveMessageWaitTimeSeconds to be more than 0. Amazon SQS will wait until there is an available message in a queue before sending a response.","correct":true},{"id":"220352e5b3779c1f2030cfd4b391b19e","text":"Modify AWS SDK to get the messages in the SQS queue by short polling. The ReceiveMessage call from the consumer sets the WaitTimeSeconds attribute to 0. As a result, the empty responses are eliminated.","correct":false},{"id":"a5cdcd2968c3566cbb7fc7bcd5fef01a","text":"Add a delivery delay in the SQS queue such as 1 minute. The delay helps to postpone the delivery of new messages to the queue for some time. When the JAVA application polls the messages from the queue, there will be a lower chance to get an empty response.","correct":false}]},{"id":"a7c939f1-277e-469f-a209-9b290e8136c9","domain":"awscsapro-domain5","question":"Your company has contracted with a third-party Security Consulting company to perform some risk assessments on existing AWS resources.  As part of a routine list of activities, they inform you that they will be launching a simulated attack on one of your EC2 instances.  After the Security Group performed all their activities, they issue their report.  In their report, they claim that they were successful at taking the EC2 instance offline because it stopped responding soon after the simulated attack began.  However, you're quite certain that machine did not go offline and have the logs prove it.  What might explain the Security company's experience?","explanation":"AWS Shield and other counter-measure technologies work to protect all AWS customers from DDoS attacks.  Unless AWS was aware of the test time and expected duration, its likely the traffic was blocked as suspicious.  AWS Firewall Manager is used to manage WAF ACLs and not dynamically blacklist IPs.  Similarly, VPC Flow Logs cannot automatically implement NACL changes as described here. Despite being a permitted service, traffic suspected of being malicious will still be blocked","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/penetration-testing/","title":"Submit a Penetration Testing Request"}],"answers":[{"id":"7d4826f179b8dc854c9cfb6e43678373","text":"The VPC Flow Logs record the spike in suspicious traffic and implement an update to the inbound NACL to block the remote IP address.","correct":false},{"id":"82a05cb45adab6d248655e827de16c6f","text":"AWS Firewall Manager is dynamically adding a blacklist entry for the Security Company's testing machine because it sees the traffic as a threat.","correct":false},{"id":"f3f963b71e307f8c28109631df115418","text":"The EC2 instance is using an ENI and the Security Company temporarily exceeded the throughput limit resulting in a throttling of their connection.","correct":false},{"id":"e3facacbe52b6423f9cf2e700d8e0b81","text":"The Security Company's traffic was seen as a threat and blocked dynamically by AWS.  AWS must grant permission before any penetration testing is done.","correct":true}]},{"id":"f679d23d-14d4-4021-9749-481bbe11046c","domain":"awscsapro-domain3","question":"A telecommunications company has decided to migrate their entire application portfolio to AWS. They host their customer database and billing application on IBM mainframes. IBM AIX servers running WebSphere provide an API layer into the mainframes. Customer-facing online applications are hosted on Linux systems. The customer service backend application resides on Oracle Solaris and makes use of gigabytes of persistent information. Their ERP and CRM systems also run on Solaris boxes. Telecom switches send call records to Linux-based applications, and their employee productivity suite runs on Windows. They need to complete the project in twelve months to satisfy budgetary constraints. Which migration strategy will provide them with the most resilient, scalable, and operationally efficient cloud environment within the project time frame?","explanation":"Since the company has twelve months to complete the project, they can plan for a highly cloud-centric migration. Refactoring the mainframe billing application to EC2 and the customer database to Aurora will require significant cost and effort, but will result in significant intermediate to long-term business value for most companies. A number of AWS Partner Network (APN) solutions are available to assist with this. The WebSphere layer can be replaced by API Gateway with HTTP, REST, or WebSocket APIs that call modules on the EC2 instances. Refactoring the customer-facing online apps to Lambda serverless and Step Functions will provide high operational efficiency. Performing a replatform of the Solaris customer service application to EC2 with Auto Scaling will achieve elasticity to avoid the excess capacity inefficiencies that were most-likely present in the on-premises environment. Many robust ERP, CRM, and employee productivity SaaS solutions exist and should be leveraged rather than trying to manage these applications with in-house staff. The call record processing Linux system can simply be rehosted to EC2. Repurchasing mainframe capacity from a third party provider only extends the rigidness of making mainframe changes whenever new business requirements arise. A Lambda/Step Functions solution will provide all the functionality needed for the online apps, and will be more economical than Elastic Beanstalk. Refactoring the customer service application to Lambda presents issues with processing the gigabytes of persistent information, so replatforming to EC2 is a better choice.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud"},{"url":"https://aws.amazon.com/blogs/apn/automated-refactoring-of-a-new-york-times-mainframe-to-aws-with-modern-systems/","title":"Automated Refactoring of a New York Times Mainframe to AWS with Modern Systems"},{"url":"https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/","title":"Build a Serverless Web Application"}],"answers":[{"id":"c875a6eb6d74f098c971af4a638f288f","text":"Repurchase mainframe capacity from a third party provider and run the customer database and billing application there. Replatform the API layer onto EC2 Linux instances. Refactor the customer-facing online applications to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the customer service, ERP, and CRM applications to EC2 Linux instances with Auto Scaling. Rehost the call record processing applications onto EC2, and repurchase SaaS applications for the employee productivity suite.","correct":false},{"id":"290da60b3bdd11f28e175fa86972386a","text":"Repurchase mainframe capacity from a third party provider and run the customer database and billing application there. Replace the API layer with Amazon API Gateway. Rehost the customer-facing online applications to Amazon Elastic Beanstalk. Refactor the customer service application to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the ERP and CRM applications to EC2 Linux instances with Auto Scaling. Rehost the call record processing applications, and repurchase SaaS applications for the employee productivity suite.","correct":false},{"id":"57d6d888b4cee16e22852910fa09cdb8","text":"Refactor the mainframe applications onto Amazon EC2 Linux instances, and migrate the customer database to Amazon Aurora. Replace the API layer with Amazon API Gateway. Rehost the customer-facing online applications to Amazon Elastic Beanstalk. Refactor the customer service application to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Repurchase SaaS solutions for the ERP and CRM systems. Rehost the call record processing applications and the employee productivity suite onto EC2.","correct":false},{"id":"5a404284de876ce2a054dcd916ed80ec","text":"Refactor the mainframe applications onto Amazon EC2 Linux instances, and migrate the customer database to Amazon Aurora. Replace the API layer with Amazon API Gateway. Refactor the customer-facing online applications to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the customer service applications to EC2 Linux with Auto Scaling. Repurchase SaaS solutions for the ERP and CRM systems. Rehost the call record processing applications onto EC2, and repurchase SaaS applications for the employee productivity suite.","correct":true}]},{"id":"6d93e859-e1a9-468f-9a05-61a2dbc2be9c","domain":"awscsapro-domain5","question":"You manage a group of EC2 instances that host a critical business application.  You are concerned about the stability of the underlying hardware and want to reduce the risk of a single hardware failure impacting multiple nodes.  Regarding Placement Groups, which of the following would be the best course of action in this case?","explanation":"Spread Placement Groups ensure your instances are each placed on separate underlying hardware so this reduces the risk of a single hardware failure taking down multiple instances.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-spread","title":"Placement Groups - Amazon Elastic Compute Cloud"}],"answers":[{"id":"e6d129a06320300aaf95634a6691b7cb","text":"You would the AWS Console to move the existing instances into a clustered placement group.","correct":false},{"id":"c8b809a2fef3f21146774b82e8f03f12","text":"You would move the instances onto a Dedicated Host.","correct":false},{"id":"b6153af57a0b8805fc28d93a2859fb9e","text":"You cannot move existing instances into a new placement group.  You would create AMIs from the existing instances and redeploy them into a clustered placement group.","correct":false},{"id":"3bde2d7fff7ead39c4035ab0600275f1","text":"You would use the AWS CLI to move the existing instances into a diversified placement group.","correct":false},{"id":"e346d1667489b5726c0367eff4ea4a34","text":"You would use the AWS CLI to move the existing instances into a spread placement group.","correct":true}]},{"id":"ebcf9ff3-82a1-48f8-b2fc-5d2aeb1d018c","domain":"awscsapro-domain3","question":"You are consulting with a company who is at the very early stages of their cloud journey.  As a framework to help work through the process, you introduce them to the Cloud Adoption Framework.  They read over the CAF and come back with a list of activities as next steps.  They are asking you to validate these activities to keep them focused.  Of these activities, which would you recommend delaying until later in the project?","explanation":"External communication usually comes much later in the process once project plans are defined and specific customer impact is better understood.","links":[{"url":"https://aws.amazon.com/professional-services/CAF/","title":"The AWS Cloud Adoption Framework"}],"answers":[{"id":"525fde29088ab22597d7d8063c7dadf6","text":"Work with internal Finance business partners to design a transparent chargeback model.","correct":false},{"id":"2f063bd85b14ae1bbeec72bf0f6c06f5","text":"Work with the Human Resources business partners to create new job roles, titles and compensation/remuneration scales.","correct":false},{"id":"153eadc71676701cd67fdf00dc6c4723","text":"Work with Marketing business partners to design an external communications strategy to be used during potential outages during the migration.","correct":true},{"id":"937d0c376f475ce2eac7a3356601b8fb","text":"Investigate the need for training for Program and Project Management staff around agile project management.","correct":false},{"id":"199907bba5306a10dbadf5a330d5f1f6","text":"Hold a workshop with IT business partners about the creation of an IT Service Catalog concept.","correct":false}]},{"id":"b00cb57f-7191-4f17-aa6d-ac687c418332","domain":"awscsapro-domain5","question":"You have a running EC2 instance and the name of its SSH key pair is \"adminKey\". The SSH private key file was accidentally put into a GitHub public repository by a junior developer and may get leaked. After you find this security issue, you immediately remove the file from the repository and also delete the SSH key pair in AWS EC2 Management Console. Which actions do you still need to do to prevent the running EC2 instance from unexpected SSH access?","explanation":"Although the SSH key pair is deleted in EC2, the public key content is still placed on the instance in an entry within ~/.ssh/authorized_keys. Someone can SSH to the instance if he has a copy of the leaked SSH private key. Users should not configure the instance to support another key pair as the old key pair still works. The correct method is deleting the instance immediately to prevent it from being compromised and launching another instance with a new SSH key pair. There is no need to use the AWS CLI command delete-key-pair as the key is already deleted from AWS EC2.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html","title":"Amazon EC2 Key Pairs"}],"answers":[{"id":"a928b9732684f81d9ce046842965f1f6","text":"No action is required as the SSH key pair \"adminKey\" is already deleted from AWS EC2. Even if someone has the SSH private key, he still cannot use the key to access the instance.","correct":false},{"id":"ee7464becbe88068a5f848419e621bba","text":"Stop and terminate the instance immediately as someone can still SSH to the instance using the key. Launch a new instance with another SSH key pair. SSH to the EC2 instance using the new key.","correct":true},{"id":"bc01758fc758191425928382b18697ca","text":"Create another SSH key pair via AWS EC2 or a third party tool such as ssh-keygen. Stop the instance and configure the instance with this new key pair in AWS Management Console. Restart the instance to activate the key pair.","correct":false},{"id":"4f9620d4caefa25fafabc98f00c6b192","text":"Use AWS CLI delete-key-pair to completely delete the key pair so that no one can use it to SSH to the instance. Configure CloudWatch logs to monitor the SSH logging events and filter the logs with the SSH key ID to see if the key pair is still used by someone.","correct":false}]},{"id":"ba6576ca-a3da-4742-8917-cf5852a133bc","domain":"awscsapro-domain2","question":"You are in the process of porting over a Java application to Lambda.  You find that one Java application's code exceeds the size limit Lambda allows--even when compressed.  What can you do?","explanation":"If your code is too large for Lambda, it might indicate the need to break the code down into more atomic elements to support microservice best practices.  If breaking the code down is not possible, you should consider deploying in a different way like ECS or Elastic BeanStalk.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/limits.html","title":"AWS Lambda Limits - AWS Lambda"}],"answers":[{"id":"7c5f7ca2bf3fe981064a0883ba4d7158","text":"Change the Java Runtime Version in the Lambda function to one that supports BIGINT. ","correct":false},{"id":"91f37922d4b3b7e70c1f900cb95370e8","text":"Enable Extended Storage in the Lambda console to permit a larger codebase to be deployed.","correct":false},{"id":"0d95df1e6f5e72a068c3fbcbc598b16d","text":"Use AWS CodeBuild to identify unused libraries and remove them from the package. ","correct":false},{"id":"5fd2567315fb3689d9b56ea422cba642","text":"Evaluate the structure of the program and break it into more modular components.","correct":true},{"id":"2c12b2cfb5b150801f171e15a1e6bfb7","text":"Consider containerization and deploy using Elastic Beanstalk.","correct":true},{"id":"a6bb1edf368a254d9604ca8e6419be1d","text":"Consider using API Gateway to offload some of the I/O your code requirements.","correct":false}]},{"id":"4c49e888-8f76-4b15-b267-7f6ec35579ca","domain":"awscsapro-domain5","question":"A client has asked you to review their system architecture in advance of a compliance audit.  Their production environment is setup in a single AWS account that can only be accessed through a monitored and audited bastion host. Their EC2 Linux instances currently use AWS-encrypted EBS volumes and the web server instances sit in a private subnet behind an ALB that terminates TLS using a certificate from ACM. All their web servers share a single Security Group, and their application and data layer servers similarly share one Security Group each. Their S3 objects are stored with SSE-S3.  The auditors will require all data to be encrypted at rest and will expect the system to secure against the possibility that TLS certificates might be stolen by would-be spoofers.  How would you help this client pass their audit in a cost effective way? ","explanation":"All the measures they have taken with Certificate Manager, S3 encryption and the EBS volumes meet the audit requirements.  There is no need for LUKS, CloudHSM or client-side encryption.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html","title":"Amazon EBS Encryption - Amazon Elastic Compute Cloud"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"Protecting Data Using Server-Side Encryption with Amazon S3-Managed  Encryption Keys (SSE-S3) - Amazon Simple Storage Service"}],"answers":[{"id":"bdddd09d0832e16504afd5c88136cf7e","text":"Leave the S3 objects alone.","correct":true},{"id":"92194f6603feb3f83a46b00bda37de5a","text":"Deploy CloudHSM and migrate the TLS keys to that service.","correct":false},{"id":"93113c2b6ca9be67acbd3561eef56481","text":"Reconfigure the EC2 EBS volumes to use LUKS OS-Level encryption.","correct":false},{"id":"0c31fb48e65443ba5bfa312a7dcc117c","text":"Encrypt the S3 objects with OpenPGP locally before re-uploading them to S3.","correct":false},{"id":"d9447af4853ab8736e49349138cac8fb","text":"Make no changes to the EBS volumes.","correct":true},{"id":"ab9ad7bfd57b97954a6f861d872c6137","text":"Continue to use the ACM for the TLS certificate.","correct":true}]}]}}}}
