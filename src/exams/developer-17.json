{"data":{"createNewExamAttempt":{"attempt":{"id":"4caf6087-32cc-4f8b-a7a7-effffa3826a7"},"exam":{"id":"dbe60cda-5c6b-4762-ad5e-6d93ad048f59","title":"AWS Certified Developer - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"3a11539f-9ee0-4b96-a649-ec7635e18dc8","domain":"deployment","question":"What is the size of one unit of read capacity?","explanation":"A read capacity unit is 4KB in size.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html","title":"Throughput Capacity for Reads and Writes"}],"answers":[{"id":"e7363d80bd01165d7bb0c75f5add39c0","text":"5 KB","correct":false},{"id":"35ef8d3515745f7c6916d7644fb5d6e5","text":"4 KB","correct":true},{"id":"d54d09275c9dfc95026a3a52d2f66173","text":"3 KB","correct":false},{"id":"bf361755334066f22d019854dd2be686","text":"1 KB","correct":false}]},{"id":"87896ba2-d675-4fce-97b9-f144f05d42f1","domain":"security","question":"You are building an S3 hosted website and your website is accessing javascript and image files located in another S3 bucket. How can you enable this? ","explanation":"Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html","title":"Cross-Origin Resource Sharing (CORS)"}],"answers":[{"id":"bef5f130edd0f036b2ca659d3295d5c7","text":"IAM roles","correct":false},{"id":"39e38061c46bfab6a44c6c8b5482763f","text":"Cross Origin Resource Sharing (CORS)","correct":true},{"id":"0e0c1a7e0b6fe582226b82afc8eec89b","text":"S3 bucket policies","correct":false},{"id":"be48e3ddc7b57744c693982774a47dad","text":"S3 ACLs","correct":false}]},{"id":"03a312db-34cd-4c47-9f7e-da02470c8c03","domain":"mon-trb","question":"You work in the security industry for a large consultancy. One of your customers uses Lambda extensively in their production environment and they require a log of all API calls made to and from their Lambda functions. How can you achieve this?","explanation":"Enabling CloudTrail for Lambda will allow you to log all API calls to an S3 bucket.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/logging-using-cloudtrail.html","title":"Logging Lambda API Calls Using AWS CloudTrail"}],"answers":[{"id":"77bca249739dbbe611e40d1dd5f2dc7e","text":"Enable Access Logs for Lambda","correct":false},{"id":"2365a4464b0b2502654aeb4ddf868a14","text":"Enable CloudTrail for Lambda","correct":true},{"id":"02ef0d69dfa15f0bee0148eeee8c3936","text":"Enable Detailed Monitoring on the Lambda function","correct":false},{"id":"e2cd3f939eef3b06ec217db11988c978","text":"Enable CloudWatch for Lambda","correct":false}]},{"id":"c2d80b09-07e7-4c6c-9b1e-c8dad1c9b669","domain":"security","question":"You are performing an audit of your IAM policies. Which of the following tools will enable you to identify which specific statement in a policy results in allowing or denying access to a particular resource or action?","explanation":"With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies attached to IAM users, groups, or roles in your AWS account. You can test which actions are allowed or denied by the selected policies for specific resources.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html","title":"Testing IAM Policies with the IAM Policy Simulator"}],"answers":[{"id":"4240f8a0b958c209f10161d60a22e254","text":"IAM Role Simulator","correct":false},{"id":"c5dcf026a75a9a24668c13a5526e1cc1","text":"IAM Statement Simulator","correct":false},{"id":"b62fc6bfda9075d9b4468c476f1d9cfe","text":"IAM Permission Simulator","correct":false},{"id":"b6632fa69795d5fabc908fe75210b177","text":"IAM Policy Simulator","correct":true},{"id":"b7dd1a111a9c83018d8060d8c22f36d8","text":"IAM Access Control Simulator","correct":false}]},{"id":"6db2c293-ed5d-41ff-84fb-803e2970a0f9","domain":"development","question":"A developer is working on a new application which will use DynamoDB. One of the DynamoDB tables that the developer must create requires an index sort key. When creating this DynamoDB table, the developer must select an Attribute Type for the sort key.\n\nWhich of the following DynamoDB data types can the developer select to use for their index sort key?","explanation":"Both partition and sort keys attributes must be defined as type string, number, or binary.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.NamingRulesDataTypes.html#HowItWorks.DataTypes","title":"DynamoDB Data Types"}],"answers":[{"id":"27226c864bac7454a8504f8edb15d95b","text":"Boolean","correct":false},{"id":"6ce976e8f061b2b5cfe4d0c50c3405dd","text":"Binary","correct":true},{"id":"27118326006d3829667a400ad23d5d98","text":"String","correct":true},{"id":"b2ee912b91d69b435159c7c3f6df7f5f","text":"Number","correct":true},{"id":"46f3ea056caa3126b91f3f70beea068c","text":"Map","correct":false},{"id":"4ee29ca12c7d126654bd0e5275de6135","text":"List","correct":false}]},{"id":"4becf869-2fee-4023-80b0-0cb3a21c2651","domain":"development","question":"You are using CodeBuild to build the source code for your new application and would like to reference a large number of environment variables in buildspec.yml. However when you try to run the build you see an error telling you that the parameters you have specified have exceeded the number of characters allowed by the buildspec file. You need to find an alternative way to store these parameters, which of the following options would you recommend?","explanation":"Use Amazon Systems Manager Parameter Store to store large environment variables and then retrieve them from your buildspec file. Amazon EC2 Systems Manager Parameter Store can store an individual environment variable (name and value added together) that is a combined 4,096 characters or less.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html","title":"Troubleshooting CodeBuild"},{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html","title":"Build Specification Reference for CodeBuild"}],"answers":[{"id":"913d4a9f910c7f6b836d623b39131480","text":"Use Systems Manager Parameter Store","correct":true},{"id":"fdc76a3ff4a2b80aefd90a5dd389f44c","text":"Store the variables as dependencies within the application code","correct":false},{"id":"31a2f312e14f62be70158dff280e56cb","text":"Store the variables as key value pairs in S3","correct":false},{"id":"18f72b8f598e9cc0d20295158be172dd","text":"Store the variables as key value pairs in DynamoDB","correct":false}]},{"id":"15a0fdc7-3a7a-42b2-a937-ccfda81d4261","domain":"mon-trb","question":"You have developed a CloudFormation stack in the AWS Management Console. You have a few small number of CloudFormation stacks saved in the Region in which you are operating in. When you launch your stack that contains many EC2 resources, you receive the error Status=start_failed. How would you troubleshoot this issue?","explanation":"Verify that you didn't reach a resource limit. For example, the default number Amazon EC2 instances that you can launch is 20. If you try to create more Amazon EC2 instances than your account limit, the instance creation fails and you receive the error Status=start_failed. Also, during an update, if a resource is replaced, AWS CloudFormation creates new resource before it deletes the old one. This replacement might put your account over the resource limit, which would cause your update to fail. You can delete excess resources or request a limit increase. Saving the template in the CLI or waiting a few minutes will have no impact. The default limit for CloudFormation stacks is 200 and the question explicitly states that there are only a very small number of existing stacks.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html","title":"Troubleshooting AWS CloudFormation"}],"answers":[{"id":"dae0c844938d270767115f5f50079227","text":"Use the Support Center in the AWS Management Console to request an increase in the number of CloudFormation stacks.","correct":false},{"id":"57b1bceff40ae164e764946db0128ea0","text":"Save the template via the AWS CLI.","correct":false},{"id":"2e0c1157e1b5a9bc0a985d53a1a60aa6","text":"Wait a few minutes before saving the template and retry the process.","correct":false},{"id":"3e994bcee300070c97a3c141bc9967bd","text":"Use the Support Center in the AWS Management Console to request an increase in the number of EC2 instances.","correct":true}]},{"id":"87fa94ce-da14-4dfe-95b7-2664251e3644","domain":"development","question":"What attribute should be set on a message when sent to a SQS queue that prevents the message from becoming visible to consumers of the queue for 300 seconds?","explanation":"DelaySeconds allows the delivery of a message to be delayed for between 0 (default) and 900 seconds before the message becomes visible to consumers of the queue for the first time.\n\nVisibilityTimeout controls the time a message is hidden from consumers *after* it has been consumed.\n\nMessageRetentionPeriod controls how long a message remains available in a queue before it is discarded.\n\nReceiveMessageWaitTimeSeconds controls polling wait time.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html","title":"Amazon SQS Delay Queues"}],"answers":[{"id":"4052d7654cde58db70c4c7bddc897056","text":"DelaySeconds","correct":true},{"id":"7d24beb123793541bb62974b1d96000c","text":"ReceiveMessageWaitTimeSeconds","correct":false},{"id":"32a09171a5c0dc0e3d105afdc109127a","text":"VisibilityTimeout","correct":false},{"id":"3ecef87637aa97f59f1688e608394dab","text":"MessageRetentionPeriod","correct":false}]},{"id":"eebfef11-98bd-48f5-9775-b70b3600480a","domain":"development","question":"You are working on updates to your .NET application which has been deployed using Elastic Beanstalk. Your environment consists of 4 EC2 instances, as well as a number of different Lambda functions and DynamoDB tables. The application requires at least 2 instances to cope with the average workload and a minimum of 3 instances to cope with peak-time traffic. The Project Manager has asked you to roll out the updates as quickly as possible. Which of the following deployment strategies do you recommend?","explanation":"An all-at-once deployment deploys to all instances simultaneously which will put all of your web servers out of action at once. Rolling with additional batch launches an extra batch of instances before starting the deployment, to maintain full capacity. However, full capacity is not required in this scenario. Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version; this is not required in this scenario. You can use a rolling update with a batch size of 25%, to ensure that 75% of your servers remain available at any time.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html","title":"Elastic Beanstalk Deployment Options"}],"answers":[{"id":"11efd9ae6f76e706e3f1b34d97584ebc","text":"Immutable","correct":false},{"id":"f4920797afb92022a9c6608efcd86317","text":"Rolling","correct":true},{"id":"ff2713a6181db42fded101c670bbd0dd","text":"Rolling with additional batch","correct":false},{"id":"d5b066eef81dedf1d0352f27d2128586","text":"All at once","correct":false}]},{"id":"ebcc48d0-3f87-4ad4-b2b9-e623ea736c5e","domain":"mon-trb","question":"You are attempting to upload a number of objects to S3, however you keep seeing the following error message: \"AmazonS3Exception: Internal Error; Service: Amazon S3;\" Which of the following is the best explanation for this kind of error?","explanation":"This is an Internal Error which indicates that Amazon S3 is unable to handle the request at that time. Internal errors or server-side errors have a 5xx status code, whereas client-side errors have a 4xx status code.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/http-5xx-errors-s3/","title":"S3 5XX Errors"},{"url":"https://docs.aws.amazon.com/systems-manager/latest/APIReference/CommonErrors.html","title":"Common Errors and Status Codes"}],"answers":[{"id":"f55fb827446b5745d21641748b94a97c","text":"This is a 500 type error, which is a client-side error","correct":false},{"id":"a78bc5137bd856fc7bf93d66a03b40ca","text":"This is a 400 type error, which is a server-side error","correct":false},{"id":"f87b00b2711524f15807d3bd5811122e","text":"This is a 400 type error, which is a client-side error","correct":false},{"id":"a529f4cb171bbd153acf7a4c3f3116a0","text":"This is a 500 type error, which is a server-side error","correct":true}]},{"id":"7e589557-fe47-4b17-8c0a-f4afe1a9c764","domain":"mon-trb","question":"Your application servers are behind an Application Load Balancer with sticky sessions configured. However during busy times you are occasionally finding that one of your application servers is becoming overloaded. Which of the following options could help avoid this from happening?","explanation":"The use of Sticky Sessions means that requests which are part of the same session get routed to the same target, which may cause the host to become throttled. ElastiCache can be accessed by multiple servers, allowing the load to be distributed more evenly.","links":[{"url":"https://aws.amazon.com/caching/session-management/","title":"Session Management"}],"answers":[{"id":"88efe13a943c7fa646b910592ecda2f9","text":"Store session state in memory","correct":false},{"id":"b225818943ba4680b8e7dc9d9c376359","text":"Store session state in RDS","correct":false},{"id":"be8019a0436605aff98fc69abd0feeec","text":"Store session state locally on the EC2 instance","correct":false},{"id":"2aa6ae3b5971bedd1a05b4d5636afbac","text":"Store session state in an ElastiCache cluster","correct":true}]},{"id":"1a967bcb-8238-4c95-b10c-e832a682b14b","domain":"refactoring","question":"Which of the following protocols does API Gateway support?","explanation":"API Gateway supports RESTful APIs, however the legacy SOAP protocol, which returns results in xml format, is also supported in pass-through mode.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html","title":"API Gateway Developer Guide"}],"answers":[{"id":"4de91f0fcb9cd3a20b6b1d64610a0a3d","text":"SOAP","correct":true},{"id":"524de3d2ade4544176f60702b36fbfdf","text":"GraphQL","correct":false},{"id":"a4d55f2834854e93ff57ecb44d1c84cc","text":"O API","correct":false},{"id":"50780f47f6839d47d60bc4555ee00c3f","text":"REST","correct":true}]},{"id":"5cd0d201-9b93-42f0-9ae7-585263307009","domain":"development","question":"You've been asked to create a Web application with an endpoint that can handle thousands of REST calls a minute.  What AWS service can be used in front of an application to assist in achieving this?","explanation":"Questions containing 'REST' are usually related to APIs, so API Gateway looks the best answer.  Elastic Beanstalk is a service which allows you to run applications without understanding the infrastructure and can be discounted, as can Global Accelerator which is a networking service that improves the availability and performance of applications.  CloudFront can be used in conjunction with API Gateway to assist in geographically disparate calls, but won't process calls by itself.","links":[{"url":"https://aws.amazon.com/api-gateway/faqs/","title":"Amazon API Gateway FAQs"}],"answers":[{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false},{"id":"9ce65e2b30ed635c84bef82218a94fdf","text":"Global Accelerator","correct":false},{"id":"2ef9dd82927a3196ca2df3fc0cdf2e0b","text":"API Gateway","correct":true},{"id":"3d6cbd7db2a4fa389808ea6f4a5fc1bc","text":"Elastic Beanstalk","correct":false}]},{"id":"eeabb3b1-06bd-4a2f-89e9-425c07670c09","domain":"security","question":"Your application uses the STS API call AssumeRoleWithWebIdentity to enable access for users who have authenticated using a Web ID provider. Which of the following best describe what is returned by a successful call to AssumeRoleWithWebIdentity?","explanation":"AssumeRoleWithWebIdentity returns a set of temporary credentials, giving the user temporary access to AWS. It also returns an Amazon Resource Name (ARN) and the assumed role ID, which are identifiers that you can use to refer to the temporary security credentials.","links":[{"url":"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html","title":"STS: AssumeRoleWithWebIdentity"}],"answers":[{"id":"5cf1fe7b63a8a99d7d3457d8b021e2c4","text":"AssumeRoleWithWebIdentity returns an ARN of the IAM user that the user is allowed to assume temporarily","correct":false},{"id":"c4dee05a317f3836954c0577fb8df356","text":"AssumeRoleWithWebIdentity returns an assumed role ID which the user is allowed to assume temporarily","correct":false},{"id":"33401d97362173f45bb04e5a8c41b8a3","text":"AssumeRoleWithWebIdentity returns a set of temporary credentials (access key ID, secret access key and security token) which give temporary access to AWS services","correct":true},{"id":"b80ac03d28d57f31929047a4ce86a48c","text":"AssumeRoleWithWebIdentity returns an ARN of the IAM role that the user is allowed to assume temporarily","correct":false}]},{"id":"787db7cd-ff6a-46b1-83e7-9fe990e22c5e","domain":"security","question":"A developer is implementing an image sharing website hosted on AWS. Images are stored in a separate S3 bucket. During testing, it is discovered that the website does not load properly because the images are being blocked by the browser. How can the developer resolve this issue?","explanation":"As a security feature, resources in one domain cannot be access by a web-page resources in a different domain. Cross-origin resource sharing (CORS) is a mechanism that enables resources on a web page to be requested from a different domain outside the domain from which the first resource was served. If we are hosting images in a separate S3 bucket, this bucket will have a different domain. To allow these resources to be accessed by the web page domain, we need to configure CORS on the S3 bucket.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html","title":"Cross-Origin Resource Sharing (CORS)"}],"answers":[{"id":"d89d59d8223823c1f20371c88b73fc27","text":"Configure CORS on the S3 bucket.","correct":true},{"id":"1a29c427267cabba7bd3106de626aa13","text":"Make the S3 bucket public.","correct":false},{"id":"8f55313a69544c54c5bf4c4a71c9d935","text":"Enable versioning on the S3 bucket.","correct":false},{"id":"216ba600e977ed0524a78ecba4335037","text":"Enable S3 Transfer Acceleration.","correct":false}]},{"id":"8caa9788-5c49-413f-b0c0-6f515af3fe5f","domain":"security","question":"An organization has mandated that all files stored in their newly created S3 bucket, 'top-secret-documents', must be encrypted using a Customer Master Key stored in KMS.\n\nWhat is the best way to enforce this requirement?","explanation":"To ensure objects are stored using a specific type of server-side encryption, you must use a bucket policy. In this case, the bucket policy must ensure the encryption type matches SSE-KMS.\n\nSetting a default encryption type on the bucket is not sufficient, as the default only applies to uploaded objects that do not specify any encryption type. For example, if the default encryption is set to AWS-KMS, but an object is uploaded with the header `x-amz-server-side-encryption: AES256`, the resulting object is encrypted using SSE-S3, not SSE-KMS.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html","title":"Protecting S3 Data Using Server-Side Encryption"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"Example bucket policy for enforcing SSE"}],"answers":[{"id":"8be8b17570d837c0301070a5b8fde1cb","text":"Add a bucket policy that denies PUT operations that don't contain the HTTP header `x-amz-server-side-encryption: SSE:C`","correct":false},{"id":"7910fc40ce77db47b28825a906611f8d","text":"Enable S3 Default encryption and select AWS-KMS","correct":false},{"id":"21b68cac2d99a4c11c51d224971ee62f","text":"Add a bucket policy that denies PUT operations that don't contain the HTTP header `x-amz-server-side-encryption: aws:kms`","correct":true},{"id":"62e28c6b06e8b3895db973aa3d03c4fd","text":"Add a bucket policy that denies PUT operations that don't contain the HTTP header `x-amz-server-side-encryption: AES256`","correct":false}]},{"id":"ee1f9799-b2a7-445f-bb89-932b725a8374","domain":"deployment","question":"A business-critical application is deployed using CloudFormation. The team would like to prevent accidental deletion of the stack. How can this be achieved most efficiently?","explanation":"Termination Protection stack option can be enabled to prevent accidental deletion of an entire CloudFormation stack. It is possible to use IAM policy to prevent deletion of a CloudFormation stack, however, this is not the optimal solution from operations and management perspective. The DeletionPolicy CloudFormation attribute applies to individual resources, not an entire stack. There is no DeletionProtection attribute in CloudFormation.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html","title":"Protecting a Stack From Being Deleted"}],"answers":[{"id":"8093dacb1a766d0f91fa60e48baa87ed","text":"Set Stack Termination Protection to Enable.","correct":true},{"id":"6983aeb4c0d42bb293c8ec93966aedbb","text":"Set the DeletionProtection to True in the CloudFormation template.","correct":false},{"id":"558cc27970e2b64a29bdc85f381b8cb9","text":"Set the DeletionPolicy to Retain in the CloudFormation template.","correct":false},{"id":"eee064b70f79422e8511f18b251253ef","text":"Create IAM Policy with Effect of Deny for 'cloudformation:DeleteStack' Action.","correct":false}]},{"id":"15af205c-dba5-4175-8236-b5a7af3e70ec","domain":"deployment","question":"You are a developer for a news, entertainment, lifestyle, and fashion website. User traffic has steadily increased month-over-month, and you are now tasked with cost optimizing the website. The website is currently served from an EC2 instance that is part of an auto-scaling group behind an elastic load balancer. Your manager and CTO have approved a complete re-structuring of the websites architecture in order to accommodate future growth. How would you optimize your application in the MOST cost-effective way?","explanation":"Serverless approaches are ideal for applications where load can vary dynamically. Using a serverless approach means no compute costs are incurred when there is no end user traffic, while still offering instant scale to meet high demand, such as a flash sale on an e-commerce site or a social media mention that drives a sudden wave of traffic. Compared to traditional infrastructure approaches, it is also often significantly less expensive to develop, deliver, and operate a web or mobile backend when it has been architected in a serverless fashion. The question allows architectural restructuring and serverless would be the best approach. Moving the application on-premise would not be cost effective as it would change your from variable to operational costs. Changing the scale-in policy would help reduce costs but would not be as cost effective as going serverless. Implementing CloudFront would increase costs overall.","links":[{"url":"https://d0.awsstatic.com/whitepapers/optimizing-enterprise-economics-serverless-architectures.pdf","title":"Optimizing Enterprise Economics with Serverless Architectures"}],"answers":[{"id":"59ba0fde539e6373d2e3b860d2ce4aca","text":"Move the application on-premise. You'll be able to fully manage the application, and purchase additional servers when appropriate.","correct":false},{"id":"ca56c5360ae4b7c7063a9f870f717088","text":"Implement CloudFront in front of the EC2 instance as the origin.","correct":false},{"id":"f4f5510799286ba3bec7cef229ad2bc8","text":"Move the website to a serverless application. Use S3 to host the website. Use a combination of Lambda and API Gateway to support dynamic API requests.","correct":true},{"id":"1a981591e9c0499dbcb77019a1598dbc","text":"Edit the scale-in policy within auto scaling to terminate instances aggressively when demand is low.","correct":false}]},{"id":"616a7b1c-bc17-42a4-b361-74af9a86607f","domain":"development","question":"A financial services company is implementing a payments processing application utilizing DynamoDB tables for its data store. To process payments, the application needs to perform a write operation on a sequence of items, and roll back and reverse all operations in case of any one faulty operation.  What is the best method to accomplish this requirement?","explanation":"DynamoDB transactions feature provides ability to group multiple items into a single atomic transaction and perform all-or-nothing coordinated operations.  This can be done programmatically using the TransactWriteItems operation. The BatchWriteItem operation does not meet the question requirements as it does not guarantee that the actions will be performed on all items as a single atomic coordinated operation. It is possible that only some of the actions in the batch succeed while the others do not. Updating the payments application is not the ideal solution.  It requires application code change and tracking all connected operations and reversing them as required is not trivial to implement.  Using the native transaction ability provided by DynamoDB is a better option.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html","title":"Amazon DynamoDB Transactions: How It Works"}],"answers":[{"id":"3b08e25699a7082805a9be123c9acbbb","text":"DynamoDB does not support atomic transactions.  Use relational database (such as RDS) that supports atomic transactions.","correct":false},{"id":"aa5647f8e2c8e7147097b79d2b2a5555","text":"Use the BatchWriteItem operation.","correct":false},{"id":"758e6d47da6512d38526cfcfa39bb5c1","text":"Update the application to manage and perform roll-back operations.","correct":false},{"id":"cb748237c6e223d0f4506cff55c6b259","text":"Use the TransactWriteItems operation.","correct":true}]},{"id":"6d133985-53e5-48d7-a1e9-6db8bb940208","domain":"security","question":"You are working as a Developer for an online retailer. Your Security Architect has requested that any files stored in S3 must be encrypted. However some teams are continuing to upload their files without encrypting them. Which of the following will ensure that only encrypted data is uploaded?","explanation":"There are a few different ways to enforce encryption, however from the provided options, the use of a bucket policy to reject requests that do not include encryption in their header is the best answer","links":[{"url":"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/","title":"How to Prevent Uploads of Un-encrypted Objects to Amazon S3"}],"answers":[{"id":"0f6a3c5c345549f63a0da92ac93e45cf","text":"Use a bucket policy that only allows PUT operations which include the x-amz-server-side-encryption parameter in the request header","correct":true},{"id":"d61960e2fc2a82679514fe6f3401150f","text":"Select the Encrypted Files Only checkbox in the S3 Permissions tab in the AWS console","correct":false},{"id":"e7d6b8da11c238d437edeadb18bf5493","text":"Create a bucket ACL that only allows PUT operations which include the x-amz-encryption parameter in request header","correct":false},{"id":"da402b6935d7bb0b6868419dc1fcf54f","text":"Tell all team members to include the x-amz-encryption parameter in request header","correct":false}]},{"id":"1a8f6ca4-1edf-4cb3-8dce-63550ef898d0","domain":"security","question":"You are working on a web application which handles confidential financial data. The application runs on a few EC2 instances which are behind an Elastic Load Balancer. How can you ensure the data is encrypted end-to-end in transit between your ELB and EC2 instances?","explanation":"Terminating secure connections at the load balancer and using HTTP on the backend might be sufficient for your application. However, if you are developing an application that needs to comply with strict external regulations, you might be required to secure all network connections. First, add a secure listener to your load balancer, then configure the instances in your environment to listen on the secure port and terminate HTTPS connections.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-endtoend.html","title":"Configuring End-to-End Encryption"}],"answers":[{"id":"2c26367f893c5f15ae242c1965051017","text":"Terminate HTTPS connections on your EC2 instances","correct":true},{"id":"c2bc73f05693654154a5a002cf77d3e1","text":"Configure a secure listener on your load balancer","correct":true},{"id":"cb9dd4ff2be4893bd209d207f1898600","text":"Perform SSL termination using Lambda","correct":false},{"id":"9f4e11571be1c566b8e30fdd753f0802","text":"Configure the instances in your environment to listen on the secure port","correct":true},{"id":"ed90effd6942a7cffc042b6cb610b798","text":"Perform SSL termination on the load balancer","correct":false}]},{"id":"97b2f9cc-962b-4c75-82c3-a44815644776","domain":"mon-trb","question":"You are developing a new application using Lambda, API Gateway, S3 and DynamoDB. You would like to record information about incoming and outgoing HTTP requests as well as latency incurred by each component. You have multiple versions of the application to cater for your Development, UAT, Performance Test and Production environments. What is the most efficient way to collect this information and group it according to which environment it relates to?","explanation":"AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, micro-services, databases and HTTP web APIs. When you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segment document as annotations and metadata. Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations","title":"X-Ray Annotations and Metadata"},{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html","title":"Searching for Traces in the AWS X-Ray Console with Filter Expressions"}],"answers":[{"id":"d2ae15c70b6c5a546c571203b1a18474","text":"Use CloudTrail to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false},{"id":"c57540f9c7ee62b718518b73d8ea536c","text":"Use X-Ray to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":true},{"id":"7596c1ed59077a183e47b002a53bb84a","text":"Use CloudFormation to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false},{"id":"8f0f2ab2fa58b6ddef1f41eb2fe56872","text":"Use CloudWatch to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false}]},{"id":"e2661600-6bfa-446a-8ff9-b760c629b0eb","domain":"deployment","question":"You need to push a docker image to your Amazon ECR repository called my-repository located in us-east-1. Which of the following commands do you need to run in order to achieve this?","explanation":"The aws ecr get-login command provides an authorization token that is valid for 12 hours. You need to run the command which was returned by the ecr get-login command to authorize you to push images to the ECR repository. For a full list of the steps, see the link below.","links":[{"url":"https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-basics.html","title":"Docker Basics for Amazon ECR"}],"answers":[{"id":"014712b54a814d72bd175deae6bdb942","text":"Run: docker push -i my-image -r 123456789012.dkr.ecr.us-east-1.amazonaws.com/my-repository","correct":false},{"id":"6d72d9334d7aba24d31008dc69bcacb1","text":"Run: docker tag -i my-image latest \r\r Then run: docker push 123456789012.dkr.ecr.us-east-1.amazonaws.com/my-repository","correct":true},{"id":"b252c8c39c36e240388f79fe234c3fef","text":"Run: aws ecr get-login --no-include-email --region us-east-1 \r\r Run the docker login command that was returned in the previous step. \r\r Then run: docker push 123456789012.dkr.ecr.us-east-1.amazonaws.com/my-repository","correct":true},{"id":"22f8f304a6f997cf4d1f481dab61b497","text":"Run: aws ecr get-login --no-include-email --region us-east-1 \r\r Then run: docker push 123456789012.dkr.ecr.us-east-1.amazonaws.com/my-repository","correct":false}]},{"id":"99e1be6c-c87a-45fd-b1a4-80ee4e4e1fe9","domain":"deployment","question":"You are creating a DynamoDB table to manage your customer orders, which of the following attributes would make a good Sort Key?","explanation":"A well designed Sort key allows you to retrieve groups of related items and query based on a range of values, e.g. a range of dates. In this case, Order Date is the best choice as it will allow users to search based on a range of dates","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-sort-keys.html","title":"DynamoDB Sort Keys"}],"answers":[{"id":"8deaf45b8d4daa4827510e469b744fdb","text":"CustomerID","correct":false},{"id":"b5a34ba8eab605ce7cebca74791768c8","text":"OrderDate","correct":true},{"id":"f6a56b98d7e5fa6571618bdc13db0c77","text":"OrderNumber","correct":false},{"id":"bb96cafb784d4109a905317afdff8f71","text":"ProductID","correct":false}]},{"id":"4a84dc95-7697-4a71-8580-d525a1a85a01","domain":"deployment","question":"You have an application running on a number of Docker containers running on AWS Elastic Container Service. You have noticed significant performance degradation after you made a number of changes to the application and would like to troubleshoot the application end-to-end to find out where the problem lies. What should you do?","explanation":"Within a micro-service architecture, each application component runs as its own service. Micro-services are built around business capabilities, and each service performs a single function. So if you want to add X-Ray to a Dockerized application, it makes the most sense to run the X-Ray daemon in a new Docker container and have it run alongside the other micro-services which make up your application.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html","title":"Running the X-Ray Daemon on Amazon ECS"},{"url":"https://aws.amazon.com/microservices/","title":"What Are Microservices?"},{"url":"https://aws.amazon.com/getting-started/projects/break-monolith-app-microservices-ecs-docker-ec2/","title":"From Monolithic Applications To Microservices"}],"answers":[{"id":"92f001f3f23ff4bd8620bed17c3978dc","text":"In the ECS console, configure the application to send telemetry records to AWS X-Ray","correct":false},{"id":"f25dfc897ebef8cc66db27b116ee6416","text":"Deploy the AWS X-Ray daemon onto an EC2 instance","correct":false},{"id":"27c3d436162806a72afb7bf932c81dfa","text":"Deploy the AWS X-Ray daemon as a new container alongside your application","correct":true},{"id":"4edd5ed4f080880075a9f5769b027d04","text":"Deploy the AWS X-Ray daemon to each of the Docker containers that run your application","correct":false}]},{"id":"bd31be6d-dcf3-4103-b281-72d44c552186","domain":"refactoring","question":"You are working on a social media application which allows users to share BBQ recipes and photos. You would like to schedule a Lambda function to run every 10 minutes which checks for the latest posts and sends a notification including an image thumbnail to users who have previously engaged with posts from the same user. How can you configure your function to automatically run at 10 minute intervals?","explanation":"You can direct AWS Lambda to execute a function on a regular schedule using CloudWatch Events. You can specify a fixed rate - for example, execute a Lambda function every hour or 15 minutes, or you can specify a cron expression.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html","title":"Using AWS Lambda with Amazon CloudWatch Events"}],"answers":[{"id":"2d428d9cc2b8ba9e3f0fb6f7b7c5d585","text":"Use AWS SWF to schedule the function","correct":false},{"id":"2255149fa723010e237589bd73a85d2d","text":"Use EC2 with cron to schedule the function","correct":false},{"id":"547b2838c15c420120f6d664b99d6dbb","text":"Use CloudWatch Events to schedule the function","correct":true},{"id":"34d04b0eb9dd5f8f69d5c60eb65f6bc8","text":"Use Lambda with cron to schedule the function","correct":false}]},{"id":"86524d3b-b3e8-46ca-97a2-e12d4edfabed","domain":"development","question":"Which of the following best describes Amazon ECS?","explanation":"ECS stands for Elastic Container Service: It manages running containers on your EC2 instances. It does not act as a scheduler and it is neither serverless nor software that you manage.","links":[{"url":"http://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html","title":"About Amazon ECS"}],"answers":[{"id":"8180f2f15ee5ff1c554855a0352e23bf","text":"The Elastic Container Scheduler is software that you can run and manage to orchestrate many running Docker containers.","correct":false},{"id":"f0fbcb4f668b638f39ce41a6972d9a94","text":"The Elastic Container Service is software that you can run and manage to orchestrate many running Docker containers.","correct":false},{"id":"a966ed9a7fc5f750acbbef6754f3ad57","text":"The Elastic Container Scheduler is a serverless system to manage running many Docker containers in a flexible and cost-effective way.","correct":false},{"id":"d3958d5a87a697631979b920c68a9ae2","text":"The Elastic Container Service is a service that manages running Docker containers on a group of your EC2 instances.","correct":true}]},{"id":"0c156bea-473c-4216-b344-51ad15046bdd","domain":"development","question":"Which of the following platforms are supported in ElasticBeanstalk","explanation":"Elastic Beanstalk provides platforms for programming languages (Java, PHP, Python, Ruby, Go), web containers (Tomcat, Passenger, Puma) and Docker containers, with multiple configurations of each.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html","title":"Elastic Beanstalk Supported Platforms"}],"answers":[{"id":"01fc3141bdcacc23a2e09a5e25ea126b","text":"Single Container Docker","correct":true},{"id":"8fd82b8864d71ed7fa12b59e6e34cd1c","text":"Chef","correct":false},{"id":"10bff31c030c63d11bd1e7ab82759f6d","text":"Java with Tomcat","correct":true},{"id":"3b2819dd4c24eda2faf2052eef449551","text":"Node.js","correct":true}]},{"id":"2d9b36ae-20bb-4dfd-94b0-0956312b6b90","domain":"development","question":"Which of the following specifies the correct run order for lifecycle hooks for an In-Place deployment using CodeDeploy?","explanation":"The logical sequence is: ApplicationStop, BeforeInstall, AfterInstall, ApplicationStart","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order","title":"In-Place Deployment Run Order of Hooks"}],"answers":[{"id":"ab9490190c13b314076b0343b84e008c","text":"BeforeInstall, ApplicationStop, AfterInstall, ApplicationStart","correct":false},{"id":"c4048c2c844fbbab9e3eca808aabd8fe","text":"BeforeInstall, AfterInstall, ApplicationStop, ApplicationStart","correct":false},{"id":"274e4d32692fa00bae2f7aceb66b5ee3","text":"ApplicationStop, BeforeInstall, ApplicationStart, AfterInstall","correct":false},{"id":"20c91632d36d53294ddd6fa1f3ee974f","text":"ApplicationStop, BeforeInstall, AfterInstall, ApplicationStart","correct":true}]},{"id":"a911b79a-68c9-41f0-af02-0f57c3ccdc66","domain":"mon-trb","question":"A company's services are protected by AWS WAF. The development team would like to enable logging on the WAF to get detailed information about traffic that is analyzed by the web ACLs in order to enhance their troubleshooting efforts. Which service can the team use to collect AWS WAF logs?","explanation":"In order to enable and configure AWS WAF logs, a Kinesis Data Firehose is required for delivery of the logs to the destination.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/logging.html","title":"Logging Web ACL Traffic Information"}],"answers":[{"id":"72e5e39b79c3d4d99d9c68a6a5e4d9f0","text":"Kinesis Data Firehose","correct":true},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"33eb5c1f2566526637e791c925c4c505","text":"VPS Flow Logs","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false}]},{"id":"d6508f82-f5de-4013-a777-d801a0816ff6","domain":"development","question":"A developer has a requirement to trigger a Lambda function once every 24 hours. What is the best way of achieving this requirement?","explanation":"CloudWatch Events allows targets to be triggered using a Schedule Expression. A Schedule Expression can define a rate; for example, every 24 hours. Or can accept a standard cron job expression.\n\nCloudWatch Events supports many targets, including Lambda.\n\nInvoking the function from a cron job running on an EC2 instance would meet the requirements, but would require additional effort and cost; therefore, is not the best solution.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html","title":"CloudWatch Events supported targets"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html","title":"CloudWatch Events Scheduling"}],"answers":[{"id":"478234fe13f76893bfa2f3f57911bcb3","text":"Schedule a trigger in CloudWatch Events","correct":true},{"id":"49b7adae681c4743b1eac8bb6cdaca00","text":"Invoke the Lambda function from a cron job running on an EC2 instance.","correct":false},{"id":"bb3413b8174c30b6418449092039dfc2","text":"Add a message into SQS that invokes the Lambda function. As part of the Lambda function's code, add a new message into SQS to re-invoke the function with a DelaySeconds equal to 24 hours.","correct":false},{"id":"9ad7f45031f00ab56d5450da9aaa7432","text":"Schedule the trigger in the Lambda Runtime Scheduler.","correct":false}]},{"id":"1a6ef8eb-4675-4004-ab47-3f8682a6e038","domain":"security","question":"Your EC2 instance needs to access files located in an S3 bucket, what is the best way to enable access?","explanation":"Using an IAM role associated with the EC2 instance is the recommended way, storing credentials locally is not recommended.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html","title":"IAM Roles"}],"answers":[{"id":"6642e43e9a2808c204a04b7c07a1f5ac","text":"Create a new IAM user and grant read access to S3. Store the user's credentials locally on the EC2 instance and configure your application to supply the credentials with each API request","correct":false},{"id":"f47981b92302bc40c2c9dc5f0ec40a23","text":"Create an IAM role with read access to S3 and assign the role to the EC2 instance","correct":true},{"id":"7e00e264a60465681fb267faf13307e9","text":"Configure a bucket policy which grants read access based on the EC2 instance name","correct":false},{"id":"246491a279b8f11f71b3c083962c0e75","text":"Create a new IAM role and grant read access to S3. Store the role's credentials locally on the EC2 instance and configure your application to supply the credentials with each API request","correct":false}]},{"id":"45bb762d-32a6-4fc7-a9e3-1377f0161979","domain":"security","question":"Which of the following activities are the responsibility of the customer?","explanation":"Security and Compliance is a shared responsibility between AWS and the customer. The customer assumes responsibility and management of the guest operating system (including updates and security patches), other associated application software as well as the configuration of the AWS-provided security group firewall. AWS is responsible for protecting the infrastructure that runs all of the services offered in the AWS Cloud. This infrastructure is composed of the hardware, software, networking, and facilities that run AWS Cloud services.","links":[{"url":"https://aws.amazon.com/compliance/shared-responsibility-model/","title":"AWS Shared Responsibility Model"}],"answers":[{"id":"2b169fd2a3342cf14cd9fdfca94943c5","text":"Security Group configuration settings","correct":true},{"id":"24950338a19d0ddaa9b785b69709702f","text":"Encryption of sensitive data","correct":true},{"id":"a66d0f8d16b1d93bbe53e387d4b62b37","text":"Controls around who can physically access the data center","correct":false},{"id":"e64e7c083b43e01c37b09547a9d7fa31","text":"Management of user credentials","correct":true},{"id":"a505eb81276d69b88b77d5b605ad4a9a","text":"Safe disposal of storage devices","correct":false}]},{"id":"764c68cc-0596-485a-b5a0-4bb272444d02","domain":"deployment","question":"Which of the following services enables you to automatically build, test and release new software whenever a developer makes an update to their code?","explanation":"CodeBuild only builds your code, it won't deploy it to your environment. CloudFormation is used to deliver Infrastructure As Code. CodeCommit manages your source code. CodeDeploy can be used to deploy code, but in isolation, it cannot create an automated release process. CodePipeline automates the build, test, and can be used to deploy phases of your release process every time there is a code change, based on the release model you define.","links":[{"url":"https://aws.amazon.com/codepipeline/","title":"AWS CodePipeline"}],"answers":[{"id":"5f6f48261d96567b3014b43c23382021","text":"CodePipeline","correct":true},{"id":"07929940c38c1e9e91ecf67c01ccfa73","text":"CodeDeploy","correct":false},{"id":"56c20df941e069e1a35a1871f662298a","text":"CodeCommit","correct":false},{"id":"f8d1a2a80933c71d9a65e3d825976bda","text":"CodeBuild","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":false}]},{"id":"9554e10d-8fab-4545-b3ea-a756133ffab3","domain":"refactoring","question":"You need to retrieve some data from your DynamoDB table, which of the following methods would consume the least provisioned Capacity Units?","explanation":"A Query is generally more efficient than a Scan operation. Eventual consistency reads use up fewer Read Capacity Units than strongly consistent reads","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html","title":"Query Vs Scan"},{"url":"https://aws.amazon.com/dynamodb/pricing/provisioned/","title":"Provisioned Capacity"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html","title":"DynamoDB Read Consistency "}],"answers":[{"id":"5d682b15ed6ff4d0b5100242fa11d4ea","text":"Scan with eventual consistency","correct":false},{"id":"8747d18821697e99310487487007af03","text":"Scan with strong consistency","correct":false},{"id":"ac9570514b07db98ad6cc89ea1cc8741","text":"Query with strong consistency","correct":false},{"id":"d87fc0cde8df11c904ff6fe25cac6bfc","text":"Query with eventual consistency","correct":true}]},{"id":"c1271c56-09c1-44e0-9e24-840abbf8ad96","domain":"development","question":"You are deploying a new version of your application using a CodeDeploy In-Place upgrade. At the end of the deployment you test the application and discover that something has gone wrong. You need to roll back your changes as quickly as possible. What do you do?","explanation":"With an In-Place upgrade you will need to redeploy the original version. Only a Blue / Green upgrade allows you to keep the original instances and roll back by routing all requests to the original instances","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html#deployments-rollback-and-redeploy-manual-rollbacks","title":"CodeDeploy Rollback and Redeploy"}],"answers":[{"id":"079d9ac5765ea898c4d08a65d8e35c53","text":"Configure your load balancer to send all incoming requests to the original instances running the old version of the application","correct":false},{"id":"63d25b1e978fde1df3b91bb3d47c31a6","text":"Use CodeDeploy to redeploy the previous version of the application","correct":true},{"id":"f7d2972260a5229eaad04f071022e255","text":"Use the CodeDeploy roll back feature to seamlessly roll back to the previous version","correct":false},{"id":"d2949de6ed3d3a5197775436b9302635","text":"Point all incoming requests to your development environment while you fix the problem with the failed deployment","correct":false}]},{"id":"847e9972-b2da-4c6c-b5b5-74fde22cf193","domain":"refactoring","question":"Your application is storing a lot of data in an S3 bucket called mybucket and is routinely exceeding 100 requests per second using a mix of GET, PUT and DELETE operations. Which of the following naming strategies will ensure low latency performance with S3?","explanation":"Prior to July 2018, AWS recommendation was to use random keyname prefixes to ensure objects were stored on separate partitions in order to give the best performance for mixed workloads. Since July 2017 this is no longer necessary, however we expect the use of random key names to still be tested in the exam so it is still important to be aware of this approach.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/","title":"Updated guidance on S3 performance - for reference"}],"answers":[{"id":"214a113d4fbf2063b6cbae4047f7e027","text":"mybucket/2018-02-07-12-00-00/cust26347456/8761file1.txt","correct":false},{"id":"a4df002d6a19b2b0edf9097af5315fce","text":"mybucket/customers/cust26347456/file1.txt","correct":false},{"id":"e327db1b1bbc74ad0be34391c8c54367","text":"mybucket/8761-2018-02-07-12-00-00/cust26347456/file1.txt","correct":true},{"id":"015205126ff1a0346b8753dbe82a2306","text":"mybucket/2018-02-07-12-00-00/cust26347456/file1.txt","correct":false}]},{"id":"f26b6227-d443-4a48-9a39-d52161ec3ef5","domain":"security","question":"What is the name of the service that allows users to use their social media account to gain temporary access to the AWS platform?","explanation":"Web Identity Federation is the services which allows users to authenticate with web Identity Providers like Facebook, Google and Amazon receive an authentication token and then exchange that token for temporary security credentials in AWS.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html","title":"Web Identity Federation"}],"answers":[{"id":"fba87e436c5368fd112fc79eddacea80","text":"Active Directory Authentication Services","correct":false},{"id":"91c75725a837ff613915a3cf3f8208e1","text":"Web Identity Federation","correct":true},{"id":"9cc6ff53d9bbecc10bacb7866e5e957d","text":"Web Confederation Services","correct":false},{"id":"876393862947dc478af3ab7b02c30ba9","text":"Facebook Sign In Service","correct":false}]},{"id":"868cb94f-ce60-4590-bfd2-60e8295cc413","domain":"security","question":"You are developing a online-banking website which will be accessed by a global customer base. You are planning to use CloudFront to ensure users experience good performance regardless of their location. The Security Architect working on the project asks you to ensure that all requests to CloudFront are encrypted using HTTPS. How can you configure this?","explanation":"Viewer Protocol Policy defines the protocols which can be used to access CloudFront content","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html","title":"Requiring HTTPS for Communication Between Viewers and CloudFront"}],"answers":[{"id":"659099387a57b1f316abf3c6afac459d","text":"Set the Session Protocol Policy to redirect HTTP to HTTPS","correct":false},{"id":"052066815497ced9f9852e55d66c6782","text":"Set the Request Protocol Policy to redirect HTTP to HTTPS","correct":false},{"id":"ae41866f6b2df14a344847e9629076db","text":"Set the User Protocol Policy to redirect HTTP to HTTPS","correct":false},{"id":"c40022183e6d5dd97e4c778332064ed2","text":"Set the Viewer Protocol Policy to redirect HTTP to HTTPS","correct":true}]},{"id":"3f74aa43-386f-49e4-bc1d-3e32a1282397","domain":"development","question":"An application uses DynamoDB table as its backend data store. Each item has size of 10KB.  The application needs to perform 100 eventually consistent read operations per second, and 50 write operations per second.  What is the provisioned WCU value required to meet these requirements?","explanation":"One write capacity unit is equivalent to one write per second for an item up to 1 KB in size.  Thus, the required WCU in this scenario is 10KB item size x 50 write operations per second = 500 WCU.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","title":"Provisioned Mode"}],"answers":[{"id":"6c9882bbac1c7093bd25041881277658","text":"250","correct":false},{"id":"7ef605fc8dba5425d6965fbd4c8fbe1f","text":"150","correct":false},{"id":"cee631121c2ec9232f3a2f028ad5c89b","text":"500","correct":true},{"id":"a9b7ba70783b617e9998dc4dd82eb3c5","text":"1000","correct":false}]},{"id":"4a430e9e-4943-4d13-9511-ba93d66d9eaf","domain":"development","question":"A developer is configuring CodeDeploy to deploy an application to an EC2 instance. The application's source code is stored within AWS CodeCommit.\n\nWhat permissions need to be configured to allow CodeDeploy to perform the deployment to EC2?","explanation":"CodeDeploy interacts with EC2 via the CodeDeploy Agent, which must be installed and running on the EC2 instance. During a deployment the CodeDeploy Agent running on EC2 pulls the source code from CodeCommit. The EC2 instance accesses CodeCommit using the permissions defined in its instance profile role; therefore, it is the EC2 instance itself that needs CodeCommit access.\n\nThe specific CodeCommit permission needed to pull code is `codecommit:GitPull`.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control-permissions-reference.html","title":"CodeCommit Permissions"},{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-configure.html","title":"Configure an Amazon EC2 Instance to Work with CodeDeploy"}],"answers":[{"id":"3801756a25b44dc5e1eaaf3a55f6c0fe","text":"Create an IAM policy with an acton to allow `codecommit:GitPull` on the required repository. Attach the policy to the EC2 instance profile role.","correct":true},{"id":"dcaaa5b141981d140fe32ea458888500","text":"Create an IAM Policy with an acton to allow `codecommit:GitPull` on the required repository. Attach the policy to CodeDeploy's Service role.","correct":false},{"id":"a5f1b9fe7264b6a78932a24f195b1064","text":"Create an IAM Policy with an acton to allow `codecommit:CreatePullRequest` on the required repository. Attach the policy to CodeDeploy's Service role.","correct":false},{"id":"ae4d1eac013f4a67d18129084085b41b","text":"Create an IAM policy with an acton to allow `codecommit:CreatePullRequest` on the required repository. Attach the policy to the EC2 instance profile role.","correct":false}]},{"id":"1c37eac2-4401-11ea-b77f-2e728ce88125","domain":"security","question":"Which of the following protocols are used to set up secure connections to AWS CodeCommit repositories?","explanation":"AWS allows you to use either the HTTPS or the SSH protocol to connect to CodeCommit repositories. Theres no option to select HTTP or RDP connections.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up.html","title":"Setting Up for AWS CodeCommit"}],"answers":[{"id":"765553e6c7ac8592c389acb9878a050a","text":"SSH","correct":true},{"id":"293c9ea246ff9985dc6f62a650f78986","text":"HTTP","correct":false},{"id":"0e8433f9a404f1f3ba601c14b026d321","text":"HTTPS","correct":true},{"id":"a66d0b3ece299ba53eafac86750cfb4a","text":"RDP","correct":false}]},{"id":"72b0a4e6-82a6-4355-aa09-cf28563f00ed","domain":"development","question":"You are developing an online gaming application which needs to synchronize user profile data, preferences and game state across multiple mobile devices. Which of the following Cognito features enables you to do this?","explanation":"Amazon Cognito Sync is an AWS service and client library that enable cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and web applications. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.","links":[{"url":"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html","title":"Amazon Cognito Sync"},{"url":"https://docs.aws.amazon.com/cognito/latest/developerguide/getting-started-with-cognito-sync.html","title":"Getting Started with Amazon Cognito Sync"}],"answers":[{"id":"0b422b4e655adc456bd9363c2dba938a","text":"Cognito Events","correct":true},{"id":"31aa670fdceaaf4289c74a1425e69d5b","text":"Cognito Streams","correct":false},{"id":"67cb5d76e7fadc7e245c0cce89ad6dbf","text":"Cognito Sync","correct":true},{"id":"9a09b38e27134b81a45f7f2ed3939edc","text":"Cognito User Pools","correct":false}]},{"id":"f71363df-7941-4a84-93e1-7a5ed2ef1c08","domain":"security","question":"You have some sensitive data that you would like to encrypt. You want to be sure that once the data is encrypted, nobody but you will be able to use the encryption key to decrypt your files. Your head of security has asked you to make sure that the key used to encrypt your files is itself encrypted under another key. Which AWS technology enables this?","explanation":"When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key, and then encrypting the data key under another key.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping","title":"Envelope Encryption"}],"answers":[{"id":"834369e6ea722aa59f60696c106ba827","text":"Re-encrypt the data key with the master key","correct":false},{"id":"c5c0340c8e4f6673916050e3cbc5e2b2","text":"Store the encryption key in an encrypted S3 bucket","correct":false},{"id":"fdf5291377e4e56d4ee76f0328313232","text":"Use envelope encryption to encrypt the data key with another key","correct":true},{"id":"8afab6f0bb19c0796620267f6a74644b","text":"Encrypt the master key with the data key","correct":false},{"id":"bd2c15ddb6cb6e3fee2adc749979e1e1","text":"Store the encryption keys in CloudHSM","correct":false}]},{"id":"51600664-99f9-48f5-97cc-ec860d378f89","domain":"development","question":"Which AWS service allows you to build and model your serverless application as a visual workflow consisting of a series of steps where the output of one stage can be input into another?","explanation":"Step Functions provide this functionality","links":[{"url":"https://aws.amazon.com/step-functions/faqs/","title":"Step Functions FAQ"}],"answers":[{"id":"42816db0ecfffdf3baff90b7f2545874","text":"Step Functions","correct":true},{"id":"1f4072738a4917bea022b11256fb46a4","text":"Simple Workflow Service","correct":false},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":false}]},{"id":"802be8c8-07cc-48a7-94c8-21d785d18f5a","domain":"deployment","question":"Your organization is developing a CI/CD environment to improve software delivery of your applications. It has already adopted a plan to execute the various phases of the CI/CD pipeline from continuous integration to continuous deployment. There are now discussions around restructuring the team make-up to implement a CI/CD environment. How would you recommend creating developer teams as a best practice to support this change in the long run?","explanation":"AWS recommends organizing three developer teams for implementing a CI/CD environment: an application team, an infrastructure team, and a tools team. This organization represents a set of best practices that have been developed and applied in fast-moving startups, large enterprise organizations, and in Amazon itself. The teams should be no larger than groups that two pizzas can feed, or about 10-12 people. This follows the communication rule that meaningful conversations hit limits as group sizes increase and lines of communication multiply. Hiring an external consulting firm will not be beneficial in the long run. Setting up a single team is not best practice. AWS CodePipeline is a continuous integration and continuous delivery service for fast and reliable application and infrastructure updates and not used for team structuring.","links":[{"url":"https://d0.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"f3054a7a017a533ace8f081538d0b664","text":"Set up an application team to develop applications. Set up an infrastructure team to create and configure the infrastructure to run the applications. Set up a tools team to build and manage the CI/CD pipeline.","correct":true},{"id":"d163f1cc494b3cde77122207a42d9de1","text":"Set up one team to own an operate all components of the CI/CD pipeline to consolidate tasks and improve efficiency.","correct":false},{"id":"b943ddf4265197477c4036cdd230b033","text":"Hire an external consulting firm to build and manage the pipeline. Provide them with the proper IAM roles to access your AWS environment.","correct":false},{"id":"3e514092970cfe2b9a3ea7331a0828cf","text":"Use CodePipeline to manage your CI/CD environment and assign team members to own different phases within your CodePipeline.","correct":false}]},{"id":"6d9f501f-bc25-4802-8cb9-adb7b7cc9724","domain":"deployment","question":"In addition to choosing the correct EBS volume type for your specific task, what else can be done to increase the performance of your volume?","explanation":"There are a number of ways you can optimise performance above that of choosing the correct EBS type.  One of the easiest options is to provide more I/O throughput than you can provision for a single EBS volume.  This can be done by striping using RAID 0.  You can join multiple gp2, io1, st1, or sc1 volumes together in a RAID 0 configuration to provide parallel read/write performance. The second option is to choose an EC2 instance type that supports EBS optimization.  This ensures that network traffic will not contend with traffic between your instance and your EBS volumes.  The final correct choice is only related to HDD based EBS volumes.  When you create a snapshot of a Throughput Optimized HDD (st1) or Cold HDD (sc1) volume, performance may drop as far as the volume's baseline value while the snapshot is in progress. This behaviour is specific to these volume types.  Therefore you should ensure that scheduled snapshots are carried at times of low usage.  The one option on the list which is entirely incorrect is the one that states \"Never use HDD volumes, always ensure that SSDs are used\" as the question first states \"In addition to choosing the correct EBS volume type for your specific task\".  HDDs may well be suitable to certain tasks and therefore they should not be discounted because they may not have have the highest specification on paper.","links":[{"url":"https://aws.amazon.com/ebs/features/","title":"Amazon EBS features"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSPerformance.html","title":"Amazon EBS Volume Performance on Linux Instances"}],"answers":[{"id":"060ad726277222bf1fdf1470ea833f1a","text":"Stripe volumes together in a RAID 0 configuration.","correct":true},{"id":"5f33f064dbad9a3f30cd0395c1ae2106","text":"Ensure that your EC2 instances are types that can be optimized for use with EBS","correct":true},{"id":"154b9cbae5d3f37e232f1a00c5364547","text":"Schedule snapshots of HDD based volumes for periods of low use","correct":true},{"id":"2b3853ec27d2a5f7a8fb114b5fcfc321","text":"Never use HDD volumes, always ensure that SSDs are used","correct":false}]},{"id":"025f7c87-6ee6-4dd9-8377-9a25598286ca","domain":"deployment","question":"Which of the following statements about a standard SQS queue is true:","explanation":"With standard queues, SQS will deliver each message at least once, but cannot guarantee the delivery order. Because each message may be delivered more than once, your application should be idempotent by design.","links":[{"url":"https://aws.amazon.com/sqs/faqs/","title":"Message Ordering"}],"answers":[{"id":"af1d8b5f4bb474a7d10ddae90292194c","text":"Messages will be delivered exactly once, but message delivery order is indeterminate.","correct":false},{"id":"58a91ca2c04fba617acd128141d111fc","text":"Messages will be delivered one or more times in first-in, first-out order.","correct":false},{"id":"de5a3c6f37dfc6ac454ea58ff54607db","text":"Messages will be delivered exactly once in first-in, first-out order.","correct":false},{"id":"f7856994f49c750fb75cc2379898cc34","text":"Messages will be delivered one or more times and message delivery order is indeterminate.","correct":true}]},{"id":"f70f0615-b415-485e-93d5-2286bc2c25cc","domain":"development","question":"What is the maximum size of an item in a DynamoDB table?","explanation":"The maximum item size in DynamoDB is 400 KB.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html","title":"Limits in Amazon DynamoDB"}],"answers":[{"id":"3c0512af07779a4b40b2b3d95dd1375b","text":"40 KB","correct":false},{"id":"5bdde53de5ac658c8aeeacac334b0152","text":"40 MB","correct":false},{"id":"1310fc461ca5af7082d588ba9c2a795b","text":"400 KB","correct":true},{"id":"462e2e8040f8c2b03228a91524dd8953","text":"400 MB","correct":false}]},{"id":"b1a8a6e6-0eb1-4c01-8d8b-4baebf067f88","domain":"development","question":"An application uses DynamoDB table as its backend data store. Each item has size of 10KB.  The application needs to perform 100 eventually consistent read operations per second, and 50 write operations per second.  What is the provisioned RCU value required to meet these requirements?","explanation":"1 RCU is equivalent to two eventually consistent reads per second of an item up to 4KB in size.  Thus, to calculate the required RCU in this scenario we need to: 1) Round up the item size to the nearest 4KB (12KB). 2) Divide by 4KB to calculate number of read units (12/4 = 3). 3) Divide by 2 to calculate the number of eventually consistent read units per item (3/2 = 1.5). 4) Multiple by operations per second to get the total RCU required (1.5*100 = 150 RCU).","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","title":"Provisioned Mode"}],"answers":[{"id":"94f6d7e04a4d452035300f18b984988c","text":"300","correct":false},{"id":"7ef605fc8dba5425d6965fbd4c8fbe1f","text":"150","correct":true},{"id":"cee631121c2ec9232f3a2f028ad5c89b","text":"500","correct":false},{"id":"a9b7ba70783b617e9998dc4dd82eb3c5","text":"1000","correct":false}]},{"id":"0dfd155c-e9fa-4a85-baba-ba51ae43d152","domain":"deployment","question":"As a developer you have built a WordPress site. Traffic to the site has increased and you have improved the site's functionality to meet the demand of your viewers since launch. Changes are coming frequently, and you are considering using AWS CloudFormation to automate the process of building test stacks, creating a change set, and executing the change set. How would you streamline this process in AWS most efficiently?","explanation":"Continuous delivery is a release practice in which code changes are automatically built, tested, and prepared for release to production. With AWS CloudFormation and CodePipeline, you can use continuous delivery to automatically build and test changes to your AWS CloudFormation templates before promoting them to production stacks. This release process lets you rapidly and reliably make changes to your AWS infrastructure. Although you can manually interact with CloudFormation to execute the various stages, this is not the most efficient method. Amazon Inspector is an automated security assessment service which evaluates the security loopholes in deployed resources specific to EC2. Config is a monitoring and governance tool that tracks changes to your AWS environment based on rules you configure.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-basic-walkthrough.html","title":"Walkthrough: Building a Pipeline for Test and Production Stacks"}],"answers":[{"id":"6e617dd2e008968aea6984c58b17c139","text":"Use Amazon Inspector to monitor your CloudFormation environment that will send an SNS notification to Lambda when a pipeline stage is complete. Subscribe the Lambda function to the SNS topic.","correct":false},{"id":"1498c0d02e9c619808313a425479642f","text":"Create a CodePipeline separated by three stages. For each stage organize actions in a pipeline. Have CodePipeline complete all actions in a stage before the stage processes new artifacts.","correct":true},{"id":"4b808c56dbbec61b1137c70708faf855","text":"Create a Config rule that will look for changes within your CloudFormation stack that will trigger Lambda functions to execute actions based on the pipeline.","correct":false},{"id":"87149b0d9dbfaac4e8ba219e908d57a1","text":"Build your test stack, create a change set, and then execute the change set by manually interacting with AWS CloudFormation.","correct":false}]},{"id":"5b6fd3fe-de07-4eb3-a61d-f96e4b4b5f1b","domain":"refactoring","question":"Which of the following is NOT a supported event source for Lambda?","explanation":"Supported event sources which can trigger Lambda functions include: CloudWatch, DynamoDB, S3, Kinesis, CodeCommit, IoT buttons, CloudFront, Cognito, SNS, SQS, SES etc. RDS cannot trigger a function directly but you could configure RDS to send notifications to SNS and then use SNS to trigger a Lambda function. ","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html","title":"Lambda Event Sources"}],"answers":[{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":true},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":false},{"id":"1399767aff0c1931602c17dcfb0d375e","text":"CloudWatch Events","correct":false}]},{"id":"e475aa36-e8a9-4d0d-81db-abf30a055ef5","domain":"mon-trb","question":"How can you configure CodeBuild to notify the DevOps team of a failure in the build process?","explanation":"CodeBuild natively supports CloudWatch Events, SNS is a subscription based notification service which integrates with CloudWatch.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-build-notifications.html","title":"Build Notifications"}],"answers":[{"id":"d6c5ab1abdce3cd74b1ebe325b88d27e","text":"Use the CodePipeline dashboard to view the CodeBuild events log","correct":false},{"id":"571ab39fe2270d1c1193587ccfd234f1","text":"Add the name of the email group to the notifications section of the CodeBuild console","correct":false},{"id":"1448724d0c2de970718ae2bfe3159256","text":"Use CloudWatch Events and SES notifications to send an email to the DevOps team","correct":false},{"id":"4a31b4240a14801eefb085b05f7985a6","text":"Use CloudWatch Events and an SNS topic to notify subscribers of build events","correct":true}]},{"id":"81a4f6d3-34ae-4753-8498-823bede20afe","domain":"deployment","question":"Your team is considering deploying an application on AWS Elastic Beanstalk. Your manager needs to know what infrastructure requirements are needed for the team, specifically in regards to maintenance, patching, and managing security. How would you explain what AWS is responsible for and what the team is responsible for to your manager?","explanation":"AWS and its customers share responsibility for achieving a high level of software component security and compliance. This shared model reduces customers' operational burden. AWS Elastic Beanstalk helps you perform your side of the shared responsibility model by providing a managed update feature. This feature automatically applies patches and minor updates for an Elastic Beanstalk supported platform version. Elastic Beanstalk publishes its platform support policy and retirement schedule for the coming 12 months. You (the customer) are responsible for the security of your application, your data, and any components that your application requires and that you downloaded. Be sure to review the Shared Responsibility Model in the URL provided.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/platforms-shared-responsibility.html","title":"Shared Responsibility Model for Elastic Beanstalk Platform Maintenance"}],"answers":[{"id":"8fc7a262705a2a78a08dbc0ea667db64","text":"You are responsible for publishing Elastic Beanstalk's platform support policy and retirement schedule.","correct":false},{"id":"f1e49a967c2ae8dc1525d208a8170996","text":"AWS is responsible for the security of your application, your data, and any components that your application requires and that you downloaded.","correct":false},{"id":"7086eb1de01b03376ee097977ba7cd94","text":"You are responsible for runtime, application server, and web server components if you opt into Elastic Beanstalk managed updates.","correct":false},{"id":"9fafdcf1a8a7c82b9e94d6080c00a245","text":"AWS is responsible for patches, minor, and major updates of operating system on its supported platform versions.","correct":true},{"id":"9d22ad24853ac3a47fe7c0e7eb2585e3","text":"You are responsible for the security of your application, your data, and any components that your application requires and that you downloaded.","correct":true}]},{"id":"d069e7a2-ff20-46e7-9fd8-64d3fc2e0c8d","domain":"development","question":"You have a load balancer configuration that you use for most of your CloudFormation stacks. This load balancer always sits in front of your application running on EC2 as it has the important function of forwarding HTTPS requests on port 443 to HTTP requests on port 80 on the instance. As demand for the application grows you need to reuse this load balancer configuration in multiple other deployments of the application and you need to use CloudFormation to do this in an automated way. What is the most efficient way to deploy the load balancer configuration?","explanation":"Nested stacks are stacks created as part of other stacks. You create a nested stack within another stack by using the AWS::CloudFormation::Stack resource. For example, assume that you have a load balancer configuration that you use for most of your stacks. Instead of copying and pasting the same configurations into your templates, you can create a dedicated template for the load balancer. Then, you just use the resource to reference that template from within other templates. Lambda would not be able to deploy infrastructure resources as efficiently as CloudFormation nested stacks. AWS CloudFormation provides two methods for updating stacks: direct update or creating and executing change sets. When you directly update a stack, you submit changes and AWS CloudFormation immediately deploys them. Use direct updates when you want to quickly deploy your updates. With change sets, you can preview the changes AWS CloudFormation will make to your stack, and then decide whether to apply those changes.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html","title":"Working with Nested Stacks"}],"answers":[{"id":"67c3f9750fe6b5768341534dcf97ba93","text":"Use AWS CloudFormation nested stacks by creating a dedicated template for the load balancer and refer to that template within other templates.","correct":true},{"id":"97cc8d8d6d33e845e203f2c7d07e776e","text":"Use AWS CloudFormation direct updates to quickly deploy the same load balancer configuration in multiple environments.","correct":false},{"id":"d8c538a0c914101c9d913c3644dfeecd","text":"Use AWS CloudFormation change sets to change the load balancer configuration based on Region/AZ where you want to deploy a copy of the application.","correct":false},{"id":"54ac82e3cb0e853af8ce69674edafd64","text":"Instead of CloudFormation, use Lambda. Let the load balancer trigger a Lambda function that has the infrastructure code embedded to deploy the configuration when prompted.","correct":false}]},{"id":"7d99fa38-f31e-4527-95a6-a88611f7731c","domain":"mon-trb","question":"A developer deployed a serverless application consisting of an API Gateway and Lambda function using CloudFormation. Testing of the application resulted in a 500 status code and 'Execution failed due to configuration' error. What is a possible cause of the error?","explanation":"When you build an API Gateway API with standard Lambda integration using the API Gateway console, the console automatically adds the required permissions. However, when you set up a stage variable to call a Lambda function through your API, you must manually add these permissions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html","title":"Invoke"},{"url":"https://docs.aws.amazon.com/apigateway/api-reference/handling-errors/#api-error-codes","title":"Error Codes (Client and Server Errors)"}],"answers":[{"id":"8fa26152f6981113cb014274ea900af1","text":"Too many API Gateway requests were created exceeding the allowed limit.","correct":false},{"id":"d69e5b40405e5538753a24c15bb80a0e","text":"The Lambda function's resource-based policy doesn't include permission for your API to invoke the function.","correct":true},{"id":"dddca3f487612188440bbfe93d1ef829","text":"IAM policy is restricting the user from invoking the API Gateway API endpoint.","correct":false},{"id":"eef7bf395052a4cb6bf0286cc9578779","text":"API Gateway is not authorized to invoke the Lambda function.","correct":false}]},{"id":"f7a67868-ec20-40ae-a334-1bca9d02bdb7","domain":"deployment","question":"You are developing a website which allows customers to purchase tickets to popular sporting events. Your application uses S3 for static web hosting, Lambda for business logic, stores transaction data in RDS and uses DynamoDB for product and stock information. After the customer has paid for their purchase, a message is sent to an SQS queue to trigger a confirmation email to be sent out to the customer including an e-ticket for their chosen event. You want to send out the email as soon as the payment has been processed, however during testing you discover that the confirmation emails are being processed a few seconds before the stock control database has finished updating. This sometimes results in selling the same ticket twice. How can you quickly fix this without re-engineering the application?","explanation":"Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html","title":"SQS Delay Queues"}],"answers":[{"id":"93afc9503f6f41866e3fb6bf957bc816","text":"Use Kinesis to stream the SQS messages, adding a delay of a few seconds","correct":false},{"id":"2c519c2e315e56078976351bc313bbf9","text":"Use a FIFO queue to ensure the messages are always processed in the correct order","correct":false},{"id":"953522fe58e17a4a74978232a8b50608","text":"Set the delay flag on the queue to 5 seconds, to ensure messages are not processed too quickly","correct":false},{"id":"dc7f3b8fe45a2bd74a545027f1178792","text":"Use an SQS delay queue to let you postpone the delivery of SQS messages by a few seconds","correct":true}]},{"id":"13a47732-f4cd-40ed-a1e4-341826169510","domain":"refactoring","question":"You need to retrieve some data from your DynamoDB table, which of the following methods would consume the greatest number of provisioned Capacity Units?","explanation":"A Query is generally far more efficient than a Scan operation. Strongly consistent reads use up double the amount of Read Capacity Units compared to eventually consistent reads","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html","title":"Query Vs Scan"},{"url":"https://aws.amazon.com/dynamodb/pricing/provisioned/","title":"Provisioned Capacity"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html","title":"DynamoDB Read Consistency "}],"answers":[{"id":"ac9570514b07db98ad6cc89ea1cc8741","text":"Query with strong consistency","correct":false},{"id":"8747d18821697e99310487487007af03","text":"Scan with strong consistency","correct":true},{"id":"5d682b15ed6ff4d0b5100242fa11d4ea","text":"Scan with eventual consistency","correct":false},{"id":"d87fc0cde8df11c904ff6fe25cac6bfc","text":"Query with eventual consistency","correct":false}]},{"id":"fe61a353-eb18-40dc-9e65-30f7106d3e6e","domain":"mon-trb","question":"Your application is running on EC2 and on Linux virtual machines in your own data center. You would like to configure your application to send data to X-Ray for troubleshooting and performance analysis. Which of the following steps will you need to complete?","explanation":"You need the X-Ray SDK and the X-Ray daemon on your EC2 instances and on-premises systems, you then need to instrument your application to send the required data to X-Ray","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html","title":"X-Ray Developer Guide"}],"answers":[{"id":"da1eaa5b340d64b20d5b526dc0de4b85","text":"Install the X-Ray SDK and the X-Ray daemon, then instrument your application to send data to X-Ray.","correct":true},{"id":"ff31ac1442cb9f28ad8f65a358816a25","text":"Install the AWS SDK and the X-Ray CLI, then instrument your application to send data to X-Ray.","correct":false},{"id":"6ffb986aa2b49eb856b38668f9a44fc2","text":"Install the AWS CLI, then instrument your application to send data to X-Ray.","correct":false},{"id":"998a1f353d1923ec24b66b6ae427d343","text":"Install the X-Ray daemon, then instrument your application to send data to X-Ray.","correct":false}]},{"id":"b9987e32-c46c-4c78-b9db-e9a5283d7b16","domain":"development","question":"You are planning to use CodeDeploy to deploy an application for the first time to a brand new fleet of EC2 instances. Which deployment approach would you recommend?","explanation":"In-Place is the one to use as you are installing to a new fleet of instances, therefore Blue/Green is not possible. Canary and Rolling updates are not an option for CodeDeploy","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html","title":"Working with Deployments in AWS CodeDeploy"}],"answers":[{"id":"ecf715d6d79a2698b7fec0357f9d721f","text":"Canary","correct":false},{"id":"53b8ba497ea2cdea89f60da12d94b46d","text":"In-Place","correct":true},{"id":"ff2713a6181db42fded101c670bbd0dd","text":"Rolling with additional batch","correct":false},{"id":"3a27747f75c4e73e94223a9e4065cd9c","text":"Blue / Green","correct":false}]},{"id":"3efc0266-4c2e-11ea-b77f-2e728ce88125","domain":"deployment","question":"Which of the following statements are true about the concept of blue/green deployment regarding development and deployment of your application?","explanation":"With blue/green deployment, you can shift traffic between two identical environments that are running different versions of your application. It allows you to easily deploy changes to your application and roll-back on changes very quickly.","links":[{"url":" https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf","title":"Blue/Green Deployments on AWS"}],"answers":[{"id":"3dae7ad4a3084f0fa3d2598654de9066","text":"The blue environment represents the current version of your application serving production traffic.","correct":false},{"id":"69da7fc6d495def048f27d73e7e55e82","text":"It allows you to shift traffic between two identical environments that are running different versions of your application.","correct":true},{"id":"72f75a1c7558a3b8e4c011917ad6105d","text":"The green environment represents the production environment.","correct":false},{"id":"545fd2a37ac85b63dad4f92c91401436","text":"The green environment represents the staging environment.","correct":false}]},{"id":"3eb49e2e-25b6-4fe6-8bbe-e3ccedcd1efd","domain":"security","question":"A developer is looking to implement a load balancing solution for web-based service oriented application deployed in AWS EC2. The solution must support path based routing and all communication to the users must be encrypted. What is the most performant method to achieve these requirement?","explanation":"The application requirement states support for path based routing. This means that we must use an Application Load Balancer as Network Load Balancer does not have this feature. It is best practice to deploy the SSL certificates on the Load Balancer. This implements SSL termination on the load balancer and off-loads this task from the application, thus reducing the load on EC2 instances. Additionally, it removes the requirement of distributing the certificate to all target EC2 instances.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html","title":"Create an HTTPS Listener for Your Application Load Balancer"}],"answers":[{"id":"7ebcbebf7f00c8f17f377813b31cc76e","text":"Use Network Load Balancer. Deploy SSL certificates on the Network Load Balancer.","correct":false},{"id":"b97b895cbdc513c48c5bfb7564a38aa7","text":"Use Network Load Balancer. Deploy SSL certificates on the EC2 instances.","correct":false},{"id":"206131d0e41aa9e13214ac701d6f08e2","text":"Use Application Load Balancer. Deploy SSL certificate on the Application Load Balancer.","correct":true},{"id":"a5a020608230cb06058ddd6291c7886a","text":"Use Application Load Balancer. Deploy SSL certificates on the EC2 instances.","correct":false}]},{"id":"9382a270-2c44-45b2-95f3-79d0cf319120","domain":"mon-trb","question":"A developer has been tasked with enabling Access Logs on the Application Load Balancer that sits in-front of their web services. As part of this task, they must configure a location to which the logs are delivered.\n\nTo what AWS service can Access Logs from an Application Load Balancer be delivered?","explanation":"S3 is the only service supported by AWS for receiving ALB access logs.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html","title":"Application Load Balancer Access Logs"}],"answers":[{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true},{"id":"a8c600be214ced26950e704d39c3ca21","text":"CloudWatch Logs","correct":false},{"id":"594025cae6dfa6b9073dc25de93ddb56","text":"Kinesis","correct":false}]},{"id":"6a5ea0b3-2527-4c20-9410-d8807d16fcd3","domain":"security","question":"An organization is hosting their static website on S3, using a custom domain name. Users have started reporting that their web browsers' are alerting them to the fact that the organization's website is \"Not Secure\" because it is not served via a secure HTTPS connection.\n\nWhat is the easiest way to start serving the website via HTTPS?","explanation":"S3 buckets do not directly support HTTPS with a custom domain name. The simplest solution is to create a CloudFront distribution and set its origin to the S3 bucket. CloudFront allows you to specify a custom domain name, and supports managed certificates via Amazon Certificate Manager.\n\nEnabling AES-256 Default Encryption on the S3 bucket only affects the object at rest.\n\nApplication Load Balancers do support SSL termination but do not support S3 as a target.\n\nAWS Shield relates to Distributed Denial of Service protection, not encryption over the wire.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-https-requests-s3/","title":"Use CloudFront to serve HTTPS requests for Amazon S3"}],"answers":[{"id":"674d0590e06799926f232e63b73894a8","text":"Add a CloudFront distribution in front of the S3 static website, which supports HTTPS with a custom domain name.","correct":true},{"id":"d76d398f54d04a531867ae84eda26050","text":"Add an Application Load Balancer in front of the S3 bucket and enable SSL termination.","correct":false},{"id":"27631f4c194c5ea5116e308788e945f2","text":"Enable AES-256 Default Encryption on the S3 bucket, which ensures all content is delivered via HTTPS.","correct":false},{"id":"6d63f6dae98a20c7db6446021c83e7f5","text":"Enable AWS Shield on the S3 bucket. Browsers automatically detect that Shield is enabled and report that the website is secure.","correct":false}]},{"id":"2863369f-3c52-497f-887b-1e6d5a13e5fe","domain":"security","question":"A developer needs to share an EBS volume with a second AWS account. What actions need to be performed to accomplish this task in the most optimal way?","explanation":"It is not possible to directly share an EBS volume with another account. In order to accomplish the required task, it is required to create an EBS volume snapshot and grant permissions to that snapshot to the second AWS account. Although EBS volume snapshots are stored in S3, they are not in a user-visible bucket. Sharing a private AMI with a second account does not meet the specific requirement as defined in the question.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html","title":"Sharing an Amazon EBS Snapshot"}],"answers":[{"id":"683e6c4bef943d4c2e5871fcc39e1ebd","text":"Create an AMI from the EC2 instance. Modify image permissions and add a second AWS account ID to share the AMI. Ensure 'create volume' permissions are added. In the second AWS account, create an EC2 instance using the shared AMI.","correct":false},{"id":"823ae734c13e3b7ed146146637c1df42","text":"Create an EBS volume snapshot. Modify EBS snapshot permissions and add the second AWS account ID to share the snapshot. In the second AWS account, create an EBS volume from the snapshot.","correct":true},{"id":"d477177fe8b8bb5a16854f7c5632856d","text":"Create an EBS volume snapshot. Modify S3 bucket policy granting the second AWS account access to the S3 object of the snapshot. In the second AWS account, create an EBS volume from the S3 object.","correct":false},{"id":"f2cb6e39eb1e99f97c84895d0c6b0cc8","text":"Create an IAM policy granting necessary actions on the specific EBS volume. Add the second AWS account ID in the Principal element.","correct":false}]}]}}}}
