{"data":{"createNewExamAttempt":{"attempt":{"id":"787c7f2c-2dab-4085-9b04-97b3ee789ba8"},"exam":{"id":"6fd68d4c-6553-4eb5-8589-7f6342bec6a5","title":"AWS Certified Developer - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"ec1a76ef-7748-442c-8335-6946ab71d4cf","domain":"mon-trb","question":"A company compliance policy mandates that all production account data must be stored across multiple geographically distant locations. In order to meet this requirement, they configured Amazon S3 Cross-Region Replication on their production account buckets. They find that S3 objects are not being replicated. What needs to be implemented to resolve this issue?","explanation":"S3 Bucket Versioning is a requirement to configure S3 Cross-Region Replication and must be enabled before S3 Cross-Region Replication can even be configured. This is therefore not part of the correct answer. S3 lifecycle policies are not related to replication of S3 data between accounts or regions. S3 lifecycle policies can be used to transition S3 objects to another Amazon S3 storage class. Using bucket event notifications would also not resolve this issue. Source bucket owner must have permissions to replicate objects on the destination S3 bucket in order for replication to succeed. This is accomplished by providing the source bucket owner permissions in the destination bucket policy. Additionally, source bucket owner must have access permissions to objects in the source bucket that are being replicated in order for replication to succeed. It is possible that IAM users other than the S3 bucket owner have permissions to put objects in the source bucket. In that scenario, the object owner must grant access permissions on the objects to the bucket owner.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-troubleshoot.html","title":"Troubleshooting Replication"}],"answers":[{"id":"45e81330579f9aff0a7899e4391df01b","text":"S3 source object owner must grant source bucket owner full access permissions.","correct":true},{"id":"bb861c75c9a7820314f89f054fc67334","text":"S3 Bucket Event Notifications must be configured.","correct":false},{"id":"7cf3b4d7efa9a9120305b06649d29c0e","text":"S3 Bucket Lifecycle Policy must be configured.","correct":false},{"id":"3a890ed88544caf9acc9c3ab791fa815","text":"Bucket policy on the destination bucket must allow the source bucket owner to replicate objects.","correct":true},{"id":"458121bce565bd7f3f18b1bb88fcd54f","text":"S3 Bucket Versioning must be enabled.","correct":false}]},{"id":"d6508f82-f5de-4013-a777-d801a0816ff6","domain":"development","question":"A developer has a requirement to trigger a Lambda function once every 24 hours. What is the best way of achieving this requirement?","explanation":"CloudWatch Events allows targets to be triggered using a Schedule Expression. A Schedule Expression can define a rate; for example, every 24 hours. Or can accept a standard cron job expression.\n\nCloudWatch Events supports many targets, including Lambda.\n\nInvoking the function from a cron job running on an EC2 instance would meet the requirements, but would require additional effort and cost; therefore, is not the best solution.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html","title":"CloudWatch Events supported targets"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html","title":"CloudWatch Events Scheduling"}],"answers":[{"id":"49b7adae681c4743b1eac8bb6cdaca00","text":"Invoke the Lambda function from a cron job running on an EC2 instance.","correct":false},{"id":"478234fe13f76893bfa2f3f57911bcb3","text":"Schedule a trigger in CloudWatch Events","correct":true},{"id":"bb3413b8174c30b6418449092039dfc2","text":"Add a message into SQS that invokes the Lambda function. As part of the Lambda function's code, add a new message into SQS to re-invoke the function with a DelaySeconds equal to 24 hours.","correct":false},{"id":"9ad7f45031f00ab56d5450da9aaa7432","text":"Schedule the trigger in the Lambda Runtime Scheduler.","correct":false}]},{"id":"0e4eaa2b-a121-40aa-ab67-94f768660e76","domain":"deployment","question":"Your application regularly sends a number of large messages exceeding 1GB in size to SQS. You would like to store these messages in S3 rather than in SQS. Which of the following can you use to manage large SQS messages stored in S3?","explanation":"You can use Amazon S3 and the Amazon SQS Extended Client Library for Java to manage Amazon SQS messages stored in S3. This includes specifying when messages should be stored in S3, referencing message objects stored in S3, getting them, and deleting them.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html","title":"Managing Large Amazon SQS Messages Using Amazon S3"}],"answers":[{"id":"a7be1fa1b34770b56b6b0cf5a4ff2842","text":"SQS Extended API for Java","correct":false},{"id":"a69f5cdbab8efbb9ff8de1f9db3d29fb","text":"SQS Extended HTTP API for Java","correct":false},{"id":"9cda9035f194932d72b2b650415eccb3","text":"SQS Extended Client Library for Java","correct":true},{"id":"6c93ec556a138f4181f32250f309a0a8","text":"SQS Extended CLI for Java","correct":false}]},{"id":"bd3b8069-4ee1-480b-8738-d0683a3de962","domain":"security","question":"A company security team wants to implement a solution for securely storing RDS database credentials.  The solution should provide automatic rotation of database credentials.  What AWS service can the team use to meet these requirements?","explanation":"AWS Secrets Manager is an AWS service that can be used to securely store, retrieve, and automatically rotate database credentials. AWS Secrets Manager has built-in integration for RDS databases. Applications use Secrets Manager API's to retrieve database credentials, enabling secure storage of sensitive information outside of the application code. Systems Manager Parameter Store provides secure storage of sensitive information. However, it does not provide automatic credentials rotation capability specified as a requirement in the question scenario. Key Management Service (KMS) is used for management of cryptographic encryption keys, not for storage of sensitive information. Resource Access Manager is not applicable here as it is used for managing access to AWS resources between multiple accounts.","links":[{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html","title":"What Is AWS Secrets Manager?"}],"answers":[{"id":"b77e6ac1dd339ec1c0d94107e6c9d3d2","text":"AWS Systems Manager Parameter Store","correct":false},{"id":"7898cb92c418aeed6974ede9cb146462","text":"AWS Secrets Manager","correct":true},{"id":"fcba7bd474eb1fdf49705827bbb6f28c","text":"AWS Key Management Service","correct":false},{"id":"7c09430feea9bf5bdf657bd178ce574c","text":"AWS Resource Access Manager","correct":false}]},{"id":"6e7680bf-f045-4583-b38b-b2ca3ae466ed","domain":"security","question":"An IT Auditor has started in your Security Team, they will need access to read files in S3 and DynamoDB as well as the ability to describe EC2 instances. You want to ensure that only the Auditor is granted this access and that the IAM policy you create cannot mistakenly be attached to any other user. Which IAM policy type should you use?","explanation":"When you use an inline policy, the permissions in the policy cannot be inadvertently attached to the wrong principal entity","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html","title":"Managed Policies and Inline Policies"}],"answers":[{"id":"60a6d5742e8f4d09d2d0edb9b15fa80d","text":"Customer Managed Policy","correct":false},{"id":"05aaffc9a37013fcae1b36c4baffceff","text":"AWS Managed Policy","correct":false},{"id":"449ff67db14fb547ec64ef11c4d33c40","text":"Inline Policy","correct":true},{"id":"ab18cb3ee046b7e5466057043f273700","text":"Custom Policy","correct":false}]},{"id":"2692273f-3f45-48a9-8db1-bd08542db08c","domain":"security","question":"You need to allow another AWS account access to resources in your AWS account, what is the recommended mechanism to configure this?","explanation":"Roles are the primary way to grant cross-account access.  With IAM roles, you can grant third parties access to your AWS resources without sharing your AWS security credentials. Instead, the third party can access your AWS resources by assuming a role that you create in your AWS account.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html","title":"IAM Roles and Cross Account Access"},{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html","title":"Providing Access to AWS Accounts Owned by Third Parties"}],"answers":[{"id":"b2cc52fe397cf7827353562b7ca92e91","text":"Configure cross account access by creating a role in your account which has permission to access only the resources they need. Allow the third party account to assume the role based on their account ID and unique external ID","correct":true},{"id":"353c57d4949d688d413dfe136b6a3e2b","text":"Provide AWS credentials to the third party so that they can log into your account and access the resources they need","correct":false},{"id":"bf90de82812c0e6a3c80cf9c3579975d","text":"Use Cognito to allow the third party to sign-up as a guest user to get temporary access to your account","correct":false},{"id":"573753accb681d032a85cdc6773bf08e","text":"Configure Web Identity Federation to allow them to log in to your account","correct":false}]},{"id":"9548796c-789c-42ea-9e90-3da3a9252c1b","domain":"security","question":"Your Security team have recently reviewed the security standards across your entire AWS environment. They have identified that a number of EC2 instances in your development environment have read and write access to an S3 bucket containing highly confidential production data. You have been asked to help investigate and suggest a way to remedy this. Which of the following can you use to find out what is going on so that you can suggest a solution?","explanation":"With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies attached to IAM users, groups, or roles in your AWS account. You can test which actions are allowed or denied by the selected policies for specific resources.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html","title":"Testing IAM Policies with the IAM Policy Simulator"}],"answers":[{"id":"17d9101067c49b2ec4b35087e24ce8eb","text":"Use the IAM Policy Simulator to identify which role or policy is granting access","correct":true},{"id":"58f20da8b31f9e9da2e5a374608338ef","text":"Use the CLI or console to check the public access permissions of the S3 bucket","correct":false},{"id":"aac02aded2357ef60d7cc0b8f32df947","text":"Use CloudTrail and Athena to identify which role or policy is granting access","correct":false},{"id":"5bc514eb754ea13140481ed2afc68d45","text":"Use the VPC flow logs to identify which EC2 instances are attempting to access the bucket","correct":false}]},{"id":"71b8b773-eed1-4335-9631-5bfe132f16c7","domain":"development","question":"You are the development lead on a large project to launch a new e-commerce website specialising in fishing supplies. Your developers are located in India, USA and the Middle East. You need to find a source code repository that everyone can use, and that will allow developers to continue to work on their code even when they are not connected to the internet. Which of the following would you suggest to the team?","explanation":"CodeCommit is based on Git, which is a distributed version control system, meaning there is no single, central place where everything is stored. In a distributed system, there are multiple backups in the event that you need one. This approach also means that you can work offline and commit your changes when you are ready.","links":[{"url":"https://aws.amazon.com/devops/source-control/git/","title":"Source Control In AWS"}],"answers":[{"id":"94efdca7e5940d3078c950c64e833082","text":"Use CodeCommit to manage your source code","correct":true},{"id":"c3a56d6c63ca88e19d3e6afc5b896c46","text":"Use CodeBuild in offline mode to manage your source code","correct":false},{"id":"4e1b47d595e44072d3890d332ead7f86","text":"Run an instance of Git in a docker container on AWS ECS","correct":false},{"id":"c2a2ffe5e9352016e58fe01b3c304de3","text":"Install Git on 2 EC2 instances in an auto-scaling group","correct":false}]},{"id":"9382a270-2c44-45b2-95f3-79d0cf319120","domain":"mon-trb","question":"A developer has been tasked with enabling Access Logs on the Application Load Balancer that sits in-front of their web services. As part of this task, they must configure a location to which the logs are delivered.\n\nTo what AWS service can Access Logs from an Application Load Balancer be delivered?","explanation":"S3 is the only service supported by AWS for receiving ALB access logs.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html","title":"Application Load Balancer Access Logs"}],"answers":[{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true},{"id":"594025cae6dfa6b9073dc25de93ddb56","text":"Kinesis","correct":false},{"id":"a8c600be214ced26950e704d39c3ca21","text":"CloudWatch Logs","correct":false}]},{"id":"91e59ee8-028b-44f1-9d03-1081d09738d3","domain":"development","question":"A clothing company needs to build a REST service to allow salespeople quick access to stock levels. The service must be accessible from an HTTP request. Which of the following solutions addresses the company's requirements?","explanation":"In an AWS Lambda integration in Amazon API Gateway, the HTTP method request from the client is mapped to a backend Lambda function invocation. Depending on your use case, you may choose to use Lambda proxy integration, Lambda non-proxy integration, or both in your API Gateway API. In a Lambda proxy integration, the entire client request is sent to the backend Lambda function as is, except that the order of the request parameters isn't preserved. In a Lambda non-proxy integration (also called a custom integration), you configure the way the parameters, headers, and body of the client's request are translated into the format that your backend Lambda function requires. ","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-getting-started-with-rest-apis.html","title":"Create a REST API with Lambda Integrations in Amazon API Gateway"}],"answers":[{"id":"01f2c91643488c78436cc962fca2f2d7","text":"Amazon EC2 and AWS Auto Scaling","correct":false},{"id":"e9f6666f3057d0c266ac855cbae770b8","text":"Amazon API Gateway and AWS Lambda","correct":true},{"id":"e8445389785666bd90da881eada7e373","text":"Amazon CloudFront and Amazon S3","correct":false},{"id":"d87496b040e2581c8e89e43a7e5ed235","text":"Amazon SQS and DynamoDB","correct":false}]},{"id":"914fca77-6b5e-4b8e-afd1-d6e423835341","domain":"refactoring","question":"Your application is trying to upload a 6TB file to S3 and you receive an error message telling you that your proposed upload exceeds the maximum allowed object size. What is the best way to accomplish this file upload?","explanation":"Amazon S3 allows a maximum object size of 5TB. However, objects 5GB or larger are required to be uploaded using the multipart upload API.","links":[{"url":"https://aws.amazon.com/s3/faqs/","title":"S3 FAQs"},{"url":"https://aws.amazon.com/blogs/aws/amazon-s3-multipart-upload/","title":"Multipart upload"}],"answers":[{"id":"eac600adbe8c5aff9f9bc513cea6c0f0","text":"Contact AWS support to increase the maximum size of your S3 object.","correct":false},{"id":"07b09839951d99bc4114d8fdf4c16454","text":"Use the S3 LargeObjectUpload API.","correct":false},{"id":"199f05e3820037e7815b1de805cc4d7d","text":"You cannot fix this, as the maximum size of an S3 object is 5TB.","correct":true},{"id":"73d0d8288e849f30fc95922108ef6a15","text":"Use the Multipart Upload API for this object.","correct":false}]},{"id":"76086e8c-3f24-4bae-ba57-f5ac18dc1ff5","domain":"development","question":"What is the name of the SAM template property that defines the point in a Lambda function's code where execution begins?","explanation":"The Handler property specifies the Lambda function's entry point. For example, if the Lambda function was written in Python, and Handler was set to lambda_function.lambda_handler, execution would begin with the lambda_handler function, contained within the lambda_function.py file.\n\nRuntime refers to the language in which the Lambda function is written. For example, python3.6 or nodejs6.10, etc.\n\nSource and Index are not valid SAM template properties.","links":[{"url":"https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md","title":"AWS Serverless Application Model Specification"}],"answers":[{"id":"bc366f2d0ba3d681e7a3899917c5d3de","text":"Runtime","correct":false},{"id":"f31bbdd1b3e85bccd652680e16935819","text":"Source","correct":false},{"id":"88fa71f0a6e0dfedbb46d91cc0b37a50","text":"Index","correct":false},{"id":"0bb4c52ba15ca41d65967d91840c66fb","text":"Handler","correct":true}]},{"id":"22556a45-7db0-48f9-85cf-654ac74d729f","domain":"security","question":"When using the AWS REST API to upload an object to S3, which of the following request headers will ensure that your data must be encrypted using SSE?","explanation":"To request server-side encryption using the object creation REST APIs, provide the x-amz-server-side-encryption request header.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"Protecting Data Using Server-Side Encryption"}],"answers":[{"id":"6d1f5944b9a6aada7e00dc385d5373bd","text":"x-s3-server-side-encryption","correct":false},{"id":"63e1961675193e5c234f582f08632a28","text":"s3-amz-server-side-encryption","correct":false},{"id":"f041d978ebb9256a023c1f9d263316ac","text":"amz-s3-server-side-encryption","correct":false},{"id":"6d38512683c3cf8052e7e47d9d12a9f6","text":"x-amz-server-side-encryption","correct":true}]},{"id":"4750f3bd-de92-4efb-ad08-06c9ea71eccb","domain":"refactoring","question":"Kinesis allows consumer applications to consume records in which order?","explanation":"Kinesis gives you the ability to consume records according to a sequence number applied when data is written to the Kinesis shard","links":[{"url":"https://aws.amazon.com/kinesis/data-streams/faqs/","title":"Kinesis FAQ"}],"answers":[{"id":"cb1ab21304b72197113dbe18c491e9c2","text":"According a sequence number assigned when the record is written to the stream","correct":true},{"id":"e71e8f5a50819b4773c68991d0c9f602","text":"Records are processed in no particular order","correct":false},{"id":"b1e05f0db70da1b8dee8592d942ed2e1","text":"According to the timestamp assigned when the record is written to the stream","correct":false},{"id":"689f6f887e0c13fb07b437de23303cac","text":"Last In First Out","correct":false}]},{"id":"5cd0d201-9b93-42f0-9ae7-585263307009","domain":"development","question":"You've been asked to create a Web application with an endpoint that can handle thousands of REST calls a minute.  What AWS service can be used in front of an application to assist in achieving this?","explanation":"Questions containing 'REST' are usually related to APIs, so API Gateway looks the best answer.  Elastic Beanstalk is a service which allows you to run applications without understanding the infrastructure and can be discounted, as can Global Accelerator which is a networking service that improves the availability and performance of applications.  CloudFront can be used in conjunction with API Gateway to assist in geographically disparate calls, but won't process calls by itself.","links":[{"url":"https://aws.amazon.com/api-gateway/faqs/","title":"Amazon API Gateway FAQs"}],"answers":[{"id":"9ce65e2b30ed635c84bef82218a94fdf","text":"Global Accelerator","correct":false},{"id":"2ef9dd82927a3196ca2df3fc0cdf2e0b","text":"API Gateway","correct":true},{"id":"3d6cbd7db2a4fa389808ea6f4a5fc1bc","text":"Elastic Beanstalk","correct":false},{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false}]},{"id":"93862748-7e4b-4c90-a475-b52a4d1f6f4c","domain":"development","question":"You are planning to deploy a new version of your application using CodeDeploy. You only have a window of 2 hours to complete the deployment and test it. Your team leader is concerned about the time it could take to roll back the upgrade if it should fail. Which deployment approach would you recommend?","explanation":"Blue / Green is the one to use as this allows you to roll back with minimal disruption. An In-Place upgrade is very disruptive to roll back as it will involve re-deploying the original version of the code and during this time your application will be unavailable. Canary and Rolling updates are not an option for CodeDeploy","links":[{"url":"https://aws.amazon.com/blogs/devops/performing-bluegreen-deployments-with-aws-codedeploy-and-auto-scaling-groups/","title":"Blue/Green Deployments with AWS CodeDeploy"}],"answers":[{"id":"53b8ba497ea2cdea89f60da12d94b46d","text":"In-Place","correct":false},{"id":"ff2713a6181db42fded101c670bbd0dd","text":"Rolling with additional batch","correct":false},{"id":"ecf715d6d79a2698b7fec0357f9d721f","text":"Canary","correct":false},{"id":"3a27747f75c4e73e94223a9e4065cd9c","text":"Blue / Green","correct":true}]},{"id":"a911b79a-68c9-41f0-af02-0f57c3ccdc66","domain":"mon-trb","question":"A company's services are protected by AWS WAF. The development team would like to enable logging on the WAF to get detailed information about traffic that is analyzed by the web ACLs in order to enhance their troubleshooting efforts. Which service can the team use to collect AWS WAF logs?","explanation":"In order to enable and configure AWS WAF logs, a Kinesis Data Firehose is required for delivery of the logs to the destination.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/logging.html","title":"Logging Web ACL Traffic Information"}],"answers":[{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"33eb5c1f2566526637e791c925c4c505","text":"VPS Flow Logs","correct":false},{"id":"72e5e39b79c3d4d99d9c68a6a5e4d9f0","text":"Kinesis Data Firehose","correct":true},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false}]},{"id":"a4d334f5-b2b6-4d3c-94f4-2b39dce36264","domain":"security","question":"You work for a large pharmaceuticals company which is conducting drug trials for a number of new products. You are using SQS to handle messaging between components of a distributed application. You need to ensure that confidential data relating to your patients is encrypted, which of the following services will you use to centrally rotate the encryption keys?","explanation":"You can use a CMK to encrypt and decrypt up to 4 KB (4096 bytes) of data. Typically, you use CMKs to generate, encrypt, and decrypt the data keys that you use outside of AWS KMS to encrypt your data. This strategy is known as envelope encryption. CMKs are created in AWS KMS and never leave AWS KMS unencrypted. To use or manage your CMK, you access them through AWS KMS.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys","title":"KMS Concepts"}],"answers":[{"id":"0e8433f9a404f1f3ba601c14b026d321","text":"HTTPS","correct":false},{"id":"1562bb9d2d9567740605dcb1ccab5c80","text":"SSE-S3","correct":false},{"id":"97029ab64afac0842857a7805cc7df88","text":"Client-side encryption","correct":false},{"id":"69500631fd60fd52ad3d029cb8aa50e0","text":"AWS KMS","correct":true}]},{"id":"8caa9788-5c49-413f-b0c0-6f515af3fe5f","domain":"security","question":"An organization has mandated that all files stored in their newly created S3 bucket, 'top-secret-documents', must be encrypted using a Customer Master Key stored in KMS.\n\nWhat is the best way to enforce this requirement?","explanation":"To ensure objects are stored using a specific type of server-side encryption, you must use a bucket policy. In this case, the bucket policy must ensure the encryption type matches SSE-KMS.\n\nSetting a default encryption type on the bucket is not sufficient, as the default only applies to uploaded objects that do not specify any encryption type. For example, if the default encryption is set to AWS-KMS, but an object is uploaded with the header `x-amz-server-side-encryption: AES256`, the resulting object is encrypted using SSE-S3, not SSE-KMS.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html","title":"Protecting S3 Data Using Server-Side Encryption"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"Example bucket policy for enforcing SSE"}],"answers":[{"id":"21b68cac2d99a4c11c51d224971ee62f","text":"Add a bucket policy that denies PUT operations that don't contain the HTTP header `x-amz-server-side-encryption: aws:kms`","correct":true},{"id":"62e28c6b06e8b3895db973aa3d03c4fd","text":"Add a bucket policy that denies PUT operations that don't contain the HTTP header `x-amz-server-side-encryption: AES256`","correct":false},{"id":"7910fc40ce77db47b28825a906611f8d","text":"Enable S3 Default encryption and select AWS-KMS","correct":false},{"id":"8be8b17570d837c0301070a5b8fde1cb","text":"Add a bucket policy that denies PUT operations that don't contain the HTTP header `x-amz-server-side-encryption: SSE:C`","correct":false}]},{"id":"ac3347a8-a8e9-482f-b9e1-cf55b44e51f4","domain":"mon-trb","question":"You are hosting your website in an S3 bucket located in us-east-1, however many of your users are located in India, Africa and Europe and they are experiencing long delays. How can you improve response times for these users?","explanation":"CloudFront can speed up the delivery of your static content to users across the globe. Creating additional buckets and replicating data is not an efficient approach. Connect Direct and ElastiCache cannot be used in the ways described.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/IntroductionUseCases.html#IntroductionUseCasesStaticWebsite","title":"CloudFront Use Cases"}],"answers":[{"id":"a4ad24886aec69baee149f06fe05ad2e","text":"Create 3 additional S3 buckets in regions local to your users and replicate the data across to the new buckets","correct":false},{"id":"019191e8a1a081a3818038b8cdceadd5","text":"Configure a CloudFront CDN","correct":true},{"id":"e225c4ebf2d1a40e5aabd2ecc669e13e","text":"Use ElastiCache to cache the content in each Region","correct":false},{"id":"ecbdb73e217f81efac6bf02c459d6091","text":"Implement Connect Direct","correct":false}]},{"id":"ee1f9799-b2a7-445f-bb89-932b725a8374","domain":"deployment","question":"A business-critical application is deployed using CloudFormation. The team would like to prevent accidental deletion of the stack. How can this be achieved most efficiently?","explanation":"Termination Protection stack option can be enabled to prevent accidental deletion of an entire CloudFormation stack. It is possible to use IAM policy to prevent deletion of a CloudFormation stack, however, this is not the optimal solution from operations and management perspective. The DeletionPolicy CloudFormation attribute applies to individual resources, not an entire stack. There is no DeletionProtection attribute in CloudFormation.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html","title":"Protecting a Stack From Being Deleted"}],"answers":[{"id":"8093dacb1a766d0f91fa60e48baa87ed","text":"Set Stack Termination Protection to Enable.","correct":true},{"id":"eee064b70f79422e8511f18b251253ef","text":"Create IAM Policy with Effect of Deny for 'cloudformation:DeleteStack' Action.","correct":false},{"id":"558cc27970e2b64a29bdc85f381b8cb9","text":"Set the DeletionPolicy to Retain in the CloudFormation template.","correct":false},{"id":"6983aeb4c0d42bb293c8ec93966aedbb","text":"Set the DeletionProtection to True in the CloudFormation template.","correct":false}]},{"id":"69332ca4-69c7-4b5c-b3fd-3dcc1b2154c6","domain":"security","question":"You would like to test the effect of a new IAM policy which you are planning to attach to a group of developers in your team. Which of the following can you use to check that the policy works as expected?","explanation":"With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies that are attached to IAM users, groups, or roles in your AWS account. You can test which actions are allowed or denied by the selected policies for specific resources.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html","title":"Testing IAM Policies with the IAM Policy Simulator"}],"answers":[{"id":"b6632fa69795d5fabc908fe75210b177","text":"IAM Policy Simulator","correct":true},{"id":"543096643aa6d28d9fac278e9257783d","text":"Amazon Inspector","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false}]},{"id":"45bb762d-32a6-4fc7-a9e3-1377f0161979","domain":"security","question":"Which of the following activities are the responsibility of the customer?","explanation":"Security and Compliance is a shared responsibility between AWS and the customer. The customer assumes responsibility and management of the guest operating system (including updates and security patches), other associated application software as well as the configuration of the AWS-provided security group firewall. AWS is responsible for protecting the infrastructure that runs all of the services offered in the AWS Cloud. This infrastructure is composed of the hardware, software, networking, and facilities that run AWS Cloud services.","links":[{"url":"https://aws.amazon.com/compliance/shared-responsibility-model/","title":"AWS Shared Responsibility Model"}],"answers":[{"id":"a505eb81276d69b88b77d5b605ad4a9a","text":"Safe disposal of storage devices","correct":false},{"id":"2b169fd2a3342cf14cd9fdfca94943c5","text":"Security Group configuration settings","correct":true},{"id":"24950338a19d0ddaa9b785b69709702f","text":"Encryption of sensitive data","correct":true},{"id":"a66d0f8d16b1d93bbe53e387d4b62b37","text":"Controls around who can physically access the data center","correct":false},{"id":"e64e7c083b43e01c37b09547a9d7fa31","text":"Management of user credentials","correct":true}]},{"id":"2689a73b-04ed-4719-9cdf-49c4ffe3eb17","domain":"deployment","question":"A developer is deploying a new application to ECS. The application requires permissions to send messages to an SQS queue. \n\nWhich role should the developer apply the policy to so that the application can access the SQS queue?","explanation":"The policy must be attached to the ECS Task's execution role to allow the application running in the container access SQS.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html","title":"Amazon ECS Task Execution IAM Role"}],"answers":[{"id":"2413a4993d7f9395b5ddf84bc7b51b62","text":"The execution role attached to the ECS Container.","correct":false},{"id":"d89a15f255b7e5a2b6cfe8aa7d628173","text":"The execution role attached to the ECS Cluster.","correct":false},{"id":"29aefbd6a89941ca26d2ef879d25237c","text":"The execution role attached to the ECS Task.","correct":true},{"id":"2d64ba22b9f8e2998aed499099374359","text":"The execution role attached to the ECS Service.","correct":false}]},{"id":"1bebd0ce-024b-4592-9a82-af0d2a38f135","domain":"deployment","question":"What action does CloudFormation take if a creation of a stack fails?","explanation":"If stack creation fails, AWS CloudFormation rolls back any changes by deleting the resources that it created.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html","title":"How Does AWS CloudFormation Work?"}],"answers":[{"id":"9bfaf040c6904ecc9d4771bb81398d24","text":"It creates a SNS notification.","correct":false},{"id":"cd021a755a83b7333bd29e58e4cf61fc","text":"It skips the creation of the resource and continues with creations of other resources in the template.","correct":false},{"id":"38a8b80a58bb9367d4f7106ea09c7f94","text":"It rolls back the stack and deletes any resources that have been created.","correct":true},{"id":"134c4b16aa963269be7f346441209384","text":"It stops the creation of the resource and any other resources in the template.","correct":false}]},{"id":"da2f7443-8627-4f31-9beb-711bd65a2174","domain":"mon-trb","question":"You are using X-Ray to monitor your application for performance and troubleshooting purposes. You would like to be able to index and filter the traces based on values specific to your project. How can you do this?","explanation":"When you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segment document as annotations and metadata. Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations","title":"X-Ray Annotations and Metadata"},{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html","title":"Searching for Traces in the AWS X-Ray Console with Filter Expressions"}],"answers":[{"id":"ced8ac214e29963404562f08022a2cdc","text":"Use Athena to run a SQL query on the traces. Index and filter the results using the X-Ray console.","correct":false},{"id":"d8b9bb01f27c4c5440eba0ad6ba60978","text":"Configure annotations in your traces so that they can be indexed and filtered in the X-Ray console, based on the annotations.","correct":true},{"id":"88ee3b02928f9daa9156cd844af82821","text":"Use DynamoDB to index and query the traces. Filter the results using a projection expression","correct":false},{"id":"bd0c045fc4ca2541fefe83505936ba0d","text":"Configure annotations in the X-Ray console and use the X-Ray daemon to filter and index the traces.","correct":false}]},{"id":"7b9da250-0466-4f74-a5f4-a48611e4ad52","domain":"deployment","question":"Which of the following could you NOT achieve using the Amazon SQS Extended Client Library for Java?","explanation":"You can use Amazon S3 and the Amazon SQS Extended Client Library for Java to manage Amazon SQS messages stored in S3. This includes specifying when messages should be stored in S3, referencing message objects stored in S3, getting them, and deleting them.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html","title":"Managing Large Amazon SQS Messages Using Amazon S3"}],"answers":[{"id":"b236aa2fd73c138dd311cd5ab9732edb","text":"Get a message object from an S3 bucket","correct":false},{"id":"d6cda81adfce3f5709327b9e75258e8a","text":"Manage large SQS Messages stored on S3","correct":false},{"id":"c864c8d423d9f1ff9520db533d3c55f0","text":"Create a new S3 bucket and move a batch of SQS messages into the bucket","correct":true},{"id":"d807fc120f6730850ff6425f9e3c48a0","text":"Specify whether messages are always stored in Amazon S3 or only when the size of a message exceeds 256 KB","correct":false},{"id":"0c34c3d6deafa4117735272f76eb3ab8","text":"Delete a message object from an Amazon S3 bucket","correct":false}]},{"id":"7d99fa38-f31e-4527-95a6-a88611f7731c","domain":"mon-trb","question":"A developer deployed a serverless application consisting of an API Gateway and Lambda function using CloudFormation. Testing of the application resulted in a 500 status code and 'Execution failed due to configuration' error. What is a possible cause of the error?","explanation":"When you build an API Gateway API with standard Lambda integration using the API Gateway console, the console automatically adds the required permissions. However, when you set up a stage variable to call a Lambda function through your API, you must manually add these permissions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html","title":"Invoke"},{"url":"https://docs.aws.amazon.com/apigateway/api-reference/handling-errors/#api-error-codes","title":"Error Codes (Client and Server Errors)"}],"answers":[{"id":"8fa26152f6981113cb014274ea900af1","text":"Too many API Gateway requests were created exceeding the allowed limit.","correct":false},{"id":"dddca3f487612188440bbfe93d1ef829","text":"IAM policy is restricting the user from invoking the API Gateway API endpoint.","correct":false},{"id":"d69e5b40405e5538753a24c15bb80a0e","text":"The Lambda function's resource-based policy doesn't include permission for your API to invoke the function.","correct":true},{"id":"eef7bf395052a4cb6bf0286cc9578779","text":"API Gateway is not authorized to invoke the Lambda function.","correct":false}]},{"id":"5953c122-dbdd-4d25-a8fe-3e1fd23a6c8f","domain":"mon-trb","question":"An application developer finds that performing a scan operation on a large DynamoDB table is taking a long time to execute.  What can be used to improve the performance and decrease the execution time of the scan operation?","explanation":"Parallel scans can be used by multiple worker threads in an application to perform a scan of a DynamoDB table much faster. Filter expression in a scan operation only filters the results.  Scan operation still performs the same amount of read operations. Projection expression is used to limit the attributes returned by the scan operation.  It reduces the size of the payload of the scan operation, but it does not affect the speed of the scan operation. Pagination can be used to divide the result set into multiple pages, but does not increase the performance of the scan operation.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan","title":"Parallel Scan"}],"answers":[{"id":"7eeca829a717872bbf53c3ff8a69e41a","text":"Use of pagination.","correct":false},{"id":"62fef50451cb13a49660c69ae13e7932","text":"Use of filter expression.","correct":false},{"id":"e262b00e3caa1a25ec14571777714e9c","text":"Use of projection expression.","correct":false},{"id":"b68d0e7d10bb7958f9b4b5fa43f06ac3","text":"Use of parallel scans.","correct":true}]},{"id":"943289f7-db1b-4439-b4c2-915168f617f9","domain":"development","question":"You work for a company which facilitates and organizes technical conferences. You ran a large number of events this year with many high profile speakers and would like to enable your customers to access videos of the most popular presentations. You have stored all your content in S3, but you would like to restrict access so that people can only access the videos after logging into your website. How should you configure this?","explanation":"All objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. Anyone who receives the pre-signed URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a pre-signed URL.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html","title":"Serving Private Content"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html","title":"CloudFront and Signed URLs"}],"answers":[{"id":"1925533bcb7e305a66391f083dd879d9","text":"Use CloudFront with HTTPS to enable secure access to the videos","correct":false},{"id":"ccfccd796a677ec7b7686f3d8c21a602","text":"Remove public read access from the S3 bucket where the videos are stored","correct":true},{"id":"07b930b04afb83395edadd40528e8336","text":"Use web identity federation with temporary credentials allowing access to the videos","correct":false},{"id":"90d89af9eb0c2a7b8b70ca4a300d9920","text":"Share the videos by creating a pre-signed URL","correct":true},{"id":"9d3d0a72e06711a3345d6dd192885795","text":"Use SSE-S3 to generate a signed URL","correct":false}]},{"id":"f7a67868-ec20-40ae-a334-1bca9d02bdb7","domain":"deployment","question":"You are developing a website which allows customers to purchase tickets to popular sporting events. Your application uses S3 for static web hosting, Lambda for business logic, stores transaction data in RDS and uses DynamoDB for product and stock information. After the customer has paid for their purchase, a message is sent to an SQS queue to trigger a confirmation email to be sent out to the customer including an e-ticket for their chosen event. You want to send out the email as soon as the payment has been processed, however during testing you discover that the confirmation emails are being processed a few seconds before the stock control database has finished updating. This sometimes results in selling the same ticket twice. How can you quickly fix this without re-engineering the application?","explanation":"Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html","title":"SQS Delay Queues"}],"answers":[{"id":"dc7f3b8fe45a2bd74a545027f1178792","text":"Use an SQS delay queue to let you postpone the delivery of SQS messages by a few seconds","correct":true},{"id":"2c519c2e315e56078976351bc313bbf9","text":"Use a FIFO queue to ensure the messages are always processed in the correct order","correct":false},{"id":"953522fe58e17a4a74978232a8b50608","text":"Set the delay flag on the queue to 5 seconds, to ensure messages are not processed too quickly","correct":false},{"id":"93afc9503f6f41866e3fb6bf957bc816","text":"Use Kinesis to stream the SQS messages, adding a delay of a few seconds","correct":false}]},{"id":"c1fc5f56-f74e-405f-a974-d9bb2e2c57e6","domain":"deployment","question":"You have deployed a new version of your Lambda function, however during testing, you notice that  your application is not behaving as expected. How can you roll back to the previous version of your code?","explanation":"Remapping the PROD alias to the previous version will allow you to quickly roll back","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda versioning and Aliases"}],"answers":[{"id":"aa97bfdc9437ce44352de51699501c4d","text":"Make a new version of your function using the original Lambda code","correct":false},{"id":"28ab809ddc3a2aee352db2592bca020a","text":"Remap the PROD alias to point to the previous version of your function","correct":true},{"id":"1ba636d1ad4c4e2a9239fc75d17ffc41","text":"Update the $LATEST alias to point to the previous version of your function","correct":false},{"id":"60682da7d7f6df421c71e7e42cf4b227","text":"Redeploy your original code to $LATEST","correct":false}]},{"id":"422c80de-8e21-4706-ab03-ce11c4cfa083","domain":"deployment","question":"You are developing a completely serverless application using Lambda and API Gateway. You need a place to persist data as key-value pairs and your application will need low latency access to the data. Which of the following is the best option for storing this data?","explanation":"DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. RDS does not store data as key-value pairs, a JSON document in S3 is not an efficient way to store the data and using an EC2 instance would not be serverless and would not be scalable.","links":[{"url":"https://aws.amazon.com/dynamodb/","title":"DynamoDB Overview"}],"answers":[{"id":"023289eeb117bb428a4288ca492fd5e8","text":"Store the data in a DynamoDB table","correct":true},{"id":"5d9c9d9ca04e432986c67415ccfdb791","text":"Store the data in JSON format on an EC2 instance","correct":false},{"id":"866706615c3cfcbd5b3dfc3bbd116f85","text":"Store the data in JSON format in an S3 bucket","correct":false},{"id":"243a3f875904542a9e0117f28650b59f","text":"Store the data in an RDS database","correct":false}]},{"id":"32c62228-3eab-45da-8f39-58fad0f1b614","domain":"refactoring","question":"You are building an application that requires an Auto Scaling group in order to scale in and out based on demand. You have the proper IAM permissions to create an Auto Scaling group, and also to create EC2 resources for the instances. What additional requirements are necessary for you to move forward?","explanation":"When you create an Auto Scaling group, you must specify the necessary information to configure the Amazon EC2 instances, the subnets for the instances, and the initial number of instances. Before you can create an Auto Scaling group using a launch template, you must create a launch template that includes the parameters required to launch an EC2 instance, such as the Amazon Machine Image (AMI) ID and an instance type. Key pairs and instance roles are optional configuration settings when creating the launch template. You do not have to create a security group beforehand as AWS will provide a default security group. Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them. These are optional configurations that can be attached to an Auto Scaling policy.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-launch-template.html","title":"Creating an Auto Scaling Group Using a Launch Template"}],"answers":[{"id":"751a4eab03ae6623d345ab81ca8518ff","text":"Create a launch template with the required Amazon Machine Image (AMI) content.","correct":true},{"id":"f5818eb3488ca8de4b28fd00844c91ab","text":"Specify a lifecycle hook to perform actions as the Auto Scaling group launches or terminates instances.","correct":false},{"id":"3f28a4f490092b9940ca1ee7b2395d69","text":"Create a launch template with the key pair and instance role template contents.","correct":false},{"id":"abc589d96f375d4ec6b80da2495c01f3","text":"Create a security group first and select the appropriate security group under network settings.","correct":false}]},{"id":"777f1a0d-0acd-4907-be1f-ceb4266d3170","domain":"development","question":"You are developing a Lambda function which takes an average of 20 seconds to execute. During performance testing, you are trying to simulate peak loads, however soon after the testing begins, you notice that requests are failing with a throttling error. What could be the problem?","explanation":"When requests come in faster than your function can scale, or when your function is at maximum concurrency, additional requests fail with a throttling error (429 status code).","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/limits.html","title":"Lambda Limits"},{"url":"https://aws.amazon.com/about-aws/whats-new/2017/05/aws-lambda-raises-default-concurrent-execution-limit/","title":"Default Concurrent Execution Limit"}],"answers":[{"id":"396cb0f44930bc3d21771b4f8c6e7afd","text":"You have reached the limit of concurrent executions for Lambda","correct":true},{"id":"67713e6254fbdabc3504f77f455cf500","text":"The deployment package is too large","correct":false},{"id":"f33d22d5ee65a1e9f86c1631c889d199","text":"The Lambda function is taking too long to execute","correct":false},{"id":"5f877189f6b1d52e35220d6fac989254","text":"Your application does not have permission to invoke the Lambda function","correct":false},{"id":"e7f379a6b188c5064bd7c1bf17994433","text":"You haven't allocated enough memory to your function","correct":false}]},{"id":"c73f812b-373b-4429-9a32-a3d71186c137","domain":"deployment","question":"Your application needs 100 strongly consistent reads on items that are 9KB in size every second. How many units of read capacity units should you provision?","explanation":"9KB rounds up to 12KB. 12KB/4KB=3 strongly consistent read capacity units each. 3*100=300 strongly consistent read capacity units.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html","title":"Throughput Capacity for Reads and Writes"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html","title":"DDB - Read Consistency"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CapacityUnitCalculations.html","title":"Calculating CU"}],"answers":[{"id":"7ef605fc8dba5425d6965fbd4c8fbe1f","text":"150","correct":false},{"id":"94f6d7e04a4d452035300f18b984988c","text":"300","correct":true},{"id":"9de6d14fff9806d4bcd1ef555be766cd","text":"350","correct":false},{"id":"3644a684f98ea8fe223c713b77189a77","text":"200","correct":false}]},{"id":"0394976d-f5c9-489a-8411-0dca671a3f67","domain":"deployment","question":"An application connects to an external third-party service with API keys being managed by AWS Secrets Manager. The development team uses CodeBuild for source code compilation activities in their CI/CD process.   Where should the reference to the third-party service API keys be specified?","explanation":"CodeBuild uses the BuildSpec file as a specification of build commands and settings.  “secrets-manager” syntax can be used to retrieve API Keys stored in AWS Secrets Manager. Build Environment variables should NOT be used for storing sensitive information as they are displayed in plain text. AppSpec file is used by CodeDeploy to specify and manage deployments. CloudFormation templates are used by CloudFormation and not by CodeBuild.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html","title":"Build Specification Reference for CodeBuild"}],"answers":[{"id":"ef56db4cbf113c83bf357241b3fe4418","text":"BuildSpec file","correct":true},{"id":"5bd169bcf0b77c844c2f4bf1bf3eac9d","text":"CloudFormation Template","correct":false},{"id":"47d4255c389a3bcdca135b3871c5238d","text":"CodeBuild Environment Variable","correct":false},{"id":"3e1115820889c27bfd8aca3774b9c1f0","text":"AppSpec file","correct":false}]},{"id":"44e023dc-183c-4d0a-8c35-a1779dcd7437","domain":"refactoring","question":"Your organization wants you to lead a development project that will perform real-time processing. The application requires the analytics and field teams to respond promptly to emerging situations based on server activity, website clicks, geo-location of devices, people, and service usage. As the development lead, what combination of services would you recommend to build out this project most efficiently and cost-effectively?","explanation":"Streaming data capture and processing is called real-time processing. The best AWS solution in this case is Amazon Kinesis. You can process data captured and stored with Kinesis sequentially and incrementally on a record-by-record basis or over sliding time windows, and use the processed data for a wide variety of analytics including correlations, aggregations, filtering, and sampling. Use AWS Lambda to process streaming data in real time. Lambda can process the data directly from Kinesis Streams, and lets you run code without provisioning or managing servers with help reduce costs. Running regular queries on Redshift is inefficient and expensive. S3 buckets will be able to store data, but won't be able to process data in real-time as efficiently as Kinesis. Amazon Aurora is a relational database solution and does not fulfill the real-time processing requirement.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html","title":"Amazon Kinesis Data Streams Terminology and Concepts"}],"answers":[{"id":"5d7d039072621b361ce8bde466f0efc4","text":"Use Amazon Kinesis to capture and store streaming data. Process streaming data with Lambda.","correct":true},{"id":"f255642584a78d5f043cde30cd580abc","text":"Use Amazon Aurora to store data in real time. Aurora will automatically replicate the data in multiple Availability Zones. Build an application on EC2 that users can call using APIs to retrieve the relevant information they need.","correct":false},{"id":"24f2f053a48d8b02ceaae4dc44a376ca","text":"Store the data on Amazon Redshift. Run queries on the Redshift cluster regularly to refresh a dashboard built on Amazon QuickSight.","correct":false},{"id":"5a50b0d5b285340be18f4a4c59ba1fd2","text":"Store all data in an S3 bucket with the correct prefixes. Develop Lambda functions for each prefix that will routinely scan and extract necessary information to another S3 bucket that will be the source for an Amazon QuickSight dashboard.","correct":false}]},{"id":"1710c298-c975-4762-8948-da98b2900d8a","domain":"development","question":"You are developing a web application which has been deployed using Lambda. Today you updated the code and uploaded the new version of your code to the Lambda console. Your test team have begun testing but have reported today that the application seems to still be using the original code. What could be the reason for this?","explanation":"The problem is that the application is referencing the function using an alias pointing to a previous version of the code. When you use versioning in AWS Lambda, you can publish one or more versions of your function. So that you can use different variations of your Lambda function in your development workflow, such as development, beta, and production. Lambda also supports creating aliases for each of your Lambda function versions. Conceptually, an AWS Lambda alias is a pointer to a specific Lambda function version. You can update aliases to point to different versions of functions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda Function Versioning and Aliases"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/how-to-manage-versioning.html","title":"Lambda Versioning"}],"answers":[{"id":"b099c8a69bf8eac2d39f02ac07d53702","text":"Your application is referencing the function using an alias which points to a previous version of the code","correct":true},{"id":"ce472c670d174e72811d57156684bb1e","text":"You forgot to publish the version","correct":false},{"id":"2fcd244fd0d8e7833b6bf94d70f01295","text":"Your application is referencing the function using an unqualified ARN","correct":false},{"id":"078229fa3cd7070c357a105fd9f1584d","text":"Your application is referencing the function using a qualified ARN","correct":false},{"id":"b67108883bf2590e9f79b9ab834a84cd","text":"Your application is referencing the function using $LATEST","correct":false}]},{"id":"4834c926-4c2e-11ea-b77f-2e728ce88125","domain":"development","question":"You want to create a continuous delivery pipeline with a build tool recommended by your Project Manager. However, you anticipate your build project to be large and complex. Which of the following AWS services will enable you to orchestrate complex pipelines?","explanation":"True to its naming, AWS CodePipeline is what you must use with the recommended build tool to make it easier to create your delivery pipelines. CodeDeploy is used for automatically deploying code, not creating pipelines. CodeBuild is for building and testing code. Although Jenkins is a build tool, it is not an AWS service.","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html","title":"What Is AWS CodePipeline?"},{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"ebf976dafc560e096501509079ba441c","text":"AWS CodePipeline","correct":true},{"id":"7be6a0f173a965818e92c1df46c6626e","text":"AWS CodeDeploy","correct":false},{"id":"2e54334c0a5ce2e3e5a5845df3ab3ada","text":"Jenkins","correct":false},{"id":"ff02d3f07f94fff5aa3bdd045211b9e2","text":"AWS CodeBuild","correct":false}]},{"id":"d32ba0f0-1563-4a30-9e61-b5edf82d628e","domain":"security","question":"You are developing a legacy application which handles confidential healthcare data. The application runs on two EC2 instances behind an Application Load Balancer. Because of the age of the application, you cannot perform TLS encryption on the EC2 instances themselves. What is the least complex way you can ensure data is encrypted in transit between your VPC, and the customer who will be accessing it?","explanation":"Handling the TLS termination process within each EC2 instance adds to the computational load on the instance as well as the operational overhead of installing an X.509 certificate on each instance. You can easily arrange for the entire HTTPS encryption and decryption process, generally known as TLS termination to be handled by an Elastic Load Balancer. Your users can benefit from encrypted communication with very little operational overhead or administrative complexity.","links":[{"url":"https://aws.amazon.com/blogs/aws/elastic-load-balancer-support-for-ssl-termination/","title":"AWS Elastic Load Balancing: Support for SSL Termination"}],"answers":[{"id":"ef68b9b0058c7aa4c84143939156e93b","text":"Perform TLS termination on the ALB","correct":true},{"id":"f81774e2c858b52c0321762e8489f624","text":"Require customers to connect through a VPN to a virtual private gateway","correct":false},{"id":"18c056fd0ece19fd238f6d245fe85321","text":"Upgrade the application to support TLS on the EC2 Instances","correct":false},{"id":"4567a70530701c962bab095e279dfb3b","text":"Perform TLS termination using Lambda","correct":false}]},{"id":"6db2c293-ed5d-41ff-84fb-803e2970a0f9","domain":"development","question":"A developer is working on a new application which will use DynamoDB. One of the DynamoDB tables that the developer must create requires an index sort key. When creating this DynamoDB table, the developer must select an Attribute Type for the sort key.\n\nWhich of the following DynamoDB data types can the developer select to use for their index sort key?","explanation":"Both partition and sort keys attributes must be defined as type string, number, or binary.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.NamingRulesDataTypes.html#HowItWorks.DataTypes","title":"DynamoDB Data Types"}],"answers":[{"id":"4ee29ca12c7d126654bd0e5275de6135","text":"List","correct":false},{"id":"27118326006d3829667a400ad23d5d98","text":"String","correct":true},{"id":"6ce976e8f061b2b5cfe4d0c50c3405dd","text":"Binary","correct":true},{"id":"46f3ea056caa3126b91f3f70beea068c","text":"Map","correct":false},{"id":"b2ee912b91d69b435159c7c3f6df7f5f","text":"Number","correct":true},{"id":"27226c864bac7454a8504f8edb15d95b","text":"Boolean","correct":false}]},{"id":"24137be2-c9b8-4be2-a4f0-0a5476fc15f9","domain":"development","question":"A developer has been tasked with migrating a large legacy web application, written in C++, to AWS. The developer wants to benefit from using Elastic Beanstalk to simplify the management of the infrastructure.\n\nWhich of the following methods would allow the developer to migrate the application with the least amount of work?\n","explanation":"Elastic Beanstalk supports Docker containers and custom AMIs via Packer. Both would allow the legacy application to be wrapped in a layer of abstraction such that Elastic Beanstalk itself would not need to support the specific language of the legacy application.\n\nThe Go platform only supports applications written in Go.\n\nThe application could be re-written in Node.js, but as it's a large application, a full rewrite is unlikely to require the least amount of work.\n\nElastic Beanstalk cannot be used to manage Lambda functions.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.html","title":"Docker on Elastic Beanstalk"},{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/custom-platforms.html","title":"Packer with Elastic Beanstalk"}],"answers":[{"id":"8711e00909d504f51f9e7cc0aea8ed4d","text":"Use Packer to generate a custom AMI that contains the application, which can then be deployed via Elastic Beanstalk.","correct":true},{"id":"680517836f5d3fd52719bcc51f4f57e9","text":"Use the Go platform, which can support any compiled language such as C++.","correct":false},{"id":"37d73c08ebaf765061f6ad4fe756430a","text":"Create a custom Lambda layer with a C++ runtime, which can be called from within Elastic Beanstalk.","correct":false},{"id":"ea130ff77bca2e340acf49c464cdd620","text":"Rewrite the application in Node.js, which can then run natively via Elastic Beanstalk.","correct":false},{"id":"dadd577f7c15163f58854c17403e2a6d","text":"Use Docker to containerize the application, which can then be deployed via Elastic Beanstalk.","correct":true}]},{"id":"ed0618e6-a6db-4b7e-8564-384ae63b0e89","domain":"development","question":"You are working for a small but busy veterinary surgery and you need to design a new DynamoDB table to store information relating to customers, their pets, and any medications that are currently being prescribed. Which of the following attributes would be a good choice for a partition key, in order to achieve maximum provisioned throughput efficiency?","explanation":"When selecting a partition key, you want to distribute the workload evenly across as many partitions as you can, to maximize provisioned throughput of your DynamoDB table. The partition key determines which partition the record will be stored on. To achieve maximum provisioned throughput, choose a partition key with a unique attribute like Customer ID, Product ID, email address, phone number etc. A partition key design that does not distribute I/O requests evenly can create hot partitions which result in throttling and uses your provisioned I/O capacity inefficiently. Values such as Medication, Species and Registration date, are not unique and in some cases may have only a few possible values which could result in hot partitions and inefficient use of provisioned throughput.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html","title":"Designing Partition Keys to Distribute Your Workload Evenly"}],"answers":[{"id":"353bd6f65060d17097c3b03141e79cce","text":"Medication","correct":false},{"id":"22ffd0379431f3b615eb8292f6c31d12","text":"Registration date","correct":false},{"id":"e1520b5997a532c7889f6e8883920ab8","text":"Species","correct":false},{"id":"d37c2bf1bd3143847fca087b354f920e","text":"Customer ID","correct":true}]},{"id":"5820892f-2759-4cd8-be11-8b2dbcc5d1b5","domain":"development","question":"Your Lambda function requires a few libraries which are not available as standard in the Lambda runtime environment. Which of the following is a recommended way to make the libraries available to your function?","explanation":"A deployment package is a ZIP archive that contains your function code and dependencies. You need to create a deployment package if you use the Lambda API to manage functions, or if you need to include libraries and dependencies other than the AWS SDK. You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html","title":"AWS Lambda Deployment Package"}],"answers":[{"id":"a74c95d1ac4aa0e191861afb10c345d4","text":"Create a deployment package containing your function code and libraries","correct":true},{"id":"c5aba36fa7ce91daccd7cb3a4e3cf9a1","text":"Add the dependencies to S3 and create an environment variable to reference them","correct":false},{"id":"d43e57bd5184ca5787e6827646b30fa5","text":"Create a handler function downloads the libraries you need","correct":false},{"id":"24030d747fefc92f6c68f2934974ba96","text":"Upload the deployment package to Lambda","correct":true},{"id":"f21d58a10f3609c9feab39647fa533cf","text":"Store the deployment package in an S3 bucket and then upload it to Lambda","correct":true},{"id":"585237ca133be02e26db990e229ab6f4","text":"Create a custom runtime which includes the libraries you need","correct":false}]},{"id":"4a84dc95-7697-4a71-8580-d525a1a85a01","domain":"deployment","question":"You have an application running on a number of Docker containers running on AWS Elastic Container Service. You have noticed significant performance degradation after you made a number of changes to the application and would like to troubleshoot the application end-to-end to find out where the problem lies. What should you do?","explanation":"Within a micro-service architecture, each application component runs as its own service. Micro-services are built around business capabilities, and each service performs a single function. So if you want to add X-Ray to a Dockerized application, it makes the most sense to run the X-Ray daemon in a new Docker container and have it run alongside the other micro-services which make up your application.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html","title":"Running the X-Ray Daemon on Amazon ECS"},{"url":"https://aws.amazon.com/microservices/","title":"What Are Microservices?"},{"url":"https://aws.amazon.com/getting-started/projects/break-monolith-app-microservices-ecs-docker-ec2/","title":"From Monolithic Applications To Microservices"}],"answers":[{"id":"4edd5ed4f080880075a9f5769b027d04","text":"Deploy the AWS X-Ray daemon to each of the Docker containers that run your application","correct":false},{"id":"27c3d436162806a72afb7bf932c81dfa","text":"Deploy the AWS X-Ray daemon as a new container alongside your application","correct":true},{"id":"92f001f3f23ff4bd8620bed17c3978dc","text":"In the ECS console, configure the application to send telemetry records to AWS X-Ray","correct":false},{"id":"f25dfc897ebef8cc66db27b116ee6416","text":"Deploy the AWS X-Ray daemon onto an EC2 instance","correct":false}]},{"id":"f71363df-7941-4a84-93e1-7a5ed2ef1c08","domain":"security","question":"You have some sensitive data that you would like to encrypt. You want to be sure that once the data is encrypted, nobody but you will be able to use the encryption key to decrypt your files. Your head of security has asked you to make sure that the key used to encrypt your files is itself encrypted under another key. Which AWS technology enables this?","explanation":"When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key, and then encrypting the data key under another key.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping","title":"Envelope Encryption"}],"answers":[{"id":"bd2c15ddb6cb6e3fee2adc749979e1e1","text":"Store the encryption keys in CloudHSM","correct":false},{"id":"c5c0340c8e4f6673916050e3cbc5e2b2","text":"Store the encryption key in an encrypted S3 bucket","correct":false},{"id":"fdf5291377e4e56d4ee76f0328313232","text":"Use envelope encryption to encrypt the data key with another key","correct":true},{"id":"834369e6ea722aa59f60696c106ba827","text":"Re-encrypt the data key with the master key","correct":false},{"id":"8afab6f0bb19c0796620267f6a74644b","text":"Encrypt the master key with the data key","correct":false}]},{"id":"a45d7c37-eb4b-4a39-9fb2-d5298cb40491","domain":"security","question":"You work for a large I.T. recruitment company that are launching a mobile application which will allow job seekers to apply for jobs online and attach their résumé to their application. Users will be able to log in to their account using Facebook and the application stores their contact and profile details in a DynamoDB table. Which of the following approaches would you recommend for enabling the users to gain access to view and update their data?","explanation":"With Web Identity Federation, you don't need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google. For most Web Identity Federation scenarios, we recommend that you use Amazon Cognito because it acts as an identity broker and does much of the federation work for you.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html","title":"Web Identity Federation"}],"answers":[{"id":"3b5209dda18fcdea46a196b06d17c586","text":"Configure Web Identity Federation with Cognito","correct":true},{"id":"e0bc3be9eb85e3ef437aac25d15f5be4","text":"Allow customers to embed user credentials in settings of the mobile app","correct":false},{"id":"f11ad00f139ee7fd7ecde821e3769c1a","text":"Configure cross-account access between the mobile app and DynamoDB ","correct":false},{"id":"ff1fd8b56d2c836edd2795619fa9b681","text":"Configure Web Identity Federation with ADFS","correct":false}]},{"id":"5d9b4539-7320-47c6-82c5-4552d4b3bf29","domain":"development","question":"You are developing an application using multiple AWS services. You need to find a solution to decouple the application components, so that they can fail independently of one another. Which of the following AWS services will enable this?","explanation":"SQS is a fully managed message queuing service that enables you to decouple and scale micro-services, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware, and empowers developers to focus on differentiating work.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html","title":"SQS Developer Guide"}],"answers":[{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":false},{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":false},{"id":"ef4ba2f338cdf4b615ed280ff0b2777d","text":"SQS","correct":true},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":false}]},{"id":"55ffe5c3-b16f-42d6-9b3e-5f4d77890f49","domain":"mon-trb","question":"You are troubleshooting a major incident which has resulted in data loss in your application. Your manager asks if you can provide a time-ordered sequence of any modifications which happened to the items in your DynamoDB table over the past 24 hours so that you can work out what happened. Which service could you use to most effectively provide this?","explanation":"DynamoDB Streams captures a time-ordered sequence of item-level modifications in a DynamoDB table and durably stores the information for up to 24 hours.","links":[{"url":"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/","title":"DynamoDB Streams Use Cases"}],"answers":[{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"48d30a1b2a41bc37892197dc4df30262","text":"Kinesis Streams","correct":false},{"id":"1f60690ba7c488a02416a7bf195f900b","text":"DynamoDB Streams","correct":true}]},{"id":"15af205c-dba5-4175-8236-b5a7af3e70ec","domain":"deployment","question":"You are a developer for a news, entertainment, lifestyle, and fashion website. User traffic has steadily increased month-over-month, and you are now tasked with cost optimizing the website. The website is currently served from an EC2 instance that is part of an auto-scaling group behind an elastic load balancer. Your manager and CTO have approved a complete re-structuring of the websites architecture in order to accommodate future growth. How would you optimize your application in the MOST cost-effective way?","explanation":"Serverless approaches are ideal for applications where load can vary dynamically. Using a serverless approach means no compute costs are incurred when there is no end user traffic, while still offering instant scale to meet high demand, such as a flash sale on an e-commerce site or a social media mention that drives a sudden wave of traffic. Compared to traditional infrastructure approaches, it is also often significantly less expensive to develop, deliver, and operate a web or mobile backend when it has been architected in a serverless fashion. The question allows architectural restructuring and serverless would be the best approach. Moving the application on-premise would not be cost effective as it would change your from variable to operational costs. Changing the scale-in policy would help reduce costs but would not be as cost effective as going serverless. Implementing CloudFront would increase costs overall.","links":[{"url":"https://d0.awsstatic.com/whitepapers/optimizing-enterprise-economics-serverless-architectures.pdf","title":"Optimizing Enterprise Economics with Serverless Architectures"}],"answers":[{"id":"1a981591e9c0499dbcb77019a1598dbc","text":"Edit the scale-in policy within auto scaling to terminate instances aggressively when demand is low.","correct":false},{"id":"59ba0fde539e6373d2e3b860d2ce4aca","text":"Move the application on-premise. You'll be able to fully manage the application, and purchase additional servers when appropriate.","correct":false},{"id":"f4f5510799286ba3bec7cef229ad2bc8","text":"Move the website to a serverless application. Use S3 to host the website. Use a combination of Lambda and API Gateway to support dynamic API requests.","correct":true},{"id":"ca56c5360ae4b7c7063a9f870f717088","text":"Implement CloudFront in front of the EC2 instance as the origin.","correct":false}]},{"id":"1a967bcb-8238-4c95-b10c-e832a682b14b","domain":"refactoring","question":"Which of the following protocols does API Gateway support?","explanation":"API Gateway supports RESTful APIs, however the legacy SOAP protocol, which returns results in xml format, is also supported in pass-through mode.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html","title":"API Gateway Developer Guide"}],"answers":[{"id":"524de3d2ade4544176f60702b36fbfdf","text":"GraphQL","correct":false},{"id":"50780f47f6839d47d60bc4555ee00c3f","text":"REST","correct":true},{"id":"a4d55f2834854e93ff57ecb44d1c84cc","text":"O API","correct":false},{"id":"4de91f0fcb9cd3a20b6b1d64610a0a3d","text":"SOAP","correct":true}]},{"id":"08861bdf-6813-43f1-9def-4255492b4533","domain":"mon-trb","question":"Your application is using SQS to send and receive messages, your application needs to receive the messages as soon as they arrive and you need to ensure the architecture is as cost efficient as possible. Which of the following approaches will optimise the cost and performance of the application?","explanation":"In almost all cases, Amazon SQS long polling is preferable to short polling and results in higher performance and reduced cost in the majority of use cases.","links":[{"url":"https://aws.amazon.com/sqs/faqs/","title":"SQS FAQs"}],"answers":[{"id":"33e24a8e438593d0ed4994ce9ea68ccd","text":"Enable Long Polling","correct":true},{"id":"8757c7e091bc29e7b9225334f00531ac","text":"Reduce the total number of message queues","correct":false},{"id":"64dab6674843dc03406bdc90b1c5b21d","text":"Lower the message Visibility Timeout","correct":false},{"id":"9ef993bf7e61fa02b5f6ac0fc8da6b18","text":"Enable Short Polling","correct":false}]},{"id":"51600664-99f9-48f5-97cc-ec860d378f89","domain":"development","question":"Which AWS service allows you to build and model your serverless application as a visual workflow consisting of a series of steps where the output of one stage can be input into another?","explanation":"Step Functions provide this functionality","links":[{"url":"https://aws.amazon.com/step-functions/faqs/","title":"Step Functions FAQ"}],"answers":[{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":false},{"id":"42816db0ecfffdf3baff90b7f2545874","text":"Step Functions","correct":true},{"id":"1f4072738a4917bea022b11256fb46a4","text":"Simple Workflow Service","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":false}]},{"id":"075f4eef-bd5d-49b2-ba18-f94cc15c377f","domain":"deployment","question":"Which of the following statements is correct?","explanation":"A primary key can either be a single-attribute partition key or a composite partition-sort key.","links":[{"url":"https://aws.amazon.com/dynamodb/faqs/#Getting_Started","title":"DynamoDB Query Functionality"}],"answers":[{"id":"e2240879e0c017447c6a1c59d0abd816","text":"In DynamoDB, a primary key must be a single-attribute","correct":false},{"id":"d7f04a911ddfe5da2f4356ffbd52b25a","text":"In DynamoDB, a primary key can be a single-attribute partition key","correct":true},{"id":"20cdfac45e66ccd0a06ccea3d171de20","text":"In DynamoDB, a primary key can be a range of values.","correct":false},{"id":"a6bead04f6018113a477c5e6ebb0e82d","text":"In DynamoDB, a primary key can be composite partition/sort key.","correct":true}]},{"id":"787db7cd-ff6a-46b1-83e7-9fe990e22c5e","domain":"security","question":"A developer is implementing an image sharing website hosted on AWS. Images are stored in a separate S3 bucket. During testing, it is discovered that the website does not load properly because the images are being blocked by the browser. How can the developer resolve this issue?","explanation":"As a security feature, resources in one domain cannot be access by a web-page resources in a different domain. Cross-origin resource sharing (CORS) is a mechanism that enables resources on a web page to be requested from a different domain outside the domain from which the first resource was served. If we are hosting images in a separate S3 bucket, this bucket will have a different domain. To allow these resources to be accessed by the web page domain, we need to configure CORS on the S3 bucket.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html","title":"Cross-Origin Resource Sharing (CORS)"}],"answers":[{"id":"1a29c427267cabba7bd3106de626aa13","text":"Make the S3 bucket public.","correct":false},{"id":"216ba600e977ed0524a78ecba4335037","text":"Enable S3 Transfer Acceleration.","correct":false},{"id":"8f55313a69544c54c5bf4c4a71c9d935","text":"Enable versioning on the S3 bucket.","correct":false},{"id":"d89d59d8223823c1f20371c88b73fc27","text":"Configure CORS on the S3 bucket.","correct":true}]},{"id":"47e074a7-e675-4918-92bf-d9a34b82803c","domain":"security","question":"Your application needs to access content located in an S3 bucket which is residing in a different AWS account, which of the following API calls should be used to gain access?","explanation":"The STS AssumeRole API call returns a set of temporary security credentials which can be used to access AWS resources, including those in a different account","links":[{"url":"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html","title":"Providing temporary access to AWS resources"}],"answers":[{"id":"c2599de6f471b00ab3948981939f8315","text":"STS:GetFederationToken","correct":false},{"id":"818a55892aa657e5ef8dae6d12ee9273","text":"IAM:AddRoleToInstanceProfile","correct":false},{"id":"5854d57d52033e05be8f9fda06330abd","text":"STS:AttachRole","correct":false},{"id":"100979100796827d7bcafe4666e4984f","text":"STS:AssumeRole","correct":true}]},{"id":"0c156bea-473c-4216-b344-51ad15046bdd","domain":"development","question":"Which of the following platforms are supported in ElasticBeanstalk","explanation":"Elastic Beanstalk provides platforms for programming languages (Java, PHP, Python, Ruby, Go), web containers (Tomcat, Passenger, Puma) and Docker containers, with multiple configurations of each.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html","title":"Elastic Beanstalk Supported Platforms"}],"answers":[{"id":"3b2819dd4c24eda2faf2052eef449551","text":"Node.js","correct":true},{"id":"10bff31c030c63d11bd1e7ab82759f6d","text":"Java with Tomcat","correct":true},{"id":"01fc3141bdcacc23a2e09a5e25ea126b","text":"Single Container Docker","correct":true},{"id":"8fd82b8864d71ed7fa12b59e6e34cd1c","text":"Chef","correct":false}]},{"id":"66c6afb4-04e0-4eda-aa5b-745c464d5cad","domain":"mon-trb","question":"You deployed a new Lambda function a few days ago and your code seems to be executing successfully, however when you check CloudWatch there isn't any log data for your function. What could be the reason for this?","explanation":"A service needs to have permissions to write log data to CloudWatch logs, Lambda is associated with an execution role which needs to grant the relevant IAM permissions","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html","title":"Using CloudWatch"}],"answers":[{"id":"dfa1603ecab090efaf75db7e3a0456e3","text":"Your code is taking too long to execute, it could be that your function does not have enough compute resources to generate the log files","correct":false},{"id":"0261bd378f9288b971d24a9076d27d22","text":"The execution role for the Lambda function did not grant permissions to write log data to CloudWatch Logs","correct":true},{"id":"122cf71e8d5e76b7cfa4409633b0e4bf","text":"There is an issue with S3 in your region","correct":false},{"id":"ec216d5f36d3705950b7957bbb31d565","text":"The CloudWatch agent has stopped","correct":false}]},{"id":"4ebf4856-d4c7-4ea6-85c6-602dba6571a3","domain":"development","question":"You are developing a latency-sensitive application which stores a lot of data in DynamoDB. Each item is 3.5KB in size. Which of the following DynamoDB settings would give you the greatest read throughput?","explanation":"A read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. Eventually consistent reads provide greater throughput than strongly consistent.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ProvisionedThroughput.CapacityUnits.Read","title":"DynamoDB Provisioned throughput"}],"answers":[{"id":"882e4855b7af9b2ffa948de81b75cf47","text":"Configure the table with 10 read capacity units and use high-performance reads","correct":false},{"id":"58dedf407e5eb2c525e6732d526eeca0","text":"Configure the table with 10 read capacity units and use strongly consistent reads","correct":false},{"id":"85f208ed09607ef8653aef5841e25848","text":"Configure the table with 15 read capacity units and configure the application to use a scan operation","correct":false},{"id":"53f0f96b15844d8d110ee8bc7174411d","text":"Configure the table with 10 read capacity units and use eventually consistent reads","correct":true},{"id":"fa75717ea7d6fc295eb8d4b34d6ac4f6","text":"Configure the table with 15 read capacity units and use strongly consistent reads","correct":false}]},{"id":"c793570e-264f-4352-8ba9-2c2f3cf6a7e0","domain":"refactoring","question":"A enterprise company is migrating their ERP system from on-premise to AWS.  The ERP system comprises of a stateful web application operating over HTTP. Various components of the system are being implemented as microservices utilizing Docker. What load balancer configuration would be a suitable solution for the ERP system migration to AWS?","explanation":"AWS Application Load Balancer receives incoming traffic and distributes the requests across targets based on evaluation of listener rules. As such, it serves as a load balancer service in AWS. More specifically, AWS Application Load Balancer works at layer 7 of the OSI model, and supports HTTP traffic. Additionally, it provides path based routing thus enabling forwarding of requests based on URL. This functionality supports microservices architecture proposed in the question. Lastly, AWS Application Load Balancer supports sticky sessions, enabling stateful applications. This meets all the requirements specified in the question scenario. Classic Load Balancer supports traffic at layer 7 (HTTP) and sticky sessions. However, it does not provide path based routing ability necessary for a microservices application design. Network load balancer operates at layer 4 of the OSI model, and thus is not appropriate for this scenario. Route53 is used to distribute traffic across geographical regions and so it is not suitable solution in this case.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html","title":"What Is an Application Load Balancer?"}],"answers":[{"id":"2c25600a1df6a32f15194f6fa7c5d056","text":"Network Load Balancer with an Elastic IP.","correct":false},{"id":"fca383fefad22bd3ef8d6ac8a24503c8","text":"Route53 with a CNAME and a CloudFront distribution.","correct":false},{"id":"9bc634e1e1185d8d1e5b96383c28b28a","text":"Application Load Balancer with sticky sessions.","correct":true},{"id":"c92cf9cdebc4be47d4192cffa30a533d","text":"Classic Load Balancer with sticky sessions.","correct":false}]},{"id":"299fae20-4401-11ea-b77f-2e728ce88125","domain":"development","question":"You are about to work on a brand-new feature that you wish to propose for the upcoming second release of a job search engine application. However, you do not want to use the production-ready code. How will you keep your work separate from it?","explanation":"AWS CodeCommit is what you need for creating the code repository necessary for developing the application. To work on the app’s new feature while keeping it separate from production-ready code, you must create a branch off of the default branch, which is the repository's base branch. You create a pull request if you want to review, comment on, and merge code changes from one branch to another. AWS CodeStar is for developing, building, and deploying applications so is not a suitable response. Creating a 'delineator' is not a valid answer; you can’t create a delineator in the repo.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/pull-requests.html","title":"Working with Pull Requests in AWS CodeCommit Repositories"}],"answers":[{"id":"daafb08efeb8922237b6264c42e7de8f","text":"Create a pull request in AWS CodeCommit.","correct":false},{"id":"168edbcd84078a04198e3014d9b51203","text":"Create a branch from your default branch in AWS CodeStar.","correct":false},{"id":"ae6130a05e4453a96fb07e1c7ee587ee","text":"Create a branch from your default branch in AWS CodeCommit.","correct":true},{"id":"adf09a361eb87ccc1da535b146808ec0","text":"Create a delineator between your code for the new feature and the production-ready code in AWS CodeCommit.","correct":false}]},{"id":"868cb94f-ce60-4590-bfd2-60e8295cc413","domain":"security","question":"You are developing a online-banking website which will be accessed by a global customer base. You are planning to use CloudFront to ensure users experience good performance regardless of their location. The Security Architect working on the project asks you to ensure that all requests to CloudFront are encrypted using HTTPS. How can you configure this?","explanation":"Viewer Protocol Policy defines the protocols which can be used to access CloudFront content","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html","title":"Requiring HTTPS for Communication Between Viewers and CloudFront"}],"answers":[{"id":"ae41866f6b2df14a344847e9629076db","text":"Set the User Protocol Policy to redirect HTTP to HTTPS","correct":false},{"id":"659099387a57b1f316abf3c6afac459d","text":"Set the Session Protocol Policy to redirect HTTP to HTTPS","correct":false},{"id":"052066815497ced9f9852e55d66c6782","text":"Set the Request Protocol Policy to redirect HTTP to HTTPS","correct":false},{"id":"c40022183e6d5dd97e4c778332064ed2","text":"Set the Viewer Protocol Policy to redirect HTTP to HTTPS","correct":true}]},{"id":"108f8299-0018-4835-91f6-329fc5f9c1de","domain":"deployment","question":"Which of the following are considered to be Serverless?","explanation":"The following AWS technologies are Serverless: DynamoDB, API Gateway, SNS, Lambda, Kinesis and S3. RDS and Elastic Beanstalk both deploy EC2 instances to run their services","links":[{"url":"https://aws.amazon.com/serverless/","title":"Serverless Computing"}],"answers":[{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":true},{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true},{"id":"2ef9dd82927a3196ca2df3fc0cdf2e0b","text":"API Gateway","correct":true},{"id":"3d6cbd7db2a4fa389808ea6f4a5fc1bc","text":"Elastic Beanstalk","correct":false}]},{"id":"dfa117ef-d974-4b50-86d3-ee991142ac42","domain":"security","question":"You are developing a Lambda function written in Python which uploads a number of sensitive files to S3. The application architect has told you to use client-side encryption to protect the files. How can you do this?","explanation":"Client-side encryption means you need to encrypt the files where they are currently stored before uploading them to S3. You can do this in Lambda by using the AWS Encryption SDK.","links":[{"url":"https://aws.amazon.com/blogs/security/how-to-encrypt-and-decrypt-your-data-with-the-aws-encryption-cli/","title":"How to Encrypt and Decrypt Your Data with the AWS Encryption CLI"},{"url":"https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/crypto-cli.html","title":"AWS Encryption SDK Command Line Interface"},{"url":"https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html","title":"What Is the AWS Encryption SDK?"},{"url":"https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/python-example-code.html","title":"AWS Encryption SDK for Python Example Code"}],"answers":[{"id":"442f843aa3641259b4c36e5565ff02c7","text":"Select the Encryption checkbox in the Lambda console ","correct":false},{"id":"0bac92f5f923eb79514a753141a293ec","text":"Select S3 default encryption","correct":false},{"id":"4de5ebab316fcf09fab5f4f030c8d0cb","text":"Encrypt the files using the AWS Encryption SDK","correct":true},{"id":"349aac318f19b866d43ee7ad35140312","text":"Use SSL to upload the files","correct":false},{"id":"46cf5e7bd245efaa706dc8411b60bf17","text":"Encrypt your local root disk before uploading the files","correct":false}]}]}}}}
