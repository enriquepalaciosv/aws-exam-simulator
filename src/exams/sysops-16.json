{"data":{"createNewExamAttempt":{"attempt":{"id":"9112156f-7b3c-4797-99d8-d818d59ae9a4"},"exam":{"id":"321a56b2-8002-46d9-bfed-36369cf20ab9","title":"AWS Certified SysOps Administrator - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"d5760f8d-25af-48b0-8e78-5d8d3d1e91cc","domain":"dep-prov","question":"You have a DynamoDB table in region ap-southeast-2 that stores users’ subscription data. As more and more users come from Europe, you want to configure a replica table that has the same name and schema in region eu-central-1. When the application writes data to a replica table, DynamoDB should automatically propagate the write to the other one. How should you implement this?","explanation":"Global Table in DynamoDB is a feature that configures DynamoDB as a fully-managed, multi-region and multi-master database. The global table consists of one replica table per Region and DynamoDB automatically keeps the table data in sync between replicas.","links":[{"url":"https://docs.aws.amazon.com/en_pv/amazondynamodb/latest/developerguide/globaltables_HowItWorks.html","title":"DynamoDB Global Tables"}],"answers":[{"id":"dafd6112e9fb08d950b54c7836f9805a","text":"Configure automatic backups for the table in another region.","correct":false},{"id":"82b856354099b33a5dbb0b6e1a12b264","text":"Create a new table in region eu-central-1 with the same name and schema. Sync the original table with the new table.","correct":false},{"id":"849e48fd8762258d9c7afa5c95203db0","text":"Configure a global table in region eu-central-1.","correct":true},{"id":"4de5416858aeddf52eafd42cce3ef87f","text":"Configure a read replica in region eu-central-1.","correct":false}]},{"id":"4871ce12-9da0-4e28-b1e4-373560dbdffa","domain":"automation","question":"A telecommunications company sends out monthly bills to their customers. Usage is accumulated during the month by nightly batch jobs that process call details. The company is in the process of migrating the billing system to AWS to reduce costs. What approach will provide them with the most cost effective solution for the compute portion of their nightly batch runs?","explanation":"AWS Batch provides allocation strategies to consider capacity and throughput in addition to cost when provisioning instances for jobs. This is a newer feature that provides more flexibility than the previous scheme that chose an instance that was the best fit based on vCPU, memory, and GPU requirements. Creating a pool of EC2 Reserved Instances might result in unused capacity if workload requirements change. Lambda is not currently available as a compute resource for AWS Batch.","links":[{"url":"https://aws.amazon.com/batch/","title":"AWS Batch"},{"url":"https://aws.amazon.com/blogs/compute/optimizing-for-cost-availability-and-throughput-by-selecting-your-aws-batch-allocation-strategy/","title":"Optimizing for cost, availability and throughput by selecting your AWS Batch allocation strategy"}],"answers":[{"id":"0bb4d60773589240e55a5a506ee84275","text":"Use AWS Batch allocation strategies to define capacity, throughput, and cost priorities for instance type provisioning.","correct":true},{"id":"96ef18a4d93470acb7dbd558eb666ca3","text":"Specify AWS Lambda as the compute resource for AWS Batch. Invoke the appropriate Lambda functions for each job.","correct":false},{"id":"3220afaa7c6fd31e7a8d35ce1e2df1fa","text":"Configure AWS Batch to choose an instance type for each job based on vCPU, memory, and GPU requirements at the lowest cost.","correct":false},{"id":"8633cd86b6633324660fa073362c2f98","text":"Schedule jobs with AWS Batch into a pool of EC2 Reserved Instances that contains enough servers for the minimum number of jobs that will be run on any one night. Use an Auto Scaling Group to provision Spot Instances to handle any additional demand.","correct":false}]},{"id":"84adda98-8315-454d-b0c1-b6478c5c0d98","domain":"mon-rep","question":"You are performing an update to all of your application servers, however some of your applications are failing following the upgrade and you notice that this seems to only be affecting servers with a specific application profile. How can you easily identify which of your systems are likely to be affected?","explanation":"AWS Config is a service that enables you to assess, audit and evaluate the configurations of your AWS resources.","links":[{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"}],"answers":[{"id":"055f466b265e26667e0bb23ddffc7970","text":"Run Command","correct":false},{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":true},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"9deb03cd21d41a691cdc24bfaab2820c","text":"Inspector","correct":false}]},{"id":"9de05324-9d1a-4252-a6ed-c8bc6e732afe","domain":"dep-prov","question":"You use a launch template to create an Auto Scaling group for an application. You need to ensure a number of EC2 instances are always online to meet minimum capacity requirements and also use lower priced instances when scaling up. Which of the following instance combinations would be the most cost effective solution to use in the Auto Scaling group?","explanation":"A combination of on-demand instances and spot instances should be launched to meet the target capacity that you specified in the spot fleet request. The on-demand instances ensure the minimum capacities are met and the spot instances provide extra capacity as the application scales up. The other options without the spot instances are not cost-efficient. And scheduled reserved instances do not run continuously and should not be selected.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html","title":"How spot fleet works"}],"answers":[{"id":"853ea21196f8eeabab96ee1faf8cb4bf","text":"On-demand instances + spot instances","correct":true},{"id":"8f81bfbb0090b9ecba50b69be972ec7b","text":"Dedicated instances + on-demand instances","correct":false},{"id":"da8d4ca080444c6f36c71ed68a3c9a8f","text":"Reserved instances + on-demand instances","correct":false},{"id":"eeea48d3cb7324682811fe139fd30093","text":"Scheduled reserved instances + spot instances","correct":false}]},{"id":"f5213e28-c41d-4552-810a-7aa8f5ba2a1c","domain":"networking","question":"A Developer is unable to connect to an EC2 instance in a VPC. A SysOps Administrator investigates the connectivity issue. Going through a troubleshooting checklist, which conditions should be checked? Select two.","explanation":"Internet gateways allow all traffic and that cannot be configured. EC2 instances always have a private IP but a public IP is required for internet access. Security groups are stateful so only an allow rule for incoming traffic is required.","links":[{"url":"","title":""}],"answers":[{"id":"08eefee7811ea91e42e46ba287ff5d6a","text":"The security group has an allow rule for incoming traffic.","correct":true},{"id":"8b476944980fce87668c22d0c40470e9","text":"The internet gateway associated with the VPC allows outgoing traffic.","correct":false},{"id":"50be1cda181093d2df7e72aed5f80ab8","text":"The instance has a public IP address.","correct":true},{"id":"578d4a14aed3e21944a0f081460e32af","text":"The security group has an allow rule for outgoing traffic.","correct":false},{"id":"692e30da816ea07b3744e393cbbd28cd","text":"The instance has a private IP address.","correct":false},{"id":"f9b2c3489abcbefefb1b289a51e33ccb","text":"The internet gateway associated with the VPC allows incoming traffic.","correct":false}]},{"id":"05d71be4-026e-433e-bd8b-eb4a3929ba63","domain":"automation","question":"A development team wants to use the latest Windows AMI whenever they launch an EC2 instance. Which service will allow them to query the AWS-managed Parameter Store namespace to retrieve the newest AMI for their CloudFormation template?","explanation":"AWS publish the latest AMI IDs for Operating Systems in AWS-managed parameters in the Parameter Store.  By using a Custom Resource in Lambda you can retrieve the relevant AMI ID and return it to the CloudFormation service, that way ensuring that your templates always use the newest AMI.","links":[{"url":"https://aws.amazon.com/blogs/mt/query-for-the-latest-windows-ami-using-systems-manager-parameter-store/","title":"Select AMI using Systems Manager Parameter Store"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html","title":"AWS Lambda-backed Custom Resources"}],"answers":[{"id":"7eb8f6238570dc713a360eae3029648f","text":"CloudFormation Mappings","correct":false},{"id":"2751cfe1530d4333f0bdac2d7b7c21bd","text":"CloudFormation using AWS Systems Manager Parameter Store","correct":true},{"id":"8c19fb5ff9d451c3f315e96ca8563b84","text":"CloudFormation Linked Parameters","correct":false},{"id":"dc0efa07b1be89f7cfd1ab666df2f949","text":"CloudFormation Custom Resource using Lambda","correct":true},{"id":"cdf3a2f6faa3abf891b952dde17eb469","text":"CloudFormation Template Transformation","correct":false}]},{"id":"900f9fd7-73bb-4c66-a64f-66d0c4d363a9","domain":"mon-rep","question":"A medical technology startup is experiencing tremendous growth in their business. As a result, the demand for AWS infrastructure for their applications is increasing dramatically each week. They've exhausted a number of AWS service limits in the past, resulting in customer experience issues. They'd like to be more proactive in requesting limit increases before problems arise. What would be the best way to monitor their service limit posture to be able to avoid potential service limit issues in the future?","explanation":"Creating a Lambda function to refresh Trusted Advisor service Limit checks gives you control over how often you'd like to test for increasing usage thresholds. CloudWatch Events will capture status events from Trusted Advisor for alerting through SNS. AWS Service Quotas allows you to view and manage your service limits from a central location, but it requires creation of CloudWatch Alarms for alerting. AWS Systems Manager does not monitor service quotas. AWS Trusted Advisor requires Amazon CloudWatch Events rules to relay service limit statuses to SQS. Any notification solution can be utilized for alerting (SNS, SQS, Slack, etc.)","links":[{"url":"https://aws.amazon.com/solutions/limit-monitor/?did=sl_card&trk=sl_card","title":"AWS Limit Monitor"},{"url":"https://docs.aws.amazon.com/servicequotas/latest/userguide/intro.html","title":"What Is Service Quotas"}],"answers":[{"id":"e4bca64a87a0aa8a60cf82026592f852","text":"Configure AWS Trusted Advisor to forward service limit statuses (OK, WARN, and ERROR) to an Amazon Simple Queue Service queue. Have a Lambda function trigger when a message is written to the queue, and publish WARN and ERROR statuses to an Amazon Simple Notification Service topic","correct":false},{"id":"621515daca03b3a42e964175ceb4cb55","text":"Regularly invoke a Lambda function to refresh AWS Trusted Advisor Service Limit checks. Create Amazon CloudWatch Events rules to send status events to an Amazon Simple Notification Service topic when individual service limit usages reach desired thresholds","correct":true},{"id":"a3e54b0eba982df36f58025b0255a252","text":"Use AWS Service Quotas to generate alarms when service limit usage reaches a desired threshold. Forward these alarms to a Lambda Slack Notifier function to write the alerts to a Slack channel","correct":false},{"id":"4a85b8ab34b877cb3e2b1e66c551633a","text":"Configure AWS Systems Manager with desired thresholds for service limit usage alerts. Create Amazon CloudWatch Events rules to trigger a Lambda Slack Notifier function that writes alerts to a Slack channel","correct":false}]},{"id":"a7e0b35e-a0a9-43fb-86a8-4677970f57ae","domain":"mon-rep","question":"Your Auto-Scaling group is configured to launch a new EC2 instance whenever it detects an unhealthy instance in your Auto-Scaling group. However, you wish to be notified when this happens. Which of the following AWS services would you join with Auto-Scaling to achieve this?","explanation":"When you use Auto Scaling to scale your applications automatically, it is useful to know when Auto Scaling is launching or terminating the EC2 instances in your Auto Scaling group. Amazon SNS coordinates and manages the delivery or sending of notifications to subscribing clients or endpoints. You can configure Auto Scaling to send an SNS notification whenever your Auto Scaling group scales.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/latest/userguide/ASGettingNotifications.html","title":"SNS Notifications With Auto Scaling"}],"answers":[{"id":"ef4ba2f338cdf4b615ed280ff0b2777d","text":"SQS","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":true},{"id":"f62772d94b939126ee608465cf5e0881","text":"SWF","correct":false}]},{"id":"5d1249a6-960b-4540-8631-50875e04850d","domain":"mon-rep","question":"A company is using a site-to-site AWS VPN connection with static routing to allow connectivity between its corporate office and a VPC in AWS. The SysOps Administrator wants to get notified if the connection goes down. What’s the most effective way to accomplish this?","explanation":"AWS support won't do this for you. The other options would work, however, creating a CloudWatch alarm is the simplest option.","links":[{"url":"https://docs.aws.amazon.com/vpn/latest/s2svpn/monitoring-cloudwatch-vpn.html","title":"Monitoring VPN Tunnels Using Amazon CloudWatch"}],"answers":[{"id":"4ca2750b50a6661dbeff47ae8524ff24","text":"Set up a cron job in an EC2 instance to confirm the TunnelState metric every minute and send an SNS notification if necessary.","correct":false},{"id":"759ca4c838619fc5548932dc516de2cf","text":"Create a CloudWatch alarm to track the TunnelState metric and send an SNS notification if necessary.","correct":true},{"id":"a05514224184194909112ecdabb8b939","text":"Write a Lambda function to check the TunnelState metric every minute and send an SNS notification if necessary.","correct":false},{"id":"b9f0b72860ab9c76417cf3f4c3ec6c98","text":"Ask AWS support to monitor the connection and send an SNS notification if necessary.","correct":false}]},{"id":"a6c51ca7-52b5-43a3-aa33-000fe7c6eb28","domain":"networking","question":"You want to configure an IPv4 subnet for 30 devices. Which subnet mask will give you the most appropriate IP address range?","explanation":"Out of the available options, /25 is the most appropriate as it gives you 123 useable IPs, /24 gives you 251 useable IPs, /28 only gives you 11 useable IPs and /16 is the largest subnet with around 65k IP addresses","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html#vpc-sizing-ipv4","title":"VPC Subnets"}],"answers":[{"id":"f338fdcb41c5dfa52b9ed8882816edc8","text":"/25","correct":true},{"id":"ff95ba81ae512ac21c01f6e4a389d550","text":"/24","correct":false},{"id":"56b06b5537fc068776178062155a9ea1","text":"/28","correct":false},{"id":"8cc8342afd8761102293089057e08fdb","text":"/16","correct":false}]},{"id":"3xre6hrv-j02a-kj8k-nkyn-5951wwipzpzd","domain":"automation","question":"Which service can you use to enable configuration management using Chef or Puppet?","explanation":"OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Config allows you to record and evaluate configuration but doesn't use Chef or Puppet, Systems Manager is an operational insights tool and Athena is used to run SQL queries on data held in S3.","links":[{"url":"https://aws.amazon.com/opsworks/","title":"OpsWorks"}],"answers":[{"id":"c42aaccedc51aac929c8ae313066f320","text":"OpsWorks","correct":true},{"id":"582ca45acfd3e21caca8b786c1413850","text":"Athena","correct":false},{"id":"8e75b153e61c22a8ea4e14aadc7cb4ee","text":"Systems Manager","correct":false},{"id":"fa535ffb25e1fd20341652f9be21e06e","text":"Config","correct":false}]},{"id":"cb3a3e4a-87a1-4810-a732-ab1818b84b1d","domain":"data-man","question":"Your company's on-premises CRM software uses file storage. The marketing department would like to add a new social media module which will require a large increase in storage capacity. There is no capital budget to purchase additional storage servers, so the decision is made to place the new module's data on Amazon Elastic File System (EFS). A leader of the application team raises concerns about the latency that could be incurred with a single application accessing data both on-premises and in the cloud. How would you architect the solution to minimize the possibility of poor application performance?","explanation":"Either AWS Direct Connect or AWS VPN is required to connect on-premises servers to Amazon EFS. Bursting Throughput mode is the default mode for EFS file systems, scaling up as the size of a filesystem grows. Provisioned Throughput mode can provide higher throughput for applications with requirements greater than those of Bursting Throughput mode. Max I/O Performance mode will only benefit highly parallelized applications, which CRM generally is not.","links":[{"url":"https://aws.amazon.com/efs/","title":"Amazon Elastic File System"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/performance.html#throughput-modes","title":"Amazon EFS Performance"}],"answers":[{"id":"9d857d0961d3644ba26dec63c2d911e2","text":"Have the CRM application connect to the AWS cloud over an AWS Direct Connect. Configure the EFS file system in Provisioned Throughput mode","correct":true},{"id":"6ebf148cfe4cebe47a593afb84737882","text":"Connect the CRM application to EFS through an AWS Service Endpoint. Deploy the EFS file system in both Provisioned Throughput and Max I/O Performance modes","correct":false},{"id":"7b828048a4ab8b3c896ce413de929519","text":"Have the CRM application connect to the AWS cloud over an AWS VPN. Implement the EFS file system in both Bursting Throughput and Max I/O Performance modes","correct":false},{"id":"ed115154d20878d7d538a3557109c6e2","text":"Connect the CRM application to the AWS cloud over an AWS Direct Connect. Configure the EFS file system in Max I/O Performance mode","correct":false}]},{"id":"4672c614-c5ab-464a-9de0-5ef85a8d081e","domain":"automation","question":"You are a SysOps Administrator for your company. The company's CIO was on vacation and didn't know that there was an AWS Region outage during her time off. She returned having no idea of the impact and wants to be alerted the next time an outage occurs whether or not she is on vacation. How would you implement a solution?","explanation":"You can use Amazon CloudWatch Events to detect and react to changes in the status of AWS Personal Health Dashboard (AWS Health) events. Then, based on the rules that you create, CloudWatch Events invokes one or more target actions when an event matches the values that you specify in a rule. Depending on the type of event, you can send notifications, capture event information, take corrective action, initiate events, or take other actions. Creating a Lambda function may be possible but is overly complicated. An AWS Config rule may also work but is not as efficient as using AWS Health directly. Amazon Inspector is used to assess security for applications deployed on EC2 and is not appropriate for this case.","links":[{"url":"https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html","title":"Monitoring AWS Health Events with Amazon CloudWatch Events"}],"answers":[{"id":"0825ebcd89b98147868bbea3888611bd","text":"Use Amazon Inspector to assess service health. Have Amazon Inspector produce reports for you to review and forward these reports to the CIO for those containing outages.","correct":false},{"id":"d34ecb714b1da223e3ec16bc80d4193b","text":"Configure an AWS Config rule that checks to see if any Regions are suffering outages. Have the configure trigger a Lambda function that will send an email to the CIO.","correct":false},{"id":"74245b5d3f6507fb9fbf04a80d0edc0a","text":"Send custom text or SMS notifications to the CIO with Amazon SNS when an AWS Health event happens by using Lambda and CloudWatch Events.","correct":true},{"id":"984716f59e8039a5c670fd67e008a4e7","text":"Create a Lambda function that parses through the AWS Service Health Dashboard to identify outages in certain Regions. Have the Lambda function email the CIO using SES.","correct":false}]},{"id":"745e3728-9374-48a2-b7ab-7ee0c4ad4ad6","domain":"security-comp","question":"Your AWS Organization includes multiple AWS accounts. To meet the security compliance, AWS Config should be enabled in all accounts. The data needs to be recorded in all regions as well. You prefer using a central place to view all the resource configurations and compliance data recorded in AWS Config. How would you configure it?","explanation":"An aggregator is an AWS Config resource type that collects AWS Config data across multiple accounts and multiple regions. You can easily add an AWS Organization and select all regions in the aggregator. After that, you can get an aggregated view of the configuration information of AWS resources, an overview of Config rules and their compliance state. You do not need to manually enable AWS Config in all regions and all accounts. And AWS Config cannot be enabled in the AWS Organizations panel.","links":[{"url":"https://docs.aws.amazon.com/config/latest/developerguide/setup-aggregator-console.html","title":"Setting up an aggregator in AWS Config"}],"answers":[{"id":"6bfb56d27b2d920c4cc3001c82233f29","text":"Create an aggregator in AWS Config. Add the AWS Organization to the aggregator and select all AWS regions.","correct":true},{"id":"2317a132eeda89bc0a88eb9b628b6d06","text":"In AWS Organizations panel, enable AWS Config for all the regions. View the centralized configuration data in AWS Organizations.","correct":false},{"id":"a856258c8005e338f9c1b40c80449899","text":"Enable AWS Config in all regions and all accounts. Select an S3 bucket to store all the configuration data.","correct":false},{"id":"4787f6c13efd28a83db2125402fa372b","text":"Enable AWS Config in all regions for each account. Configure AWS QuickSight to view the aggregated data.","correct":false}]},{"id":"de2610da-0b20-4258-b215-146e96c134d3","domain":"automation","question":"Which section of a CloudFormation template allows you to set up differing instance types based on environment type (e.g. 'Production' or 'QA')?","explanation":"","links":[{"url":"http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"CloudFormation Template Anatomy"}],"answers":[{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":false},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false},{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false},{"id":"229eb04083e06f419f9ac494329f957d","text":"Conditions","correct":true}]},{"id":"5660d734-c8d8-404c-b683-3d43db3c17e1","domain":"mon-rep","question":"You are a SysOps Administrator for your company. Your CFO notices that costs have increased steadily for the past year, and tasks you with analyzing cost and usage of the company’s AWS environment based on tagged resources. What is the most effective way to analyze your company’s AWS spend?","explanation":"Choose Cost Explorer to track and analyze your AWS usage. Cost Explorer is free for all accounts and can filter by Region, purchase option, tags, among other things. Trusted Advisor provides areas to optimize costs but doesn't provide cost and budget reports. Developing a Lambda function to calculate spend would be an administrative burden, and so would comparing different AWS Config environments. Both would require manual efforts to leverage AWS' pricing API. It's much more efficient to utilize AWS' free Cost Explorer with built-in reporting functionality.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-getting-started.html","title":"AWS Billing and Cost Management"}],"answers":[{"id":"dbbdf8cec2b6f9217bcf2d39ad3ac859","text":"Create a Lambda function that is invoked for every CloudTrail Event. Have the Lambda function calculate spend based on each AWS services’ API calls using the tag key.","correct":false},{"id":"343d6ae3fa3cd5f44bce786dbe8e8b8b","text":"Use AWS Config to evaluate the configuration of your AWS environment last year. Create a custom rule to analyze by tag. Compare last year’s configuration to this year's and calculate the difference in spend.","correct":false},{"id":"b1204a52a0664ddd8a75b740204e6b88","text":"Enable the Cost Explorer tool to track and analyze your AWS usage. Filter spend by tags.","correct":true},{"id":"c307cf10883e9081648ee198aebeb387","text":"Download cost optimization and budgeting reports from Trusted Advisor as a CSV. Filter downloaded data by tags.","correct":false}]},{"id":"9ad79f24-9def-4d1e-9419-2037fdeda2cf","domain":"data-man","question":"In your company, in order to meet compliance requirements, production files in all S3 buckets need to be replicated into S3 buckets in a different region. These files already have a prefix of PROD. Other files without this prefix should not be copied. Which of the following is the fastest and most cost-efficient way to achieve this requirement?","explanation":"Users can easily configure Cross-Region Replication to copy S3 objects to a bucket in another region. In the replication rule, the source can be the entire bucket, a prefix or tags. The solution of using the EC2 instance is not cost-efficient. The Lambda function should only copy files with the PROD prefix. The \"aws s3 cp\" command cannot replicate new or updated files to the target.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html","title":"S3 Cross-Region Replication"}],"answers":[{"id":"db95dac10149c21840a7dd4c3e6694b2","text":"Configure a Cross-Region Replication rule in S3 buckets. Only replicate the objects with the prefix of PROD to the target S3 buckets in another region.","correct":true},{"id":"b4d89a58bd92a685500e925adf4d68d6","text":"Create a T2.micro EC2 instance. Set up a cron job to sync production files to the target S3 buckets every 10 minutes.","correct":false},{"id":"d786d6ad7df12da04c1096d758233df6","text":"Use a Lambda function to copy all objects in S3 buckets to another region. Then delete the files that do not have the PROD prefix.","correct":false},{"id":"e0c7b585219f6c9ec9213169d6c29f23","text":"Use AWS CLI command \"aws s3 cp s3://sourceBucket/PROD/* s3://targetBucket/PROD/* --recursive\" to replicate production files.","correct":false}]},{"id":"904547cd-6936-40bc-a5f4-6505b1b3e42d","domain":"security-comp","question":"A public relations company has decided to manage their AWS infrastructure with AWS Systems Manager. The DevOps Team would like to automate various tasks by calling AWS Systems Manager APIs from scripts in the VPCs where the infrastructure resources reside. The Security Team is concerned about the APIs passing traffic over the public Internet. What should the DevOps Team do to ensure that AWS Systems Manager enables the company to manage it's cloud infrastructure in the most secure way?","explanation":"You can privately access AWS Systems Manager APIs from your VPC using VPC endpoints. In this way, traffic travels over the AWS network without the need to traverse the public Internet. The other three options would all require traffic to travel over the Internet, and none of these options are even supported.","links":[{"url":"https://aws.amazon.com/systems-manager/","title":"AWS Systems Manager"},{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html","title":"Create a Virtual Private Cloud Endpoint"}],"answers":[{"id":"ffe6fb1584ad7fd16d49ecf307cbde53","text":"Create an AWS Systems Manager VPC interface endpoint for each region where resources will be managed. Create a gateway endpoint for Amazon S3","correct":true},{"id":"d370f4376931467f8c8d4ccd5c7c99fc","text":"Implement a NAT Gateway in a public subnet of each VPC. Configure the AWS Systems Manager's proxy list to include each NAT Gateway's ARN","correct":false},{"id":"96ca27e0ba5ccd726c68a0afa285ae1a","text":"Configure AWS Systems Manager to use SSL connections when communicating with infrastructure resources. Enable inbound Security Group rules for the resources to allow traffic on port 443","correct":false},{"id":"a0b9906946e5e97a0fa4df37541a5494","text":"Enable VPN connectivity in the AWS Systems Manager settings. Designate the Virtual Private Gateway of each VPC to use for the IPSec tunnel","correct":false}]},{"id":"2d62b529-330e-4757-817c-853f19d31841","domain":"security-comp","question":"Regions are designed with at least two, often more, Availability Zones. What is the purpose of these availability zones?","explanation":"Regions consist of at least two Availability Zones this is with availability in mind, and to ensure fault isolation.","links":[{"url":"https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf?refid=em_","title":"AWS Security Best Practices"}],"answers":[{"id":"0d3e0fbd7b193b148b931c20c2a23388","text":"To provide a low latency connection for end users.","correct":false},{"id":"2110ad4d6cecb9bdbf10d127442c9361","text":"To allow for HA (high availability) design.","correct":true},{"id":"504bc5d3c03b93a69a8343b7b1cd2c29","text":"To provide different levels of network security.","correct":false},{"id":"9d85908f6c7d0a68f913f8c8843b4980","text":"To allow for load balancing.","correct":false},{"id":"2dbf99411ef147bbbd86e4426a961691","text":"To provide fault isolation.","correct":true}]},{"id":"d17b79ff-5e85-4d51-826c-1d7d89d1977e","domain":"data-man","question":"What is the most secure way to ensure the long-term safety of objects you store in S3?","explanation":"By default, all requests to your S3 buckets require your AWS account credentials. However, if you enable Versioning with MFA delete, *two* forms of authentication are required to permanently delete an object.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html#MultiFactorAuthenticationDelete","title":"MFA Delete"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html","title":"Using Versioning"}],"answers":[{"id":"7e4fe344631393a9e197345b8ccd4aa9","text":"Encrypt the contents of the bucket.","correct":false},{"id":"b2b33a739c212ba3d20147f3d3fac36b","text":"None of these options.","correct":false},{"id":"b24a2a262afe3872a9020f786ccf9a6a","text":"Enable both versioning and MFA delete on the bucket.","correct":true},{"id":"253efe32850426ce32c509612536c798","text":"Enable versioning on the bucket.","correct":false}]},{"id":"f43c1eae-280e-4f5b-bd96-0620dcf7ad48","domain":"data-man","question":"A company has a new Internet of Things project. You need to create a database in AWS to store customer data for further analysis. The database does not require a relational model and the data could be served using a key-value pair. You wish to quickly configure a fast and high performing database without worrying about hardware provisioning, setup and configuration, software patching or scaling. Which of the following database types would you choose?","explanation":"As the database does not require a relational model, NoSQL databases such as DynamoDB should be considered. DynamoDB offloads the administrative burdens and AWS helps to manage the DynamoDB table including provisioning, configuration, patching, etc. Amazon Aurora and QLDB do not belong to NoSQL databases. ElastiCache is a caching solution for high throughput and low latency which are not required in this scenario.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html","title":"Amazon DynamoDB introduction"}],"answers":[{"id":"a9d83c7f8f0b0f2a8f67b7097ee73e3a","text":"Amazon QLDB","correct":false},{"id":"50848259480914860b338e7baf94c29a","text":"ElastiCache","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true},{"id":"69670a9d53817d1ec89e685997343ce2","text":"Amazon Aurora","correct":false}]},{"id":"95c885d1-c528-48bf-b403-af3cf7ff29cf","domain":"dep-prov","question":"You need to automate the creation of related AWS resources. Which AWS standalone service is your best choice?","explanation":"CloudFormation automates the creation of related AWS resources, provisioning and updating them in an orderly and predictable fashion. ","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html","title":"About CloudFormation"}],"answers":[{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"a907a7338c1fb3821fbe8ed113c64b33","text":"CloudHSM","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":true},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false}]},{"id":"3961852e-ae3a-4ba3-bcea-d64a50a414fb","domain":"networking","question":"You have migrated your website to AWS and have placed it behind an Application Load Balancer and are just about to update your Route 53 records to point to the new site in AWS - Which record type do you need to create so that people browsing to \"myawssite.com\" are directed to the Application Load Balancer in AWS?","explanation":"As the URL is the Zone Apex, as CNAME record will not be allowed - requiring you to use the Alias record type. As an A record maps directly to an IP is in not appropriate to use in this case, and the CAA record type relates to certificate authorities and is not relevant in this case","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html","title":"Route 53 Supported Record Types"}],"answers":[{"id":"0e9d0445230399eb6230618d20500356","text":"Route 53 CAA Record","correct":false},{"id":"1ff0a251aaa48c9626266f0a5b0181e1","text":"Route 53 A Record","correct":false},{"id":"9e7e22b05e283291537669ab0bc663be","text":"Route 53 Alias Record","correct":true},{"id":"5a5b76c46054d7db63019bc89f3b9171","text":"Route 53 CNAME Record","correct":false}]},{"id":"46293632-1540-45c7-91df-7b6815a27847","domain":"security-comp","question":"Which of the following names is not a valid IAM role name?","explanation":"Names of users, groups, roles, policies, instance profiles, and server certificates must be alphanumeric, including the following common characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). CompanyMarketing#Role is therefore not an acceptable role name.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-limits.html","title":"Limitations on IAM Entities and Objects"}],"answers":[{"id":"a1b18d40d2aedd805efd8bc59f1d6969","text":"Company,Marketing,Role","correct":false},{"id":"7ab3a687b4689245604f8981502bdf09","text":"Company--Marketing+Role","correct":false},{"id":"e414a871abd69eafb829412a3a873a36","text":"CompanyMarketing#Role","correct":true},{"id":"dc6e80f51a713e21ec697bead75c1667","text":"Company.Marketing_Role","correct":false},{"id":"d8c8f0893b0743d016015e108e9d2ab9","text":"MarketingRole@Company","correct":false}]},{"id":"9c6bdf99-3728-43be-9772-59500b1f2276","domain":"dep-prov","question":"A team of developers plans to migrate their GraphQL-powered web application to AWS and the development lead has been instructed to use managed services whenever possible. How can the team accomplish this?","explanation":"API Gateway is used for RESTful applications. AWS AppSync is used for GraphQL powered applications. RDS and DynamoDB are managed database services but Amazon EC2 is not a managed service.","links":[{"url":"https://docs.aws.amazon.com/appsync/latest/devguide/designing-a-graphql-api.html","title":"Designing a GraphQL API"}],"answers":[{"id":"13017b9feb28c5d8fdda19e3c79df90d","text":"Use AppSync for the GraphQL API. Use DynamoDB for the managed database service.","correct":true},{"id":"58fa18d14ef959d0d59e88ef26f8c391","text":"Use Amazon EC2 for the GraphQL API. Use RDS for the managed database service.","correct":false},{"id":"4481f8f19e1b5f28da2b0f396643167b","text":"Use Lambda for the GraphQL API. Use RDS for the managed database service.","correct":false},{"id":"6f76fbab4996d8c8cb993a9c4569275b","text":"Use API Gateway for the GraphQL API. Use DynamoDB for the managed database service.","correct":false}]},{"id":"ff1ea769-7316-4292-9cb6-efc556c4af5d","domain":"networking","question":"You have an application running on an EC2 instance using IPv4. The EC2 instance is in a public subnet with a route to an Internet Gateway with target 0.0.0.0/0. As a SysOps Administrator you've been tasked with editing the network to accommodate an upgrade to the application. The upgrade adds an IPv6 address to the instance and it requires only outbound Internet access. What changes would you make?","explanation":"If you have an existing VPC that supports IPv4 only, and resources in your subnet that are configured to use IPv4 only, you can enable IPv6 support for your VPC and resources. Your VPC can operate in dual-stack mode — your resources can communicate over IPv4, or IPv6, or both. IPv4 and IPv6 communication are independent of each other. An egress-only Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances. You cannot use a NAT instance nor a NAT Gateway using the IPv6 protocol; you can only use an egress-only Internet Gateway.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html","title":"Egress-Only Internet Gateways"}],"answers":[{"id":"ed594610a3ab11624c962c4b7b60381c","text":"Set up a NAT instance and place it in the public subnet. As the NAT instance is in the same subnet as your EC2 instance, no route changes are necessary.","correct":false},{"id":"e83dc6bf30ada297b10ae6b7d524fb08","text":"Set up a NAT Gateway. Create a route from your public subnet to the NAT Gateway.","correct":false},{"id":"754f95d8f4bb6371c0314ad0727947ff","text":"Create an egress-only Internet gateway and associate it to your subnet. Specify ::/0 in the destination box and select the egress-only Internet Gateway ID in the Target list.","correct":true},{"id":"453bf5bd268d1213b9aa8e0d45ba876a","text":"This is not possible. You will need to create a new VPC that supports IPv6.","correct":false}]},{"id":"c6a8e29b-70b4-4a72-b587-8ed388d71004","domain":"security-comp","question":"You are an administrator with full admin access to S3. There are several S3 buckets within your organization that need to comply with a policy that requires all objects to be encrypted in-transit. What data encryption mechanism would you apply to fulfill this requirement?","explanation":"Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, use a master key you store within your application. Server-side encryption is encrypting data at rest. SSE-S3, SSE-KMS, and SSE-C are methods of server-side encryption and would not fulfill a data in-transit encryption policy.","links":[{"url":"https://docs.aws.amazon.com/en_pv/AmazonS3/latest/dev/UsingClientSideEncryption.html","title":"Protecting Data Using Client-Side Encryption"}],"answers":[{"id":"163a4655e01a9cbc21d84cff0d1b33a1","text":"Use Server-Side Encryption with Customer-Provided Keys (SSE-C).","correct":false},{"id":"034ffe67b63ae2e68b278955fa9e740c","text":"Use Server-Side Encryption with Keys Stored in AWS KMS (SSE-KMS).","correct":false},{"id":"6d50168a818719b2b2c1e9cd4f07f38a","text":"Use Client-Side Encryption.","correct":true},{"id":"7a36a5c2c5cf1986b38d1310f69352a6","text":"Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3).","correct":false}]},{"id":"4e911348-5056-4ece-a91d-ce96b619578f","domain":"automation","question":"The DevOps team of an insurance company has been instructed to use CloudFormation to manage the different environments of the company. Due to the size of the templates prepared exceeding the limit, the CloudFormation service rejected the processing of the template. How can the DevOps team resolve this issue?","explanation":"Due to the size of the templates exceeding the limit, dividing the CloudFormation template into smaller subparts is the solution. With this in mind, CloudFormation nested stacks will yield to the same template behavior but will involve different files","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html","title":"Using CloudFormation Nested Stacks"}],"answers":[{"id":"1d75dba4b52318bdc075c815a7690e4c","text":"Use CloudFormation custom resources","correct":false},{"id":"f163f3aff96f7db10bfa4b93ed9a0a67","text":"Use CloudFormation wait handlers","correct":false},{"id":"e29c7f5b3439e3e8355a1e6bede51fa1","text":"Minify the CloudFormation template","correct":false},{"id":"d4838aa62aeb5717c4100906a75dea2d","text":"Use CloudFormation nested stacks","correct":true}]},{"id":"3d005524-36c5-4851-8500-665f29e4c0b7","domain":"networking","question":"A team of engineers has set up a VPC with EC2 web instances in the public subnets and the RDS instances, EC2 background worker instances and the NAT Gateways in the private subnets. The team has noticed that the background worker instances do not have internet connectivity and the issue might be due to the VPC setup. How can the team resolve this issue?","explanation":"NAT gateways need to be in the public subnet in order for the resources in the private subnet to have internet connectivity. Usually, the default route for a private subnet points to the NAT gateway. If the NAT gateway is in a private subnet, then the traffic cannot reach the internet and the resources in the private subnet won't have internet connectivity.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html","title":"VPC Scenario"}],"answers":[{"id":"210235cf61b065b37cf583e72630e77c","text":"Create a new VPC with only private subnets. Transfer the background worker instances and the NAT Gateways to the new VPC.","correct":false},{"id":"85f3551b459e74e7bf48aa77dbe7a729","text":"Create a new VPC with only private subnets. Transfer the background worker instances to the new VPC. Set up a peering connection to the new VPC from the original VPC.","correct":false},{"id":"4fd2e84c5c3112b4c0f92f51aed239f1","text":"Set up new NAT Gateways in the private subnets until the background worker instances start getting internet connectivity.","correct":false},{"id":"73cbf7879ad8bbb9f58a4e7bb9a999c9","text":"Turn off the NAT Gateways in the private subnets. Create new NAT Gateways in the public subnets.","correct":true}]},{"id":"85c8beb3-4040-4c16-80a2-28699caea7f7","domain":"mon-rep","question":"Your company is running dozens of EC2 instances. What kind of a solution would give near real-time visualizations of multiple EC2 instance metrics at once?","explanation":"You can gather the necessary metrics together in CloudWatch Dashboards for complete operational visibility.","links":[{"url":"https://aws.amazon.com/cloudwatch/","title":"CloudWatch"}],"answers":[{"id":"eaa240abb4f0731fca4c2e20cbbbfefe","text":"Add the metrics into a CloudWatch Dashboard.","correct":true},{"id":"af39109a1cd1a96dc8fc03cad521e886","text":"Visualize the metrics with QuickSight.","correct":false},{"id":"1ac4cde26a39fcabf5405c352b7bdff9","text":"Send the metrics to S3 and visualize them with S3 analytics.","correct":false},{"id":"beef56bb880b285559c0254491f4d5c9","text":"Organize the metrics into a CloudFront panel.","correct":false}]},{"id":"c7e5ebf3-7eae-45bf-a02b-28bc06a6d575","domain":"mon-rep","question":"You have several CloudWatch Log Groups and Lambda Functions send logs to them. You need to use a tool to quickly search and analyze the log data in the log streams. The tool should automatically discover information in the Lambda logs such as the timestamp, max memory used and execution duration. It should also help you to perform the query using simple and pre-built query languages. Which tool is the best one for you to choose?","explanation":"CloudWatch Logs Insights is the most suitable tool to perform pre-build queries on CloudWatch Logs. Users do not need to transfer or transform the log streams. The log fields contained in the Lambda logs are automatically discovered. AWS Athena only performs queries on S3 objects. For Amazon ElasticSearch or Kinesis stream, extra configuration steps are required.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html","title":"Analyze log data with CloudWatch Logs Insights"}],"answers":[{"id":"79930c2280cb2398212ed1465b34b81e","text":"Export the log data to an Amazon ElasticSearch Service. Use ElasticSearch to perform queries or analysis.","correct":false},{"id":"7edc1b3e500b915d821d4db1db01280c","text":"Use CloudWatch Logs Insights to select the log groups and perform queries.","correct":true},{"id":"01551b4fd4b50ed9ceb2be1d0e338932","text":"Use AWS Athena to run queries on the log streams. The query language of Athena is based on SQL.","correct":false},{"id":"2c4abdd6514dcf68f6bace3f1a769e4f","text":"Stream the log data to an Amazon Kinesis stream and perform real time queries or analysis in the stream.","correct":false}]},{"id":"afb14785-ba2f-4fea-b819-21804b895752","domain":"mon-rep","question":"You use the CloudWatch Agent to collect system-level metrics in an EC2 instance and send them to AWS CloudWatch. Then you create a CloudWatch alarm based on the mem_available metric. The alarm status is OK for some time but it suddenly becomes “Insufficient data”. The EC2 instance is still up and running. Its status checks also pass. How would you resolve the problem?","explanation":"The CloudWatch agent is responsible for transmitting metrics data to CloudWatch. If the agent is not running properly, CloudWatch will not receive the metrics and the alarm will become \"Insufficient data\". You should check the running status of the CloudWatch agent process. Adjusting alarm threshold does not help as the CloudWatch alarm still does not receive data from the agent.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html","title":"Amazon CloudWatch Alarms"}],"answers":[{"id":"00e24de8460e88befdad35029ca80303","text":"Adjust the defined threshold of the alarm. The threshold may be too high for the reported data.","correct":false},{"id":"f116df32e8b60a8f3d2d7c1d5d4182eb","text":"Check if the CloudWatch agent is running properly. Restart the process if it is not running.","correct":true},{"id":"86fa960aa5069a37c3dec9725cba235a","text":"Create an image for the EC2 instance and launch a new EC2 instance using the AMI. Terminate the original instance.","correct":false},{"id":"d6dcf40b4663a8af0717c19f3c8c3c6e","text":"Delete the CloudWatch alarm and recreate a new one as the connection may get hung.","correct":false}]},{"id":"4c1c93f7-a7c8-4c21-be81-6719e4c149e6","domain":"dep-prov","question":"A company has an existing static blog site hosted on top of Amazon S3 that has been running for weeks. The development team has been instructed to upgrade the blog site to serve dynamic content. The development manager has mandated that managed services and serverless architecture patterns must be used as much as possible. How can the development team accomplish this?","explanation":"For a serverless architecture, the API Gateway, Lambda, and DynamoDB combo would allow a user to prepare serverless API endpoint that allows custom logic to be performed inside a Lambda function. The DynamoDB table(s) would contain the data being processed by the Lambda function.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html","title":"API Gateway - Getting Started with Lambda Integration"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-on-demand-https-example-configure-event-source_1.html","title":"Lambda - Configure Event Source"}],"answers":[{"id":"3de8f0c485401d6141e7f0aca4b8c68e","text":"Use API Gateway, Lambda, and DynamoDB for the API serving the dynamic content.","correct":true},{"id":"12c194830010e504e8e7e72650cb6952","text":"Use API Gateway, CodeBuild, and DynamoDB for the API serving the dynamic content.","correct":false},{"id":"5780728868d4030a2f6f36eedfd3f2d5","text":"Use API Gateway, CodeBuild, and RDS for the API serving the dynamic content.","correct":false},{"id":"ab2bbd6f5fb42ed01a72bc28be974191","text":"Use API Gateway, SQS, and RDS for the API serving the dynamic content.","correct":false}]},{"id":"9efe0b3f-2e6a-4918-81b2-c1a827654892","domain":"networking","question":"Which of the below statements about subnets and CIDR blocks as part of a VPC in AWS is correct?","explanation":"The maximum size for a subnet is /16 so this is a correct answer. Default behaviour is that every time you create a subnet it is added to the main route table - allowing other subnets using the main route table to route to it. As for the incorrect statements - the minimum size of a subnet is /28 (16 addresses) therefore /30 is incorrect, subnets are local to a single AZ and cannot span multiple AZs (although VPCs in which they live can). Every VPC will need an IPv4 CIDR allocated - even if the intent is just to use IPv6","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html","title":"VPC Subnets"}],"answers":[{"id":"22b96919d7cd98b93972df376d49d356","text":"By default, all subnets you create can route to each other","correct":true},{"id":"badb9d21322bff4ed8278d92ebc7174b","text":"The maximum size for a subnet is /16 with 65,536 addresses","correct":true},{"id":"70426e4ee2fa35fde1533f02d72886a1","text":"A subnet can span multiple AZs","correct":false},{"id":"ed1e31d6b69887380df735258ef1bd1a","text":"A subnet that will only be used for IPv6 doesn't need an IPv4 CIDR associated ","correct":false},{"id":"32df7fa7ee27b28e38ab223822a82204","text":"The minimum size for a subnet is /30 with 4 addresses","correct":false}]},{"id":"658273db-ca15-4e4c-a5a8-3a0cf2b97575","domain":"security-comp","question":"Which of the following are valid IAM access keys?","explanation":"Access Key and Secret Access Key are the two valid Key types.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html","title":"About Access Keys"}],"answers":[{"id":"524e9dc335bb89ea9e51c6deb0b395f0","text":"Private Access Key","correct":false},{"id":"2e3a68ed5a089edce456e8a3453a470d","text":"Access Key","correct":true},{"id":"bbb5588677dde59ad92c21dfd6bb0a06","text":"Public Access Key","correct":false},{"id":"dd3e3ce4a46ce581c8a9c659187f78ba","text":"Secret Access Key","correct":true}]},{"id":"bc7cd5cf-6c7b-4f45-bb7c-5700451bd9aa","domain":"data-man","question":"You are managing an S3 bucket that contains business critical objects for your operations department. You are tasked with optimizing the storage costs of the bucket. You've identified 130 objects in the bucket that need to be available but can be recreated by your department. What would you do to optimize your costs?","explanation":"Amazon recommends using One Zone-IA if you can recreate the data if the Availability Zone fails, and for object replicas when setting cross-region replication (CRR). Use Standard-IA for your primary or only copy of data that can't be recreated. S3 Standard would not be a cost-effective solution, and S3 Glacier does not provide millisecond retrieval times and would not fulfill the availability requirement.","links":[{"url":"https://docs.aws.amazon.com/en_pv/AmazonS3/latest/dev/storage-class-intro.html","title":"Amazon S3 Storage Class"}],"answers":[{"id":"daeec30860ad314818a31cb7ea5ba05e","text":"Set the storage class to S3 Standard","correct":false},{"id":"9b7e1e30e477deece7a8b6d9eecd9331","text":"Set the storage class to S3 Standard-IA","correct":false},{"id":"98c96ddb15112194ea3d2c44c1addd3b","text":"Set the storage class to S3 One Zone-IA","correct":true},{"id":"901f665e98aabd9853d4e0f9d3031ac2","text":"Set the storage class to S3 Glacier","correct":false}]},{"id":"8c1dec5f-627d-4f25-af24-119b092ca2ef","domain":"high-avail","question":"A client asks you how they can make their current database running on AWS highly available. The client is running a MySQL RDS database in us-west-1. The client does not currently have a Multi-AZ deployment. The client wants to know what the benefits are with a Multi-AZ deployment as it would incur additional costs to their AWS bill. How would you explain the benefits to the customer?","explanation":"Amazon RDS Multi-AZ deployments provide enhanced availability and durability for databases. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby. It does not lower latencies nor does it increase read performance. It cannot tolerate the failure of a single AWS Region as failure of a Region would implicate failure of all the Availability Zones within that Region.","links":[{"url":"https://aws.amazon.com/rds/details/multi-az/","title":"Amazon RDS Multi-AZ Deployments"}],"answers":[{"id":"834345e14f96992b8d4b2e785595f697","text":"Multi-AZ tolerates the failure of a single Region. It also allows higher availability during maintenance tasks.","correct":false},{"id":"aa4c2ced080ae70c33f36fe825b972ce","text":"Multi-AZ lowers latencies for application servers when they are accessing the database in multiple Availability Zones.","correct":false},{"id":"4983004598ef3be1bb46819c748a09ab","text":"Multi-AZ tolerates the failure of a single Availability Zone. It also allows higher availability during maintenance tasks.","correct":true},{"id":"6772f4dc3173a8dd298656b65d2b6efa","text":"Multi-AZ makes it faster for application servers to access the database by reading data at a quicker rate.","correct":false}]},{"id":"f77c40bd-922a-4bd5-8688-355a24e5b7f4","domain":"mon-rep","question":"As a SysOps Administrator you are auditing the patches across all the RDS instances within the us-east-1 Region of your AWS environment. You need to check the OS of the instances to ensure that the latest patches are installed and that the proper security requirements are being met. What AWS service could you use to complete your task?","explanation":"You can view whether a maintenance update is available for your DB instance by using the RDS console, the AWS CLI, or the Amazon RDS API. If an update is available, it is indicated in the Maintenance column for the DB instance on the Amazon RDS console. You can use Amazon Inspector service to create and run security assessments for your Amazon EC2 instances. AWS Artifact is used for gathering central compliance-related information that matters to you. Trusted Advisor online tool that provides you real time guidance to help you provision your resources following AWS best practices and would not necessarily provide OS patching related recommendations. These are best viewed directly in the RDS console.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html","title":"Maintaining a DB Instance"}],"answers":[{"id":"f5182b63a0b85cdb1fba4efaac2c38d6","text":"Use Amazon Inspector to run a report that shows the current security status of your RDS instances.","correct":false},{"id":"d289311b7f2b2249f93a154c2a78e69a","text":"Use the AWS Trusted Advisor dashboard to view recommendations.","correct":false},{"id":"289e4c4b17e574f33583898b34a8f37a","text":"Check maintenance of the instances on the Amazon RDS console.","correct":true},{"id":"af6beaf0a652352efd6b43acbef87765","text":"Check AWS Artifact to view the patch requirements of the instances in your AWS environment.","correct":false}]},{"id":"fb5f0c80-43c7-455e-9e9a-095d0883e920","domain":"networking","question":"Your company has set up a new server environment in a VPC. You already have applications running in several other VPCs in the same AWS Region. There is a requirement for the newly set up VPC to be able to connect to and communicate with the resources in the already existing VPCs. How would you set up the VPC so that they can communicate with each of the other VPCs?","explanation":"A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account within a single region. A Direct Connect connection is used to connect an on-premises data center and AWS. You cannot set up an IPSec tunnel between different VPCs. Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. Remember VPC connections are not transitive -- if there exists a connection between A and B, and another between B and C, you cannot assume connection between A and C. A and C must have their own VPC connection.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html","title":"VPC Peering"}],"answers":[{"id":"4883b19e9bcd0d2be72e729819f32630","text":"Configure VPC peering connection between the new server environment VPC and each of the existing VPCs.","correct":true},{"id":"5b7aae32966b04d80669d81a98bcd002","text":"Set up an IPSec tunnel between the new server environment VPC and each of the existing VPCs.","correct":false},{"id":"771101ce4f7e94bd471af7490926cde0","text":"Configure a Storage Gateway connection between the new server environment VPC and each of the existing VPCs.","correct":false},{"id":"ac4676fa941307afa0aacecfbb409545","text":"Configure a Direct Connect link between the new server environment VPC with each of the existing VPCs.","correct":false}]},{"id":"39376479-c045-4faf-a0b1-6dfddce09dba","domain":"mon-rep","question":"Which of the following is a valid AWS namespace?","explanation":"CloudWatch namespaces are containers for metrics.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/aws-namespaces.html","title":"AWS Namespaces"}],"answers":[{"id":"15b7736c9e06de8c034b67c82b7bee1b","text":"AMAZON/EC2","correct":false},{"id":"96f49c22a5ef2aba5cfdcdf4d4c7cce1","text":"AWS/DynamoDB","correct":true},{"id":"f7986f757fc3e62f5d2152aafbb2dde0","text":" AWS/ApiGateway","correct":true},{"id":"be6132c54d50d8ce33a982685bcdb6fd","text":"CUSTOM/KMS","correct":false}]},{"id":"fae36faf-783b-4c76-83ed-511f56063f62","domain":"mon-rep","question":"A MySQL database is created in AWS RDS. The DB instance stores sensitive customer data and its running status needs to be closely monitored. You want to use the Amazon Simple Notification Service (SNS) to receive notifications whenever there is a configuration change for the instance such as when the master password for the DB instance has been reset. How would you meet this requirement?","explanation":"Amazon RDS uses the Amazon SNS to provide notifications. You can create an event subscription and subscribe to specific event categories. RDS does not have streams and you cannot register a DB instance in an AWS Config rule. Lambda function makes things complicated and is not as simple as Event Subscription in RDS.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html#USER_Events.Messages","title":"Using Amazon RDS event notification"}],"answers":[{"id":"6321b05f3e9c6a31e5173e1e87d3aa62","text":"Configure an event subscription in RDS and subscribe to the “configuration change” event category. Use an SNS topic to receive notifications.","correct":true},{"id":"a70af04178c8de89d82a07fef3eb2d30","text":"Register the RDS instance in an AWS Config rule. Whenever there is a configuration change, the rule triggers a notification to an SNS topic.","correct":false},{"id":"80e872148df53129ffa1664bafab3bdd","text":"Enable the RDS streams to save the configuration events. Configure an SNS topic to receive notifications.","correct":false},{"id":"29c262e90545613cde223c6ace0c200a","text":"Create a Lambda function to monitor the configuration change events for the DB instance. Send a notification to an SNS topic whenever such an event happens.","correct":false}]},{"id":"27109f2b-2906-43cb-90b4-3e2bcfad7ab8","domain":"high-avail","question":"You are a consultant working for a global company. They are hosting their companies CRM web application on-premise across servers in three different countries. Amazon Route 53 is being used as their DNS Provider. When the servers in one of the countries goes down, traffic starts to drop instead of being redirected to the two working sites. Corporate policies prohibit client data being stored or transferred through a Public Cloud Provider. What would the simplest solution be to their issue?","explanation":"Multivalue Answer Routing will perform simple health checks on IP addresses before sending traffic to them. This has advantages over a simple routing, where an outage of one of the IP Addresses would result in failures to connect. Corporate policies prohibit in this scenario the storage or transfer of client data from the CRM through a Public Cloud Provider. This effectively rules out the migration to EC2, or the use of CloudFront as a CDN. Transferring DNS to an on-premise service may allow for more flexibility and abilities to write special health checks, but it would certainly not be the most simple option","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Amazon Route 53 Routing Policies"}],"answers":[{"id":"bad6a2c2cd6a244c640ffb67243421bd","text":"Migrate the servers to EC2 in three different regions to prevent outages in local datacentres","correct":false},{"id":"dcfd51b6eec1175ce379fb23d19ec48f","text":"Transfer their DNS Zone to on-premise DNS servers to allow administrators more power to respond to outages","correct":false},{"id":"abb8f1f2096c4f271cf6c11ab4be0ab4","text":"Implement CloudFront as a CDN for their website, ensuring global fault tolerance","correct":false},{"id":"1ed8875652253a24ebf466592454a1be","text":"Use a Multivalue Answer Routing Policy on their Route 53, including the Health Checks to detect outages","correct":true}]},{"id":"c5bc5186-77bf-47ff-86b0-a493d3d5f79e","domain":"security-comp","question":"Which of the following names is not a valid name for an IAM server certificate?","explanation":"Names of users, groups, roles, policies, instance profiles, and server certificates must be alphanumeric, including the following common characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). cert#company.name is therefore not an acceptable server certificate name.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-limits.html","title":"Limitations on IAM Entities and Objects"}],"answers":[{"id":"88f1b3c406c7b70cdc2ffdd948270841","text":"cert#company.name","correct":true},{"id":"fcac63bd07e8b093f02e341d42d24e5d","text":"Certificate-CompanyName","correct":false},{"id":"50720e862209ce24ecae4c6b1a483523","text":"cert=Company.Name","correct":false},{"id":"9f60f06229708587864e4896426180c2","text":"cert@companyname","correct":false},{"id":"946270e321436de010ee43f1baaf703b","text":"Certificate_CompanyName","correct":false},{"id":"bacefe00cb2a7a91e03569ee24b503a8","text":"certificate,,companyname","correct":false}]},{"id":"571b9603-45b6-45b1-aa47-68849ac814bb","domain":"automation","question":"A big-box retailer runs their in-store point-of-sale system on EC2 linux instances. All of the infrastructure is managed as part of a CloudFormation stack. The web servers are part of an Auto Scaling Group. The application only needs to be available during business hours from 9:00am until 6:00pm. What would be the best way to scale the web servers cost efficiently based on demand?","explanation":"Authoring the CloudFormation template to include an AutoScaling:ScheduledAction resource to increase the Auto Scaling Group's MinSize and MaxSize values at 9:00am, and another AutoScaling:ScheduledAction resource to decrease the Auto Scaling Group's MinSize and MaxSize values at 6:00pm will save costs for the retailer during non-business hours. CloudFormation conditions control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update, but don't control the actions of an Auto Scaling Group. Using an Auto Scaling Group scheduled action provides more streamlined automation than using a Lambda function. CloudFormation mappings are key/value pairs that can be used to specify conditional parameter values, but they have no impact on the Auto Scaling Group unless they are used to create a resource.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html","title":"What is AWS CloudFormation?"},{"url":"https://s3-us-west-2.amazonaws.com/cloudformation-templates-us-west-2/AutoScalingScheduledAction.template","title":"Cloud Formation Sample Template for Time-based Auto Scaling"}],"answers":[{"id":"a0f99978bb65910b10b75a32e1b92a2a","text":"Create AutoScaling:ScheduledAction conditions in the CloudFormation template that change Maxsize and MinSize values based on business hours","correct":false},{"id":"765dccb8332e2036eb70a0b6276e7285","text":"Use CloudWatch Events to trigger a Lambda function at business opening and closing that adjusts the Auto Scaling Group's MinSize and MaxSize accordingly","correct":false},{"id":"725933a9f7ea7ef281e2decf6cefadae","text":"Configure AutoScaling:ScheduledAction mappings in the CloudFormation template with Maxsize, MinSize, and Recurrence values based on business hours","correct":false},{"id":"0db6203397832efddcbca893f96ba1b6","text":"Include AutoScaling:ScheduledAction resources in the CloudFormation template that change Maxsize, MinSize, and Recurrence values based on business hours","correct":true}]},{"id":"ca3638f9-91ac-4667-9241-b4e3015691b5","domain":"data-man","question":"Your company's legal department needs to be able to retrieve customer transactions during litigation activities from seven years of historical data. Transactional archives are stored on Amazon S3, and file sizes range from 50 MB to 8 GB. The files are stored in different formats due to being written by separate departments, one third in GZIP compressed YAML format, one third in RZIP compressed CSV format, and one third in BZIP2 compressed JSON format. Which retrieval scheme will provide the best performance at the least cost?","explanation":"S3 Select provides the capability to filter the contents of S3 objects using SQL. S3 Select works on objects stored in CSV, JSON, and Parquet format, and it works with objects that are compressed with GZIP and BZIP2 compression, as well as server side encrypted objects. Due to lack of support for RZIP compression and YAML formatted files, any answer using S3 Select to retrieve the CSV or YAML files is not valid.","links":[{"url":"https://aws.amazon.com/blogs/aws/s3-glacier-select/","title":"S3 Select and Glacier Select – Retrieving Subsets of Objects"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html","title":"Selecting Content from Objects"}],"answers":[{"id":"74d1424680f4ddebfe0cf92e16d5f99b","text":"Retrieve full objects and extract the desired records programatically from the YAML files. Use Amazon S3 Select to retrieve only the desired records from the CSV and JSON files","correct":false},{"id":"21c1e971f9150a4c6b3c3ecf5665cb9c","text":"Retrieve full objects and extract the desired records programatically from the YAML and JSON files. Use Amazon S3 Select to retrieve only the desired records from the CSV files","correct":false},{"id":"37f4b5267a8be86e5b80ad0fb6ee1b55","text":"Retrieve full objects and extract the desired records programatically from the YAML and CSV files. Use Amazon S3 Select to retrieve only the desired records from the JSON files","correct":true},{"id":"1aa1c17b6e1fe36065a076d92aec97ca","text":"Use Amazon S3 Select to retrieve only the desired records from all files regardless of format","correct":false}]},{"id":"113a7914-1249-4e12-a748-03392c0570e8","domain":"high-avail","question":"You are a Security Administrator for your company. Your CIO wants to ensure that company data is highly available in multiple AWS Regions. What would you suggest to your CIO as the most effective approach?","explanation":"Deploying a multi-AZ RDS instance would only make it fault tolerant between Availability Zones, and not AWS Regions. Creating a Lambda function and creating/deploying EBS snapshots into different AWS Regions would both be an administrative and operational burden. The easiest, most effective, way is to utilize S3 Cross Region Replication","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html","title":"S3 Cross Region Replication"}],"answers":[{"id":"6e22da724261694b4ecf8fa105ef5174","text":"Enable Cross-Region Replication on your bucket to copy objects to a destination bucket in another AWS Region.","correct":true},{"id":"f876473fb6289d5a1e3e6d449713b0e7","text":"Copy the company data to an RDS instance. Deploy a multi-AZ configuration for your RDS instance to make it highly available.","correct":false},{"id":"fbde7989c5f0da5bb91bb5593f9c8e4e","text":"Create a Lambda function that downloads data from your S3 Bucket and executes a PUT operation to upload copied objects into a new bucket in a new Region.","correct":false},{"id":"a4ca2ada50730e36dd0f3302f60ab0dc","text":"Create multiple snapshots of your company data on EBS volumes. Deploy those EBS volumes on EC2 instance in different AWS Regions.","correct":false}]},{"id":"114b6b8e-801b-43bf-8229-6086cb2bac9f","domain":"data-man","question":"Your team is migrating a MySQL database (version 5.6) from on-premises to AWS. It is known that the database load is very unpredictable and read-intensive. You need to select a database service that will automatically scale its compute capacity, based on the application's needs, and require the least amount of administration and management overhead. Which AWS Database service would you choose?","explanation":"Amazon Aurora Serverless is the correct answer as it can automatically start up, shut down, and scale the compute capacity and is suitable for unpredictable and read-intensive requests. Aurora Serverless also requires minimal management as AWS configure and manage the DB service and platform. MySQL on EC2 is eliminated as it doesn't meet any of the autoscaling or management requirements. Although RDS MySQL and Aurora Cluster are managed by AWS, they do not have the ability to automatically scale their compute capacity.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html","title":"Using Amazon Aurora Serverless"}],"answers":[{"id":"51e3de37586339e9534e5b4d38015b1f","text":"Amazon Aurora Serverless","correct":true},{"id":"58dbd4ac6f5e40d5300757915fa51606","text":"MySQL on EC2","correct":false},{"id":"e503e981db6c49edcd41880be77c5369","text":"Amazon Aurora Cluster","correct":false},{"id":"88cafd371970b1e442de7b45206d1688","text":"MySQL in RDS","correct":false}]},{"id":"5b71bc80-b7ef-4fde-86f0-1a73d1d63e4c","domain":"high-avail","question":"You are an AWS administrator and have set up an Elastic Load Balancer inside a VPC. The ELB spans several Availability Zones. The ELB sits in front of a web application running on Amazon EC2. You notice that incoming traffic is not being evenly distributed across the AZs. How would you solve this issue?","explanation":"Traffic not evenly distributed across the instances in multiple AZs means the traffic is going to only specific EC2 instances. This happens when either the instances which are not receiving the traffic are unhealthy, or the instances that are receiving the traffic are holding on to the session. Since there is no mention of unhealthy instances, disabling sticky sessions on the ELB is the best answer. Increasing the number of subnets and/or instances will not solve the problem as users will remain stuck to the original instance. Increasing the frequency of health checks will have no impact to force even distribution of traffic.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html","title":"Configure Sticky Sessions for Your Classic Load Balancer"}],"answers":[{"id":"d532ac3048a69a5902a79b51bc219bd3","text":"Increase the number of EC2 instances behind the ELB.","correct":false},{"id":"b7116d2a43d8462c83e8b93fda085c71","text":"Disable sticky sessions on the ELB.","correct":true},{"id":"c0d9e44a3e92d6d91864058257e9922c","text":"Add additional subnets within your ELB and configure your ELB to span the new subnets.","correct":false},{"id":"e23eea1cecbc63f7423de00e65f29614","text":"Increase the frequency of the health checks to the EC2 instances running your application.","correct":false}]},{"id":"ab86f355-ca8c-4f03-bc21-bd2b6add379f","domain":"high-avail","question":"You need to deploy a DynamoDB table in the production environment for an application. The read and write traffic is low during weekdays. On weekends, the traffic becomes much higher due to the increasing number of users. The traffic pattern is very predictable and there is no unexpected spike. Which kind of capacity mode would you configure for the DynamoDB table?","explanation":"In this scenario, DynamoDB Auto Scaling should be used as the pattern is predictable. On-demand mode is more suitable for unknown workloads and development environments. Reserved capacity would cause a waste of resources. And users cannot configure a schedule to automatically adjust the capacity in DynamoDB.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html","title":"DynamoDB read/write capacity mode"}],"answers":[{"id":"4fdafc1c54632989128146ba919a0e07","text":"Provisioned capacity with auto scaling.","correct":true},{"id":"b799c024f6af20489d00692976f1bfd1","text":"On demand mode without the need to configure the capacity.","correct":false},{"id":"fe576b38a59cfeaa466623938285521e","text":"Provisioned capacity on weekdays with a schedule to automatically increase the capacity on weekends.","correct":false},{"id":"35ba3d07134765e8d2b001befc79e653","text":"Reserved capacity that is able to cover the high traffic on weekends.","correct":false}]},{"id":"fa8987c8-41e8-43a5-b9d3-1446dce8e0ca","domain":"security-comp","question":"Your Security Architect would like to perform Penetration Testing on your entire AWS environment. They would like to schedule this as soon as possible, how should you approach this?","explanation":"Penetration Testing is allowed with prior approval from AWS","links":[{"url":"https://aws.amazon.com/security/penetration-testing/","title":"Pen Testing in AWS"}],"answers":[{"id":"2e72d2d8a44bb5d14dd29d0fd1e97d65","text":"Penetration Testing is not supported at any time as it can be disruptive in a multi-tenant environment","correct":false},{"id":"530b142692a4e704a1565e00789ad493","text":"You are free to perform Penetration Testing on your own environment any time you like","correct":false},{"id":"ddf28d1bf64dae2b0a151350c25ca26e","text":"There is no need to perform Penetration Testing because AWS automatically runs regular tests on your behalf and provides any security recommendations in the Trusted Advisor console","correct":false},{"id":"f61573848ecce2f46433c0841355646d","text":"You need to request approval before you can perform Penetration Testing in AWS","correct":false},{"id":"7be6630643492a9ae03b1911fb54573a","text":"This will depend on the service and the type of test you want to perform - some will require permission","correct":true}]},{"id":"9423b7e6-b070-49a7-a8c4-de6a7c446e79","domain":"networking","question":"You have configured Direct Connect between you data center and your VPC in the US West Region, you then create a new VPC in US East. How can you use your Direct Connect connection to access your VPC in US East?","explanation":"You can use an AWS Direct Connect gateway to connect your AWS Direct Connect connection to one or more VPCs in your account that are located in the same or different regions.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html","title":"Direct Connect Gateways"}],"answers":[{"id":"51c17cb2e52441d24159591098c03005","text":"Use Direct Connect ","correct":false},{"id":"ca2a3e2138030d9141c0e8f3af86f76b","text":"Use VPC peering connect to the US East VPC","correct":false},{"id":"a7f9ed179820688ffcef92bb3ce621ee","text":"Use a Direct Connect gateway to connect to the VPC using a private virtual interface","correct":true},{"id":"d4ef5b9099dae72e2ef10d48e7086223","text":"Use Direct Connect to connect to the VPC using a VPN Gateway ","correct":false}]},{"id":"1de9eb98-4b61-4178-b30f-68d5d6422439","domain":"mon-rep","question":"You need to monitor application-specific events every 10 seconds. How can you configure this?","explanation":"you need to configure a custom metric to handle application specific events and if you want to monitor at 10 second intervals, you need to use high-resolution metrics. Detailed monitoring reports metrics at 1 minute intervals.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"CloudWatch Custom Metrics"}],"answers":[{"id":"da902bf148db5983868bf3383162183a","text":"Select high-resolution metrics in CloudWatch","correct":false},{"id":"886f04b8880657a2d07a12c6355639a7","text":"Configure the application to send notifications using SNS every 10 seconds","correct":false},{"id":"f3e15ffbefd8415dda26321a2912dca1","text":"Select detailed monitoring in CloudWatch","correct":false},{"id":"8625460160031630c52a466b7a91f16b","text":"configure a high-resolution custom metric in CloudWatch","correct":true}]},{"id":"y895ku45-wsg2-9rye-087a-zdmu2wd7qtr8","domain":"mon-rep","question":"Which AWS service can be used to log API calls from the AWS console, the EC2 CLI, the AWS CLI, or the AWS SDKs.","explanation":"CloudTrail captures API calls and delivers the log files to an Amazon S3 bucket.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/APIReference/using-cloudtrail.html","title":"Logging API Calls Using AWS CloudTrail"}],"answers":[{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"a8c600be214ced26950e704d39c3ca21","text":"CloudWatch Logs","correct":false},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":true}]},{"id":"d1f23ad4-6255-449e-ba3e-93f4ea692185","domain":"dep-prov","question":"You encounter problems while detaching an EBS volume through the Amazon EC2 console. What can help you diagnose the issue?","explanation":"The 'describe-volumes' CLI command can be used to gather further information to help with diagnosing volume issues and is therefore the most appropriate. The 'detach-volume --force' CLI command forces detachment if the previous detachment attempt did not occur cleanly (for example, logging into an instance, unmounting the volume, and detaching normally). This option can lead to data loss or a corrupted file system. Use this option only as a last resort to detach a volume from a failed instance. The 'Dismount-EC2Volume' PowerShell command will perform the same action as the EC2 console and will therefore not help with additional diagnosis. The 'fix-volumes' command is not a valid CLI command.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-detaching-volume.html","title":"Detaching an Amazon EBS Volume from an Instance"}],"answers":[{"id":"8f6461e3ace3c5f5cab7f6c269e12420","text":"Run the 'fix-volumes' CLI command.","correct":false},{"id":"75f13dd600a356952c8f887bd5bcb626","text":"Run the 'Dismount-EC2Volume' PowerShell command.","correct":false},{"id":"8f52ce0a3a674b594aaed05357a3d828","text":"Run the 'detach-volume --force' CLI command.","correct":false},{"id":"6dab4100f13c64be47fc0b254385f5ab","text":"Run the 'describe-volumes' CLI command.","correct":true}]},{"id":"217a427c-502e-401f-b0e1-9a1794748dd2","domain":"dep-prov","question":"You have designed your corporate AWS infrastructure so that all applications will reside in private subnets to reduce the chance of unauthorised attack from the Internet. However, you have been reminded that some people may need SSH access to these applications from the Internet, under certain circumstances.  You decide to install a Bastion Host in a public subnet to allow SSH access through to the applications for these people.  Which of the following options can be used to reduce the chance of the Bastion Host being compromised?","explanation":"Firstly, let us discount building from a Bastion AMI as this doesn't exist and we can also remove options that include logging, because although they are useful to find who attempted access, they don't reduce the chance of attack.  Implementing any or all of the remaining options will add to the \"strength in depth\" philosophy and could prevent the Bastion Host being compromised.","links":[{"url":"https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html","title":"Linux Bastion Hosts on the AWS Cloud"}],"answers":[{"id":"e4baede75c62ff192032aa1048885c6c","text":"Remove all packages that are not used","correct":true},{"id":"205062c31d482ee937ff645c845dba33","text":"Build the Bastion Host from the AWS Bastion AMI","correct":false},{"id":"ea54b90fc7659554d53c42962e2b83a5","text":"Log all SSH access attempts","correct":false},{"id":"bac7e3fe44c6180ec86d68334b027d40","text":"Allow only TCP port 22 from 0.0.0.0/0 in the Security Group","correct":true},{"id":"f16e7d50eb7d520eb2104bb73820ae91","text":"Install Fail2Ban on the Bastion Host","correct":true}]},{"id":"fcfd9e09-dd0c-45a2-abb0-a6d92297ef92","domain":"security-comp","question":"You have just been hired as a CISO at a space exploration company that makes rockets. The company has contracts with the US airforce and has very strict IT requirements. You discover that on your first day a third party IT auditing company is on site and they are after security and compliance documents such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. Which AWS service can help you meet this need?","explanation":"All AWS accounts can get access to AWS compliance documentation using AWS Artifact","links":[{"url":"https://aws.amazon.com/artifact/","title":"Artifact FAQs"}],"answers":[{"id":"05f43441d2d29ae2bb38fc8596ca6ff7","text":"AWS Trusted Advisor","correct":false},{"id":"7c90c8f2a24f3a1a28525f19fb2c75ab","text":"AWS Inspector","correct":false},{"id":"fa092ee9faf62930336257691a3dbfe8","text":"AWS Config Manager","correct":false},{"id":"05a080cb7d6b6da90ca133767496ccb0","text":" AWS Artifact","correct":true}]},{"id":"dff26ef7-d136-42ac-8aa9-714c300a29d5","domain":"dep-prov","question":"Your company is migrating an application to AWS. The application requires you to launch 60 c5.xlarge instances running Red Hat Enterprise Linux in us-west-2. As the company's SysOps Administrator how would you ensure that the application is successfully migrated without any blockers?","explanation":"The default number of EC2 instances running in a Region per AWS account is 20. If you don't request a limit increase before the migration, the application would not work as the required number of instances is 60. The correct way to do this is to request a service limit increase by opening a Support Case in the Management Console. Trusted Advisor will show quotas and limits but you are not able to request a limit increase through Trusted Advisor. Service Catalog is a way for organizations to create and manage catalogs of IT services that are approved for use on AWS. You would still need to increase the limit before reserving instances (although there is no need to reserve in us-east-1, when the question explicitly requires the application to run in us-west-2).","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/manage-service-limits/","title":"How do I manage my AWS service limits?"}],"answers":[{"id":"dd7b4b1bc0f2865b281ba87f788f551b","text":"Use AWS Service Catalog to request an increase in the default number of instances running in us-west-2.","correct":false},{"id":"b0b7bad81a35cc66e9a225de4cd217c6","text":"Using Trusted Advisor to view the account's service limits in us-west-2. Request an increase through the Trusted Advisor console if the limit is less than 60.","correct":false},{"id":"51c6ca913a627970aad5f4f4703eec03","text":"Purchase 60 reserved instances for c5.xlarge on Red Hat Enterprise Linux in us-east-1.","correct":false},{"id":"00e82002fb3bc4e3770183ec5d74683e","text":"Submit a service limit increase through Support Center in the AWS Management Console. Specify the instance type and Region.","correct":true}]},{"id":"2bd13304-db2d-4120-9487-7e13d7008a63","domain":"dep-prov","question":"EC2 instances are launched from Amazon Machine Images (AMIs). A given public AMI:","explanation":"An AMI cannot be launched into another region. To launch an AMI into a region other that the one in which it was created, the AMI must be copied to that other region first.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html","title":"Copying an AMI"}],"answers":[{"id":"5a70861f841598d574b8ebd61e20cfc1","text":"Can only be used to launch EC2 instances in the same AWS Availability Zone (AZ) as the AMI is stored.","correct":false},{"id":"3b690d9883c0bbec853ffdc3be150208","text":"Can only be used to launch EC2 instances in the same AWS country as the AMI is stored.","correct":false},{"id":"7096f3da806d7b3014ab808eb74d213a","text":"Can only be used to launch EC2 instances in the same AWS region as the AMI is stored.","correct":true},{"id":"9ba6ee29e152b251b8fc17d5d2aca126","text":"Can be used to launch EC2 instances in any AWS region.","correct":false}]},{"id":"4c7fb750-0ec0-4c9f-8dc6-378122edf97a","domain":"automation","question":"The engineering team of a digital marketing company has a lot of AWS Lambda functions directly created and managed using the AWS Console. The CTO has mandated that the code and the deployments are managed using templates and the code is stored in a code repository to enable proper version control processes. How can the team achieve this?","explanation":"SAM templates and CloudFormation templates can be used to manage the Lambda function code. Out of all the options, only CodeCommit can be used directly as a managed service for a code repository. S3 buckets are not used directly as a code repository.","links":[{"url":"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html","title":"What is SAM"}],"answers":[{"id":"c5fe89ee64d944e98f26b2db3fa7cea2","text":"Use SAM templates to manage the lambda function code. Use CodeCommit for the code repository.","correct":true},{"id":"b670d764967a3aa65a1424279e37291c","text":"Use SAM templates to manage the lambda function code. Use S3 buckets for the code repository.","correct":false},{"id":"a85a26cd33fe99adb584452fa780dce2","text":"Use CloudFormation templates to manage the lambda function code. Use S3 buckets for the code repository.","correct":false},{"id":"f33ec453646a2074aa930a61f466fc1a","text":"Use CloudFormation templates to manage the lambda function code. Use ECR for the code repository.","correct":false}]},{"id":"1982b211-0620-43ed-9a88-f5237a42eae2","domain":"automation","question":"You're using CloudFormation templates to build out staging environments. Which section of the template would you edit in order to allow the user to specify the SSH key-name at start time?","explanation":"The parameters property type in CloudFormation allows you to accept user input when starting the template, allowing you to reference the user input as variable throughout your cloud formation template.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html","title":"About CloudFormation Parameters"}],"answers":[{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":true},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":false},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false},{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false}]},{"id":"582d1290-8099-4ef5-8f80-ea6043cc32d7","domain":"security-comp","question":"You are a Sys Ops Administrator for your organization. During a routine security audit, you discovered several vulnerabilities in the operating systems of your EC2 fleet. Your EC2 fleet consists of over 250 instances. How would you resolve the security issues within your EC2 fleet in the most effective manner?","explanation":"AWS Systems Manager Patch Manager automates the process of patching managed instances with both security related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. Amazon GuardDuty is a security monitoring service that analyzes VPC Flow Logs, CloudTrail Events, and DNS logs. It would not be effective in applying patches to EC2. Amazon Inspector could help identify EC2 with security vulnerabilities, but the findings generated by Amazon Inspector depend on your choice of rules packages included in each assessment template, the presence of non-AWS components in your system, and other factors. You are responsible for the security of applications, processes, and tools that run on AWS services. Similar to Amazon Inspector, AWS Config would not help in applying patches to your entire fleet.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html","title":"Systems Manager"}],"answers":[{"id":"6256bbd45632da9b813aac4c9411f564","text":"Have Amazon Inspector assess the instances and send metric data to CloudWatch. Set up a CloudWatch Event to trigger a Lambda function that will install the patch to instances with the vulnerability.","correct":false},{"id":"2161bb4bdfc887d3c242cee80467ae30","text":"Deploy the security patch using RunCommand with AWS Systems Manager for the entire fleet of EC2 instances.","correct":true},{"id":"20ebac0f0e1509a9f9b321e86861fdff","text":"Deploy Amazon GuardDuty which will create a unique finding ID for each vulnerability in CloudWatch. Automate security patch updates with Lambda to the instances that are associated with a GuardDuty finding ID.","correct":false},{"id":"b966c9e66f79563a38283cf5493db53e","text":"Use AWS Config to identify the EC2 instances with vulnerabilities based on security rules. Isolate these instances and install the patch updates to fix the vulnerabilities.","correct":false}]},{"id":"b91a789f-f8f6-48d3-9117-363c14c95946","domain":"security-comp","question":"A company is migrating its financial systems to AWS. In order to pass audit before go-live a SysOps engineer must provide evidence that the services in use are PCI compliant. How can you obtain the current PCI DSS Attestation of Compliance (AOC)?","explanation":"PCI Attestation documents can be retrieved from any authorised user in AWS accounts by accessing AWS Artifact.  There is no such thing as the AWS Compliance Center or AWS PCI Toolkit. Raising a ticket with Support is not required to access the documents in Artifact.","links":[{"url":"https://docs.aws.amazon.com/en_pv/artifact/latest/ug/getting-started.html","title":"Getting Started with AWS Artifact"}],"answers":[{"id":"7e539fddb94e37fb5f292115f0a24e78","text":"Contact AWS Support","correct":false},{"id":"a6bdf3896ac02a764afb902ece44813c","text":"Look in AWS Artifact","correct":true},{"id":"f5099df9b7b208e79614eb58ece30915","text":"Use the AWS PCI Toolkit","correct":false},{"id":"d20092bc27d66c67e88d2a342e48ff12","text":"Check the AWS Compliance Center","correct":false}]},{"id":"daab371e-09f1-4d56-ae18-ac01147c8d31","domain":"automation","question":"Your organisation is growing their AWS footprint and wants to build a dashboard for their hybrid infrastructure.  They use a mix of on-premises Linux and Windows machines, and new AWS EC2 instances. There are around 1500 on-premises VMs which your CTO ambitiously wants to manage with a centralised configuration tool.  How can your organization simplify the management of the patching and inventory of both the on-premises and cloud instances from one central AWS account?","explanation":"Microsoft patching is only available for on-premises instances under the 'advanced-instances' tier of Systems Manager.  The standard Systems Manager tier also only enables you to register a maximum of 1,000 servers per AWS account per AWS Region. If you need to register more than 1,000 servers or VMs in a single account and Region, then you need to use the advanced-instances tier. Since there are over 1,000 servers, and a mix of Windows and Linux workloads, the organisation needs to enable Systems Manager Advanced-Instances before it can perform inventory and patching of the whole hybrid fleet using SSM.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html","title":"Setting Up AWS Systems Manager for Hybrid Environments"}],"answers":[{"id":"b99a0eb336123271439fdc9e65a6eace","text":"You can patch on-premises Linux VMs using AWS Systems Manager, however patching of on-premises Windows instances is not supported using AWS Systems Manager.","correct":false},{"id":"59d2c9186d25dac05a2935b57bd9c708","text":"Enable AWS Systems Manager Advanced-Instances and use Systems Manager to inventory and patch the instances via the SSM agent.","correct":false},{"id":"dc3a64c07de1193f3276dfd13990fbc3","text":"Enable AWS Systems Manager Enterprise to inventory and patch instances via the SSM agent.","correct":false},{"id":"34c2b7a9390e3561fd7641103bc12575","text":"Use AWS Systems Manager Managed Instances to inventory and patch instances via the SSM agent.","correct":true}]},{"id":"b439f1df-a83a-47d6-96a0-c14944daeaff","domain":"networking","question":"A UK company is building its presence in India. It has decided to deploy dedicated infrastructure in India for the local market.  How can the company seamlessly route its Indian users to the load balancer located in ap-south-1 instead of the UK?","explanation":"Route53 geolocation routing is the only answer here which will return a different DNS record based on the source IP address of the DNS request.  The other answers would not route users correctly based on their country.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/geolocation-routing-policy/","title":"How do I direct traffic to specific resources or AWS regions based on the query's geographic location?"}],"answers":[{"id":"1844cecdf015b88aef8e88746b321731","text":"Use Route53 latency routing","correct":false},{"id":"ebeefbe1e321377375eb3065b4a2fc20","text":"Use Route53 failover routing","correct":false},{"id":"816c21d5068061f198f8df28aaf61e0d","text":"Use Route53 multi-answer routing","correct":false},{"id":"055d83107cd15485df5c1f48c53f1758","text":"Use Route53 geolocation routing policy","correct":true}]},{"id":"78cce7a4-ca99-425a-9b76-cc6d6dea1ce3","domain":"mon-rep","question":"For which of the following would you need to create a custom metric in order to monitor it in CloudWatch?","explanation":"By default, CloudWatch will provide metrics on Network, Disk and CPU. You will need to use a custom metric if you want to gather memory metrics","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html","title":"Monitoring Memory in CloudWatch"}],"answers":[{"id":"4789f23283b3a61f858b641a1bef19a3","text":"Memory","correct":true},{"id":"2b55387dd066c5bac646ac61543d152d","text":"CPU","correct":false},{"id":"380dbc8d9d2c8a17f6ebb0b2c62d3e85","text":"Disk","correct":false},{"id":"eec89088ee408b80387155272b113256","text":"Network","correct":false}]}]}}}}
