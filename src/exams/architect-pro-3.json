{"data":{"createNewExamAttempt":{"attempt":{"id":"c56f479d-7abe-4e90-b3f7-bf6a5142c543"},"exam":{"id":"3585674f-9a7d-4771-89b2-93d93262dfab","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"33803c8a-b588-4dca-8067-e500383254f3","domain":"awscsapro-domain4","question":"You work for a retail services company that has 8 S3 buckets in us-east-1 region. Some of the buckets have a lot of objects in them. There are Lambda functions and EC2-hosted custom application code where the names of these buckets are hardcoded. Your manager is worried about disaster recovery. As part of her business continuity plan, she has requested you to set up Cross-Region Replication of these S3 buckets to us-west-1, ensuring that the replicated objects are using a less expensive Storage Class because they would not be accessed unless disaster strikes. You are worried that in the event of failover due to the entire us-east-1 region being unavailable, the application code, once deployed in us-west-1, must continue to work while trying to access the S3 buckets in the new region. She has also requested you to start taking periodic snapshots of EBS Volumes and make these snapshots available in the us-west-1 region so that EC2 instances can be launched in us-west-1 using these snapshots if needed. How would you ensure that (a) the launching of EC2 instances works in us-west-1 and (b) your application code works with the us-west-1 S3 buckets?","explanation":"This question presents two problems - (1) how to ensure that EBS snapshots are created periodically and are also made available in a different region for launching required EC2 instances in case of failure of the primary region (2) how to deal with application code where S3 bucket names are hardcoded and whether this hardcoding will impact disaster recovery while trying to run in a different region. Both of these problems are real-life issues AWS customers face when designing and planning their disaster recovery solutions.\n(1)Remember that Data Lifecycle Manager can only schedule snapshot creation in the same Region. If we want to copy that snapshot into a different region, we must write our own scripts or Lambda functions for doing that. Hence, the choices that state that DLM can be used to directly create the snapshot into different regions are eliminated. Additionally, only root volume snapshots can be used to create an AMI. Non-root EBS Volume snapshots cannot be used to generate an AMI. Hence, the choices that specify using non-root volume snapshots are eliminated.\n(2)Remember that S3 bucket names are globally unique. Hence, one cannot create a second S3 bucket in the DR Region with the same name as the bucket in the primary region. Hence, the options that hint the creation of S3 buckets by the same name are eliminated. This results in a problem if S3 names are hardcoded in the application - that application will simply not run in a new region, it will fail. Hence, it is best to avoid hardcoding, and fetch the S3 bucket name from a key-value storage service like AWS Systems Manager Parameter Store at runtime. Creating this Parameter Store in each region and storing the correct bucket names in them can help in designing this non-hardcoded solution. Additionally, enabling Cross-Region Replication does not copy pre-existing content. Hence, the choices that suggest that pre-existing content will be automatically copied are eliminated.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-launch-snapshot.html","title":"Launching an Instance from a Backup"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/ec2/copy-snapshot.html","title":"Copy-snapshot documentation"}],"answers":[{"id":"fae496411f2d461e0926abfdf8ad8b64","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":false},{"id":"824dc187ab89444593b50521f60b8ff3","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":true},{"id":"02e8aed40d51e4bf9256f1ed436aa069","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false},{"id":"fc1a2efe00ef610249a55dadb0dd64fe","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false}]},{"id":"3ca7c5e0-432a-4d40-afee-ab996819b429","domain":"awscsapro-domain3","question":"You are consulting with a client to guide them on migration of an in-house data center to AWS.  The client has stipulated in the contract that the migration cannot require any more than 1 hour downtime at a time and that there is always a fallback path.  Additionally, they want an overall increase in business continuity capabilities when the migration is done.  Their landscape is as follows:  (1) Several databases with about 1TB of data combined which are heavily used 24x7 and considered mission critical; (2) About 40TB of historic files which are read sometimes but almost never updated; (3) About 150 web servers on VMware in various states of customization of which there is a current project underway to standardize them.  The client's team has suggested some next steps but because they aren't yet familiar with AWS, they are not using equivalent AWS terms.  Translating their suggestions, which of the following activities would you choose to meet the requirements, reducing costs and management where possible?","explanation":"The database migration suggestion aligns well with DMS as it can keep the databases in sync until cutover.  SAN replication sounds a lot like Storage Gateway which is a reasonable way to migrate data to AWS.  However, simply using K8s does not convert your VMs into containers or make them serverless.  We can't restore tapes to AWS.  Creating the same VM landscape on AWS just adds an additional layer of complexity that's not needed.","links":[{"url":"https://aws.amazon.com/dms/faqs/","title":"AWS Database Migration Service FAQs - Amazon Web Services"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"}],"answers":[{"id":"3a02ebcd33fe18255e4ce43e8babb730","text":"Over several months, at end of business on Friday, backup all the servers and data to tape and restore to new instances in AWS to prove out AWS capabilities and reliability.","correct":false},{"id":"75d528d2ec243c60d1478ae605c89f40","text":"Build a matching VMware environment on AWS and use third-party tools to backup and restore the VMs there.","correct":false},{"id":"801ce55cfc2a125e7d17c729ca3e2e93","text":"Create new high powered stand-alone database instances in AWS and migrate data from on-prem database.  Use log shipping to keep the databases in sync.  Once we better understand AWS, we'll rebuild the servers and repartition the tables. ","correct":true},{"id":"31ea0eccdcea45ea4fce3b9459de52d4","text":"Use some block-level SAN replication tool to gradually migrate the on-prem historic files to AWS.","correct":true},{"id":"d2dde578790a34d9e740015474ea23e4","text":"Migrate the majority of the 150 web servers to a serverless concept by moving the VMs to a Kubernetes cluster.","correct":false}]},{"id":"3b08a75a-01b7-4083-bbd1-af1acd7e5314","domain":"awscsapro-domain2","question":"A clothing retailer has decided to run all of their online applications on AWS. These applications are written in Java and currently run on Tomcat application servers hosted on VMware ESXi Linux virtual machines on-premises. Because many of the applications require extremely high availability, they've deployed Oracle RAC as their database layer. Some business logic resides in stored procedures in the database. Due to the timing of other business initiatives, the migration needs to take place in a span of four months. Which architecture will provide the most reliable and operationally efficient solution?","explanation":"Elastic Beanstalk provides the most operationally efficient solution for the application server layer. With Elastic Beanstalk, you can quickly deploy and manage applications without worrying about the infrastructure that runs those applications. VMware Cloud on AWS delivers a robust environment for running Oracle RAC environments. Oracle RAC doesn't run natively on EC2. Due to the time constraints for the project, a migration to Aurora Multi-Master is probably not feasible in four months, especially when the migration of stored procedure code is involved. RDS Oracle Multi-AZ provides active/passive failover, whereas Oracle RAC is active/active, providing no-downtime failovers. Oracle Recovery Manager can run on EC2 and access the database via the VMware Cloud ENIs to perform backups to S3 over a VPC Endpoint.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_Java.html","title":"Creating and Deploying Java Applications on AWS Elastic Beanstalk"},{"url":"https://aws.amazon.com/vmware/","title":"VMware Cloud on AWS"},{"url":"https://d1.awsstatic.com/VMwareCloudonAWS/aws_reference_architecture_oracle_rac_on_vmware_cloud.pdf?did=wp_card&trk=wp_card","title":"Oracle RAC on VMware Cloud on AWS"}],"answers":[{"id":"2a2d87b2ff0208d44d780f2cf354f3f2","text":"Use a certified Tomcat AMI to deploy the application servers on EC2 instances with Auto Scaling across multiple Availability Zones. Install Oracle RAC on EC2 across multiple Availability Zones. Connect the Tomcat servers to the database via JDBC. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":false},{"id":"3cac9f7ec8cba84a36f70ab25c667485","text":"Use a certified Tomcat AMI to deploy the application servers on VMware Cloud on AWS EC2 instances with Auto Scaling across multiple Availability Zones. Configure Oracle on Amazon RDS with Multi-AZ. Connect the Tomcat servers to the database instances with VMware Cloud ENI route table entries. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":false},{"id":"b555cc3afefbe63e6b58594cd86b4fa5","text":"Deploy AWS Elastic Beanstalk to run the Tomcat servers in multiple Availability Zones. Migrate the database to Amazon Aurora Multi-Master. Connect the Tomcat servers to the database via JDBC. Leverage Aurora's Multi-AZ and automated backup capabilities to achieve high availability.","correct":false},{"id":"69173b8054edfd0b9be84428b694bc53","text":"Implement AWS Elastic Beanstalk to run the Tomcat servers in multiple Availability Zones. Run Oracle RAC on VMware Cloud on AWS in multiple Availability Zones. Connect the Tomcat servers to the database instances with VMware Cloud ENI route table entries. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":true}]},{"id":"2c688b4f-f267-472d-a68f-db7c9070bfae","domain":"awscsapro-domain5","question":"An application has a UI automation test suite based on Selenium and the testing scripts are stored in a GitHub repository. The UI tests need a username and password to login to the application for the testing. You check the test scripts and find that the credentials are saved in the GitHub repository using plain text. This may bring in some potential security issues. You suggest saving the username and password in a secure, highly available and trackable place. Which of the following methods is the easiest one?","explanation":"AWS Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. The credentials can be stored as ciphertext. The service is highly scalable, available, and durable. It also integrates with CloudTrail so the usage is easy to track. DynamoDB, DocumentDB and S3 are not designed to store parameters. These services need more configurations and are not as simple as AWS Parameter Store.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html","title":"AWS Systems Manager Parameter Store"}],"answers":[{"id":"33442a701fc2a058eea89266ea439ee4","text":"Create a table in DynamoDB that has a primary key for username and a sort key for password. Select the encryption type as \"KMS - AWS managed CMK\". Enable the on-demand read/write capacity mode.","correct":false},{"id":"fed8d54a1ce26b883744ac61a13e5ac9","text":"Configure an Amazon DocumentDB cluster with the d5.r5.large instance class. Create a schema for username and password. Enable Auto Scaling for the cluster and set up the default number of instances to be 3.","correct":false},{"id":"d540b786961136abf5dfd186e84fcca0","text":"Save the username and password in AWS Parameter Store. Select the SecureString type to encrypt the data with the default key in Key Management Service (KMS). Modify the scripts to fetch the parameter values through AWS SDK.","correct":true},{"id":"26e06a6fc6a107b0dd5aac356ad3908d","text":"Edit a JSON file to store the username and password and upload the file to an S3 bucket. Encrypt the S3 bucket with SSE-S3. Modify the S3 bucket policy to only allow the testing machines to get the file.","correct":false}]},{"id":"431e43bc-ccbc-480f-9915-210bc7773d2b","domain":"awscsapro-domain5","question":"You are in the process of migrating a large quantity of small log files to S3 for long-term storage.  To accelerate the process and just because you can, you have created quite sophisticated multi-threaded distributed process deployed across 100 VMs which can load hundreds of thousands of files at one time.  For some reason, the process seems to be throttled somewhere along the chain.  You try many things to try to uncover the source of the throttling but nothing works.  Reluctantly, you decide to turn off the KMS encryption setting for your S3 bucket and the throttling goes away.  You turn AMS-KMS back on and the throttling is back. Given the troubleshooting steps, what is the most likely cause of the throttling and how can you correct it?","explanation":"Through a process of elimination, it seems you have identified the variable that is causing the throttling.  KMS, like other AWS services, does have rate limiters which can be increased via Support Case.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/limits.html","title":"Limits - AWS Key Management Service"}],"answers":[{"id":"60ff77ab365c15bb11771e94e3dc271d","text":"You have exceeded the number of API calls for your account.  You must create a new account.","correct":false},{"id":"fe629daf7473efc279d7d8ee6f5a5806","text":"You are maxing out your SYNC requests to S3.  You need to request a limit increase via a Support Case.","correct":false},{"id":"05aeb0bc36d7b53aa30bf9e22b6cd120","text":"You are maxing out your PUT requests to S3.  You need to change over to multi-part upload as a workaround.","correct":false},{"id":"1dd4f25e52404e18ddec0b8711a82a13","text":"You are hitting the KMS encrypt request account limit.  You must request a limit increase via a Support Case.","correct":true},{"id":"01148eae3319190a0b228c6d02c9572c","text":"You are maxing out your network connection.  You must split the traffic over multiple interfaces.","correct":false}]},{"id":"e8bba7f5-4c0d-42dd-ad7a-74f042ce3dd9","domain":"awscsapro-domain3","question":"Due to a dispute with their co-location hosting company, your client is forced to move some applications as soon as possible to AWS.  The main application uses IBM DB2 for the data store layer and a Java process on AIX which interacts via JMS with IBM MQ hosted on an AS400.  What is the best course of action to reduce risk and allow for fast migration?","explanation":"For a fast migration with minimal risk, we would be looking for a lift-and-shift approach and not spend any time on re-architecting or re-platforming that we don't absolutely have to do.  Amazon MQ is JMS compatible and would provide a shorter path to the cloud than SQS.  DMS does not support DB2 as a target.","links":[{"url":"https://aws.amazon.com/amazon-mq/features/","title":"Amazon MQ Features – Amazon Web Services (AWS)"},{"url":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html","title":"Targets for Data Migration - AWS Database Migration Service"}],"answers":[{"id":"4dba67b80e52ce08ef39ca56bf0ddd57","text":"Install DB2 on an EC2 instance and migrate the data by doing an export and import.  Spin up an instance of Amazon MQ in place of IBM MQ.  Install the Java process on a Linux-based EC2 system.","correct":true},{"id":"6d8610d6575127564b286722b73ce4be","text":"Install DB2 on an EC2 instance and use DMS to migrate the data.  Encapsulate the Java program in a Docker container and deploy it on ECS.  Spin up an instance of Amazon MQ.","correct":false},{"id":"21cbd80d1d890d3668ba4f841d7901df","text":"Use a physical-to-virtual tool to convert the AIX DB2 server into a virtual machine.  Use AWS CLI to import the VM into AWS and launch the VM.  Deploy the Java program as a Lambda function.  Launch a version of IBM MQ from the AWS Marketplace.","correct":false},{"id":"fbc4f7d74e4d62a2bdce9ca3f55f9fd2","text":"Use DMS and SCT to migrate DB2 to Aurora.  Update the Java application to use SQS and install it on a LInux-based EC2 system.  ","correct":false},{"id":"3700f4c1ab4e0778c4d0ae131d9c277d","text":"Deploy the Java processes as Lambda functions.  Install DB2 on an EC2 instance and migrate the data by doing an export and import.","correct":false}]},{"id":"05e085a9-4de3-46fe-9470-10c7f2faba57","domain":"awscsapro-domain5","question":"You are consulting with a client who is in the process of migrating over to AWS.  Their current on-prem Linux servers use RAID1 to provide redundancy.  One of the big benefits they are looking forward to with moving to AWS is the ability to create snapshots of EBS volumes without downtime.  Right now, they intend on migrating the servers over to AWS and retaining the same disk configuration.  What is your advice for them?","explanation":"Because RAID is based upon multiple volumes being in sync, taking snapshots of an individual volume that's part of a active and mounted RAID array would not create a proper backup.  You must first unmount the RAID volume and then create the snapshots of the component volumes.  This of course means any data on the RAID volume would be unavailable.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html","title":"RAID Configuration on Linux - Amazon Elastic Compute Cloud"}],"answers":[{"id":"21a6a5218cf9778e0184eed7897c54ce","text":"Consider using RAID6 rather than RAID1 on AWS for performance reasons.","correct":false},{"id":"73b207bc7a947de1eed26bc058b4b67b","text":"If snapshots without downtime are the priority, do not use RAID.","correct":true},{"id":"ead9938f5d4fc4d2df30763406b6a8e5","text":"Consider using RAID10 when on AWS because it offers the best of both RAID0 and RAID1.","correct":false},{"id":"bee3a756d6bfddce4e9917e171a4b0e2","text":"Consider using RAID0 when on AWS for performance reasons.","correct":false},{"id":"99a9d29ef15a0ced996c1510ff6d8f6a","text":"EC2 does not support RAID configurations.","correct":false}]},{"id":"edc23d5d-e9ea-4713-b8c2-e35a1aa13626","domain":"awscsapro-domain2","question":"You are helping a company design a fully cloud-based Customer Service application.  Over 50% of their Customer Service Representatives are remote and that number increases and decreases seasonally.  They need the ability to handle inbound and outbound calls as well as chatbot capabilities.  Additionally, they want to provide a self-service option using interactive voice response to customers who do not need to speak to a person.  Which design is feasible and makes most efficient use of AWS services?","explanation":"AWS Connect is Amazon's \"call center in a box\" solution that enabled interactive voice response with Lex and inbound and outbound calling.  Additionally, you can use Lex to build a chatbot.  AWS Workspaces is a managed DaaS that is we suited for deploying to remote workers.","links":[{"url":"https://aws.amazon.com/connect/","title":"Amazon Connect Overview"},{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html","title":"What Is Amazon WorkSpaces? - Amazon WorkSpaces"}],"answers":[{"id":"8ed07bd636bec143bc1f7e2ce888bb03","text":"Create a standard Customer Service Rep desktop and deploy using AWS Workspaces. Setup AWS Connect for inbound and outbound calling.  Leverage Alexa for Business to create chatbot and interactive voice response components.  Store call logs in Redshift and analyze using Quicksight.","correct":false},{"id":"4086f40e467d8d2fadaac8b5b9a2d49b","text":"Setup AWS Connect for inbound and outbound calling.  Make use of Polly and Lex for interactive voice response components.  Create a standard Customer Service Rep desktop and deploy using AWS Workspaces.  Leverage Lex to create a chatbot component.","correct":true},{"id":"29d3b6d296574b52ab6aa78419ca5aad","text":"Create a standardized Customer Service Rep desktop and deploy via CloudFront.  Use Translate and AWS Connect to create a chatbot component.  Leverage Polly to create an interactive voice response component.  Use Alexa for Business for the inbound and outbound calling.","correct":false},{"id":"22128c4eb6db1f8d7e92b3fbb7155565","text":"Use AWS Comprehend to create the chatbot and interactive voice response components.  Use Asterisk PBX from AWS Marketplace to handle the inbound and outbound calling.  Create a standardized Customer Service Rep desktop and deploy using Service Catalog.","correct":false},{"id":"da552d1c23cba5e3e05a5dca7bdc0ca5","text":"Setup Twilio with Lambda to manage inbound and outbound calling.  Create a standard Customer Service Rep desktop Windows AMI and deploy via Service Catalog.  Leverage Polly for creating a chatbot and Translate for an interactive voice response system.","correct":false}]},{"id":"de88bc69-44a8-4a12-b28f-0a5e86db3939","domain":"awscsapro-domain1","question":"You have been entrusted to act as the interim AWS Administrator following the departure of the erstwhile Administrator in your company. You notice that there are several existing roles called role-engineer, role-manager, role-qa, role-dba, role-data-scientist, etc. When a new person joins the company, the new IAM user simply assumes the right role while using AWS - this allows central management of permissions and eliminates the need to manage permissions on a per-user basis.\nA new QA hire joins the company a few days later. You create an IAM User for her. You attach a Policy to the new IAM User that allows Action STS AssumeRole on any Resource. However, when this employee logs in the same day and tries to switch roles to role-qa, she is denied and is unable to assume the role-qa Role.\nWhat could be one reason why this is happening and how can it be best fixed?","explanation":"In order to allow an IAM User to successfully assume an IAM Role, two things must happen. First, the Policy attached to the User must allow the action STS AssumeRole. This is already true according to the question. Second, the Trust Policy of the Role itself must allow the User in question to assume the Role. This second condition can be met if we specify the arn of the User in the Principal element of the Trust Policy. In general, this question can be answered if the candidate is familiar with the concept of Principal in a Role, see link - A Principal within an Amazon IAM Role specifies the user (IAM user, federated user, or assumed-role user), AWS account, AWS service, or other principal entity that is allowed or denied to assume or impersonate that Role. Trust Policy is different than the Policy permissions - think of Policy Permissions as [what can be accessed] and Trust Policy as [who can access].\nTrust Policy cannot belong to an IAM User, hence the choice that claims the problem to be an unmodified User Trust Policy is incorrect. IAM changes are instantly effective, so the choice that points at the need of a time delay is also incorrect. Among the other two choices, the knowledge needed to pick the right one is an awareness of the Principal element.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html","title":"AWS JSON Policy Elements - Principal"},{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html","title":"IAM Roles"}],"answers":[{"id":"b3bf261fca3a734ad312d3ac0e5d0589","text":"You have not modified the Trust Policy of the IAM User to trust the Role role-qa. To fix this, add a Condition to the IAM Policy attached to the new user that filters on the role and specify the arn of role-qa","correct":false},{"id":"dbb48c05e238c18bdb9c17ee265e387b","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Condition element of the Trust Policy of the Role","correct":false},{"id":"765f20e64b35dbc08c2bc319bcbe7e1a","text":"Sufficient time has not passed since you made the changes. It takes up to 12 hours to propagate IAM role changes. To fix this, ask her to try again the next day.","correct":false},{"id":"e6dacaf19a289e1f73855c5e904b21fb","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Principal element of the Trust Policy of the Role","correct":true}]},{"id":"e24f7f76-6908-4ad9-820a-11790bfdcec6","domain":"awscsapro-domain3","question":"You are helping a client migrate over an internal application from on-prem to AWS.  The application landscape on AWS will consist of a fleet of EC2 instances behind an Application Load Balancer.  The application client is an in-house custom application that communicates to the server via HTTPS and is used by around 40,000 users globally across several business units.  The same exact application and landscape will be deployed in US-WEST-2 as well as EU-CENTRAL-1.  Route 53 will then be used to redirect users to the closest region.  When the application was originally built, they chose to use a self-signed 2048-bit RSA X.509 certificate (SSL/TLS server certificate) and embedded the self-signed certificate information into the in-house custom client application.  Regarding the SSL certificate, which activities are both feasible and minimize extra administrative work?","explanation":"You can import private certificates into Certificate Manager and assign them to all the same resources you can with generated certificates, including an ALB.  Also note that Certificate Manager is a regional service so certificates must be imported in each region where they will be used.  The other options in this question would either require you to update the certificate on the client or requires unnecessary steps to resolve the challenge.","links":[{"url":"https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html","title":"Importing Certificates into AWS Certificate Manager - AWS Certificate  Manager"}],"answers":[{"id":"111997579381183b07a22fad8574e76c","text":"Create a new Certificate Authority within Certificate Manager and import the existing certificate.  Generate a new certificate, CA chain and private key and push an update for the application.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false},{"id":"d4976a6c33ee189e6b681dffc83cbac5","text":"Import the existing certificate and private key into Certificate Manager in both regions.  Assign that imported certificate to the Application Load Balancers using their respective regionally imported certificate.","correct":true},{"id":"28694bd7f280694a43741563f6933ad6","text":"Create a new public SSL/TLS certificate using Certificate Manager and configure the common name and OU to match the existing certificate.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false},{"id":"0663551d15f5b15af587ac8bf75a2566","text":"Purchase a new public SSL/TLS certificate from a third-party CA.  Upload the certificate to Certificate Manager and assign that certificate to the Application Load Balancers.","correct":false},{"id":"0c4320d1dd787a5bae2b43479b645d94","text":"Use Service Catalog to push an update of the in-house app which includes an updated certificate and CA chain.  Generate a new private certificate using OpenSSL. Import the new certificate to Certificate Manager in US-EAST-1.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false}]},{"id":"b533b3c1-222f-4f33-99da-2c828e98ff91","domain":"awscsapro-domain5","question":"You have run out of root disk space on your Windows EC2 instance.  What is the most efficient way to solve this?","explanation":"We can easily increase the size of an EBS from the console or the CLI (using modify-volume) but then we also need to allow the OS to expand the resized volume so we can use it.  For Windows Server, we could use Disk Manager.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/expand-ebs-root-volume-windows/","title":"Expand the EBS Root Volume of Your EC2 Windows Instance"}],"answers":[{"id":"b893490015da5047b17b1220d43f4a1c","text":"From the AWS CLI, use the \"modify-instance\" command for EC2 to resize the volume to a larger size.  Using RDP, connect to the Windows instances and use Disk Manager to expand the volume.","correct":false},{"id":"38862a719074689f75df6a20a42f7df7","text":"Use AWS System Manager Run service to remotely execute a PowerShell script using AWS Tools for PowerShell to expand the volume using the ModifyInstance command.","correct":false},{"id":"346ac05b4353d34c630eb6d233f8a35d","text":"Compress all files on the root volume using the built-in zip utility.  Modern versions of Windows will automatically unzip the files when they are accessed.","correct":false},{"id":"229b013eff2d5f53df7b9c3a60bd2418","text":"From the AWS Console, select Modify Volume for the EBS volume.  Enter the new size and confirm the change.  Connect to your Windows instance and use Disk Manager to extend the newly resized volume.","correct":true}]},{"id":"35fc536d-d968-472d-84d5-a7ae5d343564","domain":"awscsapro-domain1","question":"You work for a genetics company that has extremely large datasets stored in S3. You need to minimize storage costs, while maintaining mandated restore times that depend on the age of the data. Data 30-59 days old must be available immediately, and data ≥ 60 days old must be available within 12 hours. Which of the following options below should you consider?","explanation":"You should use S3 - IA for the data that needs to be accessed immediately, and you should use Glacier for the data that must be recovered within 12 hours. S3 - RRS and 1Zone-IA would not be suitable solution for irreplaceable data or data that required immediate access (reduced Durability or Availability), and CloudFront is a CDN service, not a storage solution.  The use of absolute words like 'Must' is an important clue as it will eliminate options where the case may not be possible such as with OneZone-IA.","links":[{"url":"https://aws.amazon.com/s3/faqs/#sia","title":"S3 - Infrequent Access"},{"url":"https://aws.amazon.com/s3/faqs/#glacier","title":"About Glacier"}],"answers":[{"id":"4340570ba672bfa48cd45e3f026c01d1","text":"S3 - IA","correct":true},{"id":"31e831ec49678aed7f467f791d1f8704","text":"S3 - RRS","correct":false},{"id":"e9a5105fa288ef2b71c037e42d665d91","text":"S3 - OneZone-IA","correct":false},{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":true},{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false}]},{"id":"768271e9-9fd0-4921-a473-49ec465a0b34","domain":"awscsapro-domain4","question":"Your company is preparing for a large sales promotion coming up in a few weeks.  This promotion is going to increase the load on your web server landscape substantially.  In past promotions, you've run into scaling issues because the region and AZ of your web landscape is very heavily used.  Being unable to scale due to lack of resources is a very real possibility.  You need some way to absolutely guarantee that resources will be available for this one-time event.  Which of the following would be the most cost-effective in this scenario.","explanation":"If we only need a short-term resource availability guarantee, it does not make sense to contract for a whole year worth of Reserved Instance.  We can instead use On-Demand Capacity Reservations.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html","title":"On-Demand Capacity Reservations - Amazon Elastic Compute Cloud"}],"answers":[{"id":"08d50c618757e489c5ac563be91f25f7","text":"Purchase a regional Reserved Instance.","correct":false},{"id":"0862f1bd2009c6ca8b52898831991642","text":"Use an On-Demand Capacity Reservation.","correct":true},{"id":"f10307b8e644773b68b43dade41382f1","text":"Purchase Dedicated Instances.","correct":false},{"id":"0b48767c7357e407fa6f1e958fe0f051","text":"Purchase a Dedicated Host.","correct":false},{"id":"9dc64814f005254de6e1a269849ae0b1","text":"Purchase zonal Reserved Instance.","correct":false}]},{"id":"f7e5105f-7ae9-4c04-b1e7-2d6165db236c","domain":"awscsapro-domain2","question":"You work for a specialty retail organization. They are building out their AWS VPC for running a few applications. They store sensitive customer information in two different encrypted S3 buckets. The applications running in the VPC access, store and process sensitive customer information by reading from and writing to both the S3 buckets. The company is also using a hybrid approach and has several workloads running on-premises. The on-premises datacenter is connected to their AWS VPC using Direct Connect.\nYou have proposed that an S3 VPC Endpoint be created to access the two S3 buckets from the VPC so that sensitive customer data is not exposed to the internet.\nSelect two correct statements from the following that relate to designing this solution using VPC Endpoint.","explanation":"S3 VPC Endpoint is a common topic tested in the SA-P Exam, as it enables S3 access over a private network, which is a common security requirement in many organizations. It is also a cost-effective way to establish outbound connection to S3, as the alternative is to use NAT Gateways, which are charged by the hour even if there is no traffic using them.\nOn vertical scanning of the answer choices, it should be obvious that one of the two closely worded choices is correct, and one of the other two choices is correct as well. That is because if there are 2 or 3 or 4 closely worded choices, only one (or in some rare cases, two) is correct - this is a common pattern in the SA-P test.\nFor the closely worded pair - the bucket policy of an S3 bucket will always specify who can or cannot access the bucket. It will not dictate how a VPC Endpoint behaves. Hence, the choice that suggests that a bucket policy can control a VPC Endpoint is incorrect.\nBetween the other two choices, remember that a VPC Endpoint can connect to any number of S3 buckets by default. One Endpoint for each bucket is simply not scalable, and should stand out as incorrect.\nThe remaining choice is correct because the S3 VPC Endpoint is of type Gateway Endpoint as opposed to Interface Endpoint, and a subnet needs Routes in the Routing Table for sources in the subnet to be able to connect to it. Read the links provided to understand the differences","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies-vpc-endpoint.html","title":"Example bucket policies including ones that discuss use of sourceVpce attribute"},{"url":"https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/","title":"How to set up VPC Endpoint for S3 access including Route Table for subnets accessing S3"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html","title":"VPC Endpoint documentation discussing differences between gateway endpoints and interface endpoints"}],"answers":[{"id":"19bc72413543fc85eb58b5edf43fb2db","text":"Each VPC Endpoint is a Gateway Endpoint that also requires correct routes in the Route Table associated with each subnet that wants to access the endpoint","correct":true},{"id":"96bd9d9981d98c1d98550618acedc673","text":"Bucket policies on the two S3 buckets can specify the id of each VPC Endpoint using AWS attribute sourceVpce to further restrict which S3 buckets can be accessed by the VPC Endpoint","correct":false},{"id":"55b07e707272bf499484e3f28a8fac88","text":"Bucket policies on the two S3 buckets can specify the id of each VPC Endpoint using AWS attribute sourceVpce to further restrict which VPC Endpoints can access each bucket","correct":true},{"id":"28b96a455027a61e12e57f73ae7956fc","text":"You need two VPC Endpoints, one for each S3 bucket, as a single VPC Endpoint can only access a single S3 bucket","correct":false}]},{"id":"312b233b-ecc8-4e70-ae91-665159c7f77b","domain":"awscsapro-domain2","question":"You work for an automotive parts manufacturer as a Cloud Solutions Architect and you are in the middle of a design project for a new quality vision system.  To \"help out\", your parent company has insisted on contracting with a very expensive consultant to review your application design.  (You suspect that the consultant has more theoretical knowledge than practical knowledge however.)  You explain that the system uses video cameras and special polarizing filters to identify defects on fuel injectors.  As the part passes each station, an embedded RFID serial number is read and included with the PASS/FAIL vision test result in a JSON record written to DynamoDB.  The DynamoDB table is exported to Redshift on a monthly basis.  If a flaw is detected, the part can sometimes be reworked and sent back through the process--but it does retain its unique RFID tag.  Only the latest tests need to be kept for the part.  The consultant reviews your design and seems slightly frustrated that he is unable to recommend any improvement.  Then, he smiles and asks \"How are you ensuring idempotency?  In case a part is reprocessed?\"    ","explanation":"Idempotency or idempotent capability is a design pattern that allows your application to deal with the potential of duplicate records.  This can happen when interfaces fail and some records need to be reprocessed.  In this case, we are using a unique RFID serial number as our identifier for the part.  In DynamoDB, we would just overwrite the record with the latest record using a UpdateItem SDK method.  For Redshift, an UPSERT function allows us to either insert as a new record or update if a record of the same key already exists.  Redshift can do this using a merge operation with a staging table.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateItem.html","title":"UpdateItem - Amazon DynamoDB"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-upsert.html","title":"Use a Staging Table to Perform a Merge (Upsert) - Amazon Redshift"}],"answers":[{"id":"d87a9f08720c0fd91dee999f81f6f0ed","text":"You could change your design to write the message first to an SQS queue with FIFO enabled.  The records would then be guaranteed to process in the order they arrived.","correct":false},{"id":"9dc68479e32f89bfe0afde00f41ae6c3","text":"For the target DynamoDB table, you have defined the unique RFID string as the partition key.  When copying to Redshift, you use table merge method to perform a type of UPSERT operation.","correct":true},{"id":"5a65801823c8e80af868a9ca05e34e18","text":"You will be using API Gateway and Lambda for the insert into DynamoDB so scaling is not a concern. The part can be processed as many times as needed and Lambda will scale as needed.","correct":false},{"id":"8f594d180a595027ddef4e33f2784b0f","text":"You will be using CloudWatch to monitor the DynamoDB tables for capacity concerns.  If needed, you can enable DynamoDB auto scaling to accommodate the extra volume that reprocessing might introduce.","correct":false}]},{"id":"06504582-ce03-4252-b1dc-29654ff427bb","domain":"awscsapro-domain5","question":"You have just set up a Service Catalog portfolio and collection of products for your users.  Unfortunately, the users are having difficulty launching one of the products and are getting \"access denied\" messages.  What could be the cause of this?","explanation":"For Service Catalog products to be successfully launched, either a launch constraint must be assigned and have sufficient permission to deploy the product or the user must have the same required permissions.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-launch.html","title":"AWS Service Catalog Launch Constraints - AWS Service Catalog"}],"answers":[{"id":"a987914548e48bb64be70c11a97ec644","text":"A Service Catalog Policy has not yet been applied to the account.","correct":false},{"id":"779fec840f0f81e772ba3137d7ac28ad","text":"The notification constraint did not have access to the S3 location for the product's CloudFront template.","correct":false},{"id":"8bc518c42ab2dfa122390f1a497349a2","text":"The template constraint assigned to the product does not have the proper permissions.","correct":false},{"id":"43d54acb2dbc9f2dc8a0d793553b965e","text":"The user launching the product does not have required permissions to launch the product.","correct":true},{"id":"7e0083aafd999688d628f67e003d79be","text":"The product does not have a launch constraint assigned.","correct":true},{"id":"d11a80651ecb668bdbe507d4e7398b6a","text":"The launch constraint does not have permissions to CloudFormation.","correct":true}]},{"id":"9038ce91-1730-47e6-b804-571614ac4752","domain":"awscsapro-domain5","question":"For your production web farm, you have configured an auto scaling group behind a Network Load Balancer.  Your auto-scaling group is defined to have a core number of reserved instances and to scale with spot instances.  Because of differences in spot pricing across AZs, sometimes you end up with many more instances in one AZ over another.  During times of peak load, you notice that AZs with fewer instances are averaging 70% CPU utilization while the AZ with more instances average barely above 10% CPU utilization.  What is the most likely cause of this behavior? ","explanation":"Cross-zone load balancing ensures that requests are equally spread across all available instances, regardless of AZ.  When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. When cross-zone load balancing is disabled, each load balancer node distributes traffic across the registered targets in its Availability Zone only. ","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html","title":"How Elastic Load Balancing Works - Elastic Load Balancing"}],"answers":[{"id":"19fc4224d11c7dbb028fcb29fc93ce38","text":"Cross-zone load balancing is disabled on the Network Load Balancer.","correct":true},{"id":"733135f9c29b6aef570b5c4f6748df2f","text":"CloudWatch is not accurately reflecting the true CPU load due to mesh processing on Nginx.  The real CPU load might in fact not reach the threshold which explains why the group is not scaling.","correct":false},{"id":"1d1ef2b9e2525e667cc97a8dd5e91429","text":"The TTL for sticky sessions is set too high and therefore are blocking a scale-out event until some connections are dropped.","correct":false},{"id":"dbec1ca528c5fed82acd6704fa187842","text":"The cooldown time is too short in the launch configuration.","correct":false},{"id":"61e42ddbbd5b30ec8e73f350f7782986","text":"At the time a scale event is triggered, there are no more available resources in the AZ or region of the instance type you have configured in your auto scaling group.","correct":false}]},{"id":"4b00251a-a278-4d88-b715-955b4752a79a","domain":"awscsapro-domain2","question":"You'd like to create a more efficient process for your company employees to book a meeting room.  Which of the following is the most efficient path to enabling this improved business experience?","explanation":"With Alexa for Business, you can enlist Alexa-enabled devices to perform tasks for employees like retrieve information, start conference calls and book meeting rooms.","links":[{"url":"https://aws.amazon.com/blogs/business-productivity/announcing-room-booking-for-alexa-for-business/","title":"Announcing Room Booking for Alexa for Business | Business Productivity"}],"answers":[{"id":"e43eb2c75536fac2d714b862f4fec490","text":"Invest in a voice-to-text API from the AWS Marketplace.  Create a custom Lambda function that calls the API and books a conference room.  Equip each conference room with Amazon Dash buttons and configure them to invoke the Lambda function.","correct":false},{"id":"a723a332c2ed052968becb9b6824e2a4","text":"Sign-up for AWS Alexa for Business. Create conference rooms in the console and place an Alexa device in each conference room.","correct":true},{"id":"12cb35ea295a9a96bfba94741cb4f0df","text":"Sign-up for Amazon Chime.  Create conference rooms in the console and place speakerphones in each conference room.","correct":false},{"id":"5445641c568edaf760839d9f5bb7169c","text":"Configure an Alexa device with a custom skill backed by a Lambda function.  Use Amazon Lex to convert the audio sent to the Lambda function into an actionable skill.  ","correct":false}]},{"id":"5f6d53a1-1b9c-46fe-9a8e-a5706e72914d","domain":"awscsapro-domain4","question":"Which of the following is an example of buffer-based approach to controlling costs?","explanation":"The buffer-based approach to controlling costs is discussed in the Cost Optimization Pillar of the AWS Well-Architected Framework.  A buffer is a mechanism to ensure that applications can communicate with each other when they are running at different rates over time.  By decoupling the throughput rate of a process, you can better govern and smooth demand--creating a less volatile and reactionary landscape.  As a result, costs can be reduced by optimizing for the steady state. ","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"f867e23e60a3917c1ebe5e2c4ced818c","text":"An auto-scaling fleet is created to dynamically adjust available compute resources based network connection events as reported by CloudWatch.","correct":false},{"id":"878d4fde3965b2e5f84c543b2cca1dfc","text":"A production ERP landscape is scaled up during the month-end financial close period to provide some padding for the additional processing and reports so they do not impact the normal business processes.","correct":false},{"id":"26437f21964d56c3e373f55d997101ed","text":"A mobile image upload and processing service makes use of SQS to smooth an erratic demand curve.","correct":true},{"id":"5bc62ebd024ca5594793ca76f08cd960","text":"A public-facing API is created using API Gateway and Lambda.  As a serverless architecture, it scales seamlessly in step with demand.","correct":false}]},{"id":"6dc7fe81-03aa-45d6-b8e1-6dc3b70914e0","domain":"awscsapro-domain1","question":"A company owns multiple AWS accounts managed in an AWS Organization. You need to generate daily cost and usage reports that include the activities of all the member accounts. The reports should track the AWS usage for each resource type and provide estimated charges. The report files also need to be delivered to an Amazon S3 bucket for storage. How would you create the required reports?","explanation":"The consolidated billing feature in AWS Organization does not generate billing reports automatically. You need to configure the AWS Cost and Usage Reports in the master account and use an S3 bucket to store the reports. The generated reports include activities for all the member accounts and it is not required to create a report in each member's account. The option of CloudWatch Event rule and Lambda function may work however it is not a straightforward solution.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-reports-costusage.html","title":"AWS Cost and Usage Report"}],"answers":[{"id":"967b7874a080c033470777ce955a4550","text":"In the master account of the AWS Organization, generate the AWS Cost and Usage Reports and save the reports in an S3 bucket. Modify the bucket policy to allow the billing reports service to put objects.","correct":true},{"id":"149237c5674e21794204a4a0ca00bee2","text":"Create a CloudWatch Event rule that runs every day. Register a Lambda function target which calls the PutReportDefinition API to get cost reports of all AWS accounts and store them in an S3 bucket.","correct":false},{"id":"033f176fcb8f66b1ee9fb950c8741cda","text":"Enable the consolidated billing feature in the AWS Organization which automatically generates a daily billing report. Predefine an S3 bucket to store the reports. Make sure the S3 bucket has a bucket policy to allow the AWS Organization service to write files.","correct":false},{"id":"b66bf1016802f0edb247437b5fda31cb","text":"Login in each AWS account using the root IAM user, configure the daily Cost and Usage Report and set up a central S3 bucket to save the reports from all AWS accounts. Store the reports in different folders in the S3 bucket.","correct":false}]},{"id":"34351bd0-7925-4246-bb61-c64bbf4d5baf","domain":"awscsapro-domain4","question":"An application in your company that requires extremely high disk IO is running on m3.2xlarge EC2 instances with Provisioned IOPS SSD EBS Volumes. The EC2 instances have been EBS-optimized to provide up to 8000 IOPS. During a period of heavy usage, the EBS volume on an instance failed, and the volume was completely non-functional. The AWS Operations Team restored the volume from the latest snapshot as quickly as possible, re-attached it to the affected instance and put the instance back into production. However, the performance of the restored volume was found to be extremely poor right after it went live, during which period the latency of I/O operations was significantly high. Thousands of incoming requests timed out during this phase of poor performance.\nYou are the AWS Architect. The CTO wants to know why this happened and how the poor performance from a freshly restored EBS Volume can be prevented in the future. Which answer best reflects the reason and mitigation strategy?","explanation":"Data gap cannot be the reason for high disk I/O latency. Whether the data being requested is on the disk or not cannot be responsible for the extended period of high disk I/O latency, as all operating systems index the contents in some way. They do not scan the whole disk to conclude that something is missing. Hence, the choice that suggests data gap as the reason is eliminated.\nEBS Optimization works straight away after a freshly restored volume is attached to an EBS optimized instance. Hence, the choice that suggests that EBS Optimization takes some time to kick in is eliminated.\nThere is nothing called set-up-cache command. The option that suggests that there is an inbuilt caching mechanism that needs to be activated is completely fictional, and is eliminated.\nThe only correct option is the one that correctly states that every new block read from a freshly restored EBS Volume must first be downloaded from S3. This is because EBS Snapshots are saved in S3. Remember that EBS Snapshots are incremental in nature. Every time a new snapshot is taken, only the data that changed is written to that particular snapshot. Internally, it maintains the pointers to older data that was written to S3 as part of previous snapshots. These blocks of data continue to reside on S3 even after an EBS Volume is restored, and is read the first time they are accessed. Linux utilities like dd or fio can be used after restoring an EBS Volume to read the whole volume first to get rid of this latency problem when the instance is put back in production.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-initialize.html","title":"Initializing Amazon EBS Volumes"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html","title":"Amazon EBS–Optimized Instances"}],"answers":[{"id":"d3a1c3b2669127dabe2eaf2c490fcd30","text":"A freshly restored EBS Volume needs pre-warming to activate the inbuilt caching mechanism. To fix this, update the restoration process to run the set-up-cache command on the freshly restored EBS Volume first before the instance is put back in production. Also, include random I/O tests to ensure that desired I/O levels are reached before putting the instance back to production.","correct":false},{"id":"ae32d192c3829287819a74ded72e0da7","text":"The latest snapshot did not have the most current data. It only had the data from the last time a snapshot was taken. The requests timed out because of this data gap. To mitigate this, increase the frequency of taking EBS snapshots.","correct":false},{"id":"1a47200401a925a2ad1df3d3286e26dc","text":"When a data block is accessed for the first time on a freshly restored EBS Volume, EBS has to download the block from S3 first. This increases the I/O latency until all blocks are accessed at least once. To fix this, update the restoration process to run tools to read the entire volume before putting the instance back to production.","correct":true},{"id":"51e87b2dbdfcb476b026779380119b06","text":"A freshly restored EBS Volume cannot utilize EBS Optimization Instances straight away, as the network traffic and EBS traffic traverse the same 10-gigabit network interface. Only after the entire volume is scanned by an asynchronous process, EBS Optimization kicks in. This increases the I/O latency until the volume is ready to utilize EBS Optimization. To fix this, update the restoration process to wait and run random I/O tests on a freshly restored EBS Volume. Put the instance back to production only after the desired I/O levels are reached.","correct":false}]},{"id":"c2e1b0c2-2dd7-479b-a78e-8ddd1c6d2448","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a problem.  The client has several Ubuntu Linux servers in a private subnet within a VPC.  The servers are configured to use IPv6 only and must periodically communicate to the Internet to get security patches for applications installed on them.  Unfortunately, the servers are unable to reach the internet.  An internet gateway has been deployed in the public subnet in the VPC and default routes are configured.  Which of the following could fix the issue?","explanation":"With IPv6 you only requires an Egress-Only Internet Gateway and an IPv6 route to reach the internet from within a VPC.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/configure-private-ipv6-subnet/","title":"Configure Private IPv6 Subnets"}],"answers":[{"id":"5250cb1aeb48afd18fee3003d4798203","text":"Assign IPv6 EIP's to the servers and configure a default route to the existing internet gateway","correct":false},{"id":"024977f9219f0562163fc06724bc8c99","text":"Implement a Classic Load Balancer in front of the servers and register the servers to the load balancer.","correct":false},{"id":"3528a1941badb3d7623a6df18c766d0a","text":"Create an Internet Gateway in the private subnet and configure the default route for the private subnet to the gateway.","correct":false},{"id":"8afd1250611c55c8d3301faba9716c97","text":"Implement a NAT Instance in the public subnet and configure the instance as the default route for the private subnet.","correct":false},{"id":"3bd793f68f596bb23004279fc494f9a6","text":"Implement an Egress-only Gateway in the public subnet and configure an IPv6 default route for the private subnet to the gateway.","correct":true}]},{"id":"cb24982e-2c2d-43d4-872f-2dabbdb7e367","domain":"awscsapro-domain2","question":"You are helping an IT Operations group transition into AWS.  They will be created several instances based off the latest Amazon Linux 2 AMI.  They are unsure of the best way to enable secure connections for all members of the group and certainly do not want to share credentials. Which of the following methods would you recommend?","explanation":"Of the provided options, the only one that upholds AWS best practices for providing secure access to EC2 instances is to use AWS Session Manager.  ","links":[{"url":"https://aws.amazon.com/blogs/aws/new-session-manager/","title":"New – AWS Systems Manager Session Manager for Shell Access to EC2 Instances  | AWS News Blog"}],"answers":[{"id":"c594a26a8a9141cdd5615a0761fb2438","text":"Create a bastion host and use it like a jump-box.  Paste each administrators private key into the known_hosts file on the bastion host.","correct":false},{"id":"e36f501af80c8f03e08321d565cc900e","text":"Share the single private SSH key with each administrator in the group.","correct":false},{"id":"93ecb84b9fc90ec0bc4db84968ab5ebf","text":"Allow administrators to update the SSH key of the instance in the AWS console each time they need access to a system.","correct":false},{"id":"cce617457cd8701c595d236b7fa4ed7c","text":"Allow each administrator to create their own SSH keypair and assign them all to the SSH Key for the instance upon each launch.","correct":false},{"id":"b1ab01927f47a067694506df9d5249e3","text":"Configure IAM role access for AWS Systems Manager Session Manager.","correct":true}]},{"id":"9fc0785a-d5cb-47e3-bc2f-829b5a36ba26","domain":"awscsapro-domain3","question":"You work for a Genomics company which has decided to migrate its DNA Sequencing application to the AWS Cloud. The application is containerized. Currently, container image A works on genomics data residing on an on-premises file server, validating the data and updating the metadata in a local database. When it is done, engineers manually trigger 100 or more instances of container image B that process this data in parallel by reading the metadata, creating output files. When all these container instances have done their job, engineers manually trigger container image C that validates the results, cleans up and sends notifications.\nThe CTO has decided to use S3 for storing the input and output data files. She has also mandated that the parallel processing phase should run on a fleet of Spot EC2 instances to reduce compute costs. She also wants to automate the workflow, so that engineers do not have to manually trigger the next set of actions. The requirement is to minimize administrative overhead and custom development for the migration.\nAs the AWS Architect, which of the following approaches should you recommend?","explanation":"AWS ECS does not natively provide workflow management. In an ECS service definition file, you cannot specify a sequence of tasks with execution dependencies such that one will be run only after the previous one completes. Hence, the two ECS choices are ruled out.\nDistraction warning - Fargate does not allow you to specify Spot instances as it is serverless in nature (it absolves you from specifying server details). This effectively creates a distraction - when the candidate rules out ECS Fargate due to this reason, they may be relieved to see the ECS EC2 choice and jump to a conclusion because it is relatively easy to remember that EC2 launch type actually lets you select Spot instances. However, this distraction is designed to take focus away from the fact that neither of these two choices is correct. Both of the choices require service definition files to set up execution workflows. Task instances mentioned in an ECS service definition file are executed in parallel - ECS does not control the sequence of tasks.\nAWS SWF does not let you specify Spot instances either. Also, SWF is usually used in cases where human intervention is needed in the workflow.\nThis leaves AWS Batch as the correct answer. AWS Batch is indeed the most suitable AWS service for this scenario as it meets all requirements.","links":[{"url":"https://docs.aws.amazon.com/batch/latest/userguide/create-compute-environment.html","title":"How to create a compute environment for AWS Batch"},{"url":"https://docs.aws.amazon.com/batch/latest/userguide/example_array_job.html","title":"Example AWS Batch Array Job Workflow"},{"url":"https://aws.amazon.com/ec2/spot/containers-for-less/get-started/","title":"How to run ECS clusters in EC2 Spot Instances"}],"answers":[{"id":"757ddde350053553e44844d066c91386","text":"Use AWS ECS with Fargate Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"e46ada36d33a9e5b23aa37ee94c4c5d6","text":"Use AWS ECS with EC2 Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"49755d6c34da495b8c91964f52946d29","text":"Use AWS SWF workers and deciders to manage the workflow. Configure the workers to use EC2 Spot Instances","correct":false},{"id":"ceb4c03a526e8ddb01ada7a40bb60001","text":"Use AWS Batch, setting up an array job with 100 or more copies preceded by pre-requisite and follow-up jobs where the workflow is controlled by dependencies between jobs. Also, use Spot as the Provisioning Model for compute environment","correct":true}]},{"id":"115e1b30-23e4-4f3f-9c13-a0086f6af223","domain":"awscsapro-domain2","question":"You are working with a pharmaceutical company on designing a workflow for processing data.  Once a day, a large 2TB dataset is dropped off at a pre-defined file share where the file is processed by a Python script containing some proprietary data aggregation routines.  On average, it takes 20-30 minutes to complete the processing.  At the end, a notification has to be sent to the submitter of the dataset letting them know processing is complete.  Which of the following architectures will work in this scenario?","explanation":"While it may not be the most cost-effective, the EFS option is the only one that can work.  A processing time of 20-30 minutes rules out Lambda (at present with a processing limit of 15 minutes).  If we create an EBS volume with a full OS on it and mount as root for a new instance with the data set included, we still would not be able to dismount the root volume without shutting down the instance.  This would not let us issue an SES SDK call.  The database is also far too large for SQS.","links":[{"url":"https://aws.amazon.com/efs/features/","title":"Amazon Elastic File System (EFS) | Cloud File Storage | Product Features"}],"answers":[{"id":"5f8da9106eba72e74c1a0d6415a235af","text":"Create an S3 bucket to store the incoming dataset.  Once the dataset has been fully received, use S3 Events to launch a Lambda function with the Python script to process the data.  When finished, use an SDK call to SNS to notify when the processing is complete.  Store the processed data back on S3.","correct":false},{"id":"dee7711bae75cac018761467694d89e3","text":"Use SQS to take in the data set.  Use a Step Function to Launch Lambda functions in a fan-out architecture for data processing and then send an SNS message to notify when the processing is complete.  Store the processed data on S3.","correct":false},{"id":"1831d929918e3c425b20e476c1716ccc","text":"Load inbound dataset on an EBS volume.  Stand up an EBS-optimized instance and mount the data volume as the root volume.  Once the data processing is complete, unmount the EBS volume and issue an SDK call to SES to notify of completion.  Configure SES to trigger an instance shutdown after the notification is sent.","correct":false},{"id":"3e1542fcef251cf86c8bdcc89d83aaa7","text":"Stand up memory optimized instances and provision an EFS volume. Pre-load the data on the EFS volume.  Use a User Data script to sync the data from the EFS share to the local instance store.  Use an SDK call to SNS to notify when the processing is complete, sync the processed data back to the EFS volume and shutdown the instance. ","correct":true}]},{"id":"599dee9a-6ae7-4c85-a7c6-49edc6ae7d6b","domain":"awscsapro-domain5","question":"A development team is comprised of 20 different developers working remotely around the globe all in different timezones.  They are currently practicing Continuous Delivery and desperately want to mature to true Continuous Deployment.  Given a very large codebase and distributed nature of the team, enforcing consistent coding standards has become the top priority.  Which of the following would be the most effective to address this problem and get them closer to Continuous Deployment?","explanation":"Including an automated style check prior to the build can move them closer to a fully automated Continuous Deployment process.  A style check only before UI testing is too far in the SDLC.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"1f40591b9d9dbe7a2371e5e82ec05997","text":"Introduce a peer review step into their deployment pipeline during the daily stand-up, requiring sign off for each commit.","correct":false},{"id":"e60e97c4fb6cbf6c1dcf3e806624762f","text":"Require all developers to use the Pair Programming feature of Cloud9.  The commits must be signed by both developers before merging.","correct":false},{"id":"ec61b60c7eeb3bf9ca9c4149c09c5f3d","text":"Issue a department directive that standards must be followed and require the developers to sign the document.","correct":false},{"id":"f4cd7f15eb32d8ddd77234b38d0b35b8","text":"Incorporate a code style check right before user interface testing to ensure standards are being followed.","correct":false},{"id":"db4ecdbd1c7c8fda5d3e0792a15411ab","text":"Include code style check in the build stage of the deployment pipeline using a linting tool.  ","correct":true},{"id":"628453003287afe2200912bb38d0456b","text":"After integrating and load testing, run a code compliance check against the binary created during the build.","correct":false}]},{"id":"edf9ffa9-02ec-4341-a179-577cd590543e","domain":"awscsapro-domain1","question":"You work for a technology product company that owns two AWS Accounts - Prod and DevTest, both belonging to the same Organizational Unit (OU) under AWS Organizations Root. There are three different teams in your company - Dev Team, Testing Team and Ops Team. While Dev and Testing Team members have IAM Users created in the DevTest account, the Ops Team members have IAM Users created in the Prod account. There is an S3 bucket created in the Prod account that Testing Team members need access to - they need both read and write access. What is the best way to give the Testing Team members access to the Prod account S3 bucket?","explanation":"Cross-Account Access is best achieved using IAM Cross-Account Roles. The solution that suggests that the Testing Team members have to sign in to a different AWS account every time they need to access the S3 bucket is not correct as it is inefficient and unproductive.\nThere is no such AWS Organization feature that can directly enable Cross-Account Access from the console. There no way to select or deselect groups or users in this manner. Hence, the option that suggests using these is eliminated.\nThe remaining two options are a play in words. Carefully read both options. The Role must be created in the Trusting Account, in this case Prod Account because it has the Resource (S3 bucket) that needs to be accessed by the someone from another AWS Account, i.e., the Trusted Account.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html","title":"Tutorial - Delegate Access Across AWS Accounts Using IAM Roles"},{"url":"https://aws.amazon.com/organizations/","title":"AWS Organizations"},{"url":"https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/","title":"How to Enable Cross-Account Access to the AWS Management Console"}],"answers":[{"id":"87953f71b8915388e4ecb3f7b2378374","text":"Create an IAM Role in the DevTest Account that allows access to the Prod Account, thus establishing trust between the Accounts. Attach a Trust Policy to the Role that grants the Testing Team members permission to assume the Role. Attach an Access Policy to the Testing Team's IAM Users in the DevTest Account that allows them to call STS AssumeRole for the specific Resource whose value is the ARN of the Role in Prod Account. Attach a Bucket Policy to the S3 Bucket that specifies the IAM Users of Testing Team members as Principal.","correct":false},{"id":"56e4fe89e8a8ffb36bfcd67126afa0d5","text":"Create an IAM Role in the Prod Account that allows the DevTest Account to assume it, thus establishing trust between the Accounts. Attach an Access Policy to the Role that allows access to the S3 bucket. Attach a Bucket Policy to the S3 Bucket that specifies the ARN of the Role as Principal. Attach an Access Policy to the Testing Team's IAM Users in the DevTest Account that allows them to call STS AssumeRole for the specific Resource whose value is the ARN of the Role in Prod Account.","correct":true},{"id":"e661ebc45484fb5a715bc96c884b4024","text":"Enable Cross-Account Access at the AWS Organizational Unit (OU) level from the console. Deselect the Dev Team UAM Users from Cross-Account Access Setup Wizard. This will allow only the Testing Team members to be able to access the Prod account. Write a bucket policy for the S3 bucket that lists the Testing Team members as Principals who are allowed to access the bucket.","correct":false},{"id":"e91ff3381d499ccd06e7891c37a77891","text":"Create IAM Users for the Testing Team members in the Prod account. Create a Testing IAM Group in the Prod account and add the IAM Users of the Testing team members to the Group. Assign an access policy to the Testing Group in the Prod account that grants Read and Write access to the correct S3 bucket. Testing team members will sign into the Prod account to access the S3 bucket.","correct":false}]},{"id":"3d98c88b-ccae-4d7a-804a-b329b8afbdb5","domain":"awscsapro-domain1","question":"You work for a freight truck operating company that operates a website for tracking the realtime location of trucks. The website has a 3-tier architecture with web and application tiers communicating with a PostgreSQL database. Under average load, the website requires 8 web servers and 3 application servers, with average CPU consumption at 85%. Though the experience will suffer, the application can still operate with 6 web servers and 2 application servers; however, in that case, the CPU usage is close to 100%. You are deploying this application in us-west-2 region which has four AZs (Availability Zones). Select the architecture that provisions maximum availability and the ability to withstand the loss of up to two Availability Zones at the same time, being cost-efficient as well.","explanation":"The key to answering this kind of question is using a simple mathematical formula. If the requirement states that the least number of Availability Zones which will still be functioning after a catastrophic loss is X, then the servers I need per AZ is n such that n * X = minimum number of EC2 Instances required by the application.\nLet us apply this formula to the web-tier. The minimum number needed for the application to function is 6. If I am spreading my instances across 4 AZ-s, then X = 2 because I must be able to withstand the loss of two AZ-s. Therefore, n * 2 = 6. Thus, n = 3. Which means I need 3 web servers in each of my AZ-s. Let us reverse calculate with that number to be sure. If I have 3 web servers per AZ, and I have 4 AZ-s, I have 12 web servers running. Now if 2 AZ-s fail, I will still have 6 web servers running. Thus, my application will still perform, which is the requirement.\nThere are a few additional interesting aspects to this kind of problems. First, note that some of the answer choices use 3 AZ-s instead of all 4. This is because the exam wants to penalize inattentive reading, in case you miss that detail. Second, using the above formula sometimes results in multiple correct answers. In that case, we should choose the one with the lower total number of EC2 instances because that will be the lower-cost option. Thirdly, some of the answer choices would be correct if you are trying to make the architecture withstand the failure of 1 AZ (and not 2). This again is trying to penalize inattentive reading in case you are in a hurry, read the question partially, and try to find the correct answer based on 1 AZ going down instead of two, which is the stated requirement.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html","title":"AWS Regions and Availability Zones"},{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html#arch-AutoScalingMultiAZ","title":"Distributing Instances Across Availability Zones"}],"answers":[{"id":"0d286ae78f17320b7244d3c53be500ba","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning 3 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 3 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":false},{"id":"2a80c20c82cfb8e8bd358b60ee1952a5","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning all 4 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 3 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":true},{"id":"3bff2d20d9b822da03cdfc315586449e","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning all 4 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 2 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":false},{"id":"698838baa59961738cfcdc08cbdcd653","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning 3 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 6 EC2 Instances in each Availability Zone for the web-tier. Deploy 2 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS with Read Replicas.","correct":false}]},{"id":"a7c939f1-277e-469f-a209-9b290e8136c9","domain":"awscsapro-domain5","question":"Your company has contracted with a third-party Security Consulting company to perform some risk assessments on existing AWS resources.  As part of a routine list of activities, they inform you that they will be launching a simulated attack on one of your EC2 instances.  After the Security Group performed all their activities, they issue their report.  In their report, they claim that they were successful at taking the EC2 instance offline because it stopped responding soon after the simulated attack began.  However, you're quite certain that machine did not go offline and have the logs prove it.  What might explain the Security company's experience?","explanation":"AWS Shield and other counter-measure technologies work to protect all AWS customers from DDoS attacks.  Unless AWS was aware of the test time and expected duration, its likely the traffic was blocked as suspicious.  AWS Firewall Manager is used to manage WAF ACLs and not dynamically blacklist IPs.  Similarly, VPC Flow Logs cannot automatically implement NACL changes as described here. Despite being a permitted service, traffic suspected of being malicious will still be blocked","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/penetration-testing/","title":"Submit a Penetration Testing Request"}],"answers":[{"id":"82a05cb45adab6d248655e827de16c6f","text":"AWS Firewall Manager is dynamically adding a blacklist entry for the Security Company's testing machine because it sees the traffic as a threat.","correct":false},{"id":"f3f963b71e307f8c28109631df115418","text":"The EC2 instance is using an ENI and the Security Company temporarily exceeded the throughput limit resulting in a throttling of their connection.","correct":false},{"id":"7d4826f179b8dc854c9cfb6e43678373","text":"The VPC Flow Logs record the spike in suspicious traffic and implement an update to the inbound NACL to block the remote IP address.","correct":false},{"id":"e3facacbe52b6423f9cf2e700d8e0b81","text":"The Security Company's traffic was seen as a threat and blocked dynamically by AWS.  AWS must grant permission before any penetration testing is done.","correct":true}]},{"id":"083b20e3-95ff-4b8a-b655-aedf1de67c6c","domain":"awscsapro-domain4","question":"The security monitor team informs you that two EC2 instances are not compliant reported by an AWS Config rule and the team receives SNS notifications. They require you to fix the issues as soon as possible for security concerns. You check that the Config rule uses a custom Lambda function to inspect if EBS volumes are encrypted using a key with imported key material. However, at the moment the EBS volumes in the EC2 instances are not encrypted at all. You know that the EC2 instances are owned by developers but you do not know the details about how the instances are created. What is the best way for you to address the issue?","explanation":"The key must have imported key material according to the AWS Config rule. It should be a new key created in KMS. Existing KMS cannot import a new key material and AWS Managed Key such as aws/ebs cannot be modified either. CloudHSM is more expensive than KMS and is not required in this scenario. Besides, when the new encrypted EBS volume is attached, it should be attached to the same device name such as /dev/xvda1.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html","title":"How to import key material in AWS Key Management Service (AWS KMS)?"}],"answers":[{"id":"03c530623f9019c80c05daa34ad8fab1","text":"Create a new EBS key from CloudHSM with imported key material. Create a new EBS volume encrypted with the new key. Attach the volume to the EC2 instance. Use Linux dd command to copy data from non-encrypted volume to encrypted volume. Unmount the old volume after the sync is complete.","correct":false},{"id":"62f46fb2afeb695bf73a050f1662cd44","text":"Create a Customer Managed Key (CMK) in KMS with imported key material. Create a snapshot of the EBS volume. Copy the snapshot and encrypt the new one with the new CMK. Then create a volume from the snapshot. Detach the original volume and attach the new encrypted EBS to the same device name of the instance.","correct":true},{"id":"5a6b8e38c5d149a42e259d93323f12aa","text":"Modify the AWS Managed Key (AWS/EBS) in KMS to include an imported key material. Create a snapshot of the EBS volume. Then create a new volume from the snapshot with the volume encrypted. Detach the original volume and attach the new encrypted EBS to another device name of the instance.","correct":false},{"id":"94975e581f509af38c350ecdd5b951f5","text":"Import a new key material to an existing Customer Managed Key (CMK) in KMS. Create an AMI from the EC2 instance. Then launch a new EC2 instance from the AMI. Encrypt the EBS volume in the new instance. Terminate the old instance after the new one is in service.","correct":false}]},{"id":"19591d08-60c8-494e-9c39-d69c6c3390f0","domain":"awscsapro-domain4","question":"Your company's AWS migration was not planned out very well across the enterprise.  As as result, different business units created their own accounts and managed their own resources.  Recently, an internal audit of costs show that there may be some room for improvement with regard to how reserved instances are being used throughout the enterprise.  What is the most efficient way to ensure that the reserved instance spend is being best used?","explanation":"The discounts for Reserved Instances can be shared across accounts that are linked with Consolidated Billing but Reserved Instance Sharing must be enabled.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-ri-consolidated-billing/","title":"EC2 Reserved Instance Consolidated Billing"}],"answers":[{"id":"af8967525080f0a545bb6fbafc8a8f3c","text":"Setup Consolidated Billing for a single account and link all the other various accounts in the organization.  Ensure that Reserved Instance Sharing is turned on.  The discounts for specific reserved instances will automatically be applied to the consolidated invoice.","correct":true},{"id":"617beec9b1a6fd3ca1cec7764ddfd9fb","text":"Use AWS Organizations to organize accounts under one organizational unit.  Use AWS Budgets to analyze utilization of reserved instances.  Reassign those RIs to other accounts that are currently using On-Demand instances.","correct":false},{"id":"f58169ec8f86bed7c84e4510ed491661","text":"Load CloudTrail instance usage data into Redshift for analytics.  Use QuickSight to create some utilization reports on existing reserved instances.  Relocate on-demand instances into regions where reserved instances are underutilized.","correct":false},{"id":"34d8e4aeeb67918e8bd27daba0923994","text":"Use Cost Explorer reports to analyze coverage of reserved instances.  Where there are coverage gaps, purchase more reserved instance capacity for that account. Where there is excess, place those on the Reserved Instance Marketplace.","correct":false}]},{"id":"49107f33-5b31-4d7e-a2cb-95f3ce8a2d75","domain":"awscsapro-domain1","question":"Your customer has setup AWS Organizations to help manage a collection of AWS Accounts.  They are running into a problem though and need your help.  They have created accounts for each business unit and applied SCPs to those OUs. However, they notice that root accounts in in those sub-accounts can still change root access keys and disable MFA.  How do you instruct your customer?","explanation":"Service Control Policies can control many aspects but they cannot restrict root account actions of changing root access keys or disabling MFA.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"94a2f948d3d2f9c317a6ebb1f5a24ea5","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the specific sub-accounts.","correct":false},{"id":"194cc2d07b5c378b62b1e090f0aea956","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the entire OU in the root account.","correct":false},{"id":"3d722952f024bcec9174a311c17dcc14","text":"You can not use SCPs to restrict root account activities of changing the root password or managing MFA settings.","correct":true},{"id":"77df34553819fdc2e31fb79762948993","text":"You can establish a trust with the top-level account and use the \"organizations:ServicePrinciple\" condition key to restrict root access at the sub-account level.","correct":false}]},{"id":"071d48ba-80e7-420e-969e-98cb2bcfbaa3","domain":"awscsapro-domain2","question":"Across your industry, there has been a rise in activist hackers launching attacks on companies like yours.  You want to be prepared in case some group turns its attention toward you.  The most common attack, based on forensic work security researchers have done after other attacks, seems to be the TCP Syn Flood attack.  To better protect yourself from that style of attack, what is the least cost measure you can take?","explanation":"AWS Shield Standard is offered to all AWS customers automatically at no charge and will protect against TCP Syn Flood attacks without you having to do anything - this meets the requirements of protecting TCP Syn Flood attacks at the lowest cost possible, as described in the question. A more robust solution which is better aligned to best practice would involve a load balancer in the data path, however as this would provide more functionality than required at a higher cost, is not the correct option for this question.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"c50dd258b1fac18bd9368b07bf0fbc11","text":"Implement AWS Shield Advanced and configure it to generate CloudWatch alarms when malicious activity is detected.","correct":false},{"id":"7d8280921ead2af66cf214b774f94306","text":"Re-architect your landscape to use an application load balancer in front of any public facing services.","correct":false},{"id":"6f87c7b3eda4188070a6635a49710939","text":"Implement AWS WAF and configure filters to block cross-site scripting match conditions.","correct":false},{"id":"82f7ef9f4b3e8e05aef157162915fecf","text":"Subscribe to a Business or Enterprise Support Plan.  Engage AWS DDoS Response Team and arrange for a custom mitigation.","correct":false},{"id":"aec18df10393205d72a60b52cf05bab9","text":"This type of attack is automatically addressed by AWS.  You do not need to take additional action.","correct":true}]},{"id":"374fde7d-232a-4cfe-b5d6-7755d564c6ca","domain":"awscsapro-domain1","question":"You are consulting for a company that has decided to partially migrate some resources to AWS from their two data centers (DC1 and DC2).  Their first order of business is to design a robust, redundant and cost-effective network connection between their data centers and AWS.  They already have redundant links between DC1 and DC2.  Which of the following architectures provides the highest availability at the least cost?","explanation":"A common and cost effective way to provide a redundant link to AWS with Direct Connect is a VPN connection.  In the event that the Direct Connect path fails at DC1, your on-prem router can redirect traffic over the VPN at DC2 via the DC1-DC2 link.  Having dual Direct Connect links is definitely redundant but more expensive than a VPN.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/","title":"Multiple Data Center HA Network Connectivity – AWS Answers"}],"answers":[{"id":"77c9fb7459e1b19f6f655d63556016e8","text":"Configure a Direct Connect connection from DC1 to a Virtual Private Gateway on AWS.  Setup a VPN connection from DC2 to a Virtual Private Gateway on AWS.  Configure a dynamic route across DC1 and DC2 for both paths with a route priority favoring the Direct Connect path to AWS.","correct":true},{"id":"bcac2f1d84c5f367ede3342f0adda492","text":"Ensure that DC1 and DC2 have separate ISPs.  Setup VPN connections from DC1 and DC2 to a Virtual Private Gateway on AWS.  Create static routes at each DC to use the local VPN to AWS.  Use CloudTrail to monitor traffic on the Virtual Private Gateway and trigger a script to update the static route if one of the VPN connections goes down.","correct":false},{"id":"425adb8435a7c170eef3698fa729f5ea","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure a default route in both DC1 and DC2 to route traffic to the local Direct Connect link.","correct":false},{"id":"abd8b38f393b6a0d0b42f68dca5ec24d","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure BGP to dynamically route traffic across the nearest Direct Connect link.","correct":false}]},{"id":"dd8b46c7-d1d5-4326-a092-927b9333fd2a","domain":"awscsapro-domain5","question":"You are helping a company transition their website assets over to AWS.  The project is nearing completion with one major portion left.  They want to be able to direct traffic to specific regional EC2 web servers based on which country the end user is located.  At present, the domain name they use is registered with a third-party registrar.  What can they do?","explanation":"You can use Route 53 if the domain is registered under a third-party registrar.  When using Geolocation routing policies in Route 53, you always want to specify a default option in case the country cannot be identified.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy - Amazon Route 53"}],"answers":[{"id":"381d1621ab4c93aca8cc780f05e98c50","text":"Initiate a domain transfer request with the current registrar.  Once the request goes through, create a public hosted zone in Route 53.  Create SRV records for each regional EC2 instance using a Geolocation routing policy.  Create an alias record for the top-level domain and link that to the SRV records.","correct":false},{"id":"81efc0b140e618c378c9e9bc59dd4ca8","text":"Create a public hosted zone for the domain in Route 53.  Update the DNS entries in the registrars database to use AWS DNS Servers as defined in the NS record on Route 53.  Create A-type records for all EC2 instances. Configure CNAME records for the main FQDN that point to regional A records using a Geolocation routing policy.  Create another CNAME record as a default route.","correct":true},{"id":"294ce2854fbe727bd4f4917543d45bec","text":"Create a private hosted zone for the domain in Route 53.  Update the DNS record entries in the registrars database to use AWS DNS Servers.  Once the DNS changes are fully propagated across the internet and the TTL has expired, convert the private hosted zone to a public hosted zone.  Create A-type records for all the regional EC2 instances and configure a Geo-proximity policy for each record, ensuring the bias across all records sums to 100.","correct":false},{"id":"724dab26998c86ab7ddc7faa063285b8","text":"You cannot use Route 53 routing policies unless AWS is the registrar of record for your domain.  A workaround could be to configure your own top-level DNS server using BIND.  Ensure the NS and SOA records point to this instances.  Create A-type records pointing to the IP addresses of the regional EC2 web servers.  Dynamically redirect requests using customized BIND rules and a third-party IP geolocation database.","correct":false}]},{"id":"230f422f-7118-4096-8dce-59c642fb55c8","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a new Direct Connect connection.  The connection is up and you can ping the AWS peer IP address, but the BGP peering session cannot be established.  What should be your next logical troubleshooting steps?","explanation":"Because the connection is up and we can ping the AWS peer, the problem must be at a higher level on the OSI model than the Physical or Data layers.  BGP uses TCP port 179 to communicate routes so we should check that no NACL or SG is blocking it.  Additionally, we should make sure the ASNs are properly configured in the proper ranges.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/Troubleshooting.html","title":"Troubleshooting AWS Direct Connect - AWS Direct Connect"}],"answers":[{"id":"45d4c1753395277878b9a17343628c52","text":"Ensure that the VLAN is configured properly between your on-prem router the provider. ","correct":false},{"id":"16e5aea88df69cc18f99e3f066ec99c1","text":"Ensure that the local ASNs and AWS-side ASNs are properly configured.","correct":true},{"id":"edd3f9408cecbbf9182678ccc51d7981","text":"Ask your network provider to provide you with a cross connect completion notice and compare the ports with those listed on your LOA-CFA","correct":false},{"id":"8fc27418eee2ce07b64bc672007d2c1b","text":"Contact the co-location provider and request a written report for the Tx/Rx optical signal across the cross connect.","correct":false},{"id":"81977d7a1eb5714746851077b93f44d6","text":"Power cycle all the equipment to clear ARP table cache.","correct":false},{"id":"3d2a55832b90f19a2137e8715525d717","text":"Make sure no firewalls or ACLs are blocking TCP port 179 or any high-numbered ephemeral ports.","correct":true}]},{"id":"08a68d51-48ba-43b7-b0c3-c24e04bb33a8","domain":"awscsapro-domain3","question":"You have just completed the move of a Microsoft SQL Server database over to a Windows Server EC2 instance.  Rather than logging in periodically to check for patches, you want something more proactive.  Which of the following would be the most appropriate for this?","explanation":"The default predefined patch baseline for Windows servers in Patch Manager is AWS-DefaultPatchBaseline.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html","title":"Default and Custom Patch Baselines - AWS Systems Manager"}],"answers":[{"id":"4b5715f588231d7445bb512181ea13a2","text":"Make use of Server Manager and the AWS-LinuxWindowsDefaultPatchBaseline pre-defined baseline","correct":false},{"id":"42c46b2fcd3034cf79b83f0d5dc37f7d","text":"Make use of Patch Manager and the AWS-DefaultPatchBaseline pre-defined baseline","correct":true},{"id":"b40084bb5d5139b353505e5f942afc34","text":"Make use of Patch Manager to apply patches as you have defined in the Patch Groups","correct":false},{"id":"78beead90b3e8deb4bf1ee7e3544a309","text":"Make use of AWS Batch to apply patches as they appear on the RSS feed from Microsoft","correct":false},{"id":"5ee836bac8680ff00d457a8d7f90fad6","text":"Make use of Patch Manager and the AWS-WindowsDefaultPatchBaseline pre-defined baseline","correct":false}]},{"id":"945797fb-5147-44fc-a50c-aaa636c3705b","domain":"awscsapro-domain3","question":"Your organisation currently runs an on-premise Windows file server.  Your manager has requested that you utilise the existing Direct Connect connection into AWS, to provide a method of storing and accessing these files securely in the Cloud.  The method should be simple to configure, appear as a standard file share on the existing servers, use native Windows technology and also have an SLA.  Choose an option which meets these needs.","explanation":"To choose the correct option, we can start by eliminating services which don't have an SLA, in this case only Storage Gateway doesn't have an SLA so we can remove that as an option.  Next we can rule out EFS and S3 as they don't use native Windows technology or provide a standard Windows file share, therefore the only correct answer is to use Amazon FSx for Windows File Server.","links":[{"url":"https://aws.amazon.com/fsx/windows/faqs/","title":"Amazon FSx for Windows File Server FAQs"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs"},{"url":"https://aws.amazon.com/efs/faq/","title":"Amazon EFS FAQs"}],"answers":[{"id":"e953bc59adceffe1274c4bc66d83b365","text":"Create an AWS Storage Gateway for Files server and map the generated SMB share to the Windows file server, then synchronise the files","correct":false},{"id":"5a76242c7735a5846218b183930c3a41","text":"Map an Amazon Elastic File System (EFS) share to the Windows file server and use RoboCopy to copy files across","correct":false},{"id":"437caccc8e39a279a232207ec3ca741a","text":"Map an SMB share to the Windows file server using Amazon FSx for Windows File Server and use RoboCopy to copy the files across","correct":true},{"id":"af082c9581ff0bb5e1c9cc6daf7d72e0","text":"Write a Powershell script which uses the CLI to synchronise the files into an S3 bucket","correct":false}]},{"id":"6a9b273b-601e-4f90-ae40-9b87c2313945","domain":"awscsapro-domain4","question":"You are consulting for a media company that sells archival footage of old Hollywood movies.  Their customer base is largely other production companies who like to include these old clips in modern projects and pay well for the licenses.  Unfortunately, as digitization as increased, their business has decreased since companies can get clips, often illegally, from the internet.  As a result, the company has had to shift significant resources and budget to the Legal department and away from the IT department.  You had been leasing a large SAN for storage of the media but its lease is ending and you need to find a new home for the collection.  The company prides itself on fulfilling customer requests quickly and has a target of 10-15 minutes from the time a customer requests a clip until that customer is able to download that clip.  The company gets about 4-5 requests per month but licensing fees for those requests are usually tens of thousands of dollars.  You have been challenged to come up with the most cost-effective way to store the 20TB of digitized media but still meet customer needs.  Which of the following would you choose? ","explanation":"Due to the relatively infrequent requests and the SLA for retrieving the requests, Glacier and Expedited Retrieval provide the best combination of meeting the requirements and minimizing cost.","links":[{"url":"https://aws.amazon.com/s3/pricing/","title":"Cloud Storage Pricing | S3 Pricing by Region | Amazon Simple Storage Service"}],"answers":[{"id":"354579a5bfb828ebee6304cac6ea1406","text":"Use Storage Gateway to migrate the media files to S3.  Create a Lifecycle policy to move the files to Reduced Redundancy storage after 1 day. Upon a customer request, create a pre-signed URL for the resource for customer downloading.","correct":false},{"id":"da98bfd22729a645118d35c438df8bfd","text":"Use Snowball to transfer the data to AWS S3.  Create a Lifecycle policy that will archive all the media clips older than 1 day to Glacier.  Upon a customer request, initiate an Expedited retrieval of the media file.  Once restored, email the customer a pre-signed URL for downloading.","correct":true},{"id":"29c0c01d440c4ac095577d1f09b935f2","text":"Use Snowball to transfer the data to S3.  Use Elastic Transcoder to compress the media files into a smaller format.  Store the media files in small groups on EFS volumes.  Upon a customer request, mount the EFS volume from a T3.micro spot instance web server and provide the customer with a download URL.","correct":false},{"id":"a0f489ddd5ea014aed1b848b3027c8eb","text":"Use DMS to import the data into S3.  Create a Lifecycle policy to move the data to One-Zone Infrequent Access state.  Upon customer request, create a pre-signed URL with an expiration of 1 day and email to the customer for downloading.","correct":false}]},{"id":"f37f4967-9ef0-4cec-b63f-15b52dc44ca2","domain":"awscsapro-domain2","question":"You are an AWS solutions architect in a company. A team is building up a new application using AWS resources including application load balancers. In order to capture detailed information about the requests to the load balancers, all application load balancers need to activate the access logs and save the log files in an S3 bucket. The access logs should be encrypted when they are stored in the S3 bucket for data protection. How would you enable the encryption for access logs?","explanation":"The encryption at rest should be configured in the S3 bucket rather than the ELB access logs. For access logs of application load balancers, only the server-side encryption with Amazon S3-managed encryption keys (SSE-S3) is supported. Users cannot store the access logs in an S3 bucket where encryption with SSE-KMS is configured. For ELB access logs, you cannot perform the client-side encryption before the files are transferred in the S3 bucket. As a summary, the server-side encryption with SSE-S3 is the correct method.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html","title":"Access Logs for Your Application Load Balancer"}],"answers":[{"id":"6050b8b3764542173c87d57af013f345","text":"Create a Lambda function using Python boto3 to enable access logging for the application load balancers and activate the encryption feature. Configure an S3 bucket policy to allow the \"delivery.logs.amazonaws.com\" service to put objects.","correct":false},{"id":"efeff66ea347b9c1586198badebdd3f1","text":"Generate a symmetric encryption key locally, upload the key to S3 and enable client-side encryption with Amazon S3 by modifying the default encryption in the bucket properties. Make sure the encryption key is stored safely.","correct":false},{"id":"7105831f473423801649e7f1302abac6","text":"In the AWS S3 Management Console, enable server-side encryption with Amazon S3-Managed Keys (SSE-S3) by modifying the default encryption to be AES-256.","correct":true},{"id":"01a3559ab89c1a90f64a2d125b0ca2d4","text":"Create a customer managed key (CMK) in AWS Key Management Service (KMS). Add the \"delivery.logs.amazonaws.com\" service as the key user. Enable server-side encryption with AWS KMS-Managed Keys (SSE-KMS) by selecting the CMK in the S3 bucket.","correct":false}]},{"id":"4c49e888-8f76-4b15-b267-7f6ec35579ca","domain":"awscsapro-domain5","question":"A client has asked you to review their system architecture in advance of a compliance audit.  Their production environment is setup in a single AWS account that can only be accessed through a monitored and audited bastion host. Their EC2 Linux instances currently use AWS-encrypted EBS volumes and the web server instances sit in a private subnet behind an ALB that terminates TLS using a certificate from ACM. All their web servers share a single Security Group, and their application and data layer servers similarly share one Security Group each. Their S3 objects are stored with SSE-S3.  The auditors will require all data to be encrypted at rest and will expect the system to secure against the possibility that TLS certificates might be stolen by would-be spoofers.  How would you help this client pass their audit in a cost effective way? ","explanation":"All the measures they have taken with Certificate Manager, S3 encryption and the EBS volumes meet the audit requirements.  There is no need for LUKS, CloudHSM or client-side encryption.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html","title":"Amazon EBS Encryption - Amazon Elastic Compute Cloud"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"Protecting Data Using Server-Side Encryption with Amazon S3-Managed  Encryption Keys (SSE-S3) - Amazon Simple Storage Service"}],"answers":[{"id":"bdddd09d0832e16504afd5c88136cf7e","text":"Leave the S3 objects alone.","correct":true},{"id":"93113c2b6ca9be67acbd3561eef56481","text":"Reconfigure the EC2 EBS volumes to use LUKS OS-Level encryption.","correct":false},{"id":"ab9ad7bfd57b97954a6f861d872c6137","text":"Continue to use the ACM for the TLS certificate.","correct":true},{"id":"92194f6603feb3f83a46b00bda37de5a","text":"Deploy CloudHSM and migrate the TLS keys to that service.","correct":false},{"id":"d9447af4853ab8736e49349138cac8fb","text":"Make no changes to the EBS volumes.","correct":true},{"id":"0c31fb48e65443ba5bfa312a7dcc117c","text":"Encrypt the S3 objects with OpenPGP locally before re-uploading them to S3.","correct":false}]},{"id":"1239c235-107c-4f5e-8bac-9dc824c00680","domain":"awscsapro-domain5","question":"You are helping a client with some process automation.  They have managed to get their website landscape and deployment process encapsulated in a large CloudFormation template.  They have recently contracted with a third-party service to provide some automated UI testing.  To initiate the test scripts, they need to make a call out to an external REST API.  They would like to integrate this into their existing CloudFormation template but not quite sure of the best way to do that.  Help them decide which of the following ideas is feasible and incurs the least extra cost.","explanation":"To integrate external services into a CloudFormation template, we can use a custom resource.  Lambda makes a very good choice for this scenario because it can handle some logic if needed and make a call out to an external API.  Using an EC2 instances to make this call is excessive and we likely would not have the ability to configure the third-party API to poll an SQS queue.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html","title":"AWS Lambda-backed Custom Resources - AWS CloudFormation"}],"answers":[{"id":"f761e84ee0cd0f689465458a41b69fae","text":"Include an SQS queue definition in the CloudFormation template.  Define a User Script on the deployed EC2 instance which will insert a message into the SQS queue only once it has fully booted.  Configure the external REST API to use long polling to check the queue for new messages in order to initiate the testing process.","correct":false},{"id":"43569df3ec4b7db0265dea4051c04644","text":"Add a small EC2 instance definition to the CloudFormation template.  Define a User Script for that instance which will install a custom application from S3 to call out to the external REST API endpoint using the POST method to trigger the testing process.  Add a CleanUp parameter to the EC2 instance definition that will shut down the instance once the activity has completed.","correct":false},{"id":"97a123d11bbf3be0e3e1788e2f0874ac","text":"Add an API Gateway deployment to the CloudFormation template.  Add the DependsOn parameter to the API Gateway resource to ensure that the call to the external API only happens after all the other resources have been created.  Create a POST method and define it as a proxy for the external REST API endpoint.  Using SWF, call the API Gateway endpoint to trigger the testing process.","correct":false},{"id":"6b3b26e17f2323a91f04f792f0c2d20c","text":"Create a Lambda function which issues a call out to the external REST API using the POST method.  Define a custom resources in the CloudFormation template and associate the Lambda function and execution role with the custom resource.  Include DependsOn to ensure that the function is only called after the other instances are ready.","correct":true}]},{"id":"f02ba751-479b-4ff0-a09b-8f18a63177b5","domain":"awscsapro-domain3","question":"An automotive supply company has decided to migrate their online ordering application to AWS. The application leverages a Model-View-Controller architecture with the user interface handled by a Tomcat server and twenty thousand lines of Java Servlet code. Business logic also resides in two thousand lines of PL/SQL stored procedure code in an Oracle database. The company's technology leadership has directed your team to move the database to a more cost-effective offering, and to adopt a more cloud-native architecture. Business objectives dictate that the application must be live in the AWS cloud in sixty days. Which migration approach will provide the most scalable architecture and meet the schedule objectives?","explanation":"This solution will require trade-offs between schedule requirements and architectural desires. Converting twenty thousand lines of Model-View-Controller code to a serverless architecture in sixty days is unreasonable, so moving the Tomcat MVC as-is to EC2 for the initial migration is the best approach. We can migrate to a serverless user interface in a later phase. Database Migration Service will suit our needs well for moving the application data to Aurora, but the most scalable architecture strategy is to migrate the stored procedure code out of the database so that database nodes won't need to be resized when the business logic needs more compute resources. Under normal circumstances, recoding two thousand lines of PL/SQL code to Python Lambda functions within a sixty day time frame will not be a problem.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/blogs/database/migrate-your-procedural-sql-code-with-the-aws-schema-conversion-tool/","title":"Migrate Your Procedural SQL Code with the AWS Schema Conversion Tool"},{"url":"https://aws.amazon.com/lambda/","title":"AWS Lambda"}],"answers":[{"id":"2cf84f7f8daee7548143ad181423c7cb","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"97a348ed01424c357958b49bcc030935","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"db222d8a15bda541fc4147908131cfd6","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service to move the application data into Amazon Aurora. Convert the stored procedure code to AWS Lambda Python functions, and modify the Servlet code to invoke them","correct":true},{"id":"5ab40b2a54c4e82a7aafa05c8fc9a458","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service to migrate the application data and stored procedures to an Amazon RDS Oracle instance","correct":false}]},{"id":"8765bd56-057b-488c-9a0a-f5bd413dd240","domain":"awscsapro-domain5","question":"Due to new corporate policies on data security, you are now required to use encryption at rest for all data.  You have some EC2 Linux instances on AWS that were created without encryption for the root EBS volume.  What can you do that meet the requirement and reduce administrative overhead?","explanation":"AWS does support encrypted root volumes but conversion from unencrypted root to an encrypted root requires a bit of a process. You must first create an AMI then copy that newly created AMI to the same region, specifying that you want to encrypt the EBS volumes during the copy.  You can then create a new instance with an encrypted root volume from the copied AMI.  You can use either a generated key from KMS or your own CMK imported into KMS.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html","title":"AMIs with Encrypted Snapshots - Amazon Elastic Compute Cloud"},{"url":"https://aws.amazon.com/blogs/aws/new-encrypted-ebs-boot-volumes/","title":"New – Encrypted EBS Boot Volumes | AWS News Blog"}],"answers":[{"id":"4430b7492a6c058c3574ce4e8ea43955","text":"Stop the instances and temporarily detach the EBS volumes.  Attach the root volumes to another EC2 instance and mount them a data volume.  Use a encryption tool like GPG or OpenPGP to recursively encrypt all the files on the mounted root volumes.  Detach and reattach the encrypted EBS volumes to the original instances and restart.  Import the encryption keys in KMS as a CMK.","correct":false},{"id":"50c17b27f0bd0390ae321943f7db5c3d","text":"Create a certificate in CMS for the encryption key.  Stop the instances and temporarily detach the root volumes.  Via the AWS CLI, enable encryption on the root volumes using the \"ebs modify-volume\" argument with the flag of \"encryption=<CMS ARN>\" to specify the certificate.","correct":false},{"id":"9fbbb2e71386cb7f7a7ed77129a1a960","text":"Create an encrypted EFS instance and mount-points in the respective subnets.  Log into the instance and mount an encrypted EFS mount-point.  Copy all the root files over to the EFS mount point.  Edit the FSTAB file to mount the EFS mount point as the root volume instead of the root EBS device and reboot.","correct":false},{"id":"1e30ccf0b75e9e70fd76c6e041510c75","text":"At present, EC2 does not support encrypted root volumes.  Create new encrypted EBS data volumes and attach the new volumes to the existing instances.  Use RSYNC to migrate all the non-OS data over to the encrypted data volumes.","correct":false},{"id":"180e9aecdb74f204b1df00ffe6fa8b56","text":"Stop the instances and create AMIs from the instances.  Copy the AMIs to the same region and select the \"Encrypt target EBS snapshots\".  Redeploy the instances using the AMI copies you made with encrypted root volumes.","correct":true}]},{"id":"1520156f-0918-4ab4-a759-ce33a931c744","domain":"awscsapro-domain5","question":"Your company has an online shopping web application. It has adopted a microservices architecture approach and a standard SQS queue is used to receive the orders placed by the customers. A Lambda function sends orders to the queue and another Lambda function fetches messages from the queue and processes them. On some occasions the message in the queue cannot be handled properly. For example, when an order has a deleted production ID, the message cannot be consumed successfully and is returned to the queue. The problematic messages in the queue keep growing and the ability to process normal messages is affected. You need a mechanism to handle the message failure and isolate error messages for further analysis. Which method would you choose?","explanation":"It is not a good idea to adjust the retention period or simply delete the messages that fail to be processed as the question asks for a mechanism to isolate the messages for further troubleshooting. A redrive policy should be used to auto-forward error message to a dead letter queue. Then you can analyze the contents of messages to diagnose the producer’s or consumer’s issues. One thing to note is that a standard queue can only have another standard queue as the dead letter queue. Therefore a FIFO dead letter queue is incorrect as this scenario uses a standard SQS queue and requires a standard dead letter queue.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","title":"Amazon SQS dead-letter queues"}],"answers":[{"id":"3c8453a6c57faf61e761f771bab6f1af","text":"Create a standard queue as the dead letter queue and configure a redrive policy to put error messages to the dead letter queue. Analyze the contents of messages in the dead letter queue to diagnose the issues.","correct":true},{"id":"64fd54fe78b534f0eac9222a6e32747d","text":"Create a FIFO (First-In-First-Out) queue as the dead letter queue and use a redrive policy to forward problematic messages to this new queue. Create a Lambda function to read the message contents in the FIFO queue for further analysis.","correct":false},{"id":"a7eb53a7df09334677590165f666c58f","text":"Modify the error handling logic of the Lambda function to delete the messages whenever the processing is unsuccessful with an error or exception. The error messages do not return to the queue and the normal message handling is not blocked.","correct":false},{"id":"f7b3898cfcb4851b120c9b14d044ab90","text":"Decrease the message retention period of the queue to 1 day. When the messages are not processed properly and put back in the queue, they can be quickly deleted when the retention period expires.","correct":false}]},{"id":"65288d1e-af34-43b4-9be7-0c1696c649fc","domain":"awscsapro-domain2","question":"What backup and restore options are available to you when using RDS for Oracle?","explanation":"Amazon RDS for Oracle can use the standard backup methods for RDS which is Snapshot and Point In Time Recovery.  You can also use Data Pump to export logical data to binary files, which you can later import into the database as well as the standard 'exp' and 'imp' utilities.  RMAN is not supported in RDS as a backup mechanism, although you can run certain RMAN commands against the database using the rdsadmin.rdsadmin_rman_util package.  Replication Backups is not a valid function within RDS for Oracle.","links":[{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"Amazon RDS for Oracle FAQs"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Oracle.html","title":"Oracle on Amazon RDS"},{"url":"https://aws.amazon.com/rds/oracle/features/","title":"Amazon RDS for Oracle features"}],"answers":[{"id":"4cff81b1f97c64130a64b1ea818c7f24","text":"Oracle Export/Import Utilities","correct":true},{"id":"d93f3c575a1b1947fc462dc84cd425bc","text":"Replication Backups","correct":false},{"id":"bf7eb3e57b2ec9b7b52b41ebe6c155ce","text":"RDS Snapshot and Point In Time Recovery","correct":true},{"id":"5cd4a59bc9ce2cd88904b1edee24be49","text":"Oracle Recovery Manager (RMAN)","correct":false},{"id":"d7b065bec7ec0dbd1d1985543b28aec5","text":"Oracle Data Pump Export and Import","correct":true}]},{"id":"b00cb57f-7191-4f17-aa6d-ac687c418332","domain":"awscsapro-domain5","question":"You have a running EC2 instance and the name of its SSH key pair is \"adminKey\". The SSH private key file was accidentally put into a GitHub public repository by a junior developer and may get leaked. After you find this security issue, you immediately remove the file from the repository and also delete the SSH key pair in AWS EC2 Management Console. Which actions do you still need to do to prevent the running EC2 instance from unexpected SSH access?","explanation":"Although the SSH key pair is deleted in EC2, the public key content is still placed on the instance in an entry within ~/.ssh/authorized_keys. Someone can SSH to the instance if he has a copy of the leaked SSH private key. Users should not configure the instance to support another key pair as the old key pair still works. The correct method is deleting the instance immediately to prevent it from being compromised and launching another instance with a new SSH key pair. There is no need to use the AWS CLI command delete-key-pair as the key is already deleted from AWS EC2.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html","title":"Amazon EC2 Key Pairs"}],"answers":[{"id":"a928b9732684f81d9ce046842965f1f6","text":"No action is required as the SSH key pair \"adminKey\" is already deleted from AWS EC2. Even if someone has the SSH private key, he still cannot use the key to access the instance.","correct":false},{"id":"ee7464becbe88068a5f848419e621bba","text":"Stop and terminate the instance immediately as someone can still SSH to the instance using the key. Launch a new instance with another SSH key pair. SSH to the EC2 instance using the new key.","correct":true},{"id":"bc01758fc758191425928382b18697ca","text":"Create another SSH key pair via AWS EC2 or a third party tool such as ssh-keygen. Stop the instance and configure the instance with this new key pair in AWS Management Console. Restart the instance to activate the key pair.","correct":false},{"id":"4f9620d4caefa25fafabc98f00c6b192","text":"Use AWS CLI delete-key-pair to completely delete the key pair so that no one can use it to SSH to the instance. Configure CloudWatch logs to monitor the SSH logging events and filter the logs with the SSH key ID to see if the key pair is still used by someone.","correct":false}]},{"id":"872cd65b-287b-4fdb-b59f-f07f7bff707f","domain":"awscsapro-domain5","question":"Your client is a small engineering firm which has decided to migrate their engineering CAD files to the cloud.  They currently have an on-prem SAN with 30TB of CAD files and growing at about 1TB a month as they take on new projects.  Their engineering workstations are Windows-based and mount the SAN via SMB shares.  Propose a design solution that will make the best use of AWS services, be easy to manage and reduce costs where possible. ","explanation":"At present, EFS doesn't support Windows-based clients.  Storage Gateway-File Gateway does support SMB mount points.  The other options introduce additional unneeded costs.","links":[{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/limits.html","title":"Amazon EFS Limits - Amazon Elastic File System"}],"answers":[{"id":"7cad520a624f4c3b174014f339f732df","text":"Use AWS CLI to sync the CAD files to S3.  Use EC2 and EBS to create an SMB file server.  Configure the CAD workstations to mount the EC2 instances.  Setup Direct Connect to ensure performance is acceptable.","correct":false},{"id":"f52e177dc6a594ed2c0852c91b6133d3","text":"Order a Snowmobile to migrate the bulk of the data.  Setup S3 buckets on AWS to store the data.  Use AWS WorkDocs to mount the S3 buckets from the engineering workstations.","correct":false},{"id":"340da8cc24330ac5b143b526c880e4f7","text":"Order a Snowball appliance to migrate the bulk of the data.  Setup an EFS share on AWS and configure the CAD workstations to mount via SMB.  ","correct":false},{"id":"9c4821e0d9178e80636a5d4c7d0c6441","text":"Setup Storage Gateway-File Gateway and configure the CAD workstations to mount as iSCSI.  Use a Snowball appliance to sync data daily to S3 buckets at AWS.","correct":false},{"id":"c15496feac5aa7ce58d0c5d4813a5a29","text":"Use AWS CLI to sync the CAD files to S3.  Setup Storage Gateway-File Gateway locally and configure the CAD workstations to mount as SMB.","correct":true}]},{"id":"c29d0343-9d60-4882-b1f3-2897ef7e889a","domain":"awscsapro-domain2","question":"Your team starts using Docker to manage a web application. The Docker image is pushed to AWS ECR and the application is hosted in Amazon Elastic Container Service (Amazon ECS) containers. After the application is deployed, you find that there are occasions where the application is attacked by SQL injection or cross-site scripting. You would like to set up certain rules to protect the web application from the common application-layer exploits so that specific traffic patterns that you define are filtered out. How would you implement this?","explanation":"AWS WAF is a web application firewall that can protect an Amazon CloudFront distribution, an Amazon API Gateway API, or an Application Load Balancer. It cannot associate with an ECS cluster. However you can configure the Amazon ECS to use an Application Load Balancer and then associate the WAF ACL with the ELB. CloudFront distribution can not use the ECS cluster as the origin. AWS Shield Advanced is only for DDoS attacks and is not suitable.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html","title":"Use AWS WAF to protect applications that are hosted in Amazon Elastic Container"}],"answers":[{"id":"d804d87fdada57fe73d5fb20b76721f4","text":"Configure rules to protect the application from common attacks including SQL injection and cross-site scripting in AWS Firewall Manager. Use an Application Load Balancer to distribute traffic to the ECS cluster. In AWS Firewall Manager, apply the rules in the Application Load Balancer.","correct":false},{"id":"51f6c122390f4f18e845faf465045044","text":"Set up access control lists (ACLs), rules, and conditions in AWS WAF to define acceptable or unacceptable traffic. Configure Amazon ECS to use an Application Load Balancer to distribute the traffic. Use WAF to protect the application behind the Application Load balancer.","correct":true},{"id":"9ee0fc90381d7cb22768fda1080073af","text":"Configure an ACL in AWS WAF and the ACL contains rules to block common attack patterns, such as SQL injection or cross-site scripting. Setup a CloudFront distribution to use the ECS cluster as the origin. Associate the WAF ACL with the CloudFront distribution.","correct":false},{"id":"7db2f0b98cf72e4dcc0d93100a6230cd","text":"Enable AWS Shield Advanced to protect the application from common attacks such as SQL injection, DDoS and cross-site scripting. In AWS Firewall Manager, associate AWS Shield with the ECS cluster name so that the ECS cluster is monitored by AWS Shield and the traffic is filtered based on the created rules.","correct":false}]},{"id":"d8bfc54e-024a-4fbb-9daa-9a218a10b738","domain":"awscsapro-domain5","question":"Your company has an Inventory Control database running on Amazon Aurora deployed as a single Writer role. Over the years more departments have started querying the database and you have scaled up when necessary.  Now the Aurora instance cannot be scaled vertically any longer, but demand is still growing.  The traffic is 90% Read based.  Choose an option from below which would meet the needs of the company in the future.","explanation":"This question is about scaling, and if you have scaled up to the maximum level (db.r4.16xlarge) the next step is to consider scaling out.  In this case the application is Read heavy, which lends itself perfectly to adding extra Read replicas and using Read-Write splitting to help future growth.  Changing the max_connections value or using Query plan optimisation may make performance more efficient, but they are not long term solutions. Adding Multi-AZ simply adds High Availability.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html","title":"Managing Performance and Scaling for Aurora DB Clusters"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-replicas-adding.html","title":"Adding Aurora Replicas to a DB Cluster"}],"answers":[{"id":"ce644fa65dde55c85ae051c5081ba860","text":"Use Query plan management to allow the optimizer to choose the most efficient plan for each job and make transactions quicker","correct":false},{"id":"d1fe9035612273c2d0c6b9f4229e1d3d","text":"Convert Aurora to a Multi-AZ Deployment in three or more zones","correct":false},{"id":"15cd367246a8991991de371856908334","text":"Create multiple additional Readers within the Aurora cluster and alter the application to make use of Read-Write splitting","correct":true},{"id":"c6229282cfe5ed1024d22e629709b94d","text":"Increase the maximum number of connections into the database by changing the max_connections parameter","correct":false}]},{"id":"c1333471-d052-4710-bcdb-facadc095d70","domain":"awscsapro-domain5","question":"You are setting up a corporate newswire service for a global news company.  The service consists of a REST API deployed on EC2 instances where customers can retrieve the latest news articles in real-time that happen to contain their company name.  This allows companies to monitor all news sources for stories where they are mentioned.  Because of the worldwide reach of the new site, you want to position servers around the globe.  You want to publish one subdomain name globally (api.domain.com) and have the requesters directed to the nearest region based on latency.  In each region, you want to be able to accommodate blue-green deployments without downtime as well.  What steps do you take?","explanation":"We want to use weighted routing records for local instances so we have the ability to adjust weights and shift traffic during blue-green deployments.  Latency-based routing would take care of funneling requests to the site with the lowest latency.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html","title":"How Health Checks Work in Complex Amazon Route 53 Configurations - Amazon  Route 53"}],"answers":[{"id":"a60aaaf971cbd546c1ec57e08ea38274","text":"We would first create geo-spatial records for the local resources in each region (us-east-2.api.domain.com) and assign equal weights.  Next, we create latency-based routing records for the top level subdomain (api.domain.com) and direct those to the regional records as an alias.  We must also disable Health Check on the latency record to ensure the localized Health Check is used.","correct":false},{"id":"b850f1c18972d022213271c5d673e07f","text":"First setup weighted routing records for the local instances in the region in Route 53.  Assign equal weights with all sharing the same regional subdomain name (us-east-2.api.domain.com).  Next, create latency alias records by creating multiple entries for api.domain.com--each pointing to the regional subdomains.","correct":true},{"id":"41e30b8e30cbd737ced85953d7e3e939","text":"Use CloudFormation to create a distribution of the website.  Create an alias record for the subdomain (api.domain.com) in Route 53 and assign it to the CloudFront distribution.  To ensure no lag in news retrieval, set the maximum TTL on the CloudFront distribution to 0.","correct":false},{"id":"6e56101de6ed2ca97550d5025ddf559a","text":"Using Route 53, we first create the top-level api.domain.com with a geolocation policy.  We then create latency-based routing records for the instances in each region (us-east-2.api.domain.com).  Next, we configure the countries closest to each region in the geolocation policy to direct them to the regional records.","correct":false}]},{"id":"663fbd6a-87bd-4fa6-a0ea-428ba2de5b51","domain":"awscsapro-domain5","question":"You manage a relatively complex landscape across multiple AZs.  You notice that the incoming requests vary mostly depending on the time of day but also there is a more unpredictable component resulting in smaller spikes and valleys for your resources.  Fortunately, you manage this landscape via OpsWorks Stacks.  What options, if any, are available to you as part of the OpsWorks featureset.","explanation":"OpsWorks Stacks offers three types of scaling: 24/7 for instances that remain on all the time; time-based for instances that can be scheduled for a certain time of day and on certain days of the week; and load-based scaling which will add instances based on metrics.  All this can be configured from within the OpsWorks Stack console.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/best-practices-autoscale.html","title":"Best Practices: Optimizing the Number of Application Servers - AWS OpsWorks"}],"answers":[{"id":"3622d494dceb973760a46dea038d1dc2","text":"If you need the ability to dynamically scale, you will need to use OpsWorks for Chef Automate.  OpsWorks Stacks does not support scaling.","correct":false},{"id":"216c997091da6e24174ad1b83d0be8b9","text":"You would define a baseline level of resources within the OpsWorks Stack Console to cover the average load.  But for the periodic load, that requires a scheduled auto-scaling policy.  Similarly, for the volatile spikes, you must use a stepped auto-scaling policy defined in an auto scaling group. ","correct":false},{"id":"75ab4de4ea42c1971b0ee09ae04ca591","text":"You would define a baseline level of resources and configure them for 24/7 instances.  Then you could define a time-based instances to cover certain times of day.  Finally, you could cover the volatile spikes with a load-based instances.  All this can be done within OpsWorks Stacks.","correct":true},{"id":"b7ff5b06f51facca179494cb2bb00e55","text":"You can enabled CloudFormation Anticipated Scaling that uses past CloudWatch metrics and machine learning to automatically design a scaling policy optimized for the incoming request patterns.","correct":false}]},{"id":"5e92e555-6e42-4128-ae19-302b81d9fe84","domain":"awscsapro-domain2","question":"A university wants to create a regional website on AWS for end-users to download past research papers from. Most of the site will be static, as it will display metadata about the whitepapers like author, topic, date, excerpts and price. It will not show any link to download the full whitepaper. There will, however, be a payment link for each whitepaper. If users click on the payment link, the Javascript browser-side code will connect directly to a 3rd-party payment processing service which will return success on completion of payment. On success, the Javascript code will make a REST call to your website back-end to fetch a pre-signed S3 URL for that whitepaper and show a new page with the download link.\nThe pre-signed download URL must be a https URL, as you must protect the whitepaper authors from their valuable IP from being pirated using man-in-the-middle attacks while being downloaded. The website itself may be served over HTTP. None of the whitepapers must be publicly accessible without payment, though the website will not deal with any authentication or user profiles.\nIdentify the most suitable solution that will strike a good balance between quickness and convenience, cost and security for building this website.","explanation":"Admittedly verbose and lengthy, this type of question will test the time-management ability along with technical knowledge. It is clearly stated in the question that the website itself does not need to be served over HTTPS, only the pre-signed download URL (which will be used to download the full whitepaper after successful payment) needs to be HTTPS. Therefore, the choice that eliminates S3 static website citing the reason that S3 static websites do not support HTTPS is incorrect. While it is actually correct that S3 static websites do not support HTTPS, the reason for not choosing S3 static websites as a solution cannot be this fact, as the requirement clearly states that the website may be served over HTTP. Only the part that needs HTTPS can be developed outside of S3 static website, as it must be, as S3 static websites are, well, static, and would not be of much help running server-side code to generate pre-signed URLs. Also, the same choice uses EC2 instances and ELB - an approach that is not as quick or convenient as using a S3 static website.\nThe choice that uses Cloudfront and Origin Access Identity (OAI) is incorrect as we cannot use OAI if the origin is an S3 static website endpoint. OAI can only be used if the origin is an S3 bucket (and not a website). Also, using Cloudfront will unnecessarily increase the cost, as the requirement states that the website is regional.\nBoth the remaining choices are actually working solutions. Both will work. However, the best answer is the one that uses S3 static website, as it is the least amount of effort. Using API Gateway with Lambda proxy integration to serve the entire website is more work compared to using an S3 static website. Even when using S3 static website, API Gateway and Lambda needs to be used to generate the pre-signed URLs after the end-user pays for the whitepapers, but that is a small part compared to the full website.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/","title":"Various ways to chain Cloudfront and S3 for hosting websites"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html","title":"Restricting Access to Amazon S3 Content by Using an Origin Access Identity"}],"answers":[{"id":"06ec95a0c14f828594dd852570049d26","text":"Use S3 static website hosting to host the website, including the metadata, whitepapers and Javascript files, where the client-side Javascript code will read the metadata stored as JSON and parse it to render in a tabular format. If an end-user makes a payment, have the client-side Javascript call an API Gateway over https to retrieve the pre-signed URL. Deploy a Lambda function as the API Gateway backend. Generate the pre-signed URL in the Lambda function using the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the S3 Static website endpoint","correct":true},{"id":"1ffc7a9d39eeaa99af079745d4ed6ae0","text":"S3 static website cannot be used at all as they do not support https. Use S3 to host the metadata and whitepapers. Use a couple of EC2 instances in different Availability Zones to run the website, with an ELB distributing traffic to them. The website components deployed on the EC2 instances will handle both serving the website (by reading the S3 bucket using an EC2 Role and parsing the JSON metadata to generate HTML) and the subsequent request for the full whitepaper. While responding to the request for pre-signed URL, it will use the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the ELB DNS Name","correct":false},{"id":"01d44a44c543778d9db261fe16c999bd","text":"Serve the entire website using API Gateway with proxy Lambda integration. When the base website is requested, the Lambda function will read an S3 bucket that hosts the metadata and whitepapers. It will parse the JSON metadata and generate HTML for listing the whitepaper metadata in a tabular format. If an end-user makes a payment, have the client-side Javascript call a different method on the API Gateway to retrieve the pre-signed URL. Generate the pre-signed URL in a second Lambda function using the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the API Gateway URL","correct":false},{"id":"5bfca7bbb3e2e03a395345f5959db09b","text":"Use S3 static website hosting to host the website, including the metadata, whitepapers and Javascript files, where the client-side Javascript code will read the metadata stored as JSON and parse it to render in a tabular format. Deploy a Cloudfront web distribution to enable https between end-users and Cloudfront, using the static website endpoint as the origin. Use Origin Access Identity to restrict access to the S3 content only to the Cloudfront web distribution. If an end-user makes a payment, have the client-side Javascript call an API Gateway over https to retrieve the pre-signed URL. Deploy a Lambda function as the API Gateway backend. Generate the pre-signed URL in the Lambda function using the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the Cloudfront endpoint","correct":false}]},{"id":"722221be-beb9-4a2b-8ae1-dff52b80125c","domain":"awscsapro-domain3","question":"You work for a technology company with two leased data centres (one on the east coast and one on the west coast) and one owned on-premises data centre. Management has decided to move the two leased data centres to the AWS cloud - one to us-east-1 and the other to us-west-1. The on-premises data centre will still continue running workloads which are not ready to move to the cloud.\nThis on-premises data centre must be always connected to the VPC-s in us-east-1 and us-west-1 for (a) the continuous replication of several databases and (b) the need to access some data residing on the on-premises data centre from applications running in both the AWS regions. The peak bandwidth required for these connections is (a) 500 Mbps between us-east-1 and on-premises, and (b) 8 Gbps between us-west-1 and on-premises. The applications would still be able to function at lower bandwidth, but the experience will be poor, which is not desirable. Both these connections must be Highly Available with 99.999% uptime. The connectivity solution must be cost-effective as well.\nAs the AWS Architect, what connectivity solution would you propose, so that all Bandwidth, HA and cost-effectiveness requirements are met?","explanation":"We can eliminate the VPC Peering solution immediately, as VPC Peering is for connecting two VPC-s on AWS. VPC Peering cannot be used to connect an AWS VPC with an on-premises network.\nOut of the remaining choices, the one that proposes connecting to us-east-1 using VPN and us-west-1 using Direct Connect comes very close to fulfilling all requirements. It suffers from two problems, however. One - the peak bandwidth requirement for us-east-1 is 500 Mbps. A VPN connection cannot be expected to provide 500 Mbps most of the time, as the true bandwidth someone can get from a VPN connection depends on a lot of factors including internet traffic it is sharing the route with. Secondly, if we are paying for a Direct Connect connection for the other region anyway, why not just use that one for this region too? Now, there is something called Direct Connect Gateways that makes it possible to share multiple AWS Regions using the same Direct Connect connection. The knowledge of Direct Connect Gateways is important for the AWS SA-P exam. Hence, this question tests this knowledge. The correct answer is the only one that uses Direct Connect Gateway.\nThe other choice that uses two separate Direct COnnect connections (one for each region) is not cost-effective, especially because since 2017, Direct Connect Gateways make it possible to connect to multiple AWS Regions using the same Direct Connect connection.\nRegarding HA, it is always a good practice to set up a VPN connection as a back-up for Direct Connect. The only requirement to do this is that the back-up VPN connection must also use the same Virtual Private Gateway on the AWS VPC side, otherwise traffic cannot fail over easily.\nNote about Direct Connect Gateways - they not only allow a customer to connect to two AWS Regions using a single Direct Connect connection, they also let the connected Regions communicate with each other! (This is why the VPC CIDR-s in us-east-1 and us-west-1 in the correct answer have to be non-overlapping.) There may be questions testing this aspect as well. Before Direct Connect Gateways existed, VPC Peering would be the only way for Inter-Region VPC Access. There is also another solution now - Transit Gateway, but this was announced late 2018. Usually, topics do not start appearing on the exam unless they have been more than 6 months in GA. Expect Transit Gateways to start appearing in questions now as well!","links":[{"url":"https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/","title":"AWS Direct Connect Gateway for Inter-Region VPC Access"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/","title":"Configuring VPN as back up of Direct Connect"},{"url":"https://aws.amazon.com/directconnect/sla/","title":"Direct Connect SLA puts uptime target at 99.9%. Therefore, if we need more than that, we should set up VPN as back up"}],"answers":[{"id":"3ea4a9645ac22c0b2c6c427764a7ae01","text":"Set up a Direct Connect Gateway. Associate the Virtual Private Gateways from both the us-east-1 and us-west-1 VPC-s with this Direct Connect Gateway. Then set up a single 10 Gbps Direct Connect connection between the on-premises data centre and the Direct Connect Gateway, using a Private Virtual Interface. Ensure that the VPC CIDR-s in the two AWS Regions are non-overlapping. To increase HA, set up separate back-up VPN connections between the on-premises data centre and each of the two AWS Regions","correct":true},{"id":"f0671c30ae58d58c0d64162640e69527","text":"Use two Direct Connect connections - an 1 Gbps one between the on-premises data centre and us-east-1, and a 10 Gbps one between the on-premises data centre and us-west-1. For each Direct Connect connection, set up a back-up VPN connection that must use the same Virtual Private Gateway as the Direct Connect circuit","correct":false},{"id":"608dc6c035921ef09c640ebb70de9ddd","text":"Use VPC Peering to connect the on-premises network with both the us-east-1 and us-west-1 VPC-s independently. Bandwidth provided by VPC Peering is virtually unlimited, limited only by the instance sizes used. Also, VPC peering connections are fault-tolerant and scalable, so no back-up connectivity is needed","correct":false},{"id":"85b6b49a01a28a90d71ef3c20ca9da8d","text":"Connect the on-premises data centre and us-east-1 using redundant site-to-site VPN connections as its bandwidth requirements do not require a costly Direct Connect connection. The redundant VPN connections must use different customer gateways and will provide an HA solution for that region. Connect the on-premises data centre with us-west-1 using a 10 Gbps Direct Connect circuit. Set up a back-up VPN connection for this region such that it uses the same Virtual Private Gateway as the Direct Connect circuit","correct":false}]},{"id":"4e6b1423-41e3-4d39-93b1-c5c47705477b","domain":"awscsapro-domain2","question":"Given an IP CIDR block of 56.23.0.0/24 assigned to a VPC and the single subnet within that VPC for that whole range, how many usable addresses will you have?","explanation":"For VPCs and subnets, you can use IP addresses that are in RFC1918 or not.  If you choose addresses not in the RFC1918 ranges, you will not be able to route traffic to the internet directly with those addresses.  You would have to use a NAT.  For a /24 netmask, you can expect 251 usable addresses because of the 5 reserved addresses.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html","title":"IP Addressing in Your VPC - Amazon Virtual Private Cloud"}],"answers":[{"id":"c52f1bd66cc19d05628bd8bf27af3ad6","text":"254","correct":false},{"id":"3873a2be62d17c29ac441293ab21e143","text":"You cannot assign the entire CIDR range of a VPC to a single subnet.","correct":false},{"id":"19f3cd308f1455b3fa09a282e0d496f4","text":"251","correct":true},{"id":"6aa49d907a3637314f53838d83286e5d","text":"Zero.  You must use a private IP range as defined in RFC1918 for VPCs.","correct":false},{"id":"f7efa4f864ae9b88d43527f4b14f750f","text":"4096","correct":false}]},{"id":"3f7fa126-1155-4aa3-802d-e9eeb75f5e5a","domain":"awscsapro-domain3","question":"You work for a Clothing Retailer and have just been informed the company is planning a huge promotional sale in the coming weeks.  You are very concerned about the performance of your eCommerce site because you have reached capacity in your data center.  Just normal day-to-day traffic pushes your web servers to their limit.  Even your on-prem load balancer is maxed out, mostly because that's where you terminate SSL and use sticky sessions.  You have evaluated various options including buying new hardware but there just isn't enough time.  Your company is a current AWS customer with a nice large Direct Connect pipe between your data center and AWS.  You already use Route 53 to manage your public domains.  You currently use VMware to run your on-prem web servers and sadly, the decision was made long ago to move the eCommerce site over to AWS last.  Your eCommerce site can scale easily by just adding VMs, but you just don't have the capacity.  Given this scenario, what is the best choice that would leverage as much of your current infrastructure as possible but also allow the landscape to scale in a cost-effective manner?","explanation":"A Target Group for an ALB can contain instances or IP addresses.  In this case, we can define the private IP addresses of our on-prem web servers along side the private IP addresses of any EC2 instances we spin up.  The caveat is that we can only use private IP addresses when defining a target group in this way.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancing-via-ip-address-to-aws-on-premises-resources/","title":"New – Application Load Balancing via IP Address to AWS & On-Premises  Resources | AWS News Blog"}],"answers":[{"id":"4df24111c113846bfe0505ad0c84d9a3","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define two target groups:  one containing the public IP addresses of your on-prem load balancer and one including an auto scaling group of additional EC2 instances created from the imported AMI.  Assign both target groups to the ALB using the same listener port.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"6d3db4c52e96931f925f17fe8e9fd50f","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using public IP addresses of your on-prem web servers and additional EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"3e47f65e4524f53faba23e6995b592f5","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":true},{"id":"77592781918fa63474b5efbd5cc9555f","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an NLB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the NLB as an alias record.","correct":false}]},{"id":"951e49a8-9395-4b49-b623-1fb9e88a9639","domain":"awscsapro-domain5","question":"Your production web farm consists of a minimum of 4 instances.  The application can run on any instance type with at least 16GB of RAM but you have selected m5.xlarge in your current launch configuration.  You have defined a scaling policy such that you scale out when the average CPU across your auto scaling group reaches 70% for 5 minutes.  When this threshold is reached, your launch configuration should add more m5.xlarge instances.  You notice that auto scaling is not working as it should when your existing instances reach the scaling event threshold of 70% for 5 minutes.  Since you are deployed in a heavily used region, you suspect there are capacity issues.  Which of the following would be a reasonable way to solve this issue?","explanation":"If you do not have capacity reserved via a zonal RI or on-demand capacity reservation, it is possible that the AZ is out of available capacity for the type of instance you need.  You can reserve capacity or you can also increase the possible instance types in hopes that some other similarly equipped instance capacity is available.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html","title":"Types of Reserved Instances (Offering Classes) - Amazon Elastic Compute  Cloud"}],"answers":[{"id":"a791a52f0967a913925bc47c7ae1c0a6","text":"Version the launch configuration to include additional instances types that also have at least 16GB of RAM.","correct":false},{"id":"04211a7288d23e07315243e9b492dcb4","text":"Provision some zonal reserved instances of m5.xlarge to ensure you have capacity when you need it. ","correct":true},{"id":"989bc9eb46719d5cf9d9b69d191ee8d8","text":"Lower the CPU threshold to 60% so the scaling event triggers earlier and therefore has a better chance of getting resources.","correct":false},{"id":"ea27906009e93ed8060db15ee28fa8ae","text":"Reduce the warm-up and cooldown period in the scaling policy to allow more time to provision resources.","correct":false},{"id":"04c8568f7761456b829145690b8d0cba","text":"Provision some regional reserved instances of m5.xlarge to ensure you have capacity when you need it. ","correct":false}]},{"id":"f3efb368-9a43-4e1d-bf0a-1cf55bb918a8","domain":"awscsapro-domain4","question":"An AWS customer makes use of a wide variety of AWS services across multiple AWS regions. As the usage cost keeps increasing, the customer wants to get the detailed billing and cost information of his AWS account. In the meantime, the customer needs to quickly analyze the cost and usage data, visualize it, and share it through data dashboards so that he can get a better understanding of the billing and resource utilization. Which of the following methods would you choose?","explanation":"Users can get the billing and usage information from the AWS Cost and Usage Reports. The reports downloaded in an S3 bucket can be further processed and analyzed by QuickSight. The reports cannot be created in AWS Cost Explorer or billing dashboard. The question also asks for data visualization and dashboards. QuickSight, AWS's Business Intelligence service, is the most appropriate service for this requirement. Athena, Redshift or Elastic MapReduce are not suitable.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-upload-aws-cost-usage-reports-to-redshift-and-quicksight/","title":"Upload AWS Cost & Usage Reports to QuickSight"}],"answers":[{"id":"fc09f5c144565de3b8472bde413e896a","text":"Create a data pipeline that downloads the monthly cost and usage reports from AWS Cost Explorer and uploads the reports to an S3 bucket. Set up a specific billing table with Amazon Athena and analyze the billing and usage data using SQL query commands.","correct":false},{"id":"78f31d3919e8b532d502611700598d75","text":"Create the hourly AWS Cost and Usage Reports and enable the data integration with Amazon QuickSight. Analyze and visualize the data in QuickSight by creating interactive dashboards and generating custom reports.","correct":true},{"id":"36fb0d3b243e97c8ed14e32e6a2bc329","text":"Generate the billing reports from the AWS management console and upload the files to an S3 bucket. Create an Amazon Elastic MapReduce (EMR) cluster by specifying data inputs and outputs. Process the data in the cluster and generate reports for the admin IAM user.","correct":false},{"id":"d3ef8ffbe6a0ff9378c5e248f2f7e165","text":"Download the AWS Cost and Usage Reports from the AWS billing dashboard in an S3 bucket which is owned by the administrators. Integrate the reports with an Amazon Redshift cluster. Analyze and view the data in the Redshift data warehouse.","correct":false}]},{"id":"580790f0-3491-4b4c-a9b8-42b36a787cf5","domain":"awscsapro-domain2","question":"A new project needs a simple, scalable Amazon Elastic File System (EFS) to be used by several Amazon EC2 instances located in different availability zones. The EFS file system has a mount target in each availability zone within a customized VPC. You have already attached a security group to EC2 instances that allows all outbound traffic. In the meantime, how would you configure the EFS volume to allow the ingress traffic from these EC2 instances?","explanation":"EC2 instances connect to the Amazon EFS file system through the mount targets using the Network File System (NFS) port. The mount targets use a security group to control the access from EC2 instances. It should have an inbound rule to allow the NFS port (TCP:2049). The source of the security group would be the EC2 security group rather than the VPC CIDR. Either IAM role or network ACL cannot be used to control inbound access to an EFS system.","links":[{"url":"https://docs.aws.amazon.com/efs/latest/ug/network-access.html","title":"Security Groups for Amazon EC2 Instances and EFS Mount Targets"}],"answers":[{"id":"070a2bfc6b40165751997ed4d5d4e295","text":"Attach a security group to the mount targets in all availability zones. Allow the NFS port 2049 for its inbound rule. Identify the EC2 security group name as the source in the rule of the security group.","correct":true},{"id":"0fb49dc49facb19b17db23bdff392a1d","text":"Configure a new security group to allow inbound traffic from the VPC CIDR IP range such as 10.10.0.0/16. Associate the security group with the EFS file system directly to allow the ingress traffic from EC2 instances in the VPC.","correct":false},{"id":"10fa29df072c2421e53a66048a92cbe3","text":"Make sure the attached IAM role in EC2 instances has the \"elasticfilesystem:*\" permission. The ingress traffic in the EFS volume is automatically allowed if the IAM role has enough permissions to interact with Elastic File Systems.","correct":false},{"id":"b346d4a177db41a1cfb51b80ae95708a","text":"Add an inbound rule in the network ACL to allow the ingress traffic from the NFS TCP port 2049. The source of the rule should be 0.0.0.0/0. Apply the network ACL in all the subnets where the EFS mount targets exist.","correct":false}]},{"id":"482e75c9-071e-4a10-83f4-575f9c15b885","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They have just accidentally deleted the private key portion of their EC2 key pair.  Now, they are unable to SSH into their Amazon Linux servers.  Unfortunately the keys were not backed up and are considered gone for good.  What can this customer do to regain access to their instances?","explanation":"The two methods that AWS recommends if you lose a private key for an EC2 key pair are using Systems Manager Automation or using a secondary instance to edit the authorized_keys file.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2reset.html","title":"Reset Passwords and SSH Keys on Amazon EC2 Instances - AWS Systems Manager"}],"answers":[{"id":"be45655cb1d64dff71a97aa729bc4e4a","text":"Open the TELNET port (port 23) on the Security Group for the server.  Use a TELNET client to attach to the instances using the root account and password.  Modify the authorized_key file with the new public key.","correct":false},{"id":"bec3d01a56c851f61a1a09c852635db7","text":"Generate and upload a new key pair.  Stop the instances and select the new key pair from the dropdown on the Instance Settings sub-menu in the Console.","correct":false},{"id":"492c38d2fe2c3b96608bb8436592fe26","text":"Use the AWS CLI with the EC2 ModifyInstance action to enable SSH password-only access for the ec2-user account.  Attach using a password rather than an SSH key.  Modify the authorized_key file for the new public key.","correct":false},{"id":"509a77ae9827c5fcb60ccecc62fc9853","text":"Stop the instances, detach its root volume and attach it as a data volume to another instances.  Modify the authorized_keys file, move the volume back to the original instance and restart the instances.","correct":true},{"id":"79457a1b908d4a36cfeba625be909d40","text":"Use AWS Systems Manager Automation with the AWSSupport-ResetAccess document to create a new SSH key for your current instance.","correct":true},{"id":"f17aa014620843abd81fa849982566b0","text":"Create a new key pair in KMS then assign the new public key to the required EC2 instance.","correct":false}]},{"id":"c2d46981-3dac-4e68-81d1-9eedf0cbf264","domain":"awscsapro-domain2","question":"Your company is preparing a special event for its 100th year in business.  As part of that event, the event committee would like to create a kiosk where employees can browse the thousands of photographs captured over the years of the employees and company events.  In a brainstorming session, one event staff member suggests the crazy idea of allowing employees to quickly pull up photographs which they appear in.  What AWS service might be able to make this a reality?","explanation":"AWS Rekognition is a service that can detect and match faces in a photograph.  The kiosk could include a camera that allows event-goers to snap a picture of themselves and then it could scan the photo archive for facial matches.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition – Video and Image - AWS"}],"answers":[{"id":"438e06c02cba48ce4ffbf026d97488b4","text":"Amazon DeepView","correct":false},{"id":"cbe32e918d6a248e3e8ed74e6f0b72f6","text":"AWS Comprehend","correct":false},{"id":"07c025c347c7483abdd039cd36be4220","text":"AWS Rekognition","correct":true},{"id":"ade161cea9b509c72570ba6ae5238e5f","text":"Kinesis for Video","correct":false},{"id":"42e6f83b4b8205e6c2e62ddafdd3bbe3","text":"Amazon Chime","correct":false}]},{"id":"42a413d2-b7c0-4f63-ab1c-37b8ec9b724a","domain":"awscsapro-domain2","question":"You have been contracted by a small start-up to help them get ready for their new product release--a web-based application that lets users browse through detailed photographs of the world's most famous paintings.  The company is expecting a huge debut with very heavy traffic so the solution should be robust and scalable with the least amount of hands-on management.  A key feature of their app is that they have created separate web sites specifically optimized for three different form factors: mobile phone, tablet and desktop.  As such, they need the ability to detect the device and direct the requester to the proper version of the site.  Which architecture will do this and meet the requirements? ","explanation":"A common use case for Lambda@Edge is to implement some front-end logic for incoming requests as close to the requester as possible.  In this case, we can use a custom Lambda function to attempt to determine the device type in the HTTP request.  We can then direct them to the CloudFront distribution that is optimized for their form-factor.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/11/lambda-at-edge-now-supports-content-based-dynamic-origin-selection-network-calls-from-viewer-events-and-advanced-response-generation/","title":"Lambda@Edge Now Supports Content-Based Dynamic Origin Selection, Network  Calls from Viewer Events, and Advanced Response Generation"}],"answers":[{"id":"ea4ee05f24a93c3b1188072a0f4777cc","text":"Configure a Network Load Balancer to use SNI to direct the request to different EC2 web servers based on device type.  Configure multiple auto scaling groups to maintain a minimum number of servers for each device type.","correct":false},{"id":"dbb8c65d7e02773f600d4e87fecc45b5","text":"Build a custom Lambda function to dynamically redirect the requester to the proper S3 origin based on device type.  Associate a CloudFront distribution with a Lambda@Edge function.","correct":true},{"id":"f28bf27096a077030c408fe2417a1d0e","text":"Create multiple distributions in CloudFront for each needed origin.  Use Route 53 to dynamically direct the requester to the appropriate alias based on device type.","correct":false},{"id":"d6b70dc60b5f361f7b92d61ac7964558","text":"Configure the Requester Interrogation feature on CloudFront to identify the device used by the requester.  Redirect the requester to the desired S3 origin based on the device type.  ","correct":false},{"id":"2e04372e48d55855d33ef7c797110044","text":"Use Amazon AppSync to detect the type of device issuing the inbound request.  Use a Lambda function to redirect to the proper CloudFront distribution based on device type returned from AppSync.","correct":false}]},{"id":"9b8fcbdc-35da-425a-a77a-d5d7b2f6682b","domain":"awscsapro-domain5","question":"You have been asked to help develop a process for monitoring and alerting staff when malicious or unauthorized activity occurs.  Your Chief Security Officer is asking for a solution that is both fast to implement but also very low maintenance.  Which option best fits these requirements?","explanation":"AWS GuardDuty is a managed service that can watch CloudTrail, VPC Flow Logs and DNS Logs, watching for malicious activity.  It has a build-in list of suspect IP addresses and you can also upload your own lists of IPs.  GuardDuty can trigger CloudWatch events which can then be used for a variety of activities like notifications or automatically responding to a threat.  AWS Macie is a service to discovery and classify potentially sensitive information.  CloudWatch alone lacks the business rules that are provided with GuardDuty.","links":[{"url":"https://aws.amazon.com/guardduty/faqs/","title":"Amazon GuardDuty FAQs – Amazon Web Services (AWS)"}],"answers":[{"id":"68a4943d35341bc4e1fae5493607eaa0","text":"Use AWS Glue to direct all CloudTrail logs into Redshift.  Use QuickSight as a presentation layer with custom reports for visualizing malicious and unauthorized behavior.  Run the reports periodically and email them to the Security Officer. ","correct":false},{"id":"256dea4919c6b38545573ed3fe0fee5c","text":"Configure VPC Flow Logs to capture all traffic going in and out of the VPC.  Use ElastiSearch to process the logs and trigger a Lambda function whenever malicious or unauthorized behavior is found.","correct":false},{"id":"0809c23cd3f14044c6f063646502e2e7","text":"Use AWS SageMaker to implement a Linear Learner algorithm that periodically reviews CloudFront logs for malicious and unauthorized behavior.  When the ML model finds something suspicious, trigger an SES email to the Security Officer.","correct":false},{"id":"8a07e363b2fe9a6ac84afe772979dec2","text":"Configure CloudWatch to create an event whenever malicious or unauthorized behavior is observed.  Trigger an SMS message via SNS to the Security Officer whenever the event happens. ","correct":false},{"id":"069bd8db7abe860a4270a1b362abf0f0","text":"Enable AWS GuardDuty to monitor for malicious and unauthorized behavior.  Configure a custom blacklist for the IPs which you have seen suspect activity in the past.  Setup a Lambda function triggered from a CloudWatch event when anomalies are detected. ","correct":true},{"id":"759d9a95c918d18810232d19f92f6d79","text":"Enable AWS Macie to monitor for malicious and unauthorized behavior.  Configure a custom whitelist for the IPs that were wrongly flagged.  Setup a Lambda function triggered from a CloudWatch event when anomalies are detected. ","correct":false}]},{"id":"c3ac5de9-a343-4cde-af1b-6c9f89824d2f","domain":"awscsapro-domain5","question":"An external auditor is reviewing your process documentation for a Payment Card Industry (PCI) audit.  The scope of this audit will extend to your immediate vendors where you store, transmit or process cardholder data.  Because you do store cardholder data in the AWS Cloud, the auditor would like to review AWS's PCI DSS Attestation of Compliance and Responsibility.  How would you go about getting this document? ","explanation":"AWS Artifact provides on-demand downloads of AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use. You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls.","links":[{"url":"https://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html?icmpid=docs_artifact_console","title":"What Is AWS Artifact? - AWS Artifact"}],"answers":[{"id":"d7cb47dd1f374d3ed079b14cc6f2cd75","text":"Submit a Support Case requesting the document","correct":false},{"id":"d9208942349d1c6f7dbaba3661069bc1","text":"AWS WorkDocs","correct":false},{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false},{"id":"1d16d307ee989a80e421198a01993a9c","text":"AWS IAM Console","correct":false},{"id":"09e838e873f25f954fef911d50b3d1ab","text":"AWS Pinpoint","correct":false},{"id":"63df0d05cd43af35c95cf04d92aaf685","text":"AWS Legal Services website","correct":false},{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":true}]},{"id":"254eeafd-1183-4117-936e-f6f7ffa9d88a","domain":"awscsapro-domain2","question":"A client is having a challenge with performance of a custom data collection application.  The application collects data from machines on their factory floor at up to 1000 records per second.  It uses a Python script to collect data from the machines and write records to a DynamoDB table.  Unfortunately, under times of peak data generation, which only last 1-2 minutes at a time, the Python application has timeouts when trying to write to DynamoDB.  They don't do any analytics on the data but only have to keep it for potential warranty issues.  They are willing to re-architect the whole solution if it will mean a more reliable process.  Which of the following options would you recommend to give them the most scalable and cost-efficient solution?","explanation":"The application is likely running into throttling when writing to DynamoDB.  Kinesis Firehose makes for a good option in this case to accommodate streaming records.  Since we do not have to perform any analytics, we can simply store it on S3 using Firehose.","links":[{"url":"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html","title":"What Is Amazon Kinesis Data Firehose? - Amazon Kinesis Data Firehose"}],"answers":[{"id":"61f03e55a88efffb8060d5d3e07f247d","text":"Turn on DynamoDB Auto Scaling and configure appropriate upper and lower limits.","correct":false},{"id":"0bd59f25e091570f7b171c3dd76b0206","text":"Change the application design to use SQS and a custom process on an EC2 spot fleet to throttle inbound messages into DynamoDB.","correct":false},{"id":"6ac1d0de8c41824a325523f050a3784e","text":"Change the application design to write the data records to EMR.  Use a Pig script to transfer the data from EMR to DynamoDB periodically.","correct":false},{"id":"80999a665ea6597ad65e91f9d4a84b3f","text":"Change the application design to use Kinesis Streams to take in the data.  Provision at least 5 shards to ensure enough peak capacity.  Configure the Kinesis Streams to load the data into DynamoDB.  Increase the RCU and WCU for the DynamoDB table to match peak needs.","correct":false},{"id":"7e2676bbdbc658bd6a68274495e2376f","text":"Change the application design to use Kinesis to take in the data.  Use Kinesis Firehose to spool the data files out to S3.  Use S3 Lifecycle to transition the files to Glacier after a few days.","correct":true},{"id":"b30cd1d2aae071493c960798a422ade4","text":"Change the application design to use SWF to take in the data.  Use Amazon Elasticache in front of the DynamoDB database as a buffer to throttle the writes.","correct":false}]},{"id":"512696b6-6160-45f7-8000-d664f78a86aa","domain":"awscsapro-domain2","question":"You have been contracted by a Security Company to build a face recognition service for its customer, Department of Corrections, in the AWS Cloud. Whenever a new inmate or personnel joins a facility, their facial image will be taken by an application running on a laptop, and stored centrally, along with metadata like their name. They will have a second application getting a live image feed from cameras installed throughout the secure areas of the facility. Whenever the second application receives an image from a camera, it needs to check against the pool of images stored centrally to check if there is a match. If a match is found, the location must be saved along with the timestamp and name in a database which can later be used to query the location of a person at or near a given time-period.\nHow will you, as the AWS Architect, design this suite of applications?","explanation":"This question tests the knowledge of the various Machine Learning technologies and services offered by AWS. Amazon Rekognition is the AWS AI service for image and video analysis. The question also offers AWS Sagemaker, AWS Personalize and AWS Comprehend as alternatives. AWS Sagemaker is used to build and train Machine Learning models. Hence, it is not relevant in this scenario, as the use case is not about training a model. Amazon Personalize is an ML service that enables developers to create individualized recommendations for customers using their applications. This use case is not related to recommendations, hence we can eliminate AWS Personalize. Amazon Comprehend is a natural language processing (NLP) service that uses ML to find relationships and insights in text. We can eliminate Comprehend as this is an image analysis scenario, as opposed to text analysis.","links":[{"url":"https://aws.amazon.com/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/","title":"Build Your Own Face Recognition Service Using Amazon Rekognition"},{"url":"https://aws.amazon.com/machine-learning/","title":"Various components of Machine Learning on AWS"}],"answers":[{"id":"3c168275751e0bed48552554f05bad14","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Personalize from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in a Dynamodb database. Later, when a match is required, query Amazon Personalize with the image taken by security cameras. Personalize will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the Dynamodb database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to an RDS database for later querying","correct":false},{"id":"3fac11da1f4ec1911a5b1c4262c16bb4","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Comprehend from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in an RDS Postgresql database. Later, when a match is required, query Amazon Comprehend with the image taken by security cameras. Comprehend will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the RDS database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to a second RDS database for later querying","correct":false},{"id":"cc5d32350e93736f1e3ec5e359029c80","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Sagemaker Image Classification Algorithm from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in an RDS Postgresql database. Later, when a match is required, query Amazon Sagemaker with the image taken by security cameras. Sagemaker will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the RDS database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to a second RDS database for later querying","correct":false},{"id":"a24bc02aa584b134a2abecb810d5b32c","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Rekognition from the Lambda function to index the face in the image as a Collection, returning a Face id. Store this face id and Name of the person in a Dynamodb database. Later, when a match is required, query the Amazon Rekognition Collection with the image taken by security cameras. Rekognition will return a set of face ids that have potentially matched. If a face id is found whose confidence value is higher than a predefined set value, query the Dynamodb database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to an RDS database for later querying","correct":true}]},{"id":"63da01c2-9c4d-4abd-a482-06ede2baf728","domain":"awscsapro-domain2","question":"A popular royalty free photography website has decided to run their business on AWS. They receive hundreds of images from photographers each week to be included in their catalog. Amazon S3 has been selected as the image repository. As the business has grown, the task of creating catalog entries manually has become unsustainable. They'd like to automate the process and store the catalog information in Amazon DynamoDB. Which architecture will provide the most scalable solution for automatically adding content to their image catalog going forward?","explanation":"Calling the S3 API to upload the images will suffice for this use case. Streaming ingest is not needed for this volume of data. AWS Step Functions will orchestrate the process of discovering both the image metadata with a Lambda function and the image object data with Rekognition. Rekognition will not return the image metadata. AWS Elemental MediaStore is used for originating and storing video assets for live or on-demand media workflows, not image recognition. Kinesis Video Analytics is not a currently supported service.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://github.com/aws-samples/lambda-refarch-imagerecognition","title":"Serverless Reference Architecture: Image Recognition and Processing Backend"}],"answers":[{"id":"dd8565c85d19c2867d3a6768f512b404","text":"Programmatically call the S3 API to upload the images. Trigger an AWS Lambda function to kick off execution of a state machine in AWS Step Functions. Create state machine sub-steps to invoke Lambda functions which extract image metadata, detect objects in the image with Amazon Rekognition, and store the discovered data in DynamoDB.","correct":true},{"id":"f98112f4f94f4a8d66edee560976acc2","text":"Programmatically call the S3 API to upload the images. Trigger an AWS Lambda function to send the image's S3 key to AWS Elemental MediaStore, which will extract the image's metadata, discover image patterns through machine learning, and deposit artifacts back into S3. Invoke a Lambda function to write the artifact data to DynamoDB.","correct":false},{"id":"42367a003a702450700b58798073122b","text":"Deploy Amazon Kinesis Data Firehose to ingest images into S3. Invoke a Lambda function to pass the image's S3 key to Amazon Rekognition, which will extract the image metadata and detect objects in the image. Invoke a Lambda function to store the discovered data in DynamoDB.","correct":false},{"id":"0710b6a1f2a807da2cfc1a940e7014e9","text":"Deploy Amazon Kinesis Data Streams to ingest the images with two consumers. Setup Amazon Kinesis Firehose as the first consumer to deposit the images into S3. Configure Amazon Kinesis Video Analytics as the second consumer to extract the image's metadata and object information. Invoke a Lambda function to store the discovered information in DynamoDB.","correct":false}]},{"id":"0906c4cf-83a1-4cec-b2ab-c010dcdee73f","domain":"awscsapro-domain1","question":"The alternative energy company you work for has four different business units, each of which would like to run workloads on AWS. Each business unit has it's own AWS account, and a shared services AWS account has been created. An established process for tracking software license usage exists for on-premises applications, but the finance department has concerns that the self-serve nature of the cloud may result in license overages for applications deployed on AWS. You've been tasked with setting up a governance model whereby users are only given access to a standard list of products. Which architecture will provide an effective way to implement the governance requirements and manage software license usage on AWS?","explanation":"AWS Service Catalog allows organizations to create and manage catalogs of approved products for use on AWS. Products are defined as CloudFormation Templates. Software license information can be associated with Service Catalog products through tags. AWS Step Functions can orchestrate the process of incrementing usage counts and notifying of over-usage situations when products are launched by users. AWS License Manager is a robust solution for managing software licenses, but it needs to be coupled with Service Catalog to meet the requirement for limiting access to a standard set of products. A Lambda trigger is not currently available for Service Catalog product deployments. Elastic Container Registry provides tagging at the repository level, not at the individual container image level.","links":[{"url":"https://aws.amazon.com/servicecatalog/","title":"AWS Service Catalog"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://aws.amazon.com/blogs/mt/tracking-software-licenses-with-aws-service-catalog-and-aws-step-functions/","title":"Tracking software licenses with AWS Service Catalog and AWS Step Functions"}],"answers":[{"id":"41d11d1142468e8b0d2a29674d1eaa2c","text":"Deploy AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Populate Service Catalog product tags with software license information. Create an Amazon DynamoDB table to store software license usage counts. Have Amazon CloudWatch detect when a user deploys a Service Catalog product. Launch an AWS Step Functions process to increment license counts in the DynamoDB table, and send notifications when overage thresholds are met","correct":true},{"id":"62788b95bb6b249e6511d14023a20364","text":"Implement AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Create an Amazon DynamoDB table to store software license usage counts. Trigger an AWS Lambda function to run each time a Service Catalog product is launched. Have the Lambda function increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"33db838110cfb8b002590cbff630b825","text":"Create Amazon Machine Images for all of the instance configurations that will be deployed. Implement AWS License Manager license configurations and attach them to the AMIs. Create AWS CloudFormation StackSets for the AMIs in the shared AWS account and make them available to users in each business unit","correct":false},{"id":"e2441352bdc2d4db956149f0a10b6738","text":"Create Docker images for each of the standardized applications that will be deployed and register them with Amazon Elastic Container Registry (ECR). Populate ECR tags with software license metadata. Create an Amazon DynamoDB table to store software license usage counts. Whenever a container is launched in Amazon Elastic Container Service, trigger an AWS Lambda function to increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false}]},{"id":"e04f4064-644c-4e96-a040-90164b4f91b0","domain":"awscsapro-domain2","question":"A fast-food restaurant chain would like to automate their drive-thru capabilities with chatbot speech technology to capture full orders for the kitchen crew without human interaction. The automated service will need to be able to offer various up-sell options based on menu items selected (for example, suggesting fries if a burger is ordered). Line item and total order price will need to be displayed for the customer as the order progresses. When customers use vocabulary not recognized by the service, that speech will need to be handled properly in future interactions. Ordering volume increases dramatically around lunch and dinner-time at the fast-food restaurants. Which architecture will provide the most scalable solution for the automated ordering service?","explanation":"Lex provides the chatbot logic for the conversation, but Lex needs Polly to render the speech. Lex integrates seamlessly with Lambda, providing the capability to read and write database information. Product pricing would most likely be part of a larger corporate ERP system which would more likely be using a relational database like Aurora than DynamoDB. Up-sell logic can be coded directly in Lex. Lex captures unrecognized utterances, which can be fed to a SageMaker model to determine future actionable options. A Lambda function can apply the SageMaker inferences to Lex's order flow logic, but SageMaker can't do that on it's own. All of the services used in this solution are managed services which scale automatically, except for Aurora which is only used for reads in this use case. Transcribe is not needed to convert speech to text as Lex does this inherently.","links":[{"url":"https://aws.amazon.com/machine-learning/?nc2=h_ql_prod_ml","title":"Machine Learning on AWS"},{"url":"https://aws.amazon.com/lex/","title":"Amazon Lex"},{"url":"https://aws.amazon.com/polly/","title":"Amazon Polly"},{"url":"https://aws.amazon.com/sagemaker/","title":"Amazon SageMaker"}],"answers":[{"id":"4b2c3233ba5ed40f66a5c98230dc85c0","text":"Have Amazon Lex handle all chatbot interaction with the drive-thru device. Configure Lex to call an AWS Lambda function to retrieve menu item pricing information from Amazon DynamoDB, and to deposit order line items in Amazon DynamoDB. Determine order up-sell options in the Lambda function. Send unrecognized customer utterances to Amazon SageMaker, and regularly invoke a Lambda function to update Lex's order flow logic from SageMaker's inferences.","correct":false},{"id":"c3610e508d769c91f319fbaef356928e","text":"Have the drive-thru device speak order transaction statements from Amazon Polly. Send customer utterances to Amazon Lex, which calls an AWS Lambda function to retrieve menu item pricing information from Amazon Aurora, and to deposit order line items in Amazon DynamoDB. Determine order up-sell options in Lex. Send unrecognized customer utterances to Amazon SageMaker, and regularly invoke a Lambda function to update Lex's order flow logic from SageMaker's inferences. Have Lex send order transaction statements to Polly.","correct":true},{"id":"c24a3f6c66e92b76fe292d174d28c80d","text":"Have the drive-thru device speak order transaction statements from Amazon Polly. Send customer utterances to Amazon Transcribe. Have Transcribe send the text of the speech to Amazon Lex, which calls an AWS Lambda function to retrieve menu item pricing and up-sell information from Amazon DynamoDB. The Lambda function also deposits order line items in Amazon DynamoDB. Determine order up-sell options in the Lambda function. Send unrecognized customer utterances to Amazon SageMaker, and have SageMaker update Lex's order flow logic. Have Lex send order transaction statements to Polly.","correct":false},{"id":"e142e997b7988dd2168b7c5bcc72a973","text":"Have Amazon Lex handle all chatbot interaction with the drive-thru device. Configure Lex to call an AWS Lambda function to retrieve menu item pricing information from Amazon Aurora, and to deposit order line items in Amazon DynamoDB. Determine order up-sell options in Lex. Send unrecognized customer utterances to Amazon SageMaker, and have SageMaker update Lex's order flow logic.","correct":false}]},{"id":"49f16801-2cc1-48c8-a517-f9192f516318","domain":"awscsapro-domain3","question":"A tire manufacturing company needs to migrate a .NET simulation application to the AWS cloud. The application runs on a single Windows Application Server in their datacentre. It reads large quantities of data from local disks that are attached to the on-premises Application Server. The output from the application is small in size and posted in a queue for downstream processing. On the upstream side, the data acting as the input for the .NET simulation app is generated in the tire-testing Lab during the daytime by processes running on a Linux Lab Server. This data is then copied from the Linux Lab Server to the Windows Application Servers by a nightly process that also runs on the Linux Lab Server. This nightly process mounts the Application Server disks using a Samba client, this is made possible by the Application Server also acting as a Windows File Share Server. When the nightly process runs, it overwrites the input data from last night because of disk space constraint on the Application Server. This is undesirable as the data is permanently lost on a daily basis.\nThe migration is being undertaken because the .NET simulation application needs more CPU and RAM. The company does not want to spend on expensive hardware any more. However, the nightly process is not migrating, nor is the Linux Lab Server. The code of the simulation applications, as well as the nightly process, may change a little as a result of the migration, but leadership wants to keep these changes to a minimum. They also want to stop losing the daily test data and keep it somewhere for possible analytical processing later on.\nAs the AWS architect hired to shepherd this migration and many more possible migrations in the future, which of the following architectures would you choose as the best one, considering the minimization of code changes as the topmost goal, followed by cost-effectiveness as the second but important priority? The data has no security requirement.","explanation":"The two parts of this question are - (a) Do I use EFS or EBS for storing the data from the Lab Servers? (b) Do I copy data from each night to S3 using a NAT Gateway (thereby using the public internet) or a VPC Endpoint (thereby using the private network to copy)?\nThe answer to the first question is EBS because Windows EC2 instances cannot mount EFS, as EFS only supports Linux EC2 instances.\nThe answer to the second question is VPC Endpoint because NAT Gateways are very costly - they are charged 24-7 for just running, in addition to having data transfer rates. S3 VPC Endpoints are a cost-effective mechanism to copy data to S3. Note that the S3 put-object cost will be the same for both cases. The question tries to distract the candidate by stating that there is no security requirement, trying to confuse the candidate into selecting NAT Gateway in case they perceive the only distinction between NAT Gateway and S3 VPC Endpoint to be the usage of public network versus private.\nThis is an example of highly verbose question describing a complex scenario. There will definitely be quite a few such questions in the AWS SA-P exam that are challenging in terms of time management. You may use vertical scanning of the answer choices to spot the differences first. That way, you can focus on determining which of the variations is correct because you would know what is different between them.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html","title":"EFS Support for Windows EC2 Instances"},{"url":"https://aws.amazon.com/vpc/pricing/","title":"Search for NAT Gateway Pricing here"}],"answers":[{"id":"c7311a2612bfc63a966317d468a5b4dd","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"fabdb7e05e5bb1c78dd6e166134202ca","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"3ae5ae41663ec801882e10e7fa394613","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":true},{"id":"220d73c7cceb53661a402d59038118ee","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false}]},{"id":"74aec97e-c092-4588-8da4-43dca3ddd0eb","domain":"awscsapro-domain5","question":"You are trying to help a customer figure out a puzzling issue they recently experienced during a Disaster Recovery Drill.  They wanted to test the failover capability of their Multi-AZ RDS instance.  They initiated a reboot with failover for the instance and expected only a short outage while the standby replica was promoted and the DNS path was updated.  Unfortunately after the failover, they could not reach the database from their on-prem network despite the database being in an \"Available\" state.  Only when they initiated a second reboot with failover were they again able to access the database.  What is the most likely cause for this?","explanation":"The routes for all subnets in an RDS subnet group for a Multi-AZ deployment should be the same to ensure all master and stand-by units can be reached in the event of a failover.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/avoid-route-table-issues-rds/","title":"Avoid route table issues RDS Multi-AZ fail over"}],"answers":[{"id":"1105b24fe7a3c1eb0a86d0fa9a3cdc62","text":"This was most likely an AWS error.  They should submit a support ticket with the RDS instance identifier and the approximate time of the failover test via the Support Center.","correct":false},{"id":"260ae29233a6047971894eff9d05fa55","text":"They initiated a failover with an IAM account that did not have sufficient rights to perform the reboot.  This resulted in an incomplete failover that was only corrected by executing the failover again to reset the DNS entries.","correct":false},{"id":"46b9b5188d94533190c30de816355165","text":"They used the AWS Console to issue the reboot.  You can only force a failover of RDS by using the AWS CLI and adding the --force-failover parameter to the \"aws rds reboot-db-instance\" command.","correct":false},{"id":"f637d9886a1ea7b52d608046e233e504","text":"The subnets in the subnet group did not have the same routing rules.  The standby subnet did not have a valid route back to the on-prem network so the database could not be reached despite being available.","correct":true},{"id":"1dbb40d7bdb88fbe54fead30b1bf5f12","text":"There was a lag in the state update on the AWS console showing \"Available\".  If they would have waited longer, it likely would have changed to \"Degraded\".  A failover can take 30 minutes or more because AWS automatically creates a snapshot before promoting the standby.","correct":false}]},{"id":"91e4ebb5-18ac-45d6-867e-4c2eddefc075","domain":"awscsapro-domain4","question":"Your company has come under some hard times resulting in downsizing and cuts in operating budgets.  You have been asked to create a process that will increase expense awareness and your enhance your team's ability to contain costs.  Given the reduction in staff, any sort of manual analysis would not be popular so you need to leverage the AWS platform itself for automation.  What is the best design for this objective?","explanation":"AWS Budgets is specifically designed for creating awareness and transparency in your AWS spending rate and trends.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing Your Costs with Budgets - AWS Billing and Cost Management"}],"answers":[{"id":"8e9e7154ed9b1f92015c5fde5ca7a2a3","text":"Use Kinesis Streams to Ingest CloudTrail log streams.  Create a Lambda function to parse the log stream and insert a record into DynamoDB whenever a pay-per-use activity happens.  Use DynamoDB streams to alert the team when a threshold has been exceeded.","correct":false},{"id":"0e5c5be553400912c2517757d9aba6b2","text":"Provide access to Cost Explorer for your team for transparency.  Allow them to periodically review the accrued costs for the month and take the appropriate action.","correct":false},{"id":"ce7a1fa4194d00b76a1491e66b2a10ec","text":"Use AWS Budgets to create a budget.  Choose to be notified when monthly costs are forecasted to exceed your updated monthly target.","correct":true},{"id":"aadb36f869250c141ae2eeec088d3bb3","text":"Implement a Cost Allocation Tagging strategy.  Create a periodic aggregation process in AWS Glue to read costs based on the tags.  Configure Glue to send the report to SNS where it can be either emailed or texted to the team depending on their own preference.","correct":false},{"id":"be1b52e799fe44ef89f78b167401c067","text":"Export AWS bill data into Redshift.  Use Quicksight to create reports per cost center.  Provide access for users to QuickSight to monitor their usage.","correct":false}]},{"id":"5d35c6d3-3eaf-49d0-b64e-611d74d40af0","domain":"awscsapro-domain2","question":"You need to design a new CloudFormation template for several security groups. The security groups are required in different environments such as QA, Dev and Production. The allowed CIDR range in the security group ingress rule depends on environments. For example, the allowed inbound address range is 10.0.0.0/16 for non-production and 10.1.0.0/16 for production. You prefer to maintain a single template for all the environments. What is the best method for you to choose?","explanation":"CloudFormation has an optional Conditions section that contains statements to determine whether or not entities should be created or configured. Then you can use the \"Fn::If\" function to check the condition and return different values. You do not need to use Jenkins to pre-process the template and there are no \"Fn::Case\" and \"Fn::Switch\" intrinsic functions.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html","title":"CloudFormation conditions"}],"answers":[{"id":"7afa8c2f0fa2ef8966917c74044516dd","text":"CloudFormation does not provide functions for conditions. Use Terraform instead. Create a variable file to manage different CIDR IP ranges. Pass in the environment variable value and use \"Terraform apply\" to deploy all the security group resources at one time.","correct":false},{"id":"d273c3216496e7d41cb97094f83d3e33","text":"Create an environment variable and pass it to the CloudFormation template. In the security group resources, use Fn::Case and Fn::Switch to check the variable value and return different CIDR IP range.","correct":false},{"id":"c4a393b1cce09df81069eb93f2d38054","text":"Use the environment type as an input parameter and create a condition based on the parameter. In the AWS::EC2::SecurityGroupIngress resource, use Fn::If to check the condition and return related source CIDR range.","correct":true},{"id":"d2661ed9bcd0f8b132885660604a665a","text":"Create a Jenkins pipeline with an environment variable. Depending on the variable value, modify the template script to use the correct CIDR IP range. Deploy the CloudFormation stack with the updated template.","correct":false}]},{"id":"90c0c2e2-4a39-4731-80b8-5f4d64c3d896","domain":"awscsapro-domain2","question":"You have been asked to design a landscape that can facilitate the upload very high resolution photos from mobile devices, gather metadata on objects in the photos and store that metadata for analysis.  Which of the following components would you use for this use-case for quickest implementation and best scalability?","explanation":"DynamoDB and S3 represent the most reasonable and scalable choices in this list for metadata storage (DynamoDB) and file upload (S3).  Kinesis has size limits on the inbound object so it would not be appropriate for use cases that involve potentially large files like photos.  Amazon Rekognition is image processing service that can extract metadata on objects in a photograph.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition – Video and Image - AWS"}],"answers":[{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true},{"id":"8d4c0b2cef256d21ab680366c8b1c6bf","text":"EMR","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true},{"id":"6fa977a31b941055e5cc04cc2000fb84","text":"Rekognition","correct":true},{"id":"594025cae6dfa6b9073dc25de93ddb56","text":"Kinesis","correct":false},{"id":"803b0d23fbee2cf79d83376ef09a3eee","text":"Polly","correct":false}]},{"id":"02a9611c-591c-4280-bb83-6c65c7c4921f","domain":"awscsapro-domain5","question":"A sporting goods retailer runs WordPress on Amazon EC2 Linux instances to host their customer-facing website. An ELB Application Load Balancer sits in front of the EC2 instances in Auto Scaling Groups in two different Availability Zones of a single AWS region. The load balancer serves as an origin for Amazon CloudFront. Amazon Aurora provides the database for WordPress with the master instance in one of the Availability Zones and a read replica in the other. Many custom and downloaded WordPress plugins have been installed. Much of the DevOps teams' time is spent manually updating plugins across the EC2 instances in the two Availability Zones. The website suffers from poor performance between the Thanksgiving and Christmas holidays due to a high occurrence of product catalog lookups. What should be done to increase ongoing operational efficiency and performance during high-volume periods?","explanation":"ElastiCache Memcached will provide in-memory access speeds for the catalog read transactions. A WordPress plugin is required to leverage caching. WordPress can access an EFS Mount Target for file sharing across all instances. Aurora offers a MySQL option, and WordPress requires MySQL, so the solution would have been set up that way already. CodeDeploy could update plugins on all instances, and will work well for the custom in-house code, but triggering the updates of downloaded plugins will need to be orchestrated. Aurora Auto Scaling will distribute catalog reads across multiple replicas for increased performance, but not to the extent of in-memory caching. Elastic File System is a managed service providing operational advantages over NFS file shares. ElastiCache Redis will provide the in-memory read performance desired, but changing the wp-config.php file won't provide access to it, as a plugin is needed for that. WordPress does work with S3, but a shared file system is easier to implement.","links":[{"url":"https://aws.amazon.com/getting-started/projects/build-wordpress-website/","title":"Build a WordPress Website"},{"url":"https://github.com/aws-samples/aws-refarch-wordpress?did=wp_card&trk=wp_card","title":"Hosting WordPress on AWS"}],"answers":[{"id":"bc643d3342a5a675e65e5baed00e88b9","text":"Deploy Amazon ElastiCache Memcached as a caching layer between the EC2 instances and the database. Install a WordPress plugin to read from Memcached. Implement Amazon Elastic File System to store the WordPress files and create mount targets in each EC2 subnet.","correct":true},{"id":"17b12e57cd85610e888cda82b5a8a145","text":"Migrate the WordPress database to RDS MySQL since MySQL is WordPress's native database and WordPress is performance optimized for MySQL. Implement AWS CodeDeploy to update WordPress plugins on all EC2 instances.","correct":false},{"id":"f060160b20c4408b2442010d3ea4d387","text":"Use Amazon ElastiCache Redis as a caching layer between the EC2 instances and the database. Change wp-config.php to point to the Redis caching layer, and have Redis point to Aurora. Move the WordPress files to S3 and have WordPress access them there.","correct":false},{"id":"aeb27370afc3b672eb0a1afcb28e9176","text":"Implement Aurora Auto Scaling to increase the number of replicas automatically as demand increases. Create an NFS file share to hold the WordPress files. Access the file share from the EC2 instances in both Availability Zones.","correct":false}]},{"id":"777fd0a9-391f-4072-b147-a64a2016f5a1","domain":"awscsapro-domain2","question":"You are working with a company to design a DR strategy for the data layer of their news website.  The site serves customers globally so regional diversity is required.  The RTO is defined as 4 hours and RPO have been defined as 5 minutes. Which of the following provide the most cost-effective DR strategy for this client?","explanation":"While Multi-AZ RDS may be a best practice, the question only stipulates regional resilience.  So, we are looking for options that create regional diversity and fall within our RPO and RTO.  Those options would be cross-region bucket replication and cross-region RDS replicas.  The RPO given means that we must not loose anything more than 5 minutes of data, so any sort of backup that is less frequent than every 5 minutes is eliminated.","links":[{"url":"https://aws.amazon.com/blogs/aws/cross-region-snapshot-copy-for-amazon-rds/","title":"Cross-Region Snapshot Copy for Amazon RDS | AWS News Blog"},{"url":"https://aws.amazon.com/blogs/aws/new-whitepaper-use-aws-for-disaster-recovery/","title":"New Whitepaper: Using AWS for Disaster Recovery | AWS News Blog"}],"answers":[{"id":"cfe51da2c91348146ba1bb989b4c2225","text":"Configure RDS to perform daily backups then copy those to another region.","correct":false},{"id":"aa26654aaaf6239fe7acd7dc4e952d5a","text":"Write a script to create a manual RDS snapshot and transfer it to another region.  Use AWS Batch to run the script every three hours.","correct":false},{"id":"0890c4152307ffd34ebeb6ea7c500814","text":"Write a script to export the RDS database to S3 every hour then use cross-regional replication to stage the exports in a backup region.","correct":false},{"id":"035a09840d025b52ad7c808976c94da2","text":"setup cross-region replication for S3 buckets.","correct":true},{"id":"7c7f5169f7bfea8c0aa5d79d8f1f1565","text":"Configure RDS Read Replicas to use cross-region replication from the primary to a backup region.","correct":true},{"id":"b3810d00767bfbb6b85fe44c1c3d2dd1","text":"Configure RDS to use multi-AZ and automatically fail over in the event of a problem.","correct":false}]},{"id":"3440b6ff-6fe9-495d-a765-f69f6b82a628","domain":"awscsapro-domain3","question":"You work for a health record management company which operates a view-only portal for end-users to check their health records online. Users can also raise disputes if anything is incorrect. This 2-tier website that supports only HTTPS will be moved to the AWS Cloud. There are 5 web servers on-premises and an F5 Load Balancer that controls traffic to these web servers. SSL is terminated at the web servers. An Oracle Real Application Cluster (RAC) serves as the database tier. Due to the sensitive nature of personal health information, the web site uses mutual authentication - the server requests browsers for a valid client certificate before establishing a trusted session.\nSelect two alternate working architectures for this application to be migrated to AWS so that code changes are minimized. Choose two responses each of which can act as an independent and functional solution, and also result in minimum application code changes.","explanation":"The areas tested by this question are as follows.\nFirst, if an ELB is not terminating SSL, its listener protocol cannot be HTTPS - it has to be TCP. Additionally, AWS ELB does not support terminating client-side certificates. Therefore, if a website requires client SSL certificates, and if it also uses AWS ELB, the ELB must let the target EC2 instances to terminate SSL and validate the client certificate. This requires the protocol to be TCP/443. Both these facts (one, SSL is not terminated at the Load Balancer level, and two, mutual authentication is used) are stated explicitly in the question so that the candidate identifies at least one of them and is thus able to conclude that HTTPS is not the correct protocol choice for ELB. Hence, amongst the two choices that use ELB, the one that says TCP is the correct one.\nSecondly, RDS does not support Oracle RAC. Hence, the database tier must use EC2 instances. Thus, for the second alternate solution that uses Route 53 multi-value answer records instead of ELB, we should select the option that deploys EC2 instances for the database tier.","links":[{"url":"https://forums.aws.amazon.com/thread.jspa?threadID=109180","title":"Discussion Forums - HTTPS Client certificate validation while using client ELB"},{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"RDS FAQ-s, search for phrase - Is Oracle RAC supported on Amazon RDS"}],"answers":[{"id":"e7a1d88af1880a82062437a4c1005adf","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":true},{"id":"7250b61db9c9e69abf4f9e7bd2bfb268","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring HTTPS/443 as listener","correct":false},{"id":"11a4ad1f948d7b2fd3631cb763fa8a00","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring TCP/443 as listener","correct":true},{"id":"307f49d40547367c203a0fcfa1a46be8","text":"Migrate the database to RDS Oracle and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":false}]}]}}}}
