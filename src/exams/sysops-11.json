{"data":{"createNewExamAttempt":{"attempt":{"id":"5c8d4843-48a3-428c-9cf4-83b285f919ad"},"exam":{"id":"45248101-5ee2-4c4f-9045-1cdbaea5bf3f","title":"AWS Certified SysOps Administrator - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"0bf0b6fd-f8bb-4fb9-96f8-16387c91cb91","domain":"data-man","question":"You deploy a Java application in an EC2 instance. The application needs to access some configuration files stored in an S3 bucket. You attach an IAM role to the EC2 instance and it is successful to list the objects in the S3 bucket. However, when the application tries to get the objects, the operation is denied. Which of the provided options can help you to troubleshoot and fix the issue? (Select TWO.)","explanation":"It is appropriate to check if the policies in the IAM role allow s3:GetObject for the S3 objects. On the other hand, as the bucket policy may deny the operation, you should check the policy permissions as well. To get the objects in the S3 bucket, the resource should be arn:aws:s3:::BucketName/*. The security group cannot cause the problem and bucket ACL should not have a public access for this scenario.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html","title":"Using Bucket Policies and User Policies for S3 buckets"}],"answers":[{"id":"ca81853a663c326bf386c5f454ae69e8","text":"Check if the IAM role allows the s3:GetObject action for the S3 bucket resource.","correct":true},{"id":"c668df8a129a97014c06d55477be79f6","text":"Check if the bucket policy allows the s3:GetObject action for the resource of arn:aws:s3:::BucketName","correct":false},{"id":"43bcca9066c5d4d6c828aeb2bd9556c6","text":"Check if the security group of the EC2 instance allows port 80 for both inbound and outbound.","correct":false},{"id":"f287552689a30e359d65144c6a4588aa","text":"Verify if the bucket policy allows the IAM role to get objects for the resource of arn:aws:s3:::BucketName/*","correct":true},{"id":"5b96428e89f2a5e79a8b901d719b1791","text":"Make sure that the bucket Access Control List allows the List Object operation for the public access.","correct":false}]},{"id":"1de9eb98-4b61-4178-b30f-68d5d6422439","domain":"mon-rep","question":"You need to monitor application-specific events every 10 seconds. How can you configure this?","explanation":"you need to configure a custom metric to handle application specific events and if you want to monitor at 10 second intervals, you need to use high-resolution metrics. Detailed monitoring reports metrics at 1 minute intervals.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"CloudWatch Custom Metrics"}],"answers":[{"id":"da902bf148db5983868bf3383162183a","text":"Select high-resolution metrics in CloudWatch","correct":false},{"id":"886f04b8880657a2d07a12c6355639a7","text":"Configure the application to send notifications using SNS every 10 seconds","correct":false},{"id":"f3e15ffbefd8415dda26321a2912dca1","text":"Select detailed monitoring in CloudWatch","correct":false},{"id":"8625460160031630c52a466b7a91f16b","text":"configure a high-resolution custom metric in CloudWatch","correct":true}]},{"id":"17885db5-c61d-4edf-b0e3-e9d449d8e618","domain":"mon-rep","question":"Which of the following EC2 instance metrics are sent to Amazon CloudWatch by default? Select three.","explanation":"CPU utilization, disk I/O and network traffic are visible to the hypervisor running the instance and are sent to CloudWatch by default. For the others, you would need to install CloudWatch Agent on the instance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html","title":"Available CloudWatch Metrics for Your Instances"}],"answers":[{"id":"ec1e54ae04652319df5c011f228c07ac","text":"Free disk space","correct":false},{"id":"2105454033539f83d3b07265aac88d7a","text":"The amount of swap space currently in use","correct":false},{"id":"fb8326e1edbd06b1bf6ea0332e089055","text":"CPU utilization","correct":true},{"id":"c4903df1e41e0ba0b4636e753d8c7661","text":"Disk read and write operations","correct":true},{"id":"613b1188dd73dfdb768f39cfad3cc9a3","text":"Memory utilization","correct":false},{"id":"b4e5bb2b6842990e919682b3d6d5726c","text":"Volume of incoming and outgoing network traffic","correct":true}]},{"id":"08eb44ba-042a-4ced-b833-888d2943bb07","domain":"dep-prov","question":"You are experiencing issues with an Application Load Balancer you have recently set up. You have not changed any of the default logging settings. In this situation, which of the following monitoring tools is the most appropriate to help with troubleshooting client requests?","explanation":"You can use Amazon CloudWatch to retrieve statistics about data points for your load balancers and targets as an ordered set of time-series data, known as metrics. You can use these metrics to verify that your system is performing as expected and they are enabled by default. Access Logs capture detailed information about requests sent to your load balancer but are disabled by default. You can use request tracing to track HTTP requests but this functionality requires that access logging be enabled. You can use AWS CloudTrail to capture detailed information about the calls made to the Elastic Load Balancing API and store them as log files in Amazon S3. You can use these CloudTrail logs to determine which calls were made, the source IP address where the call came from, who made the call, when the call was made, and so on. CloudTrail logs are not appropriate for troubleshooting client requests.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html","title":"Monitor Your Application Load Balancers"}],"answers":[{"id":"6c5c81f47915de5f03d2577e8fae1c34","text":"Access logs","correct":false},{"id":"50ed91980adb1dac23689554eb719277","text":"CloudWatch metrics","correct":true},{"id":"8c6ded942a243b91e65d037ab4e21f7d","text":"CloudTrail logs","correct":false},{"id":"6a292fdb897093b64ef80b39e7db0a4a","text":"Request tracing","correct":false}]},{"id":"f0c663f6-c133-4064-93f1-5a68ec604f86","domain":"networking","question":"A company is developing a software product on AWS. The product requires some dependencies on an external software application developed by another company. In order for the product to run properly, it must connect with the external software that has a configured AWS PrivateLink to run some tasks. Both companies want the connection between the product and the external application to be secured privately and not over the open Internet. How would you configure this connection?","explanation":"An interface VPC endpoint is required to use AWS PrivateLink. In this case, since the external software application has configured a PrivateLink, connecting the interface VPC endpoint to the PrivateLink will provide private connectivity. A VPN connection and Direct Connect are best suited for connectivity between AWS and an on-premise data center. A VPC endpoint is more appropriate in this case. Cross account access would not apply in this case.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html#what-is-privatelink","title":"What Is Amazon VPC?"}],"answers":[{"id":"a295a681bdc68fccdefa0245a0f3cb22","text":"Create a IAM role for the product. Enable cross account access for the product to communicate with the external software application to run its dependencies.","correct":false},{"id":"396ed1524965dd42f15a5584a5df2111","text":"Set up a VPN connection between a gateway endpoint on your VPC and a customer gateway in the external company's VPC. Encrypt the IPSec tunnel to ensure private connectivity.","correct":false},{"id":"125e90bd89dd9a655f80754756185ade","text":"Put the product within a VPC and configure a VPC endpoint for the external application. Use the elastic network interface in the subnet with a private IP address.","correct":true},{"id":"82d6bd6a8ad9b046abc832a10aab46ec","text":"Configure a Direct Connect connection between your software product and the external software application's VPC. Data traversed over this connection will be private.","correct":false}]},{"id":"4672c614-c5ab-464a-9de0-5ef85a8d081e","domain":"automation","question":"You are a SysOps Administrator for your company. The company's CIO was on vacation and didn't know that there was an AWS Region outage during her time off. She returned having no idea of the impact and wants to be alerted the next time an outage occurs whether or not she is on vacation. How would you implement a solution?","explanation":"You can use Amazon CloudWatch Events to detect and react to changes in the status of AWS Personal Health Dashboard (AWS Health) events. Then, based on the rules that you create, CloudWatch Events invokes one or more target actions when an event matches the values that you specify in a rule. Depending on the type of event, you can send notifications, capture event information, take corrective action, initiate events, or take other actions. Creating a Lambda function may be possible but is overly complicated. An AWS Config rule may also work but is not as efficient as using AWS Health directly. Amazon Inspector is used to assess security for applications deployed on EC2 and is not appropriate for this case.","links":[{"url":"https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html","title":"Monitoring AWS Health Events with Amazon CloudWatch Events"}],"answers":[{"id":"0825ebcd89b98147868bbea3888611bd","text":"Use Amazon Inspector to assess service health. Have Amazon Inspector produce reports for you to review and forward these reports to the CIO for those containing outages.","correct":false},{"id":"984716f59e8039a5c670fd67e008a4e7","text":"Create a Lambda function that parses through the AWS Service Health Dashboard to identify outages in certain Regions. Have the Lambda function email the CIO using SES.","correct":false},{"id":"74245b5d3f6507fb9fbf04a80d0edc0a","text":"Send custom text or SMS notifications to the CIO with Amazon SNS when an AWS Health event happens by using Lambda and CloudWatch Events.","correct":true},{"id":"d34ecb714b1da223e3ec16bc80d4193b","text":"Configure an AWS Config rule that checks to see if any Regions are suffering outages. Have the configure trigger a Lambda function that will send an email to the CIO.","correct":false}]},{"id":"85c8beb3-4040-4c16-80a2-28699caea7f7","domain":"mon-rep","question":"Your company is running dozens of EC2 instances. What kind of a solution would give near real-time visualizations of multiple EC2 instance metrics at once?","explanation":"You can gather the necessary metrics together in CloudWatch Dashboards for complete operational visibility.","links":[{"url":"https://aws.amazon.com/cloudwatch/","title":"CloudWatch"}],"answers":[{"id":"1ac4cde26a39fcabf5405c352b7bdff9","text":"Send the metrics to S3 and visualize them with S3 analytics.","correct":false},{"id":"eaa240abb4f0731fca4c2e20cbbbfefe","text":"Add the metrics into a CloudWatch Dashboard.","correct":true},{"id":"af39109a1cd1a96dc8fc03cad521e886","text":"Visualize the metrics with QuickSight.","correct":false},{"id":"beef56bb880b285559c0254491f4d5c9","text":"Organize the metrics into a CloudFront panel.","correct":false}]},{"id":"78ad6dfb-14bd-44bf-a5a4-4af2076aae23","domain":"networking","question":"As your company's lead network administrator, you are helping the development team set up a VPC for an application in their AWS account. The application requires a network configuration such that the web servers of the application have connectivity to the Internet, and the database servers have VPN-only connectivity to the corporate network servers. What VPC set up would support this desired configuration? (Select all that apply)","explanation":"The scenario requires a VPC with an internet gateway, a virtual private gateway, a public subnet, and a VPN-only subnet. One route table has a route to the virtual private gateway in a private subnet. Another route table is explicitly associated with the public subnet. The custom route table has a route to the internet (0.0.0.0/0) through the internet gateway. A NAT instance in another private subnet would not allow Internet connectivity. The Direct Connect connection is unnecessary. The requirements does not allow placing database servers in public subnets.","links":[{"url":"https://docs.aws.amazon.com/en_pv/vpc/latest/userguide/VPC_Route_Tables.html","title":"Route Tables"}],"answers":[{"id":"31d58d613e8532ad19cdb0c73912f36d","text":"Place the database servers in a private subnet. Associate a route table to the private subnet that has a route to a virtual private gateway.","correct":true},{"id":"4d167d5d0ddda70960f354da47b72e33","text":"Place the web servers in a private subnet. Associate a route table to the private subnet that has a route to a NAT instance in another private subnet.","correct":false},{"id":"f8f4044343da77cd7ab9710d8b6f5067","text":"Place the web servers in a public subnet. Associate a route table to the public subnet that has a route to the Internet through the Internet Gateway.","correct":true},{"id":"d7235003c798151a119f9b00b31cce7f","text":"Place the database servers in a public subnet with Direct Connect. Set up a Direct Connect connection to the servers in your on-premises environment.","correct":false},{"id":"ac748fb5daa7fcaff151922fb2c482a4","text":"Place the database servers in a public subnet. Associate a route table to the public subnet that has a route to the Internet through the Internet Gateway.","correct":false}]},{"id":"81ef2ee0-06b2-4d5e-8316-6c4839e8bfc4","domain":"mon-rep","question":"Your organization is running EC2 instances in AWS as well as some VMs in your own datacentre - Both are being monitored by a 3rd party solution running in the non-AWS datacentre but offers an fully functional API that can return statistics and status values. You would like to be notified via SMS of any issues identified by your 3rd party monitoring system if anything goes wrong with either the instances in AWS or the VMs in your DC - Which AWS service can help with this? ","explanation":"AWS SNS is AWS's fully managed push notification system, and can be integrated into 3rd party apps via API, and is therefore the correct answer. SQS is used for creating, managing and using queues and is not relevant by itself here. AWS SMS is actually a server migration service.","links":[{"url":"https://aws.amazon.com/sns/faqs/","title":"AWS SNS"}],"answers":[{"id":"74e06b58e00302916a205d2bf24e9837","text":"AWS SNS","correct":true},{"id":"cdc05958362b09ba911028eaf41c71d5","text":"AWS SQS","correct":false},{"id":"0d681c4f2ebaca40d81399ace225fa7b","text":"None - as the monitoring system is 3rd party and not hosted on AWS it cannot integrate with AWS services","correct":false},{"id":"7b21a7f2969497577b6023f19ba52a9a","text":"AWS SMS","correct":false}]},{"id":"64ee23de-9703-4090-983d-d36552dac361","domain":"networking","question":"You are a SysOps Administrator running security checks throughout your AWS environment. One of your tasks is to clean up the environment and remove any idle resources that are no longer in use. You identify a VPC that was configured and used by a team that no longer works at the company and you are looking to delete the VPC. The VPC has a few running instances, a route table, a NAT Gateway, and an Internet Gateway. When you try to delete the VPC you get an error. How would you troubleshoot this situation?","explanation":"In AWS, you will get the following error if you attempt to delete the VPC with a network interface in-use: 'The VPC contains one or more in-use network interfaces, and cannot be deleted until those network interfaces have been deleted. View in-use network interfaces in the VPC.' Moreover, you cannot delete a subnet that has instances in it. The best answer would be to terminate the instances before deleting the VPC. There is no need to take snapshots before deleting a VPC (unless for backup purpose), and detaching the Internet Gateway is unnecessary as well. Assuming the role of the VPC creator is unnecessary if you already have the proper IAM permissions to do so.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html","title":"VPCs and Subnets"}],"answers":[{"id":"dc869ce95aa68ff14f2265e56e10d33d","text":"Stop and terminate the running instances. Delete all the resources in the VPC before deleting the VPC itself.","correct":true},{"id":"bdba6bd2f5a22439d78f3ae5512c9213","text":"Detach the Internet Gateway from the VPC. Delete the Internet Gateway to restrict public traffic into the VPC. Delete the VPC.","correct":false},{"id":"64ab90b44ef9b90a49072f93bb50a615","text":"Assume the role of the creator of the VPC. The credentials for the VPCs creator is required to delete the VPC.","correct":false},{"id":"4fd6f1d4acf6323fa52621653e2a6150","text":"Take a snapshot of the EBS volumes before deleting the VPC. Upload the snapshots into S3 and proceed deleting the VPC.","correct":false}]},{"id":"4f95f73a-fabd-44b4-a043-0bdee8159a99","domain":"security-comp","question":"You are a SysOps Administrator running security checks throughout your AWS environment. Your AWS account recently launched a new application and you need to ensure access to the application's web servers is restricted to certain ports. How would you implement a policy so that SSH traffic from port 3389 is restricted?","explanation":"The restricted-common-ports checks whether the incoming SSH traffic for the security groups is accessible to the specified ports. The rule is COMPLIANT when the IP addresses of the incoming SSH traffic in the security group are restricted to the specified ports. This rule applies only to IPv4. Amazon Inspector helps to identify security vulnerabilities as well as deviations from security best practices in applications, but not for security groups.","links":[{"url":"https://docs.aws.amazon.com/config/latest/developerguide/restricted-common-ports.html","title":"AWS Config rules"}],"answers":[{"id":"3783cbf28ccda5bf3b6aa20d913bb34e","text":"Set up and activate the restricted-common-ports AWS Config rule.","correct":true},{"id":"3bd73de961085bba965e8589f52a6c08","text":"Install the Amazon Inspector agent on your application to run automated security assessments to identify and restrict any SSH traffic originating from port 3389.","correct":false},{"id":"c2e5ba2c7f9b96b874027fd22e836d7d","text":"Restrict the application to run only on Linux/Unix instances.","correct":false},{"id":"6e274e1e00f14fe4302875974cf5b4c1","text":"Architect your application using the IPv6 communications Internet Protocol.","correct":false}]},{"id":"b91a789f-f8f6-48d3-9117-363c14c95946","domain":"security-comp","question":"A company is migrating its financial systems to AWS. In order to pass audit before go-live a SysOps engineer must provide evidence that the services in use are PCI compliant. How can you obtain the current PCI DSS Attestation of Compliance (AOC)?","explanation":"PCI Attestation documents can be retrieved from any authorised user in AWS accounts by accessing AWS Artifact.  There is no such thing as the AWS Compliance Center or AWS PCI Toolkit. Raising a ticket with Support is not required to access the documents in Artifact.","links":[{"url":"https://docs.aws.amazon.com/en_pv/artifact/latest/ug/getting-started.html","title":"Getting Started with AWS Artifact"}],"answers":[{"id":"a6bdf3896ac02a764afb902ece44813c","text":"Look in AWS Artifact","correct":true},{"id":"d20092bc27d66c67e88d2a342e48ff12","text":"Check the AWS Compliance Center","correct":false},{"id":"7e539fddb94e37fb5f292115f0a24e78","text":"Contact AWS Support","correct":false},{"id":"f5099df9b7b208e79614eb58ece30915","text":"Use the AWS PCI Toolkit","correct":false}]},{"id":"3xre6hrv-j02a-kj8k-nkyn-5951wwipzpzd","domain":"automation","question":"Which service can you use to enable configuration management using Chef or Puppet?","explanation":"OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Config allows you to record and evaluate configuration but doesn't use Chef or Puppet, Systems Manager is an operational insights tool and Athena is used to run SQL queries on data held in S3.","links":[{"url":"https://aws.amazon.com/opsworks/","title":"OpsWorks"}],"answers":[{"id":"8e75b153e61c22a8ea4e14aadc7cb4ee","text":"Systems Manager","correct":false},{"id":"c42aaccedc51aac929c8ae313066f320","text":"OpsWorks","correct":true},{"id":"fa535ffb25e1fd20341652f9be21e06e","text":"Config","correct":false},{"id":"582ca45acfd3e21caca8b786c1413850","text":"Athena","correct":false}]},{"id":"3a6ecc9f-2e62-4807-b2f7-0d4f0e032cc3","domain":"networking","question":"You're configuring an Elastic Load Balancer. What can you do to ensure that a user request always goes to the same server?","explanation":"You can use the sticky session feature (also known as session affinity) to enable the load balancer to bind a user's session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html","title":"Sticky Sessions for Your Classic Load Balancer"}],"answers":[{"id":"fe11d265f3c285c0d795d272a9eff45a","text":"Use Multi-Zone Load Balancing","correct":false},{"id":"4ff5c8a41331ce8a8718cf02c632ff5d","text":"Enable sticky sessions","correct":true},{"id":"55c493073aa412bee5b26fa084e13092","text":"Enable Zonal Failover","correct":false},{"id":"6586c5993a69b3d3955cc5bf228a0792","text":"Enable Connection Draining","correct":false}]},{"id":"386d9e39-b0a3-498b-9157-f845a768869a","domain":"mon-rep","question":"Which of the following are valid EC2 Auto-Scaling instance health statuses?","explanation":"Auto-scaling EC2 instances are either healthy or unhealthy.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/latest/userguide/healthcheck.html","title":"Health Checks for Auto Scaling Instances"}],"answers":[{"id":"a9409139570c30da21e2629ed2d65361","text":"Compromised","correct":false},{"id":"46c4c4d980dfe025ae5b35aa0011dde4","text":"Alarm","correct":false},{"id":"396d45b57c2fbe3318e7b93272a2686b","text":"Healthy","correct":true},{"id":"2b329fc84084b45754e42488aca3114b","text":"Unhealthy","correct":true}]},{"id":"4999e9be-1508-4e8c-bf2f-c663e9e4792b","domain":"security-comp","question":"You are the security administrator for your company tasked with setting up proper IAM permissions for your company's AWS users. Your CFO has tried to create an IAM user for someone in the finance department. The CFO has asked you why she is getting an access denied message when her IAM policy has the following statement:\r {\r\n  \"Version\": \"2012-10-17\",\r\n  \"Statement\": {\r\n    \"Effect\": \"Allow\",\r\n    \"Action\": \"iam:CreateUser\",\r\n    \"Resource\": \"*\"\r\n  }\r\n} \r How would you troubleshoot this issue?","explanation":"Use permissions boundaries to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. The CFO may have a permissions boundary only allowing it to access certain AWS services, not including IAM. Editing the IAM policy to add the CreateRole and CreateGroup API calls would not matter if there is an active permissions boundary. The change in Effect to Deny would have the opposite desired outcome. The resource policy is not relevant as it would have no impact on IAM permissions.","links":[{"url":"https://docs.aws.amazon.com/en_pv/IAM/latest/UserGuide/access_policies_boundaries.html","title":"Permissions Boundaries for IAM Entities"}],"answers":[{"id":"77af352e9767ed879035e1f548900d8a","text":"Change the Effect:Allow to Effect:Deny.","correct":false},{"id":"29cfaa54d99cf8d21527cc4cbbc91dbc","text":"Edit the IAM policy to allow the CreateRole and CreateGroup API calls.","correct":false},{"id":"8c3a9236f32602e84fc9dc7586ec3a2d","text":"Check the permissions boundary to ensure its permissions overlap with permissions in the IAM policy.","correct":true},{"id":"57770d7d7e08acce1c636dc781add121","text":"Check the resource policy of what the IAM User is attempting to access. Make sure there is no DENY actions on the resource policy.","correct":false}]},{"id":"aa05a3d6-0a00-44fb-97e7-cf343aa7ca84","domain":"dep-prov","question":"Your Dev team in Ireland needs an AMI that was created in us-east-1. The Irish Dev team have a copy of the AMI and are attempting to use it to launch instances in eu-west-1, however they are unable to make it work. Which of the following is the most likely cause of the problem?","explanation":"Copying a source AMI results in an identical but distinct target AMI with its own unique identifier. AWS does not copy launch permissions, user-defined tags, or Amazon S3 bucket permissions from the source AMI to the new AMI. After the copy operation is complete, you must apply launch permissions manually.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html","title":"Copying an AMI"}],"answers":[{"id":"dffd2baaa68b8abf554da0ad66c1ce59","text":"The Dev team has failed to manually re-create the launch permissions.","correct":true},{"id":"b3118e4c7922300a27c60efc715a92d6","text":"The Dev team does not have launch permissions to use the AMI to launch instances.","correct":false},{"id":"d81e2ae842a8098be3e06c2c3b5042f5","text":"You cannot use AMIs created by a different account","correct":false},{"id":"b1a80cc35924d3526e2b247428ef3a0b","text":"The US based account which created the AMI has not shared it with the Ireland based account.","correct":false}]},{"id":"wprzr3tn-okej-0h4b-ndf8-ap946jyfvo5r","domain":"automation","question":"Your company has moved to AWS so it can use \"scripted infrastructure\". You would like to apply version control to your infrastructure, so that you can roll back infrastructure to a previous stable version if needed. You would also like to quickly deploy testing and staging environments in multiple regions. What services should you use to achieve this?","explanation":"CloudFormation, plus a version control system such as GitHub, would be the correct choice if the goal was to employ infrastructure-as-code.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html","title":"About CloudFormation"}],"answers":[{"id":"a9b7ffe77958257f25106e8512657c4c","text":"CloudWatch, plus a version control system such as GitHub.","correct":false},{"id":"9c90a68675503a6fe9fa5d118c48fa11","text":"Elastic BeanStalk, plus a version control system such as GitHub.","correct":false},{"id":"0fe9a555daecf33ab759c807a79e3027","text":"Opsworks, plus a version control system such as GitHub.","correct":false},{"id":"3944bd95d89aa956870cc4058a727391","text":"CloudFormation, plus a version control system such as GitHub.","correct":true}]},{"id":"4e911348-5056-4ece-a91d-ce96b619578f","domain":"automation","question":"The DevOps team of an insurance company has been instructed to use CloudFormation to manage the different environments of the company. Due to the size of the templates prepared exceeding the limit, the CloudFormation service rejected the processing of the template. How can the DevOps team resolve this issue?","explanation":"Due to the size of the templates exceeding the limit, dividing the CloudFormation template into smaller subparts is the solution. With this in mind, CloudFormation nested stacks will yield to the same template behavior but will involve different files","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html","title":"Using CloudFormation Nested Stacks"}],"answers":[{"id":"d4838aa62aeb5717c4100906a75dea2d","text":"Use CloudFormation nested stacks","correct":true},{"id":"f163f3aff96f7db10bfa4b93ed9a0a67","text":"Use CloudFormation wait handlers","correct":false},{"id":"1d75dba4b52318bdc075c815a7690e4c","text":"Use CloudFormation custom resources","correct":false},{"id":"e29c7f5b3439e3e8355a1e6bede51fa1","text":"Minify the CloudFormation template","correct":false}]},{"id":"592837f2-71d3-4e62-a250-9318ea373e4f","domain":"security-comp","question":"You are a Cloud Administrator for your company. You have many employees who need to run internal applications that access the company's AWS resources. The employees already have user identities in the company's identity and authentication system, and your CISO doesn't want to create a separate IAM user for each company employee. You've confirmed with your developers that the applications' identity stores are not compatible with SAML 2.0. How would you implement a secure solution for your employees to access AWS resources?","explanation":"If your identity store is not compatible with SAML 2.0, then you can build a custom identity broker application to perform a similar function. The broker application authenticates users, requests temporary credentials for users from AWS, and then provides them to the user to access AWS resources. Creating IAM Users for each employee is not an option. Amazon Cognito is for mobile and web-based application scenarios. A permissions boundary is not a viable solution.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html","title":"Providing Access to Externally Authenticated Users (Identity Federation)"}],"answers":[{"id":"e59f83de64b3549683dca23ce954c76f","text":"Create IAM Users for the employees and group them into IAM Groups for administrative purposes since your identity provider is incompatible with SAML 2.0.","correct":false},{"id":"65e9ecf1544f3bd1ca9f6dce4aef9dcf","text":"Create a permissions boundary around a role that correspondence to the allowed actions for your company's employees. Allow each employee to assume that role. Create a Lambda function that will automatically deny allow if the employee's permissions exceed the boundary.","correct":false},{"id":"b24b7cff8ae14f5cf1fabc8e93a55cec","text":"Build a custom identity broker application to verify that employees are signed into the company's authentication system, then obtains temporary security credentials for the employees.","correct":true},{"id":"04b52fca3576444ec1bdbe890cb9b2c8","text":"Use Amazon Cognito with public identity provider services for your employees. Link your employees' credentials with a third-party IdP that is compatible with OpenID Connect (OIDC) to grant access to AWS resources.","correct":false}]},{"id":"43f0527f-8ffc-49c2-8dd8-f55203c05f15","domain":"data-man","question":"A company has a local data center that stores satellite images and the company plans to migrate the image files to AWS S3 or Glacier. The total amount of data is about 100TB. The local network speed is slow so it is not applicable to transfer the files over the internet. Which of the provided options is the best to migrate the data to AWS?","explanation":"AWS Snowball is a recommended data transport solution that accelerates moving terabytes to petabytes of data to AWS. AWS Transfer for SFTP uses the internet so it is eliminated. VPN also relies on the network connection and it cannot accelerate the data transfer. AWS Storage File Gateway is a storage service to integrate with on-premises server. It is not used to migrate data to AWS.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html#aws-snowball","title":"Migration and Transfer solutions"}],"answers":[{"id":"49088421573ec1c3f93f7588fb78704f","text":"Create an AWS Snowball job and transfer files to the Snowball hardware. After the device is shipped back, AWS is in charge of storing data in S3 or Glacier.","correct":true},{"id":"4713a94f713bbca5a4e46d58447eb1cf","text":"Configure the VPN direct connection from the local data center to AWS VPC. Copy over the files using the high speed intranet.","correct":false},{"id":"d21889436bedfa9ee6bd66e73efdc3f0","text":"Configure the AWS Transfer for SFTP service to seamlessly migrate files to AWS S3 or Glacier.","correct":false},{"id":"8ebed8f00ea6cd51a35c7bfbc4a1b000","text":"Create a high speed AWS Storage File Gateway to map all the local files to S3 or Glacier.","correct":false}]},{"id":"7b724c4c-726d-4da7-9021-59e459427ecd","domain":"networking","question":"Placement Groups can either be of the type 'Cluster', 'Spread', or 'Partition'.  Choose options from below which are only specific to Spread Placement Groups.","explanation":"There is only one answer that is specific to Spread Placement Groups.  Whilst some of these answers are correct for either Cluster Placement Groups only, or for both Cluster and Spread Placement Groups, the question stated that only options specific to Spread Placement Groups should be chosen, which would rule out two options as they are true for both Spread & Cluster type placement groups.  The Logical grouping of instances within a single Availability Zone is only true of Cluster Placement Groups and is also incorrect.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html","title":"Placement Groups"}],"answers":[{"id":"2ce403ef492eb181bff27c45219186ad","text":"A spread placement group is a logical grouping of instances within a single Availability Zone","correct":false},{"id":"bb229a2ce0bd114f2e002f94923618d2","text":"A spread placement group is a group of instances that are each placed on distinct underlying hardware","correct":true},{"id":"5e91fe627c2bd0613378a96fab7f8bc3","text":"Spread placement groups require a name that is unique within your AWS account for the region","correct":false},{"id":"0156cb85707daf221315fb25ac1bb505","text":"An instance can be launched in one placement group at a time and cannot span multiple placement groups.","correct":false}]},{"id":"a4edb410-e5e5-40b4-9335-1da82639c2e3","domain":"networking","question":"You work for an investment bank, supporting a mission critical stock market data processing application running on EC2 and consuming real-time data feeds from your on-premises systems. Your traders are complaining that the system is sometimes very slow to refresh the data and you suspect that this is due to fluctuations in available network bandwidth between AWS and your datacentre. What improvement can you suggest to give users a consistent experience and improve performance for users?","explanation":"AWS Direct Connect is a network service that provides an alternative to using the Internet to connect customer's on premise sites to AWS.","links":[{"url":"https://aws.amazon.com/directconnect/faqs/","title":"Direct Connect FAQs"}],"answers":[{"id":"a60c49fc87050d8b3c698515938d624b","text":"Configure S3 Transfer Acceleration to move the data into AWS much faster","correct":false},{"id":"3225172eff8d104a4744f0ee6f50d836","text":"Configure an additional Elastic IP for each of your application servers to increase the network bandwidth","correct":false},{"id":"35461a11ab55ba8f2b7eb1d3fcc15eff","text":"Scale out your application servers","correct":false},{"id":"5d5c067abd490006c21d11ff221c552a","text":"Configure a Direct Connect connection between your data center and AWS","correct":true}]},{"id":"23e299c8-e80d-4333-b726-4f8ef1e89cfc","domain":"security-comp","question":"You want to restrict who can access a specific bucket that the development team use to store artifacts from their development pipeline. What kind of in-line policy will you need to use to insure this access","explanation":"Resource based policies are inline policies that restrict access to a specific resource, a good example of this is an S3 bucket policy.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html","title":"Policies and Permissions"}],"answers":[{"id":"33b02b9157e2a10758df1edb0fa38865","text":"Service Control Policy","correct":false},{"id":"8e2ddf5878aac8b5d22a6acab856040d","text":"Access Policy","correct":false},{"id":"2d53d9afa60c8a03ddae8baa0cb72fb2","text":"Resource-based Policy","correct":true},{"id":"eeec60437e6675cbd890b5f482628cfc","text":"Identity-based Policy","correct":false}]},{"id":"594aa1e8-45f7-4cc9-a4d9-0241868d6e47","domain":"high-avail","question":"Your company has an application running on Amazon EC2 instances behind an Elastic Load Balancer (ELB). The instances are in an Auto Scaling group that spans multiple Availability Zones. You are a SysOps Administrator and as your check the health of your Auto Scaling group you notice that instances that are failing the Load Balancer health checks are not being replace. How would you remedy this situation?","explanation":"If you attached a load balancer or target group to your Auto Scaling group, you can configure the group to mark an instance as unhealthy when Elastic Load Balancing reports it as OutOfService. If you configure your Auto Scaling group to use the Elastic Load Balancing health checks, Amazon EC2 Auto Scaling determines the health status of the instances by checking both the EC2 status checks and the Elastic Load Balancing health checks. After an instance has been marked unhealthy because of an Amazon EC2 or Elastic Load Balancing health check, it is almost immediately scheduled for replacement. Re-creating an Auto Scaling group or launching new instance types would not solve the issue. HTTPS port 443 will also have no impact.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Checks for Auto Scaling Instances"}],"answers":[{"id":"a26b5f88aff4c163eb6d9cad25ac904e","text":"Ensure that HTTPS port 443 is open on all the security groups attached to these instances.","correct":false},{"id":"c4cb9c892c467fcccbff0e8640bd4d5b","text":"Delete the Auto Scaling group and re-create a new one using the same configuration.","correct":false},{"id":"0cc2395369502a58ef4d3168644555f6","text":"Save the AMIs of the instances and terminate the existing instances. Re-launch the AMIs with new instances under the latest generation instance type (i.e. m4 to m5).","correct":false},{"id":"d28a2bf653d5f98af3ed710e025e87a1","text":"Configure the Auto Scaling group to determine health status using ELB health checks.","correct":true}]},{"id":"b2d3b949-889b-4bbd-8ec9-c65b764c47c3","domain":"mon-rep","question":"You are a SysOps Administrator monitoring a web app that lets users upload high-quality images and use them online. Each image requires resizing and encoding. The images are placed in an Amazon SQS queue for processing by an EC2 instance. It processes the images and then publishes the processed images where they can be viewed by users. When you monitor the EC2 instance you see that the CPU utilization is consistently at 90% and that image processing time is being delayed. The team is looking for a cost-effective solution. What would you recommend?","explanation":"You can use an Auto Scaling group to manage EC2 instances for the purpose of processing messages from an SQS queue. Set a custom metric to send to Amazon CloudWatch that measures the number of messages in the queue per EC2 instance in the Auto Scaling group, and then set a target tracking policy that configures your Auto Scaling group to scale based on the custom metric and a set target value. CloudWatch alarms invoke the scaling policy. Increasing the size of the instance may work but is not a cost-effective solution since Auto Scaling gives you the option to scale down during low demand. Kinesis Data Streams are best suited for real-time data processing, and they have a size limit of 1MB which would be too low for high-quality images. Migrating the data to DynamoDB would not be a viable, let alone cost-effective, solution.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html","title":"Scaling Based on Amazon SQS"}],"answers":[{"id":"52bcbdff61b39eafa67d9496dc77ee09","text":"Migrate the image data into DynamoDB. Attach a role to the instance to be able to access the data from DynamoDB and process the images.","correct":false},{"id":"40fe2365fa7bae167e628c9ef29bd6ca","text":"Move the images into Kinesis Data Streams where you'll be able to process the data in real time.","correct":false},{"id":"c1c44e18b3c0f81b8ec026e9e1ab5b38","text":"Place the instance in an Auto Scaling group. Use CloudWatch metrics to scale out the Auto Scaling group depending on the size of the SQS queue.","correct":true},{"id":"3a781b7e075f07b31a51c98b18d84a2e","text":"Increase the size of the instance and ensure that it is compute-optimized to boost it's capacity to process the images.","correct":false}]},{"id":"0b73a5a1-e9ff-4bf8-8f53-0fb5aa540566","domain":"high-avail","question":"You run a bespoke security application on AWS and have a very limited (but highly valuable) number of customers over the globe. Your application is extremely sensitive and you limit who can access this application. The application sits on a fleet of EC2 webservers in an autoscaling group across multiple availability zones behind an elastic load balancer. Your end customers are investments banks and this application helps to keep one of their extremely sensitive database servers secure. The financial regulations state that these databases should not be internet facing and if they need access to specific internet resources, these resources must be whitelisted on the banks firewalls by a single fixed IP address only. In order to have a single fixed IP address to give to your customers in order to connect to your application, what choice of Elastic Load Balancer should you make to meet this strict security requirement?","explanation":"Only the Network Load Balancer supports the use of a single fixed IP address. The other load balancer offerings do not provide this","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html","title":"Network Load Balancer"}],"answers":[{"id":"b3b5475001f327d331389c6f07ff7c3a","text":"Application Load Balancer","correct":false},{"id":"b4ec634f996fd486030f44e2c5fab630","text":"Classic Load Balancer","correct":false},{"id":"37e242ab2d525505933bbdb47d50d2b9","text":"Route53","correct":false},{"id":"e0f10b949b1cbe40263bfe87c11a2f5d","text":"Network Load Balancer","correct":true}]},{"id":"dc1b5bc1-79d0-49ab-9c2f-e0dea66f0361","domain":"dep-prov","question":"You are helping to migrate a customer from their on-premise data center to AWS. The customer has over 1,000 users in their Active Directory service and wants to be able to using their existing on-premise directory to quickly and easily log into AWS. The customer wants to be able to continue using Microsoft Active Directory with AWS. How would you configure this set up for the customer?","explanation":"AD Connector helps connect your on-premises Microsoft Active Directory to the AWS cloud. AD Connector is designed to give you an easy way to establish a trust relationship between your Active Directory and AWS. With AD Connector, you can streamline identity management by sourcing and managing all your user identities from Active Directory. It also enables you to reuse your existing Active Directory security policies such as password expiration, password history, and account lockout policies. Also, your users will no longer need to remember yet another user name and password combination. SimpleAD does not connect existing on-premises AD to AWS. SimpleAD is a Microsoft Active Directory compatible directory from AWS Directory Service and supports common features of an active directory. AWS Directory Service for Microsoft AD is an AWS managed service that is hosted on the AWS cloud, it does not connect your AD with AWS. Creating IAM users, groups, and roles would not be feasible with 1,000 users and is not best practice.","links":[{"url":"https://docs.aws.amazon.com/quickstart/latest/active-directory-ds/architecture.html","title":"Deployment Scenarios and Architecture"}],"answers":[{"id":"2b121dea9ea29a83bc210acdacd096e4","text":"Use AD Connector to connect the on-premise Microsoft Active Directory to AWS.","correct":true},{"id":"3d15e64aa60139df5f0cd74532823f6f","text":"Create IAM users, groups, and roles based on the current on-premise users to mirror their permissions on AWS.","correct":false},{"id":"63140d266692d56af00a75ac1e583ff5","text":"Use AWS Directory Service for Microsoft AD to connect the on-premise Microsoft Active Directory to AWS.","correct":false},{"id":"59dc56b1ea5b335ecde98e0658573ae3","text":"Use SimpleAD to connect the on-premise Microsoft Active Directory to AWS.","correct":false}]},{"id":"c4f661ee-5ea1-4e69-9440-52bea0321a6a","domain":"mon-rep","question":"You have set CloudWatch billing alarms for your instances running in eu-west-2. However, when you try to access the billing information and alarms, no information is visible. Why might this be?","explanation":"Billing and Alarm data can be accessed only from the us-east-1 region.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/free-tier-alarms.html","title":"Creating a Billing Alarm"}],"answers":[{"id":"fb7a7d16c3f39960e7afde6babd422e1","text":"Billing and Alarm data can be accessed only from the us-east-1 region.","correct":true},{"id":"7f3a3688c3f0cddb24c07255b9d13767","text":"Billing and Alarm data can be accessed only from the us-west-1 region.","correct":false},{"id":"b5ff6f2dfd759f7e70f27d7529e4462b","text":"You need to login as the account owner to see such information.","correct":false},{"id":"cd2c9fa4324b5861276dbbf7f4f593a8","text":"You need to login as the root user to see such information.","correct":false}]},{"id":"b5cf6800-4ce3-4d24-8eaa-a1279c1c6409","domain":"mon-rep","question":"An insurance company has a monolithic application hosted in an EC2 instance and a serverless application hosted in AWS Lambda. After a few months of running the application, the customers have raised multiple delays and performance issues from the applications. The Operations Engineer responsible has mentioned that the latency issues might have been caused by code-level performance issues and the Head of Operations has instructed the team to add code-level monitoring support. How can the team accomplish this?","explanation":"X-Ray can be used for adding code tracing support for both monolithic application code (e.g. a large Django monolithic project) and serverless (Lambda function) code. CloudTrail is used for auditing API call logs. CloudWatch is used for monitoring resource usage and metrics. X-Ray is a distributed tracing system.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html","title":"AWS X-Ray"}],"answers":[{"id":"7d3a7bc0301958a4e3ad624f70b462d4","text":"Use AWS CloudTrail for the monolithic application code. Use AWS X-Ray for the serverless application code.","correct":false},{"id":"359f4dd3d7689aca37514e23a8781431","text":"Use AWS X-Ray for both the monolithic application code and the serverless application code.","correct":true},{"id":"ab066c8d1096ed9b7b49b4637589b201","text":"Use AWS X-Ray for the monolithic application code. Use AWS CloudTrail for the serverless application code.","correct":false},{"id":"2eb097422fa45d569c37d094428d9a9d","text":"Use AWS CloudWatch for the monolithic application code. Use AWS X-Ray for the serverless application code.","correct":false}]},{"id":"a085468f-b362-451e-a53b-2a027f39f4b4","domain":"security-comp","question":"Your team has a new web application in AWS that has customers in different countries. As the service needs to be highly available, it must be well protected from common DDoS attacks such as SYN floods. When the web site is under attack, you can get instant support to assist you in mitigating the issue. Which option is the most suitable to achieve this requirement?","explanation":"When the AWS Shield Advanced feature is activated, you can get support from the DDoS response team. The team helps you to analyze the suspicious activity and fix the issue. AWS Shield Standard or AWS WAF do not have this service. AWS Enterprise Support plan is not cost-efficient. As there is only one web application that needs to be protected from DDoS attacks, AWS Shield Advanced is enough.","links":[{"url":"https://docs.aws.amazon.com/en_pv/waf/latest/developerguide/shield-chapter.html","title":"AWS Shield"}],"answers":[{"id":"2d4b07f7063adc3f7d26742bd531f447","text":"Enable the AWS Enterprise Support plan.","correct":false},{"id":"3bb0eac221e4844f28e6ad4ea5db5f86","text":"Enable AWS WAF rules to protect the application from DDoS attacks.","correct":false},{"id":"2f94e6d78605c4eef8c11c646aab2420","text":"Activate the AWS Shield Advanced feature.","correct":true},{"id":"f3573cee21c8731886863e4c267344fe","text":"Activate the AWS Shield Standard service.","correct":false}]},{"id":"f253314b-765f-471a-a5de-bc5f6095164a","domain":"dep-prov","question":"An application server running in an autoscaling group is terminating and relaunching every few minutes. What is the most likely cause?","explanation":"When an instance is seen to be terminating and relaunching regularly (commonly known as 'flapping' or 'thrashing'), the most likely cause is that the Autoscaling group is marking the instance as unhealthy to trigger a replacement.  This can be caused if the Load Balancer health check has been improperly configured- for instance if a missing security group rules means the ALB cannot perform health checks. or the health check too aggressively marks instances as unhealthy before launch has completed (i.e. before userdata has finished.  Termination of an autoscaling instance when using Spot fleet is common, but to see the regular launch and termination would suggest this is health-check related rather than due to spot price changes.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html#health-check-grace-period","title":"EC2 AutoScaling - Replacing Unhealthy Instances"}],"answers":[{"id":"ef10fce024d97295098b2e7c8b9de34e","text":"The Launch Configuration is using an unsupported AMI for your Availability Zone.","correct":true},{"id":"22b774b395936efd71fe7ddaf41a3f8a","text":"The autoscaling health check is marking the instance as unhealthy before it has time to initalise fully.","correct":false},{"id":"ceb2643dc2123b2807f446e103bad7ff","text":"The price for the EC2 spot instance has increased to above the maximum price for your autoscaling group Launch Configuration.","correct":false},{"id":"18d69cdccb048f832573792064f79345","text":"There is a temporary outage in the AWS Autoscaling service in that region.","correct":false}]},{"id":"fe87b3bd-e952-4f75-9b0e-3583fe879ad6","domain":"data-man","question":"Your manager has informed you that due to compliance issues, all data stored in company S3 buckets must be encrypted as soon as possible.  What is the quickest way to ensure all of this data is encrypted to meet the requirements?","explanation":"The easiest and quickest way to encrypt data in a bucket is to use Server Side Encryption, because Client Side Encryption will encrypt the files before sending to S3 and therefore will only work on newly uploaded files, we can discount any Client Side Encryption options first.  Of the remaining Server Side Encryption options, we can remove any method of managing keys ourselves, as this creates an overhead, so using S3 Managed Keys (SSE-S3) will be the quickest way to encrypt objects in a bucket.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html","title":"Protecting Data Using Server-Side Encryption"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html","title":"Protecting Data Using Client-Side Encryption"}],"answers":[{"id":"1a4d7384bee05f19390352598fbc5fb6","text":"Enable Server-Side Encryption with Customer-Provided Keys on each S3 bucket","correct":false},{"id":"56f875bcb7f1ca2bd789c15f1cdc5c37","text":"Enable Server-Side Encryption with AWS KMS-Managed Keys on each S3 bucket","correct":false},{"id":"d16f34464ccb19afe4b04ca69703c59e","text":"Encrypt new data using AWS KMS–Managed Customer Master Key and add to S3 bucket","correct":false},{"id":"873764e12ed3764f29ba07e9fcdb622a","text":"Enable Server-Side Encryption with S3-Managed Keys on each S3 bucket","correct":true}]},{"id":"5b71bc80-b7ef-4fde-86f0-1a73d1d63e4c","domain":"high-avail","question":"You are an AWS administrator and have set up an Elastic Load Balancer inside a VPC. The ELB spans several Availability Zones. The ELB sits in front of a web application running on Amazon EC2. You notice that incoming traffic is not being evenly distributed across the AZs. How would you solve this issue?","explanation":"Traffic not evenly distributed across the instances in multiple AZs means the traffic is going to only specific EC2 instances. This happens when either the instances which are not receiving the traffic are unhealthy, or the instances that are receiving the traffic are holding on to the session. Since there is no mention of unhealthy instances, disabling sticky sessions on the ELB is the best answer. Increasing the number of subnets and/or instances will not solve the problem as users will remain stuck to the original instance. Increasing the frequency of health checks will have no impact to force even distribution of traffic.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html","title":"Configure Sticky Sessions for Your Classic Load Balancer"}],"answers":[{"id":"d532ac3048a69a5902a79b51bc219bd3","text":"Increase the number of EC2 instances behind the ELB.","correct":false},{"id":"c0d9e44a3e92d6d91864058257e9922c","text":"Add additional subnets within your ELB and configure your ELB to span the new subnets.","correct":false},{"id":"e23eea1cecbc63f7423de00e65f29614","text":"Increase the frequency of the health checks to the EC2 instances running your application.","correct":false},{"id":"b7116d2a43d8462c83e8b93fda085c71","text":"Disable sticky sessions on the ELB.","correct":true}]},{"id":"5d1249a6-960b-4540-8631-50875e04850d","domain":"mon-rep","question":"A company is using a site-to-site AWS VPN connection with static routing to allow connectivity between its corporate office and a VPC in AWS. The SysOps Administrator wants to get notified if the connection goes down. What’s the most effective way to accomplish this?","explanation":"AWS support won't do this for you. The other options would work, however, creating a CloudWatch alarm is the simplest option.","links":[{"url":"https://docs.aws.amazon.com/vpn/latest/s2svpn/monitoring-cloudwatch-vpn.html","title":"Monitoring VPN Tunnels Using Amazon CloudWatch"}],"answers":[{"id":"b9f0b72860ab9c76417cf3f4c3ec6c98","text":"Ask AWS support to monitor the connection and send an SNS notification if necessary.","correct":false},{"id":"759ca4c838619fc5548932dc516de2cf","text":"Create a CloudWatch alarm to track the TunnelState metric and send an SNS notification if necessary.","correct":true},{"id":"4ca2750b50a6661dbeff47ae8524ff24","text":"Set up a cron job in an EC2 instance to confirm the TunnelState metric every minute and send an SNS notification if necessary.","correct":false},{"id":"a05514224184194909112ecdabb8b939","text":"Write a Lambda function to check the TunnelState metric every minute and send an SNS notification if necessary.","correct":false}]},{"id":"7fc0d8c0-5fc9-463e-94ff-54f852f1d819","domain":"data-man","question":"An S3 bucket stores some files for an application. As the files need to be read from users on the internet, the bucket should have Public Access. However when you modify the Bucket Policy to be public in AWS console, the operation is blocked with an \"Access denied\" error. Your IAM user has enough permissions to modify Bucket Policies. How would you troubleshoot the issue?","explanation":"Users can configure S3 buckets to block public access. When users try to enable the public access through Access Control Lists or Bucket Policies, the operation is denied. In this scenario, you should check if the public access through Bucket Policy is blocked.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access.html","title":"Block Public Access to S3 Buckets"}],"answers":[{"id":"2e51a73ed0e5f8cf18b5f4047b358a94","text":"User should configure the public access in S3 Access Control List rather than Bucket Policy. In ACL, enable the read access to the group of Everyone.","correct":false},{"id":"92ffb0eb54566b1970433d74106174e3","text":"Use AWS CLI s3api put-bucket-policy to modify the Bucket Policy.","correct":false},{"id":"ba324e78ebf4d884332673942ca4bee0","text":"Check the bucket public access settings to see if the public access through Bucket Policy is blocked. Make sure the public access is not blocked by the settings.","correct":true},{"id":"416e41b2d06f37e643dd26b55ea9bc53","text":"Check the syntax of Bucket Policy and ensure that the Action field only contains s3:GetObject and the Principal field is a wildcard.","correct":false}]},{"id":"a633d884-fccf-4ab0-bdff-72974f6fe06c","domain":"networking","question":"You have launched an EC2 instance into the public subnet of your custom VPC. The VPC's internet gateway is properly specified in the default route table and the instance's security group allows SSH traffic over port 22. However, you are still unable to SSH into your instance. Which of the following could explain this?","explanation":"To communicate with your instance, it must have either a public IP or an Elastic IP.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html","title":"AWS Elastic IP"}],"answers":[{"id":"62c19bb64f03bcc2aa277034eca8fa21","text":"You have the Internet Gatweway specified as the destination in the Route Table.","correct":false},{"id":"780a0b2b6809da8bfafa8cc49a2d62e3","text":"Your instance doesn't have an Elastic IP.","correct":true},{"id":"0111365d729d1d299458b0a3069b9a25","text":"Your EC2 instance doesn't have a public IP address.","correct":true},{"id":"3376ac0efb861775fb39e3946d485331","text":"The security group isn't properly connected to the Internet Gateway.","correct":false}]},{"id":"y895ku45-wsg2-9rye-087a-zdmu2wd7qtr8","domain":"mon-rep","question":"Which AWS service can be used to log API calls from the AWS console, the EC2 CLI, the AWS CLI, or the AWS SDKs.","explanation":"CloudTrail captures API calls and delivers the log files to an Amazon S3 bucket.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/APIReference/using-cloudtrail.html","title":"Logging API Calls Using AWS CloudTrail"}],"answers":[{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"a8c600be214ced26950e704d39c3ca21","text":"CloudWatch Logs","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":true}]},{"id":"2a16f1cd-530d-4e30-ad08-d89afae34484","domain":"mon-rep","question":"You create a new DynamoDB table with the provisioned read and write capacity units set to 5. The auto scaling feature is enabled for both read and write. And the target utilization is set as 70%. After monitoring the table for some time, you notice that there are two CloudWatch alarms related to the table. The description of one alarm is \"ConsumedWriteCapacityUnits < 150 for 15 datapoints within 15 minutes\". Which action do you need to take to address the alarm?","explanation":"DynamoDB manages the throughput capacities automatically with the auto scaling feature. The alarms are used for the feature and no action is required. There is no need to disable the feature or modify the provisioned capacities.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html","title":"Manage throughput capacity automatically with DynamoDB Auto Scaling"}],"answers":[{"id":"2131aeffdfb01948cc4943432a680327","text":"You need to adjust the provisioned read and write capacities to a higher value such as 10.","correct":false},{"id":"859412aa0d0666ab4c2b8d20707c8f41","text":"They are fake alarms. You can manually delete them from the console.","correct":false},{"id":"3a5208c005673a46313dce67b9e2dd57","text":"You should disable the auto scaling feature for the table.","correct":false},{"id":"252cd5486e4ae9685450c3e9ad208b05","text":"No action is required as the alarms are used for auto scaling for the DynamoDB table.","correct":true}]},{"id":"eab3e224-a279-4865-a746-4595551b4cfb","domain":"security-comp","question":"As a systems administrator, it's your job to grant IAM access to your entire development team as your company transitions to AWS. What's the best strategy in doing this?","explanation":"Instead of defining permissions for individual IAM users, it's usually more convenient to create groups that relate to job functions (administrators, developers, accounting, etc.). Next, define the relevant permissions for each group. Finally, assign IAM users to those groups. All the users in an IAM group inherit the permissions assigned to the group.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#use-groups-for-permissions","title":"Using Groups to Assign IAM Permissions"}],"answers":[{"id":"58d601b0c64e419e78ca3f11a589b34a","text":"Create groups based on the relevant permissions for that job function and assign each user to the appropriate group.","correct":true},{"id":"5ed32311b773a2bfca008d3086a7bd62","text":"Use Active Directory and copy its permissions.","correct":false},{"id":"f070e391c12dea8466645d67cc18ef12","text":"Use the default access provided by your identity provider.","correct":false},{"id":"d07d419ef9a8d5dfbaa087cd534554de","text":"Create IAM access specific to each user's needs.","correct":false}]},{"id":"f94237f9-9b2f-48f3-9a0a-cd4a8350cead","domain":"networking","question":"Your department has made a decision to migrate a number of applications to AWS to reduce operational costs. These applications will need connectivity from your corporate network. Multiple AWS accounts and VPCs within accounts are needed. You currently contract with a single carrier for WAN services, and this carrier is an AWS Direct Connect Partner. Your manager has tasked you with creating a highly reliable networking solution between AWS and the corporate network. Which architecture will provide a highly reliable solution with the best cost efficiency?","explanation":"A single Direct Connect with VPN backup provides highly reliable connectivity between a corporate network and AWS at a lower cost than deploying two Direct Connects. Direct Connect Gateway is not required to achieve cross-account connectivity. Public Virtual Interfaces will not provide connectivity to VPCs. Private Virtual Interfaces are used for that.","links":[{"url":"https://aws.amazon.com/directconnect/","title":"AWS Direct Connect"},{"url":"https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/","title":"Multiple Data Center HA Network Connectivity"}],"answers":[{"id":"49ba7d5b47cab8f1b382c99edd2f513a","text":"Deploy a single AWS Direct Connect through your current carrier. Monitor the Direct Connect connection with Amazon CloudWatch and invoke AWS Lambda to failover traffic to an IPSec VPN tunnel if there are any issues.","correct":true},{"id":"e406e073341c09e80aff0244163394af","text":"Implement two AWS Direct Connects through your current carrier to a single Direct Connect location to be provisioned on redundant Amazon routers. Create Private Virtual Interfaces across the different accounts and VPCs.","correct":false},{"id":"d99e60f7ff5145b088e827b533122d02","text":"Employ a single AWS Direct Connect through your current carrier. Implement a second AWS Direct Connect through another Direct Connect Partner to another Direct Connect location in the same AWS region. Use Direct Connect Gateway to bridge across the multiple accounts.","correct":false},{"id":"d4025e698a2432c8861799040385c885","text":"Employ a single AWS Direct Connect through your current carrier. Implement a second AWS Direct Connect through another Direct Connect Partner to the same Direct Connect location. Create Public Virtual Interfaces across the different accounts and VPCs.","correct":false}]},{"id":"4c7fb750-0ec0-4c9f-8dc6-378122edf97a","domain":"automation","question":"The engineering team of a digital marketing company has a lot of AWS Lambda functions directly created and managed using the AWS Console. The CTO has mandated that the code and the deployments are managed using templates and the code is stored in a code repository to enable proper version control processes. How can the team achieve this?","explanation":"SAM templates and CloudFormation templates can be used to manage the Lambda function code. Out of all the options, only CodeCommit can be used directly as a managed service for a code repository. S3 buckets are not used directly as a code repository.","links":[{"url":"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html","title":"What is SAM"}],"answers":[{"id":"c5fe89ee64d944e98f26b2db3fa7cea2","text":"Use SAM templates to manage the lambda function code. Use CodeCommit for the code repository.","correct":true},{"id":"f33ec453646a2074aa930a61f466fc1a","text":"Use CloudFormation templates to manage the lambda function code. Use ECR for the code repository.","correct":false},{"id":"a85a26cd33fe99adb584452fa780dce2","text":"Use CloudFormation templates to manage the lambda function code. Use S3 buckets for the code repository.","correct":false},{"id":"b670d764967a3aa65a1424279e37291c","text":"Use SAM templates to manage the lambda function code. Use S3 buckets for the code repository.","correct":false}]},{"id":"3b299de7-5eed-4f52-b332-e58a1788d451","domain":"networking","question":"You plan to use a placement group for separating important systems running on a small number of EC2 instances. For compliance reasons, every instance must be in a separate rack. Which of the following placement groups are the most suitable?","explanation":"Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks. Spread placement groups provide access to distinct racks, and are therefore suitable for mixing instance types or launching instances over time. Partition cluster groups are suitable for separating groups of instances on distinct hardware. Cluster placement groups are designed to place hardware as close as possible to increase performance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#concepts-placement-groups","title":"Placement Groups"}],"answers":[{"id":"5b108266549d80fa8e65d875a00a8213","text":"A Spread Placement Group","correct":true},{"id":"e0f55d640e28ee8139799619632e4a68","text":"A Partition Placement Group","correct":false},{"id":"93a08fc1ff6bbf8d8ea03e58b442f724","text":"A Dedicated Instance Placement Group","correct":false},{"id":"ea65fadc86756ddcd18793374fe31349","text":"A Cluster Placement Group","correct":false}]},{"id":"1b616846-7563-44e8-9c4f-0f58740be664","domain":"security-comp","question":"You run a popular media company with offices all over the world. Each one of your customers has their own S3 bucket, but with only 1 AWS account and thousands of buckets this is becoming difficult to manage. You notice that some strange buckets are being created that do not match your traditional naming conventions and you need to get to the bottom of who is provisioning these buckets and why. Which two AWS services can you use to solve this mystery?","explanation":"You can query CloudTrail audit logs stored in S3 using Athena to identify who ran the API calls to create these buckets","links":[{"url":"https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html","title":"Querying CloudTrail with Athena"}],"answers":[{"id":"70a3e0e15c01108addff079f46998fe9","text":"RDS & Inspector","correct":false},{"id":"a71f7a1061cf58f2ff2db3c8a80928e7","text":"CloudTrail & Athena","correct":true},{"id":"630cf5ff1c4c25e530711a6cc08154ec","text":"CloudWatch & Neptune","correct":false},{"id":"d32c3163d9a6bb35cabbe9de082741da","text":"AWS Inspector & AWS Artifact","correct":false}]},{"id":"e05ee44b-cd10-4658-9853-ff5cea9c9d32","domain":"automation","question":"The company has started experiencing deployment issues due to the increasing complexity of the application and the lack of a structured testing and release process. The DevOps team of the company plans to set up a continuous integration pipeline in AWS to improve the stability of the releases and through the enforcement of the use of automated tests. The Head of DevOps has been instructed to use managed services as much as possible to reduce the maintenance overhead of the continuous integration pipeline. How can the DevOps team accomplish this?","explanation":"CodeBuild and CodePipeline are managed services that can be used to easily build a continuous integration pipeline. CodeBuild can run the tests and CodePipeline can manage the pipeline steps for the CI testing and deployment pipeline. Given that managed services are preferred, running Jenkins in an EC2 instance is not the priority option. AppSync is for building GraphQL powered APIs and is not used for continuous integration pipeline requirements.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html","title":"Use CodePipeline with CodeBuild to Test Code and Run Builds"}],"answers":[{"id":"ce1176f044958de744d567d7ac7d0534","text":"Use Jenkins in an EC2 instance and AWS Step Functions for the continuous integration pipeline.","correct":false},{"id":"4c32bcc37cbaab42d77738b8b6875d19","text":"Use AppSync and CodeBuild for the continuous integration pipeline.","correct":false},{"id":"6574dc863e2d0855e66841be7aee54e7","text":"Use CodeBuild and CodePipeline for the continuous integration pipeline.","correct":true},{"id":"8adba880488851800b50283a6de55215","text":"Use Jenkins in an EC2 instance and CodePipeline for the continuous integration pipeline.","correct":false}]},{"id":"4f5f3bf3-a598-458a-acbb-12196bc666e1","domain":"security-comp","question":"You are building an image-sharing application, and all of the images will be stored on S3. One of the requirements is that it cannot be publicly accessible from S3 directly. Instead, you plan to use CloudFront as the global content delivery network. How would you implement this security requirement from preventing users to access images from S3 directly?","explanation":"You want to grant CloudFront exclusive rights to access your S3 bucket. Using an OAI would grant CloudFront exclusive access to the S3 bucket, and prevent other users from accessing the public content of S3 directly via S3 URL. Creating IAM users is not only administratively burdensome but it would also breach the security requirements. Cognito user pools and bucket policies breaches the same requirement. Moreover, creating a bucket policy is unnecessary when you can already use OAI.","links":[{"url":"http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html","title":"Restricting Access to Amazon S3 Content by Using an Origin Access Identity"}],"answers":[{"id":"6bec9539cf1f6ef23ae0bde74626af43","text":"Create an Amazon Cognito user pool for each user and a corresponding S3 bucket. Grant S3 bucket GET requests for each bucket to each Cognito user.","correct":false},{"id":"1395435a5e1cac22caae256b05146faa","text":"Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI.","correct":true},{"id":"effe1117f91832860bec1be56e49c8b0","text":"Create an S3 bucket policy that allows GET actions for the CloudFront distribution ARN as the principal.","correct":false},{"id":"0a9518ebba0c28ede48256e984661d84","text":"Create IAM users for CloudFront. Grant access to the relevant objects in your S3 bucket to each IAM user.","correct":false}]},{"id":"c3482594-a7b1-41c9-9efa-850e0675c155","domain":"dep-prov","question":"You are an AWS administrator for your organization. There is a new company-wide security policy in place the requires all employees to only launch AWS services on a manager-approved asset list. The security policy also outlines that employees may only launch an approved list of instance types. As a administrator, how would you ensure that AWS users comply with the new security policy with the least administrative overhead?","explanation":"With Service Catalog you can administer and manage approved assets by restricting where the product can be launched, the type of instance that can be used, and many other configuration options. The result is a standardized landscape for product provisioning for your entire organization. AWS Config would not mark AWS resources non-compliant; it would not prevent any services from being deployed. CloudTrail logs and CloudWatch alarms are possible but require are an administrative burden.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html","title":"What Is AWS Service Catalog?"}],"answers":[{"id":"7f2c17f2958061067cdadf870d7a5332","text":"Create and manage a portfolio of manager-approved AWS services and instance types with Service Catalog. Allow users to only deploy services they need from the products within your portfolio.","correct":true},{"id":"d715c4d8d3ee87dc1888b3a9fc87e5dc","text":"Deploy AWS Config with a custom managed rule the marks any service not on the manager-approved asset list as non-compliant.","correct":false},{"id":"e19013b2ad37817c9a21b16855501ff8","text":"Create a CloudTrail log for all Regions. Create a Lambda function to parse through CloudTrail logs and will rollback or terminate any service deployments that are not approved.","correct":false},{"id":"3c403961d0fb25abd23e15c2acfca88c","text":"Set a CloudWatch alarm that triggers an SNS topic whenever a non-approved service is deployed. Subscribe the necessary security personnel to the SNS topic to quickly respond.","correct":false}]},{"id":"7e182684-0217-472e-a62a-53d34cbd972c","domain":"dep-prov","question":"You have been asked to decouple an application by utilising SQS.  The application dictates that messages on the queue can be delivered more than once, but must be delivered in the order that they have arrived, and also must allow for efficient, repeated polling of the queue.  Which of the following options are most suitable?","explanation":"This question has two parts which need to be considered, the type of queue and the type of polling.  The question states that messages, \"can be delivered more than once\" but, \"must be delivered in the order that they have arrived\", which means that it can only be a FIFO queue as it is the only SQS type which will deliver messages in order, regardless of how many times the message is delivered.  The question also states that the queue, \"must allow for efficient polling\" and in this case long polling is the most efficient and cost effective option in situations where the queue will be polled constantly.  The correct answer is therefore to configure a FIFO SQS queue with long polling enabled.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html","title":"Amazon SQS FIFO (First-In-First-Out) Queues"},{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html","title":"Amazon SQS Long Polling"}],"answers":[{"id":"8ced375cf136690dd622d3bdc98e16c2","text":"Configure a standard SQS queue and use default polling","correct":false},{"id":"13aac3121a54296c93468234033a5608","text":"Configure a standard SQS queue and use long polling","correct":false},{"id":"a078e24ae55cd5b1c779ffdfdfd8fcf9","text":"Configure a FIFO SQS queue and enable long polling","correct":true},{"id":"2a55c515f67b00a75c46c6d27e2cc2ed","text":"Configure a FIFO SQS queue and enable short polling","correct":false}]},{"id":"8a1eae53-9489-4424-904d-6cf15dc1a0a0","domain":"security-comp","question":"The R&D group at your company has begun developing on AWS. Most applications have a short lifespan due to initiatives either not moving forward or entering a product life cycle in another department. Security policy dictates that AWS IAM roles be used for authentication and authorization of AWS services used by all applications. Policy also requires the removal of IAM roles that have had no activity for sixty days. Which solution will provide the most operationally efficient way to identify roles that don't comply with security policy?","explanation":"The IAM API provides information about when a role has last been used. A Lambda function invoked by an AWS Config custom rule can cycle through IAM roles to identify which are compliant and which are non-compliant. Trusted Advisor doesn't provide a check for inactive IAM roles. CloudWatch Alarm metrics don't exist for IAM role activity. Filtering CloudTrail events will miss non-compliant roles that have no API records written.","links":[{"url":"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules.html","title":"AWS Config Custom Rules"},{"url":"https://aws.amazon.com/blogs/security/continuously-monitor-unused-iam-roles-aws-config/","title":"Continuously monitor unused IAM roles with AWS Config"}],"answers":[{"id":"08d10e1bbf40689f1b82d560dabddbbc","text":"Employ AWS CloudTrail to publish to Amazon CloudWatch Logs. Filter the logs for IAM role activity and publish metrics which will alarm whenever a role hasn't been accessed for sixty days or more.","correct":false},{"id":"d7ec8812d614132d00b53e4d6ae7e537","text":"Regularly invoke a Lambda function to refresh AWS Trusted Advisor security checks. Create Amazon CloudWatch Events rules that monitor the Trusted Advisor checks and send status events to an Amazon Simple Notification Service topic when a role is no longer compliant.","correct":false},{"id":"444fe0024cfc19432a6f4acf80e90d76","text":"Configure an Amazon CloudWatch Alarm to publish metrics to an Amazon Simple Notification Service topic when the role last used date exceeds the sixty-day compliance threshold for role activity.","correct":false},{"id":"2799421eb19492036b61a12bcb8445d4","text":"Create an AWS Config custom rule that invokes an AWS Lambda function to identify IAM roles that have been inactive for sixty days or more. Have the Lambda function write compliance status back to AWS Config.","correct":true}]},{"id":"6c456d08-f571-4a63-833d-ef8c49757ef4","domain":"automation","question":"You have an application running on several Amazon EC2 instances in an Auto Scaling group attached to an Elastic Load Balancer. You check the ELB logs in the S3 bucket and realize that instances that fail the ELB health checks are not being replaced. How would you troubleshoot this issue?","explanation":"The default health checks for an Auto Scaling group are EC2 status checks only. If an instance fails these status checks, the Auto Scaling group considers the instance unhealthy and replaces it. If you've attached one or more load balancers or target groups to your Auto Scaling group, the group does not, by default, consider an instance unhealthy and replace it if it fails the load balancer health checks. To do this, configure the Auto Scaling group to use Elastic Load Balancing health checks. The listener rules determines how your load balancer routes request traffic, and the trace ID traces request through your ELB.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html","title":"Amazon EC2 Auto Scaling"}],"answers":[{"id":"9283050a2c9b87649381de2f1f54a3e7","text":"Configure request tracing on the load balancer to update the X-Amzn-Trace-Id header before sending the request to the Auto Scaling group.","correct":false},{"id":"9bc372b00a1c40e68b34b0d3e9f6edc3","text":"Update the listener rules on the load balancer to allow for health checks on the ELB.","correct":false},{"id":"ba4ba5112eec0a2ca6312a45f8834c76","text":"Check the access logs permissions. Adjust the log permissions to allow ELB to write logs to the S3 logs bucket.","correct":false},{"id":"e5f3d2525407af983596253361c1af26","text":"Configure the Auto Scaling group to use ELB health checks to have the Auto Scaling group replace the instance.","correct":true}]},{"id":"f0c47538-7997-40fc-9c82-27eea818fac2","domain":"security-comp","question":"A company has started running its e-commerce application in container workloads in AWS. The e-commerce application is running its web tier in Amazon ECS and the database tier in RDS all inside a VPC. Under the AWS shared responsibility model, which activities is AWS NOT responsible for?","explanation":"AWS takes care of the underlying software for managed services. For services such as EC2, the instance hypervisor and underlying hardware are managed and maintained by AWS. It is the customer's responsibility to monitor and manage the memory utilization of the containers in services such as ECS and EKS.","links":[{"url":"https://aws.amazon.com/compliance/shared-responsibility-model/","title":"Shared Responsibility Model"}],"answers":[{"id":"ac7ca331a57c32d9975616bf91327c82","text":"Monitoring and managing the memory utilization of the containers","correct":true},{"id":"b7a8057898c923210f320a3bcccbcbca","text":"Maintaining the underlying hardware infrastructure of the instances used by ECS","correct":false},{"id":"9abfc73643ea9090379aae3cd89400d1","text":"Patching the instance hypervisor","correct":false},{"id":"1dfb8644f2cce2dca687a152b45b7d54","text":"Patching the database instance software of RDS","correct":false}]},{"id":"0c3624fc-3f1d-4663-9593-d1b228afb36b","domain":"mon-rep","question":"An organisation has been notified of an issue with their website loading slowly.  On investigation your autoscaling group has been scaled down to two servers, when it usually operates with two. Your CTO wants you to get to the bottom of this as quickly as possible. Which service in the Console will help you understand where the reconfiguration came from?","explanation":"AWS Config logs all changes to your configuration on a timeline, and it also allows you to retrace the steps via CloudTrail to see associated events with the configuration changes. In this case you could check the autoscaling group in AWS Config and would be able to see exactly when the number of servers was changed, and who performed the change.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/TrackingChanges.html","title":"Tracking Configuration Changes with AWS Config"}],"answers":[{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":true},{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false},{"id":"7c90c8f2a24f3a1a28525f19fb2c75ab","text":"AWS Inspector","correct":false},{"id":"a8c600be214ced26950e704d39c3ca21","text":"CloudWatch Logs","correct":false}]},{"id":"bd76b805-8568-4083-a962-3eebb610d4fc","domain":"dep-prov","question":"You've been given the responsibility for creating an automated deployment approach to provision EC2 instances for every application in your department's portfolio. Each application requires a different set of software libraries, and all EC2 instances are members of Auto Scaling Groups. Deployments must complete within a limited time window as part of the department's overall DevOps strategy, and AWS CloudFormation will be used for infrastructure-as-code. Which architecture will provide for the fastest deployments in the most operationally efficient manner?","explanation":"Creating an AMI at the time of CloudFormation stack creation or update helps to ensure that the latest binaries are included in an Auto Scaling Launch Configuration while avoiding potentially long waits for Auto Scaling instances to be provisioned. Bootstrapping software downloads during Auto Scaling instance launch will potentially result in long wait times. Using CloudFormation metadata to indicate which software binaries need to be used in Auto Scaling Launch Configurations is not supported functionality. Not automatically keeping the AMIs up to date means that a significant level of effort will be needed to keep AMIs current with new software releases in each EC2 instance configuration.","links":[{"url":"https://aws.amazon.com/blogs/infrastructure-and-automation/enable-fast-bootstrapping-of-your-auto-scaled-instances-using-dynamically-created-images/","title":"Speed up instance bootstrapping by using dynamically created images"}],"answers":[{"id":"d53fd253be12fb9ac6c1f9d04b0590f3","text":"Create base Amazon Machine Images for each application's EC2 configuration. Use CloudFormation to specify that the Auto Scaling Launch Configuration use the base AMIs, and bootstrap software downloads from public sources for each application at the time of instance launch","correct":false},{"id":"02bf56251e14a50de6e835ecd38ab66a","text":"Create complete Amazon Machine Images for each application's EC2 configuration. Configure a CloudFormation Stack for each application that creates Auto Scaling Groups for each instances corresponding AMI","correct":false},{"id":"ff704c3738fe0246d98c17f8a497e999","text":"Create an EC2 instance for each application's full software configuration. For each application, have CloudFormation invoke an AWS Lambda function that creates an Amazon Machine Image for the EC2 instance. Include the AMI in the Auto Scaling Group Launch Configuration specified by CloudFormation","correct":true},{"id":"df623f37652f3a2b4441a7c1de625ab8","text":"Create base Amazon Machine Images for each application's EC2 configuration. Use CloudFormation to specify that the Auto Scaling Launch Configuration use the base AMIs, and use CloudFormation metadata to indicate which software binaries need to be used in Auto Scaling Launch Configurations during CloudFormation stack creations and updates","correct":false}]},{"id":"bc7cd5cf-6c7b-4f45-bb7c-5700451bd9aa","domain":"data-man","question":"You are managing an S3 bucket that contains business critical objects for your operations department. You are tasked with optimizing the storage costs of the bucket. You've identified 130 objects in the bucket that need to be available but can be recreated by your department. What would you do to optimize your costs?","explanation":"Amazon recommends using One Zone-IA if you can recreate the data if the Availability Zone fails, and for object replicas when setting cross-region replication (CRR). Use Standard-IA for your primary or only copy of data that can't be recreated. S3 Standard would not be a cost-effective solution, and S3 Glacier does not provide millisecond retrieval times and would not fulfill the availability requirement.","links":[{"url":"https://docs.aws.amazon.com/en_pv/AmazonS3/latest/dev/storage-class-intro.html","title":"Amazon S3 Storage Class"}],"answers":[{"id":"daeec30860ad314818a31cb7ea5ba05e","text":"Set the storage class to S3 Standard","correct":false},{"id":"9b7e1e30e477deece7a8b6d9eecd9331","text":"Set the storage class to S3 Standard-IA","correct":false},{"id":"901f665e98aabd9853d4e0f9d3031ac2","text":"Set the storage class to S3 Glacier","correct":false},{"id":"98c96ddb15112194ea3d2c44c1addd3b","text":"Set the storage class to S3 One Zone-IA","correct":true}]},{"id":"8dade21b-c268-4ccd-af0a-18c01cd2c638","domain":"automation","question":"You check the last bill of your AWS account and find that the storage of EBS snapshots charges a lot. A large number of EBS snapshots are very old and can be deleted. You want to keep 10 snapshots for an EBS volume and old snapshots are deleted automatically. The strategy should also help you to create a snapshot every 24 hours. Which is the best way of implementing this strategy?","explanation":"The Amazon EBS Snapshot Lifecycle can automate the creation, retention, and deletion of EBS snapshots. You only need to configure a lifecycle policy in the Lifecycle Manager. You do not need to configure a Lambda Function or a Cron job in an EC2 instance to implement the same policy.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html","title":"Automating the Amazon EBS Snapshot Lifecycle"}],"answers":[{"id":"6e23ea86e4cf4e8929aa5b846a7e5ad4","text":"Create a Snapshot Lifecycle Policy to automatically create new snapshots and delete old snapshots.","correct":true},{"id":"630f293528bb790c603b2f77b266a56d","text":"Use a Cron job that runs in an T2.micro EC2 instance. The job creates a new snapshot and deletes the old one every day.","correct":false},{"id":"ac159fa50438475f44d25f96b6a2bea4","text":"Configure a Lambda Function that runs every 24 hours to create a snapshot and delete the old snapshot.","correct":false},{"id":"e711537d471732823f103b6f63ab96c2","text":"Configure a Cloudwatch Event rule to execute every 24 hours. The target is a Lambda Function that creates a new snapshot and deletes the old one.","correct":false}]},{"id":"d3ba2fd5-b275-4485-a2c3-12c264fdd9ff","domain":"dep-prov","question":"You are trying to SSH into your EC2 instance and you get a \"Permission denied (publickey)\" error. Which of the following are the most likely causes of this error?","explanation":"If you connect to your instance using SSH and get any of the following errors, \"Host key not found in [directory]\", \"Permission denied (publickey)\", or \"Authentication failed, permission denied\", verify that you are connecting with the appropriate user name for your AMI *and* that you have specified the proper private key (.pem) file for your instance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html","title":"Troubleshooting: Connecting to Your Instance"}],"answers":[{"id":"51ba58459b06a9c8a6e1d38f220f5d65","text":"You have supplied an invalid or otherwise improper private key (.pem) file.","correct":true},{"id":"eb07107df739e65e6a8f465acdc3a501","text":"You have provided an incorrect username for your AMI type.","correct":true},{"id":"99e7bf54cfff54e73272a9034be64f6c","text":"There is an issue with the AWS infrastructure.","correct":false},{"id":"334acb0dcf7f1f56bc449066c8cab4b6","text":"The instance's security group is misconfigured.","correct":false}]},{"id":"67817330-582a-439c-878c-4f490334cfad","domain":"high-avail","question":"The web development team of an chatbot machine learning startup has migrated their on-premise application to AWS. The on-premise application uses a custom load balancer which was replaced by an Application Load Balancer in the new architecture setup in AWS. The customers have reported that their chat sessions are lost from time to time and they are forced to sign in again. The new architecture setup uses Route 53, Application Load Balancer, EC2 instances, and DynamoDB for the web application tier. How can the team resolve this issue?","explanation":"Instead of disabling sticky sessions, enabling sticky sessions in the Application Load Balancer would solve the requirement of having an EC2 instance stick to an existing session similar to the scenario provided. Replacing DynamoDB with RDS instances would not solve the stickiness issues. Stickiness is not handled by Route53 routing policies. Replacing the Application Load Balancer with a Classic Load Balancer would not solve the stickiness issue.","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/","title":"ELB Features"}],"answers":[{"id":"7a83fce81dc2bab71c72499c7076e2d4","text":"Replace DynamoDB with RDS instances.","correct":false},{"id":"ca8c07a031edb56f0a44f9d891ebaa8a","text":"Enable sticky sessions in the Application Load Balancer.","correct":true},{"id":"93cb25a136afcc3b5e255daab59500f5","text":"Enable sticky sessions in the Route 53 routing policy.","correct":false},{"id":"ebb04e35ab1c5519bfe7dc876f414406","text":"Replace the Application Load Balancer with a Classic Load Balancer.","correct":false}]},{"id":"466da628-4a01-43da-ba82-90215f555b37","domain":"data-man","question":"A mobile application which runs its backend data storage and processing in AWS experienced an outage last night.  According to AWS there was scheduled maintenance on a number of EC2 instances in your VPC.  You should have caught this but you did not.. Your company recently invested in Business Support for your Production account.  As a SysOps engineer you have been asked to ensure that planned maintenance on AWS services that affect you, as well as known outages in the region, are logged into your IT department ticketing system.  The notifications should be in a human-readable format.  Which services will help you to do this in an efficient manner?","explanation":"AWS Health is an API service which provides programmatic access to AWS health events for Business and Enterprise Support customers.  CloudWatch Events allows you to intercept AWS Health events, and has the ability to convert a Health event into human-readable format by performing transforms on the properties of the Health event.  AWS SNS allows the transformed events to be delivered to a variety of destinations including HTTP endpoints, or via email in order to integrate into your ticketing systems.","links":[{"url":"https://docs.aws.amazon.com/health/latest/ug/what-is-aws-health.html","title":"What is AWS Health?"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-Input-Transformer-Tutorial.html","title":"Tutorial: Use Input Transformer to Customize What is Passed to the Event Target"}],"answers":[{"id":"f9118d24e7ab0d46eccea1c43f5db58d","text":"AWS Personal Health Dashboard, CloudTrail and AWS Lambda","correct":false},{"id":"cb52f85bdee5c92ebdded1c692c43685","text":"AWS System Status, CloudTrail and SES","correct":false},{"id":"c500e0084c698c714d39fa45e73c194f","text":"AWS Health, CloudTrail and AWS Simple Notification Service","correct":false},{"id":"ab911b3e904a12c5461f69cab52931d4","text":"AWS Health, CloudWatch Events and AWS Simple Notification Service","correct":true}]},{"id":"8baa3a0f-c625-4a68-ae80-fa674af87b4c","domain":"data-man","question":"You need to establish an AWS backup and archiving strategy for your company. For compliance reasons, documents should be immediately accessible for 3 months and available for 5 years. Which of the following combinations of AWS services fulfills these requirements in the most cost effective way?","explanation":"A Lifecycle Policy will allow you to store immediately-accessible data in S3, then push that data to Glacier.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html","title":"Creating a Lifecycle Policy for an S3 Bucket"}],"answers":[{"id":"105948c34db1ced35fd86df5595195f0","text":"Use StorageGateway to move data to S3, then use lifecycle policies to move the data into Redshift for long-term archiving.","correct":false},{"id":"dfb026df2e51307a98e329f13e8ed31f","text":"Upload the data to an EBS volume, then use lifecycle policies to move the EBS snapshots you take to S3.","correct":false},{"id":"e5752808e486aa8696e865ddbbdc4a17","text":"First, use DirectConnect to upload data to S3. Then, use IAM policies to move the data into Glacier for long-term archiving.","correct":false},{"id":"d79dc0ab36fd5adac011a343358d0acf","text":"Upload data to S3 and use lifecycle policies to move the aging data into Glacier for long-time archiving.","correct":true}]},{"id":"7432af70-890b-411b-abbb-6b0b403aa3f5","domain":"mon-rep","question":"You're an ops manager and you want one of your AWS accounts to receive the price breaks associated with Consolidated Billing for Organizations. What needs to happen for you to be included in the Organization and receive the discounted pricing?","explanation":"You can invite existing AWS accounts to join your organization. When you start this process, AWS Organizations sends an invitation to the account owner, who then decides whether to accept or decline the invitation. You can use the AWS Organizations console to initiate and manage invitations that you send to other accounts. You can send an invitation to another account only from the master account of your organization.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html","title":"Inviting an AWS Account to Join Your Organization"}],"answers":[{"id":"8a358ee9ae18997a0c91f0541fad6f77","text":"You must request to be added to consolidated billing.","correct":false},{"id":"5f46574bd37f7ef74f5a95c1debbad29","text":"The Master account must ask AWS to associate your account with theirs.","correct":false},{"id":"3b0f522229b67ca4ea9eb21840d96e41","text":"The Master account must send you an invitation to join your account with theirs and you must accept.","correct":true},{"id":"8cacf7f83f0c9a58c2e264de726c764f","text":"You must ask AWS to associate your account with the Master account.","correct":false}]},{"id":"de2610da-0b20-4258-b215-146e96c134d3","domain":"automation","question":"Which section of a CloudFormation template allows you to set up differing instance types based on environment type (e.g. 'Production' or 'QA')?","explanation":"","links":[{"url":"http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"CloudFormation Template Anatomy"}],"answers":[{"id":"229eb04083e06f419f9ac494329f957d","text":"Conditions","correct":true},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false},{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":false}]},{"id":"571b9603-45b6-45b1-aa47-68849ac814bb","domain":"automation","question":"A big-box retailer runs their in-store point-of-sale system on EC2 linux instances. All of the infrastructure is managed as part of a CloudFormation stack. The web servers are part of an Auto Scaling Group. The application only needs to be available during business hours from 9:00am until 6:00pm. What would be the best way to scale the web servers cost efficiently based on demand?","explanation":"Authoring the CloudFormation template to include an AutoScaling:ScheduledAction resource to increase the Auto Scaling Group's MinSize and MaxSize values at 9:00am, and another AutoScaling:ScheduledAction resource to decrease the Auto Scaling Group's MinSize and MaxSize values at 6:00pm will save costs for the retailer during non-business hours. CloudFormation conditions control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update, but don't control the actions of an Auto Scaling Group. Using an Auto Scaling Group scheduled action provides more streamlined automation than using a Lambda function. CloudFormation mappings are key/value pairs that can be used to specify conditional parameter values, but they have no impact on the Auto Scaling Group unless they are used to create a resource.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html","title":"What is AWS CloudFormation?"},{"url":"https://s3-us-west-2.amazonaws.com/cloudformation-templates-us-west-2/AutoScalingScheduledAction.template","title":"Cloud Formation Sample Template for Time-based Auto Scaling"}],"answers":[{"id":"0db6203397832efddcbca893f96ba1b6","text":"Include AutoScaling:ScheduledAction resources in the CloudFormation template that change Maxsize, MinSize, and Recurrence values based on business hours","correct":true},{"id":"a0f99978bb65910b10b75a32e1b92a2a","text":"Create AutoScaling:ScheduledAction conditions in the CloudFormation template that change Maxsize and MinSize values based on business hours","correct":false},{"id":"725933a9f7ea7ef281e2decf6cefadae","text":"Configure AutoScaling:ScheduledAction mappings in the CloudFormation template with Maxsize, MinSize, and Recurrence values based on business hours","correct":false},{"id":"765dccb8332e2036eb70a0b6276e7285","text":"Use CloudWatch Events to trigger a Lambda function at business opening and closing that adjusts the Auto Scaling Group's MinSize and MaxSize accordingly","correct":false}]},{"id":"e2dd07ca-6d80-4c5b-87ce-95bd37a03b28","domain":"dep-prov","question":"You are helping to design a multi-instance storage solution for volume data. Your CTO believes utilizing Amazon EBS volumes is the best solution but has come to you for advice. What would you suggest?","explanation":"Amazon EBS volumes can only be attached to one EC2 instance at a time. If you need multiple EC2 instances accessing volume data at the same time, consider using Amazon EFS as a file system. Taking a snapshot and attaching new volumes from the snapshot to every instance would be an administrative burden. S3 is not used for volume data.","links":[{"url":"https://d0.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf","title":"AWS Storage Services Overview"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html","title":"Amazon EFS: How It Works"}],"answers":[{"id":"03c2ae52e51575a97a45797cd779317b","text":"Use Amazon S3. Attach an IAM role to all instances granting access to the S3 bucket hosting the data.","correct":false},{"id":"43fe6d9ea40b98964da1097e42fb0c54","text":"Use Amazon EBS volumes. Attach the same EBS volume to all instances.","correct":false},{"id":"e417bdbb8845ac556e877cff73411a3a","text":"Use Amazon EFS. Mount EFS file system on instances within a VPC.","correct":true},{"id":"b7e0cfad15004b2f664c81878bfd1091","text":"Use Amazon EBS volumes. Take a snapshot of the volume. Create a new volume from the snapshot and attach to all instances.","correct":false}]},{"id":"762031fb-db4e-4e09-bab4-09bf0fc50472","domain":"high-avail","question":"You run a very popular fashion blog and during a major event your wordpress site struggles immensely with the amount of traffic that you are receiving. The wordpress site sits across a fleet of EC2 instances in an autoscaling group which scales based on CPU utilization. You notice from your CloudWatch metrics that your webservers appear fine, however your back end Aurora database is running at 100% CPU Utilization. What can you do to alleviate the situation?","explanation":"Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html","title":"Aurora Read Replicas"}],"answers":[{"id":"98a15d02b2c6ead5dff3036e99269acd","text":"Place the Aurora database in to a the same Autoscaling group as the EC2 instances and configure the launch configuration to deploy new Aurora instances whenever a node capacity reaches 80% for 5 minutes","correct":false},{"id":"e38c819297b357216e3ddc63cecd22ec","text":"Migrate from Aurora to MySQL RDS instance with multi-AZ turned on to better handle the load","correct":false},{"id":"10239931e046b099c14736b5b9c422d0","text":" Place the Aurora database in to a separate Autoscaling group and configure the launch configuration to deploy new Aurora instances whenever a node capacity reaches 80% for 5 minutes","correct":false},{"id":"f663aa878eb8286dbadf97aa0f95be56","text":"Add additional Aurora Read Replica Nodes and update your connection string on the webservers to point to the new nodes to spread the load","correct":true}]},{"id":"78cce7a4-ca99-425a-9b76-cc6d6dea1ce3","domain":"mon-rep","question":"For which of the following would you need to create a custom metric in order to monitor it in CloudWatch?","explanation":"By default, CloudWatch will provide metrics on Network, Disk and CPU. You will need to use a custom metric if you want to gather memory metrics","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html","title":"Monitoring Memory in CloudWatch"}],"answers":[{"id":"4789f23283b3a61f858b641a1bef19a3","text":"Memory","correct":true},{"id":"eec89088ee408b80387155272b113256","text":"Network","correct":false},{"id":"380dbc8d9d2c8a17f6ebb0b2c62d3e85","text":"Disk","correct":false},{"id":"2b55387dd066c5bac646ac61543d152d","text":"CPU","correct":false}]}]}}}}
