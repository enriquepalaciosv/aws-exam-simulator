{"data":{"createNewExamAttempt":{"attempt":{"id":"dcacc691-ad8a-4d77-b7c7-671c4c93927e"},"exam":{"id":"2725332e-cc13-447e-988f-21d218ac4f06","title":"AWS Certified DevOps Engineer - Professional 2019","duration":10800,"totalQuestions":75,"questions":[{"id":"cae0814f-fa70-443b-9c43-1cca04205b54","domain":"SDLCAutomation","question":"Your senior developer wants to be able to access any past version of the binaries that are being built as part of your CI/CD pipeline. You are using CodeBuild with CodePipeline to automate your build process. How will you achieve this?","explanation":"CodeBuild has an optional 'Namespace type', which will insert the build ID into the path to the build output zip file or folder, giving you a unique directory and binary artifact for each build that runs.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/create-project.html","title":"Create a Build Project in CodeBuild"}],"answers":[{"id":"8c3218ccd025dc9444babadf01808d86","text":"Nothing needs to change, by default all artifacts are given a unique filename in S3.","correct":false},{"id":"c52ea2b58f7db8902570649989645ace","text":"Implement a CodeBuild lambda trigger which will copy each build artifact to S3 with a unique ID.","correct":false},{"id":"4eed75febec27e9d78dc57a1fbbc70f8","text":"Change the artifact namespace type to Build ID, which will insert the build ID into path of the output folder in S3.","correct":true},{"id":"baa367f7cda7f088cdd1fbab6a72d39b","text":"Change the artifacts packaging to zip, which will append a version number to each build.","correct":false}]},{"id":"5e50532a-5d17-4a6f-a367-db3eb2c87698","domain":"SDLCAutomation","question":"Your organization has been using CodePipeline to deploy software for a few months now and it has been smoothly for the majority of releases, but when something breaks during the build process it requires lots of man hours to determine what the problem is, roll back the deployment and fix it. This is frustrating both management and your customers. Your supervisor would like to assign one developer to test the build works successfully before your CodePipeline proceeds to the deploy stage so you don't encounter this issue again. How would you implement this?","explanation":"CodePipeline allows for manual approval steps to be implemented for exactly this reason","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html","title":"Add a Manual Approval Action to a Pipeline in CodePipeline"}],"answers":[{"id":"adac0bfeeb378f849ee1f8575b660f83","text":"Create a test deploy stage as well as a manual approval stage in CodePipeline. Once the assigned developer checks the testing deploy worked, they can authorize the pipeline to continue and deploy to production.","correct":true},{"id":"ca05b5864508a85819911de3b31a9c02","text":"Ask the assigned developer to run a local build first to test all changes, and then commit it to the repository which is then deployed to production only when they know there are no errors.","correct":false},{"id":"fb383cd098c54e0f94598a5029e3c99e","text":"Configure SES to email the assigned developer when CodePipeline has deployed to production. This will provide an immediate notification so they can check if there are any errors in testing. Then they can push the changes to the production git branch.","correct":false},{"id":"61692576c930a7b4f4bb412881fa1829","text":"Configure SQS to email the assigned developer when CodePipeline has deployed to production. This will provide an immediate notification so they can check if there are any errors in testing. Then they can push the changes to the production git branch.","correct":false}]},{"id":"276f651e-fe53-4841-8545-38c00909e9c5","domain":"PoliciesStandards","question":"After the runaway success of their first website, ACatGuru is building out its IT services to handle its tremendous growth. The CEO of ACatGuru would like to centrally manage all of its commonly deployed IT services, while still giving its employees access to deploy only the services defined by company policy allows. For example, only a developer can spin up a test stack, only a database administrator can spin up a database, and so on. There should also be restrictions on which products can be launched, with the ability to create a custom user interface for you users so they can avoid the complex AWS Management Console. Which AWS service is ideal for implementing these requirements and which features are required?","explanation":"AWS Service Catalog enables organizations to create catalogs of what can be deployed, and create custom interfaces allowing users to deploy them. They are organised as Products which are added to Portfolios to which users can be given access.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/dg/what-is-service-catalog.html","title":"What Is AWS Service Catalog? - AWS Service Catalog"}],"answers":[{"id":"526775c1622cd0e7b703eb8d4a83d657","text":"AWS Service Catalog","correct":true},{"id":"b8f528bf9294fdda9450ad077dfcfc66","text":"A Placement","correct":false},{"id":"725903c711beec9056a0f2891a402263","text":"A Project","correct":false},{"id":"132976a248c1f5f10f99b4aa7d7fe9d1","text":"AWS System Catalog","correct":false},{"id":"96564dfa4633550e1df29d07c3dba125","text":"A Portfolio","correct":true},{"id":"b1820ee1cf68e2e65f263ff7bb207626","text":"AWS Organizations","correct":false},{"id":"3ac62db74ae690ff5ba0cdff8b04efe0","text":"A Product","correct":true}]},{"id":"b21d657a-8e37-4f2b-9641-6e37ff00825c","domain":"SDLCAutomation","question":"Your manager wants to implement a CI/CD pipeline for your new cloud-native project using AWS services, and would like you to ensure that it is performing the best automated tests that it can. He would like fast and cheap testing, where bugs can be fixed quickly. He suggests starting with individual units of your software and wants you to test each one, ensuring they perform how they are designed to perform. What kind of tests do you suggest implementing, and what part of your CI/CD pipeline will you implement them with?","explanation":"Unit tests are built to test individual units of your software and quickly identify bugs. These can be implemented with CodeBuild.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing CI/CD on AWS"}],"answers":[{"id":"437e248b28463ad899879dda97bea983","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some unit tests for the existing code base and ensure that your developers produce component tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeCommit","correct":false},{"id":"962f2ea0d522b0054011f4e7935fd81e","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some unit tests for the existing code base and ensure that your developers produce unit tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeBuild","correct":true},{"id":"673e10c0ae89a727cede17ac866f1ae4","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build source tests for current code base and ensure that your developers produce source tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeCommit","correct":false},{"id":"464e499150ad3f7270dbfb985c67bcb0","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some compliance tests for current code base and ensure that your developers produce component tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeBuild","correct":false}]},{"id":"50424d57-02c2-4af2-84ff-10d935637fc6","domain":"IncidentEventResponse","question":"Your company has been producing Internet-enabled Microwave Ovens for two years.  These ovens are constantly sending streaming data back to an on-premises endpoint behind which sit multiple Kafka servers ingesting this data.  The latest Microwave has sold more than expected and your Manager wants to move to Kinesis Data Streams in AWS, in order to make use of its elastic capabilities.  A small team has deployed a Proof of Concept system but are finding throughput lower than expected, they have asked for your advice on how they could put data on the streams quicker.  What information can you give to the team to improve write performance?","explanation":"You should always use the Kinesis Producer Library (KPL) when writing code for Kinesis where possible, due the Performance Benefits, Monitoring and Asynchronous performance.  You should choose any answers which include KPL as a solution.  You should also check all relevant Service Limits to ensure no throttling is occurring.  Only three of these options are shown in the question, but there are many more possibilities.  The remaining options will work, but generally will give slower performance.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-producers.html","title":"Troubleshooting Amazon Kinesis Data Streams Producers"}],"answers":[{"id":"c6a098ae823197ee09ed9de745ce2cc8","text":"Use a Small Producer with the Kinesis Producer Library, but using the PutRecords operation.","correct":true},{"id":"19d55c732ada47508097e33c6e04a120","text":"Check the GetShardIterator, CreateStream and DescribeStream Service Limits.","correct":true},{"id":"95940bef689407d24dcd08ee0da18ca5","text":"Develop code using the SDK to put data onto the streams with the Kinesis Data Streams API.","correct":false},{"id":"8309467932d5a97196aca6a575a318af","text":"Develop code using the Kinesis Producer Library to put data onto the streams.","correct":true},{"id":"7080b30b6deaa847a5edb074ea4be9d8","text":"Use a Large Producer without the Kinesis Producer Library, but using the PutRecord operation.","correct":false}]},{"id":"9fa6cf61-bd26-4e40-b82b-b21521aaf0fb","domain":"MonitoringLogging","question":"You are developing an augmented reality mobile game. Initially, you want to launch this in California but plan to expand rapidly to other areas if the pilot runs successfully. For operational purposes, you need to keep track of the number of players in each area and decided to use a custom CloudWatch metric called 'Population' for this. To enable a basic level of analytics, you also want to capture more than a dozen mobile device specific values such as battery level and network signal strength. During the pilot, you specify a data collection frequency of 6 times per minute. How could you achieve this? Select all correct statements.","explanation":"Namespace names must contain valid XML characters. Possible characters are: alphanumeric characters (0-9A-Za-z), period (.), hyphen (-), underscore (_), forward slash (/), hash (#), and colon (:). CloudWatch does not aggregate across dimensions for your custom metrics and you can only assign up to 10 dimensions to a metric.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html","title":"Amazon CloudWatch Concepts: Dimensions"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"Publishing Custom Metrics"}],"answers":[{"id":"e6e3cc57cfb5d4c561244425173d7ecd","text":"For the initial launch, create a separate CloudWatch namespace for your 'Population' metric for each individual area such as 'NewEGamesLtd-ARGame>MountainView'.","correct":false},{"id":"61c6ab75e1284129ae6c91b415dc5326","text":"Assign CloudWatch dimensions to your 'Population' metric that specify the player's current area, i.e. Country=USA, State=California, City=MountainView, etc. At any time, you can then get the total number of active players on any geographical level by aggregating across the relevant dimensions.","correct":false},{"id":"d30cb8988c9d57590ddfb5479583eac6","text":"If you choose to use one or more custom high-resolution CloudWatch metrics for your analytics requirements, your data points with a period of less than 60 seconds are available for only 3 hours. After that, the data is still available but is aggregated together for long-term storage with a 1-minute resolution. After 15 days the data is aggregated again and is retrievable only with a resolution of 5 minutes.","correct":true},{"id":"d6192d0e2db11ba00651e36dc06b15df","text":"Standard resolution is for data having a one-minute granularity. When you publish a high-resolution custom metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds. If you choose to use one or more custom CloudWatch metrics for your analytics requirements, define these as high-resolution.","correct":true},{"id":"864ca9273b06458b60886b3cef9bdd16","text":"Every 10 seconds, make a 'PutMetricData' call from within the app to publish the data to a new 'Analytics' CloudWatch custom metric. Specify the individual device values as separate dimensions, e.g. --dimensions BatteryLevel=42,SignalStrength=72, etc.","correct":false}]},{"id":"eef970b2-0cae-40bb-a9bc-a2fedbd2bc02","domain":"ConfigMgmtandInfraCode","question":"Your manager wants you to look into including and provisioning a new authentication service in your CloudFormation templates. Generally this would be simple, but the authentication service he has signed a 3 year deal with is not an AWS service at all, which could make things difficult. The authentication service has an API you can use, which will hopefully ease the pain of solving this problem. How do you solve it?","explanation":"This is an excellent use case for a CloudFormation custom resource","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html","title":"Custom Resources"}],"answers":[{"id":"ccb593ba89acf906fb6a643256d93f44","text":"Use the API to provision the service with an AWS Lambda function. Use a CloudFormation custom resource to trigger the Lambda function.","correct":true},{"id":"6200195512a7053624054d12f6d6d0c7","text":"Provision the authentication service externally to your AWS resources. You will have to maintain them in parallel.","correct":false},{"id":"2bbfc07a49181a40ef8a9ecaefe0c6f9","text":"Use the API to provision the service with an AWS Lambda function. Use a CloudFormation wait conditions to trigger the Lambda function during the provisioning process.","correct":false},{"id":"8c7dd37aaf9be0d28c45046384cb9056","text":"Use the API to provision the service with a python script using the boto framework. Run the python script with a CloudWatch Events schedule.","correct":false}]},{"id":"6e4406f4-c446-4f58-9533-e04e5b45eb0d","domain":"ConfigMgmtandInfraCode","question":"After many years of running co-located servers, your company has decided to move their services into AWS.  The prime reasons for doing this is to scale elastically and to define their new infrastructure as code using CloudFormation.  Your colleagues have been defining infrastructure in a test account but now they are ready to deploy into production, they are identifying some problems.  They have have attempted to deploy the main CloudFormation Template but they are seeing the following errors; 'Invalid Value or Unsupported Resource Property', 'Resource Failed to Stabilize' and when these errors are encountered, the stack fails to rollback cleanly.  Assist your team by choosing the best ways to troubleshoot these problems.","explanation":"Most of these answers are valid troubleshooting solutions for various CloudFormation issues, but there is only one answer for the problems listed in the question. 'Invalid Value or Unsupported Resource Property' errors appear only if there is a parameter naming mistake or that the property names are unsupported. 'Resource Failed to Stabilize' errors appear because a timeout is exceeded, the AWS service isn't available or is interrupted.  Finally, any update which fails to rollback could be because of a number of reasons, but the most popular is due to the deployment account having permissions to create stacks, but not to modify or delete stacks. The answer which includes all of these pieces of advice is correct.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html","title":"Troubleshooting AWS CloudFormation"}],"answers":[{"id":"9f3b9793c5af6485de0b1b166a4e36be","text":"Ensure that your AMI has the CloudFormation helper scripts installed and that your VPC has a defined route to the Internet.  Also ensure that the user deploying the CloudFormation stack has enough permissions to modify resources.","correct":false},{"id":"c271e5866ef0c9376f5675822c410c2f","text":"Check that resources exist when specified as parameters, ensure that there is no maintenance being undertaken on any of the defined AWS services and that the user deploying the CloudFormation stack has enough permissions to modify resources.","correct":true},{"id":"7114f5881fc7b95daef2e696e207bac9","text":"Check that a DependsOn attribute has been included in the Template, and that you are not waiting for a resource which usually takes longer than the default timeout period. Finally, ensure that the user deploying the CloudFormation stack has enough permissions to create resources.","correct":false},{"id":"a577e14c499181ee450e6306c9e57820","text":"Ensure that you do not have termination protection enabled, that a DependsOn attribute has been included in the Template and that there is no maintenance being undertaken on any of the defined AWS services.","correct":false}]},{"id":"b8240606-8f74-4b39-ad43-d8374d4b275d","domain":"MonitoringLogging","question":"Your organization has been using AWS for 12 months. Currently, you store all of your custom metrics in CloudWatch. Per company policy, you must retain your metrics for 3 years before it is OK to discard or delete them. Is CloudWatch suitable?","explanation":"CloudWatch will retain metrics for 15 months, after which they will expire. If you need to keep metrics for longer you must pull them out of CloudWatch using their API and store them in a database somewhere else.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html","title":"CloudWatch Concepts"}],"answers":[{"id":"8faab65d1ba3ce5e0cb52e471493dd0b","text":"No, CloudWatch only retains metrics for 24 months. You will have to use the API to pull all metrics and store them somewhere else.","correct":false},{"id":"cf00bc1c445d1daf231db593c6f998d9","text":"Yes, CloudWatch will retain custom metrics for 5 years.","correct":false},{"id":"d829ad2942daa681d42ad4237f50827c","text":"No, CloudWatch only retains metrics for 15 months. You will have to use the API to pull all metrics and store them somewhere else.","correct":true},{"id":"47ecaf50014e4dd77522488586ae375a","text":"Yes, CloudWatch will retain custom metrics for 3 years.","correct":false}]},{"id":"ec0b5238-f2d5-437c-9d06-6d0011f2ae8c","domain":"IncidentEventResponse","question":"Your organisation has dozens of AWS accounts owned and run by different teams and paying for their own usage directly in their account.  In a recent cost review it was noticed that your teams are all using on-demand instances. Your CTO wants to take advantage of any pricing benefits available to the business from AWS. Another issue that keeps arising involves authentication. It's difficult for your developers to use and maintain their logins all of their accounts, and it's also difficult for you to control what they have access to. What's the simplest solution which will solve both issues?","explanation":"AWS Single Sign-On allows you to centrally manage all of your AWS accounts managed through AWS Organizations, and it will also allow you to control access permissions based on common job functions and security requirements.","links":[{"url":"https://aws.amazon.com/single-sign-on/","title":"AWS Single Sign-On."}],"answers":[{"id":"e31c64ffefb763cb85b2e44f4a6f7bb6","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Raise a Support request with Amazon in order to link the accounts for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Use LDAP to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":false},{"id":"87c78562df56f4879a6773d20c92b308","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Create a Billing account for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Use AWS Single Sign-On to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":true},{"id":"76f5be58299aec4cdb3dc958b35a5f6a","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Create a Billing account for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Use group-based authentication policies in IAM to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":false},{"id":"398b5351a11512e388505258eb119be1","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Create a Billing account for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Switch to role-based authentication policies in IAM to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":false}]},{"id":"dad35c13-8f81-4aac-a7b9-c2ed4a5f9335","domain":"HAFTDR","question":"With your company moving more internal services into AWS, your colleagues have started to complain about using different credentials to access different applications. Your team has started to plan to implement AWS SSO, connected to the corporate Active Directory system, but are struggling to implement a working solution.  Which of the following are not valid troubleshooting steps to confirm that SSO is enabled and working?","explanation":"The question states which are NOT valid troubleshooting steps, so we need to choose the ones which will not help us troubleshoot the issues. Firstly, you can use the User Principal Name (UPN) or the DOMAIN\\UserName format to authenticate with AD, but you can't use the UPN format if you have two-step verification and Context-aware verification enabled. Secondly, AWS Organisations and the AWS Managed Microsoft AD must be in the same account and the same region.  The answers which suggest the opposite are the ones which should be chosen.  The other answers are correct troubleshooting steps and therefore can not be chosen.","links":[{"url":"https://docs.aws.amazon.com/singlesignon/latest/userguide/prereqs.html","title":"AWS SSO PreRequisites"},{"url":"https://docs.aws.amazon.com/singlesignon/latest/userguide/troubleshooting.html","title":"Troubleshooting AWS SSO Issues"}],"answers":[{"id":"ff24f8f9a780de8999f077e354ae2eef","text":"To allow authentication using a User Principal Name, enable two-step verification in Context-aware verification mode.","correct":true},{"id":"eb8551b7f4293f316685247af6fbe823","text":"Implement AWS Organisations with 'All Features' enabled, deploy the AD Connector residing in your master account.","correct":false},{"id":"3627bfa9871db5cdd41006b26ec5dbbe","text":"Implement AWS Organisations and deploy AWS Managed Microsoft AD in two separate accounts.  It does not matter which regions they are deployed in.","correct":true},{"id":"fe88b553bb4a39e8268dcb243738d505","text":"AWS SSO with Active Directory only allows authentication using the DOMAIN\\UserName format.","correct":true},{"id":"0fa6f9fa1ae19cce6cead12014268484","text":"Ensure the number of AWS SSO permission sets are less than 500 and you have no more than 1500 AD groups.","correct":false}]},{"id":"97a8a78b-a6d1-4bf7-aaff-a59191881a65","domain":"IncidentEventResponse","question":"Your company runs a popular website for selling cars and its userbase is growing quickly. It's currently sitting on on-premises hardware (IIS web servers and SQL Server backend.)  Your managers would like to make the final push into the cloud. AWS has been chosen, and you need to make use of services that will scale well into the future. Your site is tracking all ad clicks that your customers purchase to sell their cars. The ad impressions must be then consumed by the internal billing system and then be pushed to an Amazon Redshift data warehouse for analysis. Which AWS services will help you get your website up and running in the cloud, and will assist with the consumption and aggregation of data once you go live?","explanation":"Amazon Kinesis Data Firehose is used to reliably load streaming data into data lakes, data stores and analytics tools like Amazon Redshift. Process the incoming data from Firehose with Kinesis Data Analytics in order to provide real-time dashboarding of website activity.","links":[{"url":"https://aws.amazon.com/kinesis/data-firehose/","title":"Streaming Data Firehose - Amazon Kinesis - AWS"},{"url":"https://aws.amazon.com/solutions/real-time-web-analytics-with-kinesis/","title":"Real-Time Web Analytics with Kinesis | AWS Solutions"}],"answers":[{"id":"3e05395168d2e92f31097dc8eaadfb28","text":"Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create a CloudTrail stream to push the data to Amazon Redshift warehouse for analysis.","correct":false},{"id":"ee0e1b18c96ba916c7d61c5f11a173b3","text":"Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.","correct":true},{"id":"5a744ec834a3bed2345133430519b13d","text":"Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Streamer to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.","correct":false},{"id":"ed27547c1d649b6ee7f4c6106499701b","text":"Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Athena Data Stream to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.","correct":false},{"id":"029dca01b35ce55b6a3218b46cb66d8f","text":"Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Aggregator to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.","correct":false}]},{"id":"9f7ba9e2-8030-4333-acf6-e2d27613e959","domain":"IncidentEventResponse","question":"You have an idea regarding your AWS account security. You would like to monitor your account for any possible attacks against your resources, such as port scans or brute force SSH and RDP attacks. If anything is detected, you want the report pushed to a Slack channel where anyone in your company can monitor and take action if it's their responsibility. How do you go about implementing this?","explanation":"Amazon GuardDuty is the best way to implement this. It can trigger Lambda functions on events which can be used to post to a Slack channel.","links":[{"url":"https://aws.amazon.com/guardduty/","title":"Amazon GuardDuty"}],"answers":[{"id":"e91bba567af0e13b250c7976157789ae","text":"Implement a Lambda function to monitor your VPC Flow Logs. For any odd requests, post the information to your Slack channel.","correct":false},{"id":"74245776eaafb5659f52d1c243b22462","text":"Implement a Lambda function to monitor your CloudTrail file. For any odd API calls or requests, post the information to your Slack channel","correct":false},{"id":"a594a49c80d0416db98a60a3f66adc4e","text":"Implement Amazon GuardDuty. On detected events, trigger a Lambda function to post the information to your Slack channel.","correct":true},{"id":"2b3ed3e5fd2a948326831b73fe30bd4a","text":"Implement AWS Inspector. On detected events, trigger a Lambda function to post the information to your Slack channel.","correct":false}]},{"id":"0917c160-74a4-439b-818b-248197d8fd54","domain":"IncidentEventResponse","question":"Your organization has a few million text documents in S3 that are stored in a somewhat random manner, and the amount of files is always growing. The developer that initially wrote the system in use stored everything with a random file name with some attempt at security through obscurity. Now your CEO and CFO both need to be able to search the contents of these documents, and they want to be able to do so quickly at a reasonable cost. What managed AWS services can assist with implementing a solution for your CEO and CFO, and what would the setup process involve?","explanation":"CloudSearch by itself is enough to fulfill the requirements put forward here. CloudSearch is managed, scalable can very quick to configure and get online. In comparison it would take some time to set up EC2 and install ElasticSearch or any other search tool, and would be much more difficult to scale. This involves creating a search domain and configuring the index as required, and then setting up the access policies.","links":[{"url":"https://aws.amazon.com/cloudsearch/","title":"AWS | Amazon CloudSearch - Search Service in the Cloud"}],"answers":[{"id":"d912e8309bb58d3483307e1ea428bf8a","text":"Set up access policies","correct":true},{"id":"a34f38b0356fb4da2ce3a29a66b4b415","text":"Configure your baseline","correct":false},{"id":"562f9538310fcd7c846445ad9768ab7c","text":"Implement Amazon CloudSearch","correct":true},{"id":"280209643a35ef346f22e051eafca06e","text":"Create a search domain","correct":true},{"id":"8f9d7ad9c2c6d20b41851ce8e3572aaa","text":"Configure your index","correct":true},{"id":"705d488d75db0b38eb9811a38a863d57","text":"Implement MongoDB","correct":false},{"id":"38b79d58ed1b5a50d6bbc91173d0c745","text":"Create a search index","correct":false},{"id":"e26c5d07fc30848902a99c95897441b4","text":"Set up IAM roles","correct":false},{"id":"ab25d01b99c512f0cb03129afc920a07","text":"Implement ElasticSearch","correct":false}]},{"id":"10e64485-5c7d-44b1-adcd-23a1b1b60e7d","domain":"IncidentEventResponse","question":"Your organization is building millions of IoT devices that will track temperature and humidity readings in offices around the world. The data is then used to make automated decisions about ideal air conditioning settings based on that data, and then trigger some API calls to control the units. Currently, the software to accept the IoT data and make the decisions and API calls runs across a fleet of autoscaled EC2 instances. After just a few thousand IoT devices in production, you're noticing the EC2 instances are beginning to struggle and there's just too many being spun up by your autoscaler. If this continues you're going to hit your account service limits and costs will blow out way more than you budgeted for. How can you redesign this service to be more cost effective, more efficient and most importantly, scalable?","explanation":"In this instance, Kinesis Data Analytics with the data streamed with Kinesis Data Streams is the best choice. It's also completely serverless so you are able to save costs by shutting down your EC2 servers.","links":[{"url":"https://aws.amazon.com/kinesis/data-analytics/","title":"Amazon Kinesis Data Analytics"}],"answers":[{"id":"7af4964635eff76f7316e3605f75adaa","text":"Switch to CloudWatch Data Analytics. Stream the IoT data through Kinesis Data Streams and perform your decision making and API triggering with Lambda.","correct":false},{"id":"932e73a0f9bcccb0cb228111d6e91632","text":"Switch to Kinesis Data Analytics. Stream the IoT data with Kinesis Data Streams and perform your decision making and API triggering with Lambda. Shut down your EC2 fleet.","correct":true},{"id":"ed5129b5b4a876e31ca0fe3383fa6b9e","text":"Switch to Kinesis Firehose Analytics. Stream the IoT data directly, perform your decision making and then store the results in S3. Analyze the S3 content to trigger your API calls with your EC2 fleet. Change the autoscaling metric to use the Firehose capacity.","correct":false},{"id":"9e06f33f0b3d84a2d27b1e028103a035","text":"Switch to Kinesis Data Firehose. Stream the IoT data directly, perform your decision making and then store the results in S3. Analyze the S3 content to trigger your API calls with your EC2 fleet. Change the autoscaling metric to use the Firehose capacity.","correct":false}]},{"id":"97b89b27-99f8-4cb9-88f3-9002eacb7713","domain":"MonitoringLogging","question":"You are part of a team working on an house valuation service that aggregates data via APIs from different external providers. Your Solution Architect specified that AWS Secrets Manager is to be used for the storage of all third-party API keys, client IDs and other secrets. One particular API requires two customer specific values - each of these is 1024 bytes long. Which of the following answer is correct?","explanation":"Secrets can be database credentials, passwords, third-party API keys, and even arbitrary text. The maximum length of a secret 7168 bytes.","links":[{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html","title":"What Is AWS Secrets Manager?"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/reference_limits.html","title":"Limits of AWS Secrets Manager"}],"answers":[{"id":"8c641d9b7e20b71b0bc352b7ed6f68dc","text":"Secrets must not exceed 256 characters in length.","correct":false},{"id":"42946ed8adcfa463fd89ecdf2afed049","text":"You will need to store the two values separately as two secrets and combine them programmatically.","correct":false},{"id":"147c08f6dcfd373071697491fcf3b414","text":"AWS Secrets Manager supports the management of database credentials (username and password) only. Max. length is 512 characters for each field.","correct":false},{"id":"45f37012d1d473b2ab3bc21659f75224","text":"You can store up to 7168 bytes in each secret.","correct":true}]},{"id":"9e3970ae-d3c6-4365-bd1d-ece6971ce4a0","domain":"ConfigMgmtandInfraCode","question":"Which of the following AWS services allow native encryption of data, while at rest?","explanation":"EBS, S3 and EFS all allow the user to configure encryption at rest using either the AWS Key Management Service (KMS) or, in some cases, using customer provided keys.  The exception on the list is ElastiCache for Memcached which does not offer a native encryption service, although ElastiCache for Redis does.","links":[{"url":"https://aws.amazon.com/ebs/faqs/","title":"Amazon EBS FAQs"},{"url":"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html","title":"Comparing Memcached and Redis"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html","title":"Protecting Data Using Server-Side Encryption"},{"url":"https://aws.amazon.com/efs/faq/","title":"Amazon EFS FAQs"}],"answers":[{"id":"1e26da0c1f6b77f6ef2ad9a3b8cf5a98","text":"ElastiCache for Memcached","correct":false},{"id":"4b222d59a012c5cc128037a5a473cde7","text":"Elastic Block Store (EBS)","correct":true},{"id":"e9880f1e01aac341f6553ec8a6a7e622","text":"Elastic File System (EFS)","correct":true},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true}]},{"id":"141e78ef-c6e5-4575-baf2-489bb1f3b7f8","domain":"SDLCAutomation","question":"Your e-commerce platform sees tremendous spikes in transaction volume during new product launches. Each new product event requires that iterative code changes be made during pre-launch preparation based on marketing department feedback and testing results. Slow response times for users have occurred during previous launches, so leadership has directed that load tests be conducted on the e-commerce platform after each new code release. Load testing requires significant time to manually setup and execute, which slows down the development process. You've been tasked with creating an automated load testing service on AWS to shorten testing windows. Which architecture will you recommend to provide scalable load testing capabilities?","explanation":"Simulating thousands of connected users generating a select number of transactions per second can be done without having to provision servers. A test automation framework Docker image deployed on AWS Fargate can be scaled according to user-specified test scenarios. The test scenarios can be created in a web application, and written to S3 and DynamoDB by a Lambda function front-ended by API Gateway. These test scenarios can then be read by another Lambda function to manage Fargate's container scaling to generate the desired load on the e-commerce application. Hosting the web application on Elastic Beanstalk is overkill since test scenarios will probably be entered by a single user for only a short duration of time. S3 configured for static web hosting is a more economical option. Auto Scaling EC2 instances won't generate load according to the specified test scenarios - there's no way to get those scenarios into scaling policies. Running the test automation framework in a Lambda function will risk exceeding Lambda's maximum execution time. AWS Device Farm is used for functionality testing across different browsers and mobile devices, not load testing.","links":[{"url":"https://aws.amazon.com/fargate/","title":"AWS Fargate"},{"url":"https://aws.amazon.com/solutions/distributed-load-testing-on-aws/?did=sl_card&trk=sl_card","title":"Distributed Load Testing on AWS"}],"answers":[{"id":"2fe1750d28696e75b6795c06d03d883c","text":"Deploy a web application for creating test scenarios. Host the web application on an Amazon S3 bucket configured for static web hosting. Have the web application call Amazon API Gateway APIs which route requests to an AWS Lambda function. Have the Lambda function send test scenario parameters to AWS Device Farm in a JSON file to generate the desired load on the e-commerce application. Store test results in S3 and log output in Amazon CloudWatch.","correct":false},{"id":"2916315feefab0d67d74c91444be1fcf","text":"Build a web application for entering test scenarios. Host the web application on AWS Elastic Beanstalk. Have the web application call Amazon API Gateway APIs which route requests to an AWS Lambda function. Have the Lambda function use a test automation framework to generate load to the e-commerce application according to test scenario criteria. Store test results in S3 and log output in Amazon CloudWatch.","correct":false},{"id":"18ad3b56af5e9d5741987077fd861aca","text":"Implement a web application for creating test scenarios. Host the web application on an Amazon S3 bucket configured for static web hosting. Have the web application call Amazon API Gateway APIs which route requests to AWS Lambda functions that store test scenarios on S3 and DynamoDB. Use a test automation framework to generate load to the e-commerce application. Deploy the test automation framework as a Docker image to Amazon Elastic Container Service. Use another Lambda function to scale AWS Fargate containers according to test scenario criteria. Store test results in S3 and log output in Amazon CloudWatch.","correct":true},{"id":"b8d86dc44ae5d62bf8966ca89db964b3","text":"Create a web application for entering test scenarios. Host the web application on AWS Elastic Beanstalk. Have the web application call Amazon API Gateway APIs which route requests to AWS Lambda functions that store test scenarios on S3 and DynamoDB. Use a test automation framework to generate load to the e-commerce application. Deploy the test automation framework as an Amazon Machine Image (AMI) for use on EC2 instances with Auto Scaling. Use another Lambda function to provision the initial number of EC2 instances according to test scenario criteria. Store test results in S3 and log output in Amazon CloudWatch.","correct":false}]},{"id":"ad3ca55a-a29e-4c0a-8204-2eb07854ddf2","domain":"IncidentEventResponse","question":"Your company's suite of web applications have just been overhauled to prevent some security issues and memory leaks that were slowing them all down significantly. It's working a lot more efficiently now, though your developers are still on the lookout for any network security issues the application might be leaving exposed. A weekly scan of ports reachable from outside the VPC would be beneficial. Which AWS service will you use to implement this with minimal additional overhead to the existing CI/CD process, and how will you configure it?","explanation":"Amazon Inspector is an automated security assessment service which will allow you to improve the security and compliance of your applications. A network configuration analysis checks for any ports reachable from outside the VPC. The agent is not required for this.","links":[{"url":"https://aws.amazon.com/inspector/","title":"Amazon Inspector - Amazon Web Services (AWS)"}],"answers":[{"id":"27ec757b9636ff05cdc67b897d2e0b9e","text":"Implement Amazon X-ray.","correct":false},{"id":"85ffa705a973908e8e02e22ca035a00a","text":"Implement Amazon GuardDuty.","correct":false},{"id":"69ef6b28f0e5a5c3d28ccda2f3197473","text":"Implement Amazon Inspector.","correct":true},{"id":"4d72a45ae986fba394b6ac15e0822b91","text":"Implement Amazon Macie.","correct":false},{"id":"de2ce6aca6fc8725c0f4d32837d0371f","text":"Install agent.","correct":false},{"id":"b5dabb6bbcab4c4cb776e9cfed03b42e","text":"Configure Host Assessments.","correct":false},{"id":"5263e79a1693336f8cffea30b57f14c4","text":"Configure Network Assessments.","correct":true}]},{"id":"90ad13a4-87b3-4afb-9f86-6dd9f234f8ff","domain":"MonitoringLogging","question":"Your organisation would like to implement autoscaling of your servers, so you can spin up new servers during times of high demand and remove servers during the quiet times. Your application load mainly comes from memory usage, and so you have chosen that as your scaling metric. What do you have to do next?","explanation":"Memory utilization is not a default CloudWatch metric, you will have to publish a custom metric first.","links":[{"url":"https://aws.amazon.com/autoscaling/","title":"AWS Auto Scaling"}],"answers":[{"id":"228ffd56d2bbca049ca1bb76c788e09f","text":"Configure the Auto Scaling Group with a target tracking scaling policy, use the default metric type 'Average Memory Utilization'","correct":false},{"id":"19f84b354da6659ed799e7df04b00825","text":"Configure the Auto Scaling Group to use the default metric type 'Average Memory Utilization'","correct":false},{"id":"e49d82811a3d3e790284aadeeb45eccd","text":"Publish a custom memory utilization metric to CloudWatch, as there isn't one by default.","correct":true},{"id":"e83f40180c161cdc0807d8b69b0b85ee","text":"Configure the Auto Scaling Group with scaling policy with steps, use the default metric type 'Average Memory Utilization'","correct":false}]},{"id":"00ff039c-bad1-448c-8d17-e6ad8b1491f6","domain":"SDLCAutomation","question":"You manage a team of developers who currently push all of their code into AWS CodeCommit, and then CodePipeline automatically builds and deploys the application. You think it would be useful if everyone received an email when a pull request is created or updated. How would you achieve this in the simplest way?","explanation":"Notifications in the CodeCommit console is the simplest way to implement this requirement. Triggers only trigger when someone pushes to a repository, not when a pull request is created.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-repository-email.html","title":"Configuring Notifications for Events in an AWS CodeCommit Repository"}],"answers":[{"id":"d5af6b2263080d905efb836b99bf8b19","text":"Enable notifications in the CodePipeline console. Create a notification stage after your source stage and select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.","correct":false},{"id":"439fdcb8b7e144ddacda7b944536f82b","text":"Enable triggers in the CloudCommit console. Select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.","correct":false},{"id":"ee242f6e3c35045661e6306dbd2e0153","text":"Enable notifications in the CodeCommit console. Select 'Pull request update events' as the event type and choose or create a new SNS topic for the notifications. Subscribe everyone to the SNS topic.","correct":true},{"id":"15c5b59dd67cddfeb10567fef856e7b1","text":"Enable triggers in the CodePipeline console. Create a trigger stage after your source stage and select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.","correct":false}]},{"id":"dae6c09f-c2e7-4b01-b0ec-95ed07a7bab2","domain":"HAFTDR","question":"Due to some recent performance issues, you have been asked to move your existing Product Information System to Amazon Aurora.  The database uses the InnoDB storage engine, with new products being added regularly throughout the day. As the database is read heavy, the decision has also been made to add a Read Replica during the migration process.  The changeover completes successfully, but after a few hours you notice that lag starts to appear between the Read/Write Master and the Read Replica.  What actions could you carry out to reduce this lag?","explanation":"One of the most obvious causes of Replication lag between two Aurora databases is because of settings and values, so making the storage size comparable between the source DB and Read Replica is a good start to resolving the issue, as is ensuring compatible DB Parameter settings, such as with the max_allowed_packet parameter.  Turning off the Query Cache is good for tables that are modified often which causes lag, because the cache is locked and refreshed often. No other options are correct.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html","title":"Troubleshooting for Aurora"}],"answers":[{"id":"05acabf0a4eea6213f78770124a81bcd","text":"Ensure the max_allowed_packet parameter value for the Read Replica is the same as the source DB instance.","correct":true},{"id":"5b3b3ef613f42a00bbda6ed80b4f3683","text":"Change the Read Replica instance class to have the same storage size as the source instance.","correct":true},{"id":"09fee716d530e8dbb655cb9541bb4d5c","text":"Unlike Amazon RDS for MySQL, Amazon Aurora does not exhibit replication lag.","correct":false},{"id":"82a38022b7a8f6e84ec39753340df001","text":"Set query_cache_type=0 in the DB Parameter Group, to disable the query cache.","correct":true},{"id":"94692ead9b9079bf92302461566b94a5","text":"Add additional replicas and the alter the code to initiate Read/Write splitting of the Database.","correct":false}]},{"id":"dc3d128f-8cce-4541-b6e6-be1ec6cea96d","domain":"SDLCAutomation","question":"Your CI/CD pipeline generally runs well, but your manager would like a report of some CodeBuild metrics, such as how many builds were attempted, how many builds were successful and how many builds failed in an AWS account over a period of time. How would you go about gathering the data for you manager?","explanation":"These are default CloudWatch metrics that come with CodeBuild.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html","title":"Monitoring Builds with CloudWatch Metrics"}],"answers":[{"id":"a1f83b40240e6af928c6cab6eeb010c1","text":"CloudWatch metrics will report these metrics by default. You can view them in the CloudWatch console.","correct":true},{"id":"19e6e35ed627bf15d10ddc88af1ab9be","text":"Configure a CloudWatch custom metric to track the build information, and create custom graphs in the CloudWatch console.","correct":false},{"id":"6149c7ca08a16f697f2988509bb61427","text":"Implement a lambda function to poll the CodeBuild API to gather the data and store it in CloudWatch Logs. Write a metric filter to graph the data and generate your report.","correct":false},{"id":"c5045cc818946fcb9c3f6d71744202b7","text":"Configure CodeBuild to log builds to CloudWatch Logs, and then write a metric filter which will graph the data points your manager requires.","correct":false}]},{"id":"b8ae52d2-9638-47a9-b2b0-69c471a35eab","domain":"MonitoringLogging","question":"Your organization runs a large amount of workloads in AWS and has automated many aspects of its operation, including logging. As the in-house devops engineer, you've received a ticket asking you to log every time an EC2 instance state changes. Normally you would use CloudWatch events for something like this, but CloudWatch logs aren't a valid target in CloudWatch Events. How will you solve this?","explanation":"CloudWatch Events can use a Lambda function as a target, which will solve this issue.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html","title":"Tutorial: Log the State of an Amazon EC2 Instance Using CloudWatch Events"}],"answers":[{"id":"b482ab43e71b8137c1bb08d0904d0576","text":"Use a CloudWatch dashboard, which will log EC2 state changes to CloudWatch logs if you create a text widget.","correct":false},{"id":"a68e57ea4a6bd4cfa8f565ef7cbc637e","text":"Create a Lambda function and run it on a schedule with CloudWatch Events. Make the Lambda function parse your CloudTrail logs for EC2 instance state changes and log them to another CloudWatch Logs log.","correct":false},{"id":"1fc1f373fd2b33a9a55daeb93aa03e7b","text":"Parse the EC2 CloudWatch changelog with a Lambda function each minute, and log the results to a separate log for instance state changes","correct":false},{"id":"a4e104c9dd0f3e6a03765a0680648eed","text":"Use CloudWatch Events, but use a Lambda function target. Write a Lambda function which will perform the logging for you.","correct":true}]},{"id":"7cd5551a-b9a0-4e59-bb53-d40d81dc8938","domain":"ConfigMgmtandInfraCode","question":"You have been ask to deploy a clustered application on a small number of EC2 instances.  The application must be placed across multiple Availability Zones, have high speed, low latency communication between each of the nodes, and should also minimise the chance of underlying hardware failure.  Which of the following options would provide this solution?","explanation":"Spread Placement Groups are recommended for applications that have a small number of critical instances which need to be kept separate from each other. Launching instances in a Spread Placement Group reduces the risk of simultaneous failures that might occur when instances share the same underlying hardware. Spread Placement Groups provide access to distinct hardware, and are therefore suitable for mixing instance types or launching instances over time. In this case, deploying the EC2 instances in a Spread Placement Group is the only correct option.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html","title":"Placement Groups"}],"answers":[{"id":"f3d57c381ce0c53f5ff05f7a48d8ae15","text":"The application should deployed as a service in ECS","correct":false},{"id":"112a2330d77c300b357edda7c17dddb6","text":"Deploy the EC2 servers in a Cluster Placement Group","correct":false},{"id":"72371c02b14e73370c1b01dd2523a1c1","text":"deploy the EC2 servers in a Spread Placement Group","correct":true},{"id":"b1954190e825c200d890843b70d5cb38","text":"Create a new VPC with the tenancy type of host and deploy the instances in the VPC","correct":false}]},{"id":"69f350ad-db3a-4b83-bf8e-15ea0c0df866","domain":"PoliciesStandards","question":"Your company is preparing to become ISO 27001 certified and your manager has asked you to propose a comprehensive solution to log configuration and security changes in a separate audit account.  Specifically, the solution should ensure IAM users have MFA enabled, identify S3 buckets which aren't encrypted and enforce the addition of specific tagging.  Identify which option solves the problem.","explanation":"AWS Config is the only service that will meet all of the requirements in the question as it records configuration changes and snapshots the configuration at regular intervals set by you. Data aggregation means that AWS Config data from multiple accounts can be stored in a single account.  The following built in rules; s3-bucket-server-side-encryption-enabled and iam-user-mfa-enabled identify any S3 buckets not encrypted and any IAM accounts that do not have MFA enabled.  Tagging is also available for AWS Config resources that describe AWS Config rules.","links":[{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"},{"url":"https://aws.amazon.com/cloudtrail/faqs/","title":"AWS CloudTrail FAQs"}],"answers":[{"id":"efd3ca0f78115a7a32bffa7905edd17b","text":"Enable AWS Config and create three default rules to check whether IAM users have MFA enabled, S3 buckets have server side encryption and tagging is added to resources.","correct":true},{"id":"2f426c67cf1036d34927bdb5aac020ba","text":"Enable AWS Cloudtrail to check for MFA enabled IAM users, configure Server access logging in S3 to view the encryption status and use a CloudFormation Templates to add tagging.","correct":false},{"id":"8c9a63c97e58123416200044d646218a","text":"Enable Enhance Logging in AWS Cloudwatch to track all security and configuration changes and view these using Cloudwatch Logs Insights.","correct":false},{"id":"0e1d676582953a5a30bcd07fa6996329","text":"Enable AWS Config and AWS Cloudtrail to track changes in all resources and to identify IAM user API calls with MFA enabled.","correct":false}]},{"id":"b6a6714a-e869-4cdf-8fb0-e114ed67b133","domain":"ConfigMgmtandInfraCode","question":"Your organization currently runs all of its applications on in-house virtual machines. Your CEO likes the direction AWS are taking the cloud industry and has suggested you look into what it would take to migrate your servers to the cloud. All of your servers are built and configured automatically using Chef. Which AWS services will you transition your first batch of servers to AWS in the FASTEST possible way while maintaining a centralised configuration management?","explanation":"While AWS OpsWorks allows you to use your own custom chef cookbooks with your AWS resources, it is not as easy as simply importing your existing servers into OpsWorks. The fastest solution here is to use the AWS Server Migration Service to bring your existing Chef managed machines into EC2 and to manage them with Chef the same way you have been on your in-house system.","links":[{"url":"https://aws.amazon.com/server-migration-service/","title":"AWS Server Migration Service - Amazon Web Services"}],"answers":[{"id":"ccdcca14f4397b834e728c31d87f72cb","text":"AWS CloudFormation and OpsWorks","correct":false},{"id":"673eb2698b26d29c462829b8f6baa851","text":"Use VM Import/Export to upload snapshots of your first batch of servers into AWS. Use OpsWorks for Chef Automate to deploy a new Chef server into your AWS account. Manually deploy the same recipes and cookbooks as you run on-prem, onto your new Chef server.","correct":false},{"id":"75e1b69b91db540990ec7f1b7899a81a","text":"AWS OpsWorks with EC2 VM Import/Export","correct":false},{"id":"35b2113902504f85d4b843761552eb23","text":"Leverage the AWS Server Migration Service to migrate your instances across as AMIs into AWS.  Use  custom configuration script in the Server Migration Service console to register the instances to Chef.","correct":true}]},{"id":"5c90df95-736b-4b27-a7cf-095c4dc3d4c4","domain":"HAFTDR","question":"You run a load balanced, auto-scaled website in EC2. Your CEO informs you that due to an upcoming public offering, your website must not go down, even if there is a region failure. What's the best way to achieve this?","explanation":"A latency based routing policy will keep your website as fast as possible for your customers, and will act as redundancy should one of the regions go down.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy"}],"answers":[{"id":"20703f2de437222f1e63b4b81216e053","text":"Deploy your load balancers and auto-scaled website in two different availability zones. Create a Route53 weighted Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":false},{"id":"560436ce37b94afeee434582179ddc95","text":"Deploy CloudFront in front of your instances. It will cache requests even if a region goes down and your users will not notice.","correct":false},{"id":"f20603c9e3176b9a3bdd585343418443","text":"Deploy your load balancers and auto-scaled website in two different regions. Create a Route53 Latency Based Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":true},{"id":"230b3ec4e1a08538bc506cca11e9c1a1","text":"Deploy your load balancers and auto-scaled website in two different availability zones. Create a Route53 GeoProximity Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":false}]},{"id":"6edec40a-5186-484f-84e3-d7bd3df44eb8","domain":"IncidentEventResponse","question":"Your company has recently switched from an external Email Service Provider to utilising SES to send marketing email.  You have been asked to record all bounce and complaint information from each email sent, in an S3 bucket so that it can be processed later.  You know that Kinesis Firehose can perform this task.  You have also been asked to anonymise the email addresses that are returned, in order to comply with GDPR and you will use Lambda to perform this task.  After a number of attempts, you have failed to receive any data into the S3 bucket.  Which of the following troubleshooting methods are valid in resolving your issues?","explanation":"We can immediately reject any answers that contain RedShift, as there is no need to use it to meet the needs of the question.  All of the remaining answers are possible methods of troubleshooting your issues.","links":[{"url":"https://docs.aws.amazon.com/firehose/latest/dev/troubleshooting.html","title":"Troubleshooting Amazon Kinesis Data Firehose"}],"answers":[{"id":"f71cd26ef170d12dafd69c0d3db6ddd9","text":"Make sure the Firehose ExecuteProcessingSuccess metric is available, to confirm that an attempt has been made to invoke your Lambda function.","correct":true},{"id":"b0f65e9497f5a0ee31b86e2dfd462e31","text":"Make sure the Firehose IncomingBytes and IncomingRecords metrics are available, to confirm that data is being delivered to your Firehose delivery stream successfully.","correct":true},{"id":"196fe528acbe4658755d261eab3182d4","text":"Make sure that the Firehose DeliveryToS3.Success metric is available, to confirm that Firehose has tried putting data into your S3 bucket.","correct":true},{"id":"9c2908c869a796ee883dbbd0f643986a","text":"Make sure the Firehose DeliveryToRedshift.Success metric is available to confirm that Firehose has tried to copy data from your S3 bucket to the Redshift cluster.","correct":false},{"id":"bc1a53efea21f6531d0b1a523da1b07f","text":"Make sure the Amazon Redshift STL_LOAD_ERRORS table is available, to verify the reason for the COPY failure.","correct":false}]},{"id":"bbe1fc54-7ed1-4156-930f-446e13ca5e59","domain":"ConfigMgmtandInfraCode","question":"One of your colleagues has developed a CloudFormation template which creates a VPC and a subnet for each Availability Zone within each Region the Stack is created in.  Although this template is currently functional, it requires the Region and Availability Zones to be passed into the Stack as parameters.  You have been asked by your manager to alter the template so that you can automate all of this functionality using Intrinsic Functions, and eliminate passing in parameters.  Choose options from the list which will meet the requirements of rewriting the template.","explanation":"Intrinsic functions can not be used within the Parameters section and so these options can be immediately ruled out. Using both Fn::GetAZs to return the Availability Zones and Fn:Select to choose from the list and also using Fn::FindInMap to pull back each Region from the Mappings section are both valid options to retrieve the Regions.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"Template Anatomy"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html","title":"Intrinsic Function Reference"}],"answers":[{"id":"c697110279709e26aa4a1219dcf5f6e2","text":"In the Resources section, use !Ref \"AWS::Region\" to return the Region, Fn::GetAZs to return a list of all Availability Zones and Fn:Select to choose each AZ from the list.","correct":true},{"id":"9fe027bb7b742b6639a50bc38d88cd6f","text":"In the Mappings section, make a list of each Availability Zone in each Region.  Then in the Resources section use Fn::FindInMap to pull back each AZ using !Ref \"AWS::Region\" as the key.","correct":true},{"id":"3eca8a7377d618c7e76e66d97dddfd42","text":"In the Parameters section, use !Ref \"AWS::Region\" to return the Region for each parameter, Fn::GetAZs to return a list of all Availability Zones and Fn:Select to choose each AZ from the list.","correct":false},{"id":"0f58a828daf6079e3560070d348c8d13","text":"In the Parameters section, create three individual parameters, one for each Availability Zone and use !Ref to retrieve the values in the Resources section.","correct":false}]},{"id":"b842350d-b112-40a7-999f-1d6e4259e260","domain":"IncidentEventResponse","question":"Your company sells and supports a number of Internet-enabled printers and scanners. Every time these devices have an issue, the data is sent back to a global endpoint which ingests it and places it into a Kinesis stream for processing.  Your sales team have have been meeting all of their targets and devices are sending more and more data back into your infrastructure.  You have therefore made the decision to scale out the Kinesis stream. Recently a new Cloud Engineer has joined the Operations Team and he has been given the task of re-sharding.  The task was completed successfully, but soon after he notices the following error in the logs; \"Cannot get the shard for this ProcessTask, so duplicate KPL user records in the event of re-sharding will not be dropped during de-aggregation of Amazon Kinesis records\".  The Engineer has asked you what the error is and what they should do to resolve it.  Chose your response from the following options.","explanation":"These errors are generated from code that handles KPL messages. When the KCL consumers are restarted or the lease entries are cleared, the ListShards operation will be run which will resolve the KPL messages. None of the other resolutions listed above will resolve these messages.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/introduction.html","title":"What Is Amazon Kinesis Data Streams?"}],"answers":[{"id":"ccbe7f04475bbc92d4b729572830b9b7","text":"To resolve these errors, ensure the producers use multiple threads to write to the Kinesis Data Streams service at the same time.","correct":false},{"id":"6bdec47e015462087aa8015af4c475c4","text":"To resolve these errors, either stop and start the KCL application or clear the lease entries from the DynamoDB lease table.","correct":true},{"id":"0068cac99ac759e7c30c9361ca3e3fa7","text":"To resolve these errors, re-shard the Kinesis stream to the the number of shards that were originally set before the errors occurred.","correct":false},{"id":"8401e9c6e54a8974f4e537555ccf7a94","text":"To resolve these errors, ensure that records use the the multi-record operation PutRecords, or are aggregated into a larger file before using the single-record operation PutRecord.","correct":false}]},{"id":"26b5e2d7-e00f-4d7b-b086-c09e029d1de9","domain":"HAFTDR","question":"The world wide cat news powerhouse, Meow Jones, has hired you as a DevOps Database consultant. They're currently using legacy in-house PostgreSQL databases which cost a considerable amount to maintain the server fleet, as well as operational costs for staff, and further hardware costs for scaling as the industry grows. You are tasked in finding an AWS solution which will meet their requirements. They require high throughput, push button scaling, storage auto-scaling and low latency read replicas. Any kind of automatic monitoring and repair of databases instances will also be appreciated. Which AWS service(s) do you suggest?","explanation":"Amazon Aurora will fit the needs perfectly, and the Database Migration Service can assist with the migration.","links":[{"url":"https://aws.amazon.com/rds/aurora/details/postgresql-details/","title":"Amazon Aurora Features: PostgreSQL-Compatible Edition"}],"answers":[{"id":"e96297fedff7f3062d962bbd0a387cb4","text":"An auto-scaled, load balanced EC2 fleet running PostgreSQL with data shared via EFS volumes.","correct":false},{"id":"6129915cbe4cbc9e1ddb0b74808beded","text":"Amazon Aurora, AWS Database Migration Service","correct":true},{"id":"0a663a4b84931c9af9abadcccde84f3f","text":"Keep the current PostgreSQL databases and implement an ElastiCache to cache common queries and reduce load on your in-house databases to save on upgrade costs.","correct":false},{"id":"9802017b44b6d60fc846a356c1fa9e9a","text":"A cluster of Amazon RDS PostgreSQL instances, AWS Database Migration Service.","correct":false}]},{"id":"c3e23238-408e-4b66-86f9-c652a8649dc5","domain":"MonitoringLogging","question":"Your organization deals with petabytes of data which needs to be shared with a vendor. They require full access to your private S3 buckets to perform their development work. They have extensive AWS experience already. How will you give them access?","explanation":"The best way to accomplish this is to create a cross-account IAM role with permission to access the bucket, and grant the vendor's AWS ID permission to use the role.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/","title":"Provide Cross-Account Access to Objects In S3 Buckets"}],"answers":[{"id":"10b2007be424d0cd17f0473bf718ef66","text":"Edit the bucket policy to allow the vendor AWS ID read access","correct":false},{"id":"38cbb5ba1eb48a6fd20b712b0433a77d","text":"Grant permission to the vendors AWS ID in the S3 bucket policy","correct":false},{"id":"9744a37e79095f8e3ca34aadf736f9b6","text":"Grant permission to vendor AWS ID to use the role","correct":true},{"id":"087a96fb587651ee51b6ca0824c7b131","text":"Grant the role permission to the bucket","correct":true},{"id":"c81b77222ee909b09dcad4f625bfe641","text":"Enable cross region replication","correct":false},{"id":"18e8d083cfe353b0d12b4ee66f546b75","text":"Create a cross-account IAM Role","correct":true},{"id":"8f55be78212f11fd250d67174c0dc930","text":"Create an IAM Role","correct":false}]},{"id":"b1b39162-3af1-4aa5-9cd4-4e30338d07f6","domain":"HAFTDR","question":"Your CEO wants you to start future proofing your AWS environment, so he's asked you to look into IPv6 compatibility of your existing Load Balanced EC2 stack. You make use of both Application (ALB) and Network (NLB) load balancers in your EC2-VPC. What are your findings?","explanation":"At this moment in time only the Application Load Balancer supports IPv6 in the EC2-VPC environment. Classic Load balancers only support IPv6 if you are using an EC2-Classic environment.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/01/announcing-internet-protocol-version-6-ipv6-support-for-elastic-load-balancing-in-amazon-virtual-private-cloud-vpc/","title":"Announcing Internet Protocol Version 6 (IPv6) support for Elastic Load Balancing in Amazon Virtual Private Cloud (VPC)"}],"answers":[{"id":"eda9ef0a0ac30b1db3a5bf8674687603","text":"Application and Network Load Balancers both support IPv6.","correct":false},{"id":"2a74b0b6126f559f800f0f5cef044526","text":"No Load Balancers in EC2 support IPv6.","correct":false},{"id":"12d596f99a6d191c9c1aaae49901a9e1","text":"Application Load balancers support IPv6, Network Load Balancers do not.","correct":true},{"id":"4aff7beb08ea7fe4fa49ab96903dfc41","text":"Application Load balancers do not support IPv6, Network Load balancers do.","correct":false}]},{"id":"f6f2e5d0-a72e-4d44-aaea-2e2e8fa264c3","domain":"HAFTDR","question":"Your company utilizes EC2 and various other AWS services for its workloads. As the DevOps engineer it is your responsibility to ensure all company policies are implemented. You have noticed that while you are using S3 for data archival and backups, your company policy is that your backups need to reside on company owned servers, such as those you run in your local Equinix data center. You also have another company policy that backup and archival cannot traverse the internet. What do you do?","explanation":"AWS Direct Connect is the only way to access your AWS resources from a Data Center without traversing the internet, despite the encryption offered by the other solutions.","links":[{"url":"https://aws.amazon.com/directconnect/","title":"AWS Direct Connect"}],"answers":[{"id":"f42f4a5866921fe7f0288f667c043aac","text":"Provision an AWS Direct Connect connection to your local router in your data center and your local VPC. Push backups via the Direct Connect connection.","correct":true},{"id":"32f6b2f1cc26546abf7ce835ae778156","text":"Install the EFS agent on your data center backup NAS, mount the volume on an EC2 server and copy the backups to the volume.","correct":false},{"id":"008894eae706c0e60a40a59c79342658","text":"Implement an AWS Client VPN with a company owned server in your data center. Push backups to your data center backup NAS through SSH via vpn.","correct":false},{"id":"b51577fa11c831eba7efec0c73952602","text":"Implement a Site to site VPN with a company owned server in your data center. Push backups to your data center backup NAS through SSH via vpn.","correct":false}]},{"id":"e6b3fbb9-d87d-4dc7-b056-981294c46bdc","domain":"PoliciesStandards","question":"An error has occurred in one of the applications that your team looks after and you have traced it back to a DB connection issue. There have been no network outages and the database is up and running. A colleague tells you that all credentials are now stored in AWS Secrets Manager and suspects that the problem might be caused by a recent change in that area. Select all possible reasons for this.","explanation":"Although you typically only have one version of the secret active at a time, multiple versions can exist while you rotate a secret on the database or service. GetSecretValue has an optional VersionId parameter that specifies the unique identifier of the version of the secret that you want to retrieve. If you don't specify either a VersionStage or VersionId then the default is to perform the operation on the version with the VersionStage value of AWSCURRENT.","links":[{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/enable-rotation-other.html","title":"Enabling Rotation for a Secret for Another Database or Service"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_retrieve-secret.html","title":"Retrieving the Secret Value"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_overview.html","title":"Overview of Managing Access Permissions to Your Secrets Manager Secrets"}],"answers":[{"id":"4add2a29b600d7ccdc5e4633595bd593","text":"The DB credentials and connection details in Secrets Manager have been encrypted using the default Secrets Manager CMK for the account. The application and secret are in different AWS accounts though and no cross-account access has been granted.","correct":true},{"id":"ef3ec50fbea476c0443043b89119ef3b","text":"The GetSecretValue API call in the application didn't include the version of the secret to return.","correct":false},{"id":"67a9e9da426f34e831b5de27c46e3b56","text":"The AWS credentials that are used for the call to Secrets Manager in the client-side component embedded in the application to retrieve the database password don't have the secretsmanager:DescribeSecret permission on that secret.","correct":true},{"id":"8525c502b705804602b85cbe128c73a3","text":"When secret rotation is configured in Secrets Manager, it causes the secret to rotate once as soon as you store the secret. This can lead to a situation where the old credentials are not usable anymore after the initial rotation. It is possible that the team forgot to update the application to retrieve the secret from Secrets Manager.","correct":true}]},{"id":"353f2086-4a2b-4c7a-b40d-8199509f0f8f","domain":"PoliciesStandards","question":"You have an antiquated piece of software across your Linux virtual machines that you are attempting to move away from. Unfortunately the process of extracting the data from this software is difficult and requires a lot of commands to be run on the command line. The final process of moving away from this software is executing 90 commands on each database server. Which AWS services will make this as painless as possible and easiest to set up?","explanation":"The Systems Manager Run Command on the database servers is the best way to achieve this. You do not need to run commands on all servers.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/rc-console.html","title":"Running Commands from the Console"}],"answers":[{"id":"b524668957c324a9210f76db4fc4995f","text":"Create an EC2 instance and a bash script with the commands. Run the bash script which will ssh to all of your servers and execute the commands.","correct":false},{"id":"16101d7bc0420494d4449f62f5980f41","text":"Use the Systems Manager Run Command feature to run a shell script containing the 90 commands on each database server.","correct":true},{"id":"fdcba29bfa345864d3e47b322ca54389","text":"Create a bash script with the commands and store it in S3. Use a Lambda function to copy the script to all of your database servers and execute the script.","correct":false},{"id":"77c20d947ea3cdba35cde43dc31f265a","text":"Use the scp command to copy the bash script to each server and then execute it manually.","correct":false}]},{"id":"7bce21b9-f986-4147-b616-c58ac141d19f","domain":"PoliciesStandards","question":"You work for a global service provider, deploying critical software 24 hours a day.  You have 3 AWS Accounts; 'POC' allows for Developers to stand up new technology and try out new ideas on an adhoc basis. 'QA' allows for automated builds and testing to be carried out as part of your CI/CD Pipeline and any problems here mean that software will not get pushed into production, so it's important that any issues are resolved quickly. The final account is 'Live' which is the main Production account.  It's the most critical and requires the best response times.  You need to choose the most appropriate and cost effective Support Plan which will satisfy your support needs but also allow for the full range of AWS Trusted Advisor Checks and Recommendations.","explanation":"The important phrase in the question is that any choice must \"...allow for the full range of AWS Trusted Advisor Checks and Recommendations\", only two support plans have all the Trust Advisor checks and these are Business and Enterprise.  Anything that contains the Basic or Developer plans will not be correct.  Also, it specifies that the 'Live' account is \"critical and requires the best possible response times\", so we could use the Enterprise plan for this, but we'll only get better response time for mission critical business applications such as Microsoft, SAP and Oracle tools, which is not the software we are deploying.  Therefore taking all of this into account, along with cost, 'POC', 'QA' and 'Live' should all be allocated the Business plan.","links":null,"answers":[{"id":"e52ec269985d97d389e8e39403ec69fc","text":"The 'POC' Account should be allocated the Basic Support Plan, 'QA' the Business Support Plan and 'Live' the Enterprise Support Plan.","correct":false},{"id":"eec4f4e8aab13e184a6c7abc550b5ded","text":"The 'POC' and 'QA' Accounts should use the Basic Support Plan, while 'Live' should use the Enterprise Support Plan.","correct":false},{"id":"4dbc42a7817c285587307fc6102d43d2","text":"The 'POC' Account should be allocated the Developer Support Plan, 'QA' the Business Support Plan and 'Live' the Enterprise Support Plan.","correct":false},{"id":"b802f5ecd5a395f2c899ccdbf041d677","text":"The 'POC' Account should be allocated the Basic Support Plan and both 'QA' and 'Live' the Business Support Plan.","correct":false},{"id":"92b9da164af4d9a8f9d10be3a3964a68","text":"The 'POC', 'QA' and 'Live' Accounts should all be allocated the Business Support Plan.","correct":true}]},{"id":"860e8b11-9faf-4176-8c11-716aeccf799b","domain":"HAFTDR","question":"You're assisting a developer working on a very large and read-heavy application which uses an Amazon Aurora database cluster. The feature currently being worked on requires reading but no writing, however it will be called by the application frequently and from multiple different servers so the reads need to be load balanced. Additionally your reporting team need to make ad-hoc, expensive queries which need to be isolated so that reads for production are not affected by reporting.  Which Aurora configuration fulfils both needs with minimal extra configuration?","explanation":"The Reader endpoint is appropriate in this situation. The reader endpoint provides load balanced support for read only connections to the database cluster. A custom endpoint can be used to connect to an isolated replica for report generation or ad hoc (one-time) querying,","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html","title":"Amazon Aurora Connection Management - Amazon Aurora"}],"answers":[{"id":"2b1242d9bda15b98989935f538280b8f","text":"Use the Cluster endpoint for your Production system to make its reads against high-capacity read replicas, and create an Aurora reporting endpoint pointing to a separate replica for isolation of ad-hoc reporting.","correct":false},{"id":"5ff29b324290cc87e7f79502c91662fd","text":"Use the Custom endpoint for your Production system to make its reads against a single high-capacity replica, and create another custom endpoint pointing to a dedicated reporting replica for isolation of ad-hoc reporting.","correct":false},{"id":"794b7ab3749de5b254f0dd3f2337bc74","text":"Use the Reader endpoint for your Production system to make its load-balanced reads against high-capacity read replicas, and create a custom endpoint pointing to a separate replica for isolation of ad-hoc reporting.","correct":true},{"id":"af531b8d353cecad595506c5239ca5f2","text":"Use the Aurora Instance endpoint for your Production system to make load-balanced reads against a read replicas, and create another custom endpoint pointing to a dedicated reporting replica for isolation of ad-hoc reporting.","correct":false}]},{"id":"25964f23-34d0-4ed9-9f24-4cee82e76511","domain":"ConfigMgmtandInfraCode","question":"You are building acatguru, which your organization describes as facebook for cats. As part of the sign-up process your users need to upload a full size profile image. You already have these photos being stored in S3, however you would like to also create thumbnails of the same image, which will be used throughout the site. How will you automate this process using AWS resources?","explanation":"S3 triggering Lambda to create thumbnails is a perfect example of how to automate this process","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html","title":"Using AWS Lambda with Amazon S3"}],"answers":[{"id":"a24cc963be9859fff809b1da36be8137","text":"Configure S3 to publish its event stream to an SNS topic. Subscribe a Lambda function to the SNS topic which will trigger when a file is uploaded. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":false},{"id":"30a4f470d1bf4e4d01dab0088e331a6d","text":"Create an S3 bucket notification trigger to execute a Lambda function when an object is created. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":false},{"id":"9d2bb40a4f2a61f4e70855a070faa693","text":"Create an S3 event trigger to execute a Lambda function when an object is created. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":true},{"id":"62751bd29f8b4728e9393f35cf7bf035","text":"Use CloudTrail to monitor PUT and POST calls sent to S3 and trigger a Lambda function when you identify an upload. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":false}]},{"id":"fbc8bae9-74cb-430a-aaf3-7c16cd49f9f9","domain":"MonitoringLogging","question":"You have built a serverless Node.js application which uses Lambda, S3 and a DynamoDB database. You'd like to log some simple metrics so you can possibly graph them at a later date, or analyze the logs for faults or errors. You aren't able to install the CloudWatch Logs agent into a Lambda function however. What do you do instead?","explanation":"console.log will work perfectly in Lambda and is the easiest way to log directly to CloudWatch Logs","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-logging.html","title":"AWS Lambda Function Logging in Node.js"}],"answers":[{"id":"d694da5d9149fb6683cce0b6ec647e92","text":"Use an API gateway configured to log to CloudWatch Logs","correct":false},{"id":"32ad7a8632717703ab0efe26deafc538","text":"Use the CloudWatch Logs API, which will provide putLogEvents where you can upload logs to CloudWatch.","correct":false},{"id":"856b826225eea424a5e5472bb8242afa","text":"Use the console.log commands in Lambda, it will log your logs straight to CloudWatch Logs.","correct":true},{"id":"f86fadfb6bf4f2aff4fc541fb9f962d9","text":"Use the log.console commands in Lambda, it will log your logs straight to CloudWatch Logs.","correct":false}]},{"id":"67394e5d-b7ef-4850-b0da-9548411156f3","domain":"SDLCAutomation","question":"Vector Technologies Incorporated wants to implement a CI/CD environment for their online bill payment system. The application runs in a .NET environment on AWS Elastic Beanstalk, and the database is SQL Server. Code is stored in a GitHub repository external to AWS. Changes to the database schema are needed for most code updates, and are done with a script that is also stored on GitHub. The CI/CD process should be initiated every time new source code or a new schema update script is moved to the central repo. Which architecture will provide the capability to automatically retrieve, build, and deploy application components each time changes are made?","explanation":"Code Pipeline can be invoked every time there is a code change or new schema update script in the GitHub repository branch, both of which can be written to S3. CodePipeline can then trigger CodeBuild to use the MSBuild Windows image that has been stored in ECR to build the .NET application code. The resulting package is then placed in a build artifacts S3 folder, which is picked up by CodeDeploy to be implemented on Elastic Beanstalk. CodeDeploy can also invoke a PowerShell script to run the schema update executable. An ECS container is not needed to perform this build, and will incur additional costs. CodeBuild can work directly with the MSBuild container image in ECR. NAnt provides the capability to automate software build processes, but MSBuild is still needed to compile the .NET code. CodeDeploy won't be able to run the database schema update scripts without them being embedded in an executable.","links":[{"url":"https://aws.amazon.com/codepipeline/","title":"AWS CodePipeline"},{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild"},{"url":"https://aws.amazon.com/codedeploy/","title":"AWS CodeDeploy"},{"url":"https://aws.amazon.com/quickstart/architecture/dotnet-cicd-on-aws/","title":".NET CI/CD on the AWS Cloud"},{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html","title":"CodeDeploy AppSpec File Reference"}],"answers":[{"id":"0bace657c20367e9692de64ec05c7669","text":"Write an MSBuild container image with the required tools for compiling .NET applications and push it to Amazon Elastic Container Registry (ECR). Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script and write them to S3. Have CodePipeline trigger AWS CodeBuild to create an Amazon Elastic Container Service (ECS) container from the ECR image. Use the ECS container application to retrieve the latest code updates from S3 and construct the .NET application. Have CodePipeline trigger AWS CodeDeploy to implement the .NET application on Elastic Beanstalk. Have CodeDeploy run the schema update scripts against the database.","correct":false},{"id":"3833bcf2af22e331e4ba509f55aa1236","text":"Use AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use NAnt to compile the source code and construct the .NET application. Configure CodePipeline to trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy run the schema update scripts against the database.","correct":false},{"id":"2b48c63fdb564d11a32c45fcc0205c3f","text":"Create an MSBuild container image with the required tools for compiling .NET applications and push it to Amazon Elastic Container Registry (ECR). Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use the MSBuild container image from ECR to compile the source code. Configure CodeBuild to then construct the .NET application. Also have CodeBuild create an executable to run the schema update script. Have CodePipeline trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy invoke a Powershell script to run the schema update executable.","correct":true},{"id":"a69266d62fd4ec974f90c80873f07508","text":"Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use NAnt to compile the source code and construct the .NET application. Also have CodeBuild create an executable to run the schema update script. Configure CodePipeline to trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy invoke a Powershell script to run the schema update executable.","correct":false}]},{"id":"b2d2d9d6-3ab6-4949-85a3-de49e4783ab9","domain":"ConfigMgmtandInfraCode","question":"In the past, your organisation has had security breaches due to unauthorised changes in infrastructure.  To reduce these, the InfoSec Team have implemented AWS Config so that all changes are recorded.  Any changes to the infrastructure are then monitored by the Service Desk.  However, the following errors; \"We are unable to complete the request at this time. Try again later or contact AWS Support\" are appearing in the console.  The Service Desk Manager has asked you for more information on why these errors are happening.  Choose the correct reasons from below.","explanation":"There are only two issues that would cause this error to appear in AWS Config.  The first is if you make multiple calls to the AWS Config API within a minute and the default rate limiting will then implement a temporary block.  The CloudTrail logs will show the following; \"You have exceeded the maximum request rate. Try again at a later time.\" and you should stop calling the API as frequently.  The second option will be if you exceed the set Aggregator limit (which is 50 by default).  CloudTrail will show; \"The configuration aggregator could not be created because the account already contains '50' configuration aggregators. Consider deleting configuration aggregators or contact AWS Config to increase the limit.\"  No other options listed accurately describe the message.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/config-console-error/","title":"How can I troubleshoot AWS Config console error messages?"},{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"}],"answers":[{"id":"72a656f0e09c8f5382bf1978c403720f","text":"This error is related to the Aggregator limit being reached, and you should contact AWS Support to increase the Configuration Aggregator limit from the default value.","correct":true},{"id":"c2b54e290aa3e1f5ba5b69b19e872a6a","text":"This error is related to the ConformancePackStatusReason, and you should check it to know more about the reason for failure.","correct":false},{"id":"8d84028643f27b004e826c24087d49db","text":"This error is related to exceeding the rate limiting if you use the API call GetResourceConfigHistory or ListDiscoveredResources with a Lambda function.","correct":false},{"id":"bb66cca2e29dc015297e99a6c201a347","text":"This error is related to switching to a different region when Remediation is in Progress.","correct":false},{"id":"92344dfafeacc6f8c91cc9d5159ce1ff","text":"This error is related to calling the StartConfigRulesEvaluation API more than once per minute.","correct":true}]},{"id":"e742c07d-c719-4c53-9615-292147e77480","domain":"ConfigMgmtandInfraCode","question":"In the Amazon States Language, InputPath, Parameters, ResultPath, and OutputPath filter and control the flow of JSON from state to state. Which of the following definitions are correct?","explanation":"The definition of OutputPath and ResultPath are swapped around, i.e. an OutputPath can filter the JSON output to further limit the information that is passed to the output while the ResultPath selects what combination of the state input and the task result to pass to the output","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html","title":"Input and Output Processing in Step Functions"},{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/input-output-inputpath-params.html","title":"InputPath and Parameters"}],"answers":[{"id":"84df9b15c00389ac9ecd3ba94d81a165","text":"OutputPath selects what combination of the state input and the task result to pass to the output.","correct":false},{"id":"9d857343a25dab2067293a92f66a0fbf","text":"InputPath selects which parts of the JSON input to pass to the task of the Task state.","correct":true},{"id":"cfd8d208af0b74c2e7d0fdc920690032","text":"Parameters enable you to pass a collection of key-value pairs, where the values are either static values that you define in your state machine definition, or that are selected from the input using a path.","correct":true},{"id":"1063b9865beb745109392a1abe5501c8","text":"ResultPath can filter the JSON output to further limit the information that is passed to the output.","correct":false}]},{"id":"47fefdcc-2b9b-40bc-ab6d-9c70bad210a3","domain":"SDLCAutomation","question":"CodeBuild is used to manage your project builds. In the past, quite a few builds completed successfully but you are currently dealing with a failed one after a source code change. Which of the below are INVALID statements about AWS CodeBuild build phase transitions?","explanation":"PROVISIONING comes before DOWNLOAD_SOURCE and a failed PRE-BUILD phase transitions to the FINALIZING phase.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/view-build-details.html#view-build-details-phases","title":"View Build Details in CodeBuild: Build Phase Transitions"}],"answers":[{"id":"5e1175d25bc1d48d660e2eebac6142eb","text":"If a command fails during the INSTALL phase, CodeBuild transitions to the FINALIZING phase and none of the commands in the pre_build, build, and post_build phases are run for that build's lifecycle.","correct":false},{"id":"346f35bdae6107a46bb68a12559c154d","text":"CodeBuild transitions from a failed PRE-BUILD phase to the POST_BUILD phase.","correct":true},{"id":"8e289066c9ed3349987194d5056b8fe5","text":"This failed build could be a result of a failed PROVISIONING phase which comes after the DOWNLOAD_SOURCE phase.","correct":true},{"id":"38d29cdacb55007ab2949624e436d1c0","text":"The UPLOAD_ARTIFACTS phase is always attempted, even if the BUILD phase fails.","correct":false}]},{"id":"ea7992e6-08c7-4f1b-9391-67be322b64ac","domain":"MonitoringLogging","question":"Your company runs a fleet of spot instances to process large amounts of medical data which can be killed at any time. The processing results are extremely important, and you find that sometimes the servers shut down too quickly for the logs to be copied over to S3 when a batch has completed. It's OK if a batch doesn't complete processing because it will be re-run on another spot instance, but the small case of completing and not getting the logs for that completion is causing headaches. Which is the best solution to implement to fix this issue?","explanation":"The CloudWatch logs agent will stream your logs straight into CloudWatch Logs so nothing will be missed if the server is terminated.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html","title":"Install and Configure the CloudWatch Logs Agent on a Running EC2 Linux Instance"}],"answers":[{"id":"2b9f925fe8658d7ca17378e9d8006c98","text":"Configure the CloudWatch logs agent on the AMI the spot instance is using and stream your logs to CloudWatch Logs directly","correct":true},{"id":"2b8f5ce19a375c3062f4d5f2f3d46d59","text":"Configure the application to ingest the logs directly into Kinesis streams, and use a master logging server to process those logs and copy them into S3","correct":false},{"id":"9808af87ddc6c3fed5dcb05510fc62b5","text":"Configure a cronjob to push the logs to S3 every minute","correct":false},{"id":"7da59d6d572879b3f9de3a904c1ed08e","text":"Configure the server to send the logs to SQS on completion and use a Lambda function to poll the SQS queue and save the queue contents to S3","correct":false}]},{"id":"19cc0253-1dad-4ed2-bc2a-95515248a9ac","domain":"SDLCAutomation","question":"You are developing a completely serverless application and store your code in a git repository. Your CEO has instructed you that under no circumstances are you allowed to spin up an EC2 instance. In fact, he's blocked access to ec2:* company-wide with an IAM policy. He does however still want you to completely automate your development process, you just can't use servers to do so. Which AWS services will you make use of to meet these requirements?","explanation":"CodeDeploy can deploy a serverless lambda function straight from your CodeCommit repository.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html","title":"What Is CodeDeploy?"}],"answers":[{"id":"e3ce2b50bb87a7fe18adff65e0320c08","text":"Use AWS Lambda for your compute functions, edit them directly in the Cloud9 console in the browser. Once installed they don't need to redeployed.","correct":false},{"id":"93bfcd1da129b57627d8753c0b2d65ef","text":"Use AWS Lambda for your compute functions, use CodeDeploy to deploy the functions for you. Store your code in CodeCommit and use a CodePipeline to automatically deploy the functions when you commit your code to the repository.","correct":true},{"id":"d385958d5064373b0d639a8c16f6ab09","text":"Use AWS Lambda for your compute functions, use CodeDeploy to deploy the functions for you. Store your code in GitCommit and use a CodePipeline to automatically deploy the functions when you commit your code to the repository.","correct":false},{"id":"fbc7f2f64b4e3a232325800045b1725f","text":"Move your compute functions into S3, and use CodePipeline to deploy your code from S3 into AWS Lambda.","correct":false}]},{"id":"6a6f97fb-45c8-4509-91c7-4507629d60fa","domain":"HAFTDR","question":"Your current workload with DynamoDB is extremely latency bound, and you need it to be as fast as possible. You do not have time to look at other AWS services but instead have been instructed to use features and configuration changes of the services you are currently using. What do you do?","explanation":"Implementing a DAX cluster is the ideal solution here. It meets the requirement of using the existing DynamoDB service feature, while having the ability to reduce latency from milliseconds to microseconds. DynamoDB Scan times can also be optimised by reducing the number of attributes in your table and grouping attributes as JSON blobs within a single attribute.","links":[{"url":"https://aws.amazon.com/dynamodb/dax/","title":"Amazon DynamoDB Accelerator (DAX)  Fully managed in-memory cache for DynamoDB"},{"url":"https://aws.amazon.com/blogs/database/optimizing-amazon-dynamodb-scan-latency-through-schema-design/","title":"Optimize Amazon DynamoDB scan latency through schema design | AWS Database Blog"}],"answers":[{"id":"357461d5fd66eb609146fe0e65854901","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement Elasticache Redis for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS Insights to make available Insights tracing of their DynamoDB calls.","correct":false},{"id":"1c8d38560c0975ffecdfff42f01c5965","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement a DAX cluster for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS Insights to make available Insights tracing of their DynamoDB calls.","correct":false},{"id":"3d69fc095075fdb4cc8e97f241e0fb76","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement Elasticache Memcached for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":false},{"id":"4e7fca450cdf45d6ded7827b5ef88c16","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement a DAX cluster for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":true},{"id":"b5002075c11491df649c4c4f6e960472","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement DynamoDB Cached for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":false}]},{"id":"5de9216b-07fe-47be-aed7-41a4da2b44bd","domain":"HAFTDR","question":"One of your colleagues has been asked to investigate increasing the performance of the main corporate Website, for customers in the Asia Pacific Region, using a CDN.  They have decided to use CloudFront to perform this function, but have encountered problems when configuring it.  You can see that CloudFront returns an InvalidViewerCertificate error in the console whenever they attempt to to add an Alternate Domain Name.  What can you suggest to your colleague to assist them in solving the issue?","explanation":"You must ensure that the certificate you wish to associate with your Alternate Domain Name is from a trusted CA, has a valid date and is formatted correctly.  Wildcard certificates do work with Alternate Domain Names providing they match the main domain, and they also work with valid Third Party certificates.  If all of these elements are correct, it may be that there was an internal CloudFront HTTP 500 being generated at the time of configuration, which should be transient and will resolve if you try again.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/troubleshooting-distributions.html","title":"Troubleshooting Distribution Issues"}],"answers":[{"id":"8eec3b58fb382642048a138370ae92cf","text":"Ensure the certificate is not a Wildcard certificate as these do not work with Alternate Domain Names.","correct":false},{"id":"6ef625f1455326a96638173bec379999","text":"Ensure that there is a trusted and valid certificate attached to your distribution.","correct":true},{"id":"e4f79c0e45af152d4406e7e7123c83a8","text":"There was a temporary, internal issue with CloudFront which meant it couldn't validate certificates.","correct":true},{"id":"2d089a04108e374436b3174f6acaa8e3","text":"The certificate relating to the Alternate Domain Name was imported from a Third Party CA and this will not work.","correct":false}]},{"id":"1c7dd43f-b2c7-49aa-b897-bd62f97ee183","domain":"ConfigMgmtandInfraCode","question":"A company has successfully deployed a two EC2 node Redis cluster, using CloudFormation.  In order for the cluster to work, Node one is started and populated with data before Node two is started. Some weeks later, a new Redis stack is created using the same template, but is populated by a different set of data.  In this case the stack consistantly fails to be created eventhough the template and parameters are identical to the original stack.  Which actions should be taken to ensure that the new cluster can successfully be created?","explanation":"If, as in this case, the order of resources is important, it should never be assumed that they will always stand up in the same order each time, the first stack may have deployed in the correct order by luck.  A 'DependsOn' resource should always be used to maintain order consistancy.  Also, the data that is populating Redis may be much larger and take more time in the second Stack.  Adding a Creation Policy with a large Timeout will ensure that data successfully completes the import and the EC2 instance is successfully created.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"Template Anatomy"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-creationpolicy.html","title":"CreationPolicy Attribute"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-dependson.html","title":"DependsOn Attribute"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html","title":"Working with Nested Stacks"}],"answers":[{"id":"d1b2270b9abfe6f978128e05ec79268d","text":"Use a single CloudFormation template and add a Creation Policy with a large Timeout, to the first EC2 node.","correct":true},{"id":"002f82cd165be0c2dd0d4bfba60bde24","text":"Use a single CloudFormation template and add a 'DependsOn' resource for the second EC2 node which points to the first EC2 node.","correct":true},{"id":"af31ddbdf176fd17fce491dc05ba1388","text":"Use a CloudFormation root template which uses nested stacks.  Define each Redis node in a separate template which are linked to by the root template.","correct":false},{"id":"9a01df5c5a78cacac35b87d8678c7d74","text":"Use a single CloudFormation template and add a Creation Policy and Deletion Policy with a Retain option to maintain both EC2 Redis nodes.","correct":false},{"id":"a7e04cbb4e145efffd811f7596f2fbdf","text":"Use two CloudFormation templates, one for each EC2 Redis node and manually run one at at time.","correct":false}]},{"id":"82e554d2-f867-4507-974f-04b0b9020cb9","domain":"MonitoringLogging","question":"Your company has a team of Windows Software Engineers which have recently switched from developing on-premise applications, to cloud native micro-services.  The Service Desk has been inundated with questions, but they can't troubleshoot because they don't know enough about Amazon CloudWatch.  The questions they have been receiving mainly revolve around why EC2 logs don't appear in log groups and how they can monitor .NET applications.  Choose the following options which will help troubleshoot the Amazon Cloudwatch issues.","explanation":"The question suggests we are utilising a Windows development environment, so we can discount any answers which have Linux only terms such as running shell scripts.  To run Amazon CloudWatch Application Insights for .NET and SQL Server, we will need to install the SSM Agent with the correct roles, IAM policies and Resource Groups.  We also need to ensure that the CloudWatch agent is running correctly, by starting it using the amazon-cloudwatch-agent-ctl.ps1 script, and as we are assuming defaults, Cloudwatch metrics can be found under the CWAgent namespace.  There are limits for many items in Cloudwatch, utilising Custom Metrics is not one of them.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/troubleshooting-CloudWatch-Agent.html","title":"Troubleshooting the CloudWatch Agent"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.html","title":"Amazon CloudWatch Application Insights for .NET and SQL Server"}],"answers":[{"id":"ff3e53ec436317efcebcc4a0bf3b52a8","text":"In the Amazon Cloudwatch console, select Metrics, AWS Namespaces and then your metrics should appear under 'CWAgent'.","correct":true},{"id":"815da42a7b90966156b2d66da1a16b9e","text":"Check the common-config.toml file exists and is configured correctly, and then run the following Powershell command; amazon-cloudwatch-agent-ctl.ps1 -m ec2 -a start.","correct":true},{"id":"aa7402fca9b873f6b49d144c5c67440e","text":"For each EC2 instance running .NET code, install the SSM Agent and attach the AmazonEC2RoleforSSM Role.  You will also need to create a Resource Group and IAM Policy.","correct":true},{"id":"ffd7b413b965db5c4e66d6dc2c8a73e8","text":"Check the aws-agent.yaml file exists and is configured correctly, and then run the following shell command; aws-cw-agent.sh start.","correct":false},{"id":"33668d47f8774c94713045bff628ba97","text":"If you are utilising CloudWatch Custom Metrics, ensure that you have not reached the default limit.","correct":false}]},{"id":"e96d9cdb-1a97-4abe-9aea-0f5c2016fd0f","domain":"IncidentEventResponse","question":"You run a cat video website, and have an EC2 instance running a webserver which serves out the video files to your visitors who watch them. On the same EC2 instance, you also perform some transcoding to make the files smaller or to change their resolution based on what the website visitor is requesting. You have found that as your visitors grow, this is getting harder to scale. You would like to add more web servers and some autoscaling, as well as moving the transcoding to its own autoscaling group so it autoscales up when you have a lot of videos to convert. You are also running out of disk space fairly frequently and would like to move to a storage system that will scale too, with the least amount of effort and change in the way your entire system works. What do you do?","explanation":"This is a great case for using EFS. With minimal effort you can move your cat video website to an automatically scaling storage solution which can be used by all of your EC2 instances.","links":[{"url":"https://aws.amazon.com/efs/","title":"EFS"}],"answers":[{"id":"b908baa5c72de0c770b146c5103275e0","text":"Implement S3. Modify your applications to serve videos from S3 as well as downloading files from S3, transcoding them and storing them there.","correct":false},{"id":"c9acb0ccafd771fdd4739d4cb5146111","text":"Implement S3 Glacier to save on storage costs. Use the S3 Glacier API to retrieve and store video files as required.","correct":false},{"id":"0369b9bbb9fc65a7ad7ed7be897b6824","text":"Implement EFS. Mount your volume to each server for serving content and transcoding.","correct":true},{"id":"fc82534fc7c30a5bbc7dd66625b426ae","text":"Implement Storage Gateway on each EC2 instance. Use the gateway to quickly move files between instances as required.","correct":false}]},{"id":"927aeb0d-23f7-48f5-9ca7-aa76c6403385","domain":"SDLCAutomation","question":"You are contracting for APetGuru, an online pet food store. Their website works fine, but you really want to update the look and feel to be more modern. They have given you strict guidelines that their website can not be offline at all or they will lose sales. You are thinking of using a rolling deployment so some of their servers are always online during the rollout. Just before you trigger the roll out, you receive a phone call asking if you can only install your updates on a few servers first for testing purposes. It is suggested that a few customers can be redirected to the updated site and everyone else can still use the old site until you confirm the new one is working properly. Once you're happy with the operation of the updated website, you can complete the rollout to all servers. What do you do?","explanation":"A canary deployment will allow you to complete the roll out in the way you want to.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing continuous integration / continuous delivery on AWS"}],"answers":[{"id":"864cd30cc708bcfb524df58f775ae226","text":"Use a Canary Deployment. This allows deployment to a few servers where you can observe how the website is running while still receiving a small amount of customers. If there's an error it can be rolled back. Otherwise, the rollout will continue on new servers.","correct":true},{"id":"807b35728975296049d217f0dfbb3bc8","text":"Use an In-Place Deployment. You can deploy to the \"In-Place\" servers first for testing, then once testing is verified you can continue the deployment to the \"Out-Place\" externally facing servers.","correct":false},{"id":"562add4453cef6369a9860f6a26dbe36","text":"Use a Batch Deployment. This allows you to install your new website to the small batch which traffic will be directed to, and will allow you to roll back the small batch if there's an error. Otherwise, you can complete the rollout on the large batch.","correct":false},{"id":"a93e067980f0010b2f812e5f3eef9b9f","text":"Use a Blue/Green deployment. This allows you to install the new website to the blue servers, and once you're happy with it working you can finalise the install on the green servers. If there's an issue you can roll back the blue installation.","correct":false}]},{"id":"ac6646a9-8e77-48b4-99fd-a477f558c477","domain":"SDLCAutomation","question":"Your CEO has come up with a brilliant idea for when your company migrates its first application to the Cloud. He is a big fan of infrastructure as code and automation, and thinks your deployments will never fail again if you spin up completely new servers, deploy your software to them, redirect your DNS to the new servers and then shut down the old servers. Unfortunately for him, you know this has existed for a long time and had already planned to implement this in the coming weeks. Which build and deployment strategy report do you grab from your desk to show him?","explanation":"Blue/Green deployments are certainly what your CEO thinks he has invented. Updating your Route53 alias record to a new load balancer","links":[{"url":"https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf","title":"AWS Blue Green Deployments"}],"answers":[{"id":"6b741114a9154860a54dfc92186ad064","text":"Build the data tier using Amazon RDS via CloudFormation. Deploy your web tier behind a load balancer, hosted on EC2 instances.  During deployments, utilise a rolling deployment strategy by deploying an entirely new load balancer and group of EC2 instances which run your new code base.  Once the build and deployment of the fresh environment is complete, testing can be performed by accessing the site from the new load balancer URL. You can use a Route 53 alias to direct people from the old load balancer to another once your testing team completes their checks.","correct":false},{"id":"0b5318c4c58764e77d91057e51dcc6ba","text":"Build the data tier using Amazon RDS via CloudFormation. Deploy your web tier behind a load balancer, hosted on EC2 instances.  During deployments, utilise a blue/green deployment strategy by deploying an entirely new load balancer and group of EC2 instances which run your new code base.  Once the build and deployment of the fresh environment is complete, testing can be performed by accessing the site from the new load balancer URL. You can use a Route 53 failover routing policy to redirect people from your old load balancer to another once your testing team completes their checks.","correct":false},{"id":"6ccc35631fac4729d9b44594506d7a15","text":"Build the data tier using Amazon RDS via CloudFormation. Deploy your web tier behind a load balancer, hosted on EC2 instances.  During deployments, utilise a blue/green deployment strategy by deploying an entirely new load balancer and group of EC2 instances which run your new code base.  Once the build and deployment of the fresh environment is complete, testing can be performed by accessing the site from the new load balancer URL. You can update your Route 53 simple routing policy to direct people from the old load balancer to another once your testing team completes their checks.","correct":true},{"id":"648583c850bab8423500a7340e307a54","text":"Build the data tier using Amazon RDS via CloudFormation. Deploy your web tier behind a load balancer, hosted on EC2 instances.  During deployments, utilise a Mirrored deployment strategy by deploying an entirely new load balancer and group of EC2 instances which run your new code base.  Once the build and deployment of the fresh environment is complete, testing can be performed by accessing the site from the new load balancer URL. You can use a Route 53 alias to direct people from one load balancer to another once your testing team completes their checks.","correct":false}]},{"id":"0e604476-a66d-48c8-b59d-dbb5efe66d38","domain":"SDLCAutomation","question":"AWS CodeBuild is used for a project that you look after which has a known issue with its build performance. You can trace it back to an environment variable named AWS_CODEBUILD_MAX_MEM_ALLOC that is defined in multiple places. It has a value of '4096' in your build spec declaration, '2048' in the build project definition and a value of '1024' in the start build operation call. Which of the following statements is correct?","explanation":"Its value is '1024'. Do not set any environment variable with a name that starts with 'CODEBUILD_' as this prefix is reserved for internal use.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html","title":"Build Specification Reference for CodeBuild: Build Spec Syntax: env"}],"answers":[{"id":"7201fd1c5d65552aba9eb7f9f1b7c846","text":"Its value is '4096' because the value in the build spec declaration takes highest precedence, followed by the value in the build project definition while the value in the start build operation call takes lowest precedence.","correct":false},{"id":"a09a6c2e72b77a064358b20cbf7ffeea","text":"The value in the start build operation call takes highest precedence, followed by the value in the build project definition while the value in the build spec declaration takes lowest precedence. Therefore its value is determined as '1024'.","correct":true},{"id":"3e4a9e589e2cb112cdc5aff1387285fc","text":"The variable is undefined as the 'AWS_CODEBUILD_' prefix of its name is reserved for internal use.","correct":false},{"id":"6d6b3a85646a3254ab985c43b3a11a20","text":"The value in the build project definition takes highest precedence, followed by the value in the start build operation call while the value in the build spec declaration takes lowest precedence. Because of this, its value is '2048'.","correct":false},{"id":"91e162eb52d2dd94ba6973eae901a468","text":"The value of AWS_CODEBUILD_MAX_MEM_ALLOC  is '2048' as the build project definition takes highest precedence, followed by the value in the build spec declaration while the value in the start build operation call takes lowest precedence.","correct":false}]},{"id":"78f41605-1f8d-4b51-8cba-700dc133f4f7","domain":"ConfigMgmtandInfraCode","question":"You are the only developer in your new team, and you are developing a PHP application with a Postgres database. You would like to just focus on your code and spend a minimal amount of time in the AWS console. You would also like any new developer who joins your team to be able to deploy to the environment in a simple way, as they are traditional developers and not cloud engineers. It also needs to support rolling deployments as you grow, giving you the ability to deploy your software to existing servers without taking them all offline, as well as rolling back if required. Which AWS service do you choose to meet these needs, and how do you roll back if required?","explanation":"AWS Elastic Beanstalk is the best candidate for this requirement. You can quickly deploy code without having to learn about the infrastructure that runs those applications. Rolling deployments are possible, and a manual redeploy of the old version of your code will be required to roll back.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html","title":"Deploying Applications to AWS Elastic Beanstalk Environments - AWS Elastic Beanstalk"}],"answers":[{"id":"8783d925cf8391b96952ef87581b9144","text":"Build and deploy your application using the AWS Elastic Beanstalk service. Deploy to your servers with All-at-once deployment and initiate a roll back if required using manual redeploy.","correct":false},{"id":"587b149ea3ac40bb8b426a5c63d9707e","text":"Build and deploy the application using AWS CloudFormation and AWS Autoscaling. Deploy using CloudFormation Update Stacks, and roll back on failure to the using the previous template automatically stored in S3","correct":false},{"id":"257328bbb1bc59807cf7babc510d644f","text":"Build and deploy your application using the AWS CodeDeploy service. Deploy to your servers with CodeDeploy Rolling Deploy. Initiate a roll back if required using CodeDeploy Rollback.","correct":false},{"id":"e86fdbe990df7cf472897ce49b953712","text":"Build and deploy your application using the AWS Elastic Beanstalk service. Deploy to your servers with Rolling deployment and initiate a roll back if required using manual redeploy.","correct":true}]},{"id":"fddf85cb-72e6-4fa1-9c80-07536a84a6e8","domain":"HAFTDR","question":"You have a new website design you would like to test with a small subset of your users. If the test is successful, you would like to increase the amount of users accessing the new site to half your users. If that's successful and your infrastructure is able to scale up correctly, you would like to completely roll over to the new design and then decommission the servers hosting the old design. Which of these methods do you choose?","explanation":"A weighted routing policy combined with an auto-scaling group will meet your requirements and will continue to scale if your tests are successful and you completely roll over to the new design.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted","title":"Weighted Routing"}],"answers":[{"id":"8d8f29ecca384ae73a4006c686df91b8","text":"Install the new website design in a new AutoScaling group. Create a Lambda function to modify your Route53 apex record to use the new AutoScaling group for 5% of the day. If that's successful then modify the function to change the apex record for half the day, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false},{"id":"07123797fd0770bb8b71a6b339d36bfc","text":"Install the new website design in a new AutoScaling group. Use a Weighted Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 50%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":true},{"id":"ff9caa8ea679855cc9425d3e553bf303","text":"Install the new website design in a new AutoScaling group. Use an A/B Test Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 20%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false},{"id":"1a147be1638db0d199c9779bb5488bd7","text":"Install the new website design in a new AutoScaling group. Use a Weighted Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 20%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false}]},{"id":"6e3db01a-b168-46b2-87a2-8ee861d18334","domain":"SDLCAutomation","question":"You have set up a new AWS CodeCommit repository for your company's development team. All but one member can connect to it from an SSH command line terminal on their local machines and you are now troubleshooting the problem for that user. What are some of the possible reasons for this not working and how can you resolve the issue?","explanation":"The simplest and easiest connection method to use with CodeCommit is HTTPS connections and Git credentials. It is supported by most IDEs and development tools. However, the SSH protocol is also supported. If you want to use SSH connections for your repository, you can connect to AWS CodeCommit without installing the AWS CLI.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html","title":"Setup for HTTPS Users Using Git Credentials"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-https-unixes.html","title":"Setup Steps for HTTPS Connections to AWS CodeCommit Repositories on Linux, macOS, or Unix with the AWS CLI Credential Helper"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-without-cli.html","title":"Setup for SSH Users Not Using the AWS CLI"}],"answers":[{"id":"c87085b06d5895f92cf3cc838ee49464","text":"The user didn't install the AWS CLI. You cannot connect to AWS CodeCommit without installing the AWS CLI when using SSH connections for your repository. You must install the AWS CLI first, then associate the public key in OpenSSH format with the IAM user and finally add CodeCommit to the SSH configuration.","correct":false},{"id":"6ebf3ce5aed5449aa5c0fc11fbd81b2b","text":"Instead of using the SSH protocol, the user tried unsuccessfully another connection method. With SSH connections, you create public and private key files on the users' local machine that Git and CodeCommit use for SSH authentication. You simply associate the public key with the IAM user and store the private key on the users' local machine. Besides the use of the CodeCommit console, this is currently the only supported connection method to use with CodeCommit.","correct":false},{"id":"136c49d59bc5936c51a8afe887d7fada","text":"The user might have previously configured his local computer to use the credential helper for CodeCommit. In this case, edit the .gitconfig file to remove the credential helper information from the file before using Git credentials. On macOS, you might need to clear cached credentials from Keychain Access.","correct":true},{"id":"a607ed26280fc1c3e5fe316de5339c82","text":"Because CodeCommit requires AWS Key Management Service, a policy might be attached to the already existing IAM user that expressly denies the KMS actions required by CodeCommit. If required, update the users' policies and ensure that all required permissions are granted. For a new IAM user, attach the AWSCodeCommitFullAccess or another managed policy for CodeCommit access.","correct":true}]},{"id":"a1b1f253-8d88-4b6a-882a-146b02e9e327","domain":"MonitoringLogging","question":"Your company has been moving its core application from a monolith to an ECS based, microservices architecture.  Although functionally the application is operational in its new environment, you randomly see spikes of latency throughout the day and you are concerned with the overall performance of the application.  How can you rapidly gain information about the requests each microservice is serving?","explanation":"X-Ray is the AWS service that can deal with this sort of scenario, therefore we can discount any option that doesn't include it.  The best solution would be to deploy a Docker image running the X-Ray agent as an ECS service and this could ingest data from all sources.  You could then track the path of the application and filter on this data to find the cause.","links":[{"url":"https://aws.amazon.com/xray/faqs/","title":"AWS X-Ray FAQs"},{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html","title":"AWS X-Ray Concepts"}],"answers":[{"id":"92f0c9098189dbfb752878b2b946957f","text":"Utilise the AWS SDK to communicate with the AWS X-Ray API.  These will generate the JSON segments into an S3 bucket and the use SQL statements within AWS Athena to search through the traces stored in the bucket.","correct":false},{"id":"bfef3f324b5e71c44d859b637f210e99","text":"Deploy the X-Ray daemon to all ECS Services and view the traces and performance data that is generated, within the X-Ray console.","correct":false},{"id":"5a8dd485b25fce5ab3b460060dc89f4f","text":"Utilise AWS X-Ray by deploying a Docker image containing the X-Ray daemon to ECS, which will then gather segment data.  Then examine the traces generated to track the path of a request through the application and use Filter Expressions to find traces relating to specific paths.","correct":true},{"id":"6cd41e60c1ed008cbabe084fc5f7c5f5","text":"Enable Enhanced Cloudwatch functionality in each microservice, push all performance information into Cloudwatch Logs and use Log Insights to identify poorly performing APIs.","correct":false}]},{"id":"98efc6ca-1ded-4c51-94dd-7aa4ff3e835e","domain":"SDLCAutomation","question":"Your manager has asked you to investigate different deployment types for your application. You currently use Elastic Beanstalk so you are restricted to what that service offers. Your manager thinks it would be best to only deploy to a few machines at a time, so if there are any failures only the few that have been updated will be affected, making a rollback easier. To start with, your manager would like the roll-outs to occur on 2 machines at a time but would like the option to increase that to 3 at a time in the future. Deployments are done during quiet periods for your application, so reduced capacity is not an issue.  Which deployment type do you implement, and how can you get the most reliable indication that deployments have not introduced errors to the servers?","explanation":"The rolling deployment policy is suitable. Additional batch is not specified as a requirement.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html","title":"Elastic Beanstalk Deployment Policies and Settings"}],"answers":[{"id":"afd11d75ca714f8fd9bc5435eb0279b3","text":"Implement Elastic Beanstalk's Immutable deployment policy. Within the update policy set batch size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":false},{"id":"7257c3141e580ca3d648f8ccd2e3e77e","text":"Implement Elastic Beanstalk's Two-at-once deployment policy. Within the Two-at-once update policy set update based on Health with batch size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":false},{"id":"3ab638a2611327523fa489ffd5358081","text":"Implement Elastic Beanstalk's Rolling deployment policy. Within the Rolling update policy set Rolling based on Health with batch size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":true},{"id":"88b8bcbc9a5f8d43bd4e059cf5b1ee0f","text":"Implement Elastic Beanstalk's Rolling with additional batch deployment policy. Within the policy, set update based on Health with size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":false}]},{"id":"7bb8566a-c105-47ad-9848-105db61926ef","domain":"PoliciesStandards","question":"Your CTO, in a very urgent manner, wants to know the uptime of all of your EC2 servers with an 'environment' tag set to 'prod'. How do you achieve this quickly?","explanation":"The Run Command feature of AWS Systems Manager is suitable for this task. All roles must be configured correctly and the SSM agent needs to be installed.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html","title":"AWS Systems Manager Run Command - AWS Systems Manager"}],"answers":[{"id":"898c6c270ab3457638bd087cfd1247e8","text":"Use the Batch Command feature","correct":false},{"id":"5435e85a50449d33e87b3cf25e3dcf23","text":"Ensure required roles are configured","correct":true},{"id":"92292ff4cfdeb0c7acd7901a12866431","text":"Use AWS Service Catalog","correct":false},{"id":"9d336b9d0d3b3fc25b4610e2a7a2f90d","text":"Ensure required users are configured","correct":false},{"id":"23c84837eb29f8d7c5906f9701407929","text":"Use the Run Command feature","correct":true},{"id":"2b2248d6940ab012e3eb6750edcac3fd","text":"Ensure required agent is installed","correct":true},{"id":"d165cfe0af0f1d1de31dc1657219abe7","text":"Use AWS Systems Manager","correct":true}]},{"id":"fdf88463-93d7-415a-8d3c-3bc428f88120","domain":"IncidentEventResponse","question":"You currently work for a local government department which has cameras installed at all intersections with traffic lights around the city. The aim is to monitor traffic, reduce congestion if possible and detect any traffic accidents. There will be some effort required to meet these requirements, as lots of video feeds will have to be monitored. You're thinking about implementing an application that will user Amazon Rekognition Video, which is a Deep learning video analysis service, to meet this monitoring requirement. However before you begin looking into Rekognition, which other AWS service is a key component of this application?","explanation":"Amazon Kinesis Video Streams makes it easy to capture, process and store video streams which can then be used with Amazon Rekognition Video.","links":[{"url":"https://aws.amazon.com/rekognition/video-features/","title":"Amazon Rekognition  Video - AWS"}],"answers":[{"id":"35b2a3e15fd22f4f37f605c348df07b6","text":"Amazon Kinesis Data Streams","correct":false},{"id":"bd6f317b2c9dca8a10917f4bc718f74d","text":"Amazon Kinesis Camera Streams","correct":false},{"id":"d508d8ca67770d1a3837779bbaffacbb","text":"Amazon Kinesis Video Streams","correct":true},{"id":"fd61365f43b7f51920e8e3acc7f0f3a4","text":"Amazon Kinesis Video Analytics","correct":false}]},{"id":"74ff9710-cfb7-46df-a980-0811c45f45f0","domain":"HAFTDR","question":"For a number of years, your company has been running its Billing system using Amazon Aurora for MySQL, using additional Read Replicas to run reports and to generate invoices.  Your Manager has said that RDS is now showing a RDS-EVENT-0045 error after one of the team made a configuration change, then left to go on holiday.  After restarting the replication, you briefly get an RDS-EVENT-0046 notification which quickly returns another RDS-EVENT-0045 notification and replication stops again.  Your Manager wants you to troubleshoot the problem as soon as possible, because no bills can be generated.  What steps could you take to resolve the issue?","explanation":"All of the answers have something to do with failing replication, but we can immediately discount anything which requires a change in the code as the question states it was a configuration change which caused the initial issue.  It can't be a database engine issue, because replication will only run on a transactional engine like InnoDB.  Deleting and recreating the read replicas can resolve an issue, but only if there are data inconsistencies and performing only that operation, means that those errors will come back.  In this case, the only thing it could be, is that your colleague has changed the size of max_allowed_packet.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html","title":"Troubleshooting for Aurora"}],"answers":[{"id":"52956a328038ae53dea8c74318f72c79","text":"Remove any queries using SYSDATE() from the code, redeploy and restart replication.","correct":false},{"id":"1ed2f7210119575d1441c76431e4ab4d","text":"Delete and recreate the Read replicas using the same DB instance identifier.","correct":false},{"id":"73f213792c22de894e9255b4caa4c13e","text":"Ensure the max_allowed_packet parameter on a Read Replica is the same as that of the source DB.","correct":true},{"id":"9ad744f1e763f7a5e0ede3939d14a3a3","text":"Switch the database engine from MyISAM to InnoDB and restart replication.","correct":false},{"id":"da283d585bf5dbb555cb1e9207f19fae","text":"Change and redeploy the code to stop writing to a Read Replica, then set read_only to 1.","correct":false}]},{"id":"d4166e07-35b5-4196-b355-a04793003f88","domain":"HAFTDR","question":"You currently have a lot of IoT weather data being stored in a DynamoDB database. It stores temperature, humidity, wind speed, rainfall, dew point and air pressure. You would like to be able to take immediate action on some of that data. In this case, you want to trigger a new high or low temperature alert and then send a notification to an interested party. How can you achieve this in the most efficient way?","explanation":"Using a DynamoDB stream is the most efficient way to implement this. It allows you to trigger the lambda function only when a temperature record is created, thus saving Lambda from triggering when other records are created, such as humidity and wind speed.","links":[{"url":"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/","title":"DynamoDB Streams Use Cases and Design Patterns"}],"answers":[{"id":"bfa4fc6ae9d9c31b4764750964fc2583","text":"Use CloudWatch custom metrics to plot your temperature readings and generate an event alert if it breaches your high and low thresholds.","correct":false},{"id":"86d964e70883ad4a58b196f134df686c","text":"Write an application to use a DynamoDB scan and select on your Sort key to determine the maximum and minimum temperatures in the table. Compare them to the existing records and send an SNS notification if they are breached. Run the application every minute.","correct":false},{"id":"295bff0a7cd575c34ecae937a4f12f8e","text":"Use a DynamoDB stream and Lambda trigger only on a new temperature reading. Send a SNS notification if a record is breached.","correct":true},{"id":"3cca31ba4defd9f00540ba540543c0af","text":"Modify your IoT devices to also log their data to Kinesis Data Firehose and trigger a Lambda function which will check for new high or low temperatures. Send a SNS notification.","correct":false}]},{"id":"10530be3-3d3d-4b54-875f-066cc6789c22","domain":"ConfigMgmtandInfraCode","question":"Your company APetGuru has been developing its website and companion application for a year, and everything is currently deployed manually. You have EC2 servers and load balancers running your website front end, your backend is a database using RDS and some Lambda functions. Your manager thinks it's about time you work on some automation so time isn't wasted on managing AWS resources. He would also like the added benefit of being able to deploy your entire environment to a different region for redundancy or disaster recovery purposes. Your RTO is an hour, and RPO is two hours.  Which AWS services would you suggest using to achieve this?","explanation":"AWS CloudFormation Stack Sets will suit the requirements, using the same S3 bucket for source code, and a recent RDS snapshots will allow a RPO of one hour.","links":[{"url":"https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/","title":"Use CloudFormation StackSets to Provision Resources Across Multiple AWS Accounts and Regions | AWS News Blog"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopyDBSnapshot","title":"Copying a Snapshot - Amazon Relational Database Service"}],"answers":[{"id":"530f3a5b92e4090b35ff7569343314b5","text":"Automatically take an hourly snapshot and use a CloudWatch event trigger to run a Lambda function which copies the snapshot to your DR region.  Use AWS CodeDeploy Deployment Sets to build your infrastructure in your Production region. Parameterise the deployment set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CodeDeploy Deployment Set to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.","correct":false},{"id":"3d27ef7d6046ef9df655199e0943af4c","text":"Automatically take an hourly snapshot and use a EventWatch event trigger to run a Lambda function which copies the snapshot to your DR region.  Use AWS CloudFormation Stack Sets to build your infrastructure in your Production region. Parameterise the stack set to use the snapshot ID to deploy RDS resources from snapshot.  Use Systems Manager to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CloudFormation StackSet to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.","correct":false},{"id":"0d3bb19cd25dd0111cf3edda16072d8e","text":"Automatically take an hourly snapshot using AWS Step functions which then invokes a Lambda function to copy the snapshot to your DR region.  Use AWS CodeFormation Sets to build your infrastructure in your Production region. Parameterise the CodeFormation Set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CodeFormation Set to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.","correct":false},{"id":"3373d70ebc5df7fa36010e5b49e31e69","text":"Automatically take an hourly snapshot and use Cloud Scheduler trigger to run a Lambda function which copies the snapshot to your DR region.  Use AWS CodePipeline Regional Deploy Set to deploy your infrastructure in your Production region. Parameterise the deploy set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CodePipeline Regional Deploy Set to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.","correct":false},{"id":"bd4c9a245ec35f4087247b3a463f5d41","text":"Automatically run an AWS Step function to take an hourly snapshot then run a Lambda function to copies the snapshot to your DR region.  Use AWS CloudFormation Stack Sets to build your infrastructure in your Production region. Parameterise the stack set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CloudFormation StackSet to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.","correct":true}]},{"id":"3ab068b8-2849-4c61-8ffd-d4048963f8db","domain":"HAFTDR","question":"Due to the design of your application, your EC2 servers aren't treated as cattle as advised in the cloud world, but as pets. As such, you need DNS entries for each of them. Managing each DNS entry is taking a long time, especially when you have lots of servers, some of which may last a day, a week or a month. You don't want your Route53 records to be messy, and you would prefer some kind of automation to add and remove them. Which method would you choose to solve this in the best way?","explanation":"Tagging your instance with the required DNS record is a great way to help you automate the creation of Route53 records. A Lambda function can be triggered from a CloudWatch Events EC2 start/stop event and can add and remove the Route53 records on your behalf. This will meet your requirements and automate the creation and cleanup of DNS records.","links":[{"url":"https://aws.amazon.com/blogs/compute/building-a-dynamic-dns-for-route-53-using-cloudwatch-events-and-lambda/","title":"Building a Dynamic DNS for Route 53 using CloudWatch Events and Lambda"}],"answers":[{"id":"29aa731344e5dd9aad6885f0b8c7274c","text":"Make your instance ID the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use a CloudWatch Events rule to detect when an instance is started or stopped and trigger the Lambda function.","correct":false},{"id":"d6e2786b315c66a437523d7a8fb675c1","text":"Make your instance ID the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use CloudTrail API call logs to detect when an instance is started or stopped and trigger the Lambda function.","correct":false},{"id":"b1f933ed5c535439e1e097db065978eb","text":"Tag your instance with the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use CloudTrail API call logs to monitor when an instance is started or stopped and trigger the Lambda function.","correct":false},{"id":"338d60d516c4c48ee9e575f75fe01c98","text":"Tag your instance with the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use a CloudWatch Events rule to monitor when an instance is started or stopped and trigger the Lambda function.","correct":true}]},{"id":"2783a2d1-d7aa-42d9-8a75-041a2c73010e","domain":"MonitoringLogging","question":"During a compliance audit, a deficiency is identified stating that insufficient log monitoring and alarming exists for your entire application portfolio on AWS. You have workloads running on Amazon EC2 in ten different AWS accounts and in three AWS regions. Resolving the audit deficiency requires the creation of a centralized log monitoring capability for the AWS-based applications and infrastructure, and for certain on-premises systems that they interface with. You've been tasked with creating an automated solution that will satisfy the auditors. Which architecture will you implement for the solution?","explanation":"Amazon Elasticsearch is a fully managed service that lets you collect and analyze logs and metrics, giving you a comprehensive view into your applications and infrastructure, reducing mean time to detect (MTTD) and resolve (MTTR) issues. Amazon CloudWatch collects API events from CloudTrail, networking events from VPC Flow Logs, and application and system logs from EC2 instances via the CloudWatch Logs agent. Using a single CloudWatch log group ensures that system logs share the same retention, monitoring, and access control settings. Lambda functions in each account can move log data from CloudWatch into Elasticsearch for indexing and visualization with Kibana. CloudWatch log streams are sequences of events from the same source, whereas a log group is a collection of log streams. Logstash works well for collecting logs, but is usually paired with Elasticsearch for log monitoring and analysis. AWS Systems Manager agents send status and execution information back to Systems Manager, but not the application logs, which the CloudWatch Logs agents are capable of sending.","links":[{"url":"https://aws.amazon.com/cloudwatch/","title":"Amazon CloudWatch"},{"url":"https://aws.amazon.com/elasticsearch-service/","title":"Amazon Elasticsearch Service"},{"url":"https://aws.amazon.com/solutions/centralized-logging/","title":"Centralized Logging"}],"answers":[{"id":"bc1e38ce114c629b8597b80ee4242dc1","text":"Implement CloudWatch Logs agents on all EC2 instances and set up a single log stream. Also deploy CloudWatch Logs agents on the on-premises systems and send their logs to the same log stream. Configure Amazon CloudWatch Events to trigger an AWS Lambda function on a regular schedule. Have the Lambda function load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data from CloudWatch in all accounts into an Amazon Elasticsearch domain in the primary account. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false},{"id":"b1ea8cd72d3e0e0233c2c1dd38b80375","text":"Configure Logstash on an EC2 instance in an Auto Scaling Group in the primary account. Implement CloudWatch Logs agents on all EC2 instances and set up a single log group. Also install CloudWatch Logs agents on the on-premises systems and send their logs to the same log group. Create an AWS System Manager Automation document in the primary account to load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data in all accounts into Logstash. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false},{"id":"08a40cc3203d8aca0e7363189d9cff8e","text":"Install Logstash on an EC2 instance in an Auto Scaling Group in the primary account. Implement AWS Systems Manager agents on all EC2 instances and point them to Systems Manager in the primary account. Also install AWS Systems Manager agents on the on-premises systems and point them to Systems Manager in the primary account. Create an AWS Systems Manager Automation document in the primary account to load AWS CloudTrail, VPC Flow Log, and EC2 log data from all accounts into Logstash. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false},{"id":"9dd2205bea19fc01a45c85453e73a8a0","text":"Deploy CloudWatch Logs agents on all EC2 instances and set up a single log group. Also install CloudWatch Logs agents on the on-premises systems and send their logs to the same log group. Configure Amazon CloudWatch Events to trigger an AWS Lambda function on a regular schedule. Have the Lambda function load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data from CloudWatch into an Amazon Elasticsearch domain in the primary account. Use CloudWatch Events to trigger Lambda functions in each of the other accounts to send CloudWatch data to Elasticsearch in the primary account. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":true}]},{"id":"fc10726c-c2c6-4c3e-a514-ef41e8e8ba30","domain":"IncidentEventResponse","question":"You work for a company who sells various branded products via their popular Website.  Every time a customer completes a purchase, streaming data is immediately sent back to your endpoint and is placed onto a Kinesis Data Stream.  Most of the year, traffic remains static and no scaling of the Stream is necessary.  However, during the Black Friday period, scaling is required and this is accomplished by increasing the KCL instances and also re-sharding the Kinesis Stream.  This year, when sharding some of the streams, it was noticed that an extra shard was left after the operation finished, and this meant that if an even number of shards was requested, the number of open shards became odd.  You have been asked to troubleshoot the issue and find the cause and resolution from the list below.","explanation":"This is an issue which occurs from time to time when re-sharding.  The difference between the StartingHashKey and EndingHashKey values is normally large (depending on the number of shards you have in the stream).  Occasionally, the difference can be a very low value such as 1 and this causes the UpdateShardCount to end up with an extra shard.  This can usually be resolved by finding the ShardID with next adjacent Hash Key value, and merging the small shard with the larger shard.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/introduction.html","title":"What Is Amazon Kinesis Data Streams?"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html","title":"Troubleshooting Amazon Kinesis Data Streams Consumers"}],"answers":[{"id":"0e5554f90414fbfc45af06379bc77584","text":"This occurs if a shard iterator expires immediately before you can use it.  This may be resolved by ensuring the DynamoDB storing the lease information has enough capacity to store this data, by increasing the write capacity assigned to the shard table.","correct":false},{"id":"8d38887be863e96d47c5353c44733dd0","text":"This occurs when a producer application writes to an encrypted stream without permissions on the KMS master key. This is resolved by assigning the correct permissions to an application to access a KMS key.","correct":false},{"id":"4c33b9c64a3047658345b8abf257490c","text":"This occurs when the width of a shard is very small in size in relation to other shards in the stream. This is resolved by merging with any adjacent shard.","correct":true},{"id":"c5d9d335c1cea4d9ab165da89fc4240f","text":"This occurs when an unhandled exception is thrown in the KCL from the processRecords code, and a record is skipped. This is resolved by handling all exceptions within processRecords appropriately.","correct":false}]},{"id":"430df32b-2784-4be3-83cc-ce359d07d95a","domain":"PoliciesStandards","question":"You work for a medical imaging company, dealing with X-rays, MRI's, CT scans and so on. The images and other related patient reports and documents are stored in various S3 buckets in the US West region. Your organization is very security conscious and wants to ensure that while the S3 buckets are locked down, there's no other way that the documents are being shared internally or externally other than the approved methods already in place. Audits are also important, so whatever methods of data protection are in place must work together with this, as well as providing actionable alerts if there any observed issues. How do you best achieve this? Which AWS services can help?","explanation":"Amazon Macie is a security service that uses machine learning to discover personally identifiable information in your S3 buckets. It also provides you with dashboards and alerts that show how your private data is being accessed.","links":[{"url":"https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html","title":"What Is Amazon Macie?"}],"answers":[{"id":"2cd9c2c19e59cae06d93cba7c9861016","text":"Write a lambda function to monitor CloudTrail API calls to S3 and trigger an SNS notification if anything out of the ordinary is detected.","correct":false},{"id":"2e08a3430fb173fad3d2a91519ecc6e5","text":"Don't store sensitive data in S3, the public cloud is not secure enough. Look into moving storage in-house.","correct":false},{"id":"4a68228f60a7ed4f82aa09dbc1d18425","text":"Implement Amazon Macie across your S3 buckets.","correct":true},{"id":"4535ec14d07cb09bc414719290a94777","text":"Write a lambda function which is triggered when new data is uploaded into your S3 buckets to apply an S3 policy to ensure the data is secure.","correct":false}]},{"id":"ffda164a-53b8-4683-82d5-422d4a4910b0","domain":"ConfigMgmtandInfraCode","question":"Your Manager has informed you that all 300 of the EC2 Linux instances in the company VPC will be audited next week and in preparation, a number of shell commands must be run on each of the servers, with results posted into a secure S3 bucket.  This task has to be completed by tomorrow.  What are the quickest ways to deploy and execute the commands and to retrieve the data?","explanation":"Systems Manager gives the ability to run a list of shell commands or a script, by choosing from a list of servers or by tags.  To do this you can run the Command Document called 'AWS-RunRemoteScript' to run a script located in an S3 bucket or GitHub repository or the Command Document 'AWS-RunShellScript' which will allow individual shell commands to be run on a set of servers.  Either of these options would be the quickest and most effective way of running the audit script on the 300 servers in the shortest time.","links":[{"url":"https://aws.amazon.com/systems-manager/faq/","title":"AWS Systems Manager FAQs"}],"answers":[{"id":"9c4a809d938990c6c1f061c48223d0df","text":"In AWS Systems Manager, run the Automation Document called 'AWSSupport-TroubleshootSSH',  supply the shell commands in a list and enable 'Write results to Cloudwatch'","correct":false},{"id":"8a3910c5b9b55101e20ab80338a96fd8","text":"In AWS Systems Manager, run the Command Document called 'AWS-RunRemoteScript', add the shell command to a script and store this in S3. Enable 'Write command output to an Amazon S3 bucket'","correct":true},{"id":"341a4cd5e00a3dfea4b7da604cc91b3f","text":"In AWS Systems Manager, run the Command Document called 'AWS-RunShellScript', supply the shell commands in a list and enable 'Write command output to an Amazon S3 bucket'","correct":true},{"id":"1a4604679274c8f0b28e31aa3958d321","text":"Run the script locally using the command 'ssh -i server.pem ec2-user@remoteserver-n \"bash -s\" < /scripts/audit.sh' replacing remoteserver-n with each IP of the 300 servers which will run the local script on the remote servers.","correct":false},{"id":"977b9ab61a47ae98fc0fd7f109d46b26","text":"In AWS Systems Manager, run the Automation Document called 'AWSSupport-DeployScript', add the shell command to a script and store this in S3 and enable 'Write results to Cloudwatch'","correct":false}]},{"id":"100210fd-801f-4377-bbb1-131646b5bbf4","domain":"SDLCAutomation","question":"Your developers are currently storing their code in a private github repository, however your organization has recently introduced rules that everything must live within your AWS environment, so you need to find an alternative. What do you suggest that will both continue to work with your existing developer environments but also support a possible future transition into a CI/CD environment?","explanation":"CodeCommit is the best solution because it's compatible with CI/CD supporting services such as CodeBuild and CodePipeline","links":[{"url":"https://aws.amazon.com/codecommit/","title":"AWS CodeCommit | Managed Source Control Service"}],"answers":[{"id":"909d969c995f3c405df90a78ad1dc185","text":"Move your repositories to your own CodeCommit EC2 instance from the AWS marketplace.","correct":false},{"id":"59bc8c49cd324cbe0767a72d0787fc19","text":"Move your repositories to CodeCommit.","correct":true},{"id":"efdd4ad61a489544e40886e3db7751db","text":"Move your repositories to S3.","correct":false},{"id":"dfa1a6e5db21b8bd2df6f5674aadbbd7","text":"Move your repositories to your own Git server running in EC2.","correct":false}]},{"id":"d1e18980-dcfc-4893-afce-cb19efacb6a9","domain":"IncidentEventResponse","question":"Your CEO has heard how an ELK stack would improve your monitoring, troubleshooting and ability to secure your AWS environment. Before letting you explain anything about it, he demands you get one up and running as soon as possible using whatever AWS services you need to use. How do you go about it?","explanation":"The Amazon Elasticsearch service will give you managed Elasticsearch, Logstash and Kibana without the requirement of installing, maintaining or scaling any of them and their associated infrastructure.","links":[{"url":"https://aws.amazon.com/elasticsearch-service/resources/articles/the-benefits-of-the-elk-stack/","title":"The benefits of the ELK stack without the operational overhead"}],"answers":[{"id":"c18a97d462ec5c25526d8376971d37b2","text":"Install CloudSearch, Logstash and Kibana on an EC2 instance.","correct":false},{"id":"2c6e86f82d079bd47ee2f084bbe0fa2d","text":"Install Elasticsearch, Logstash and Kibana on an EC2 instance.","correct":false},{"id":"04de12d4521b9dc826f59dc0d42f97f5","text":"Use the Amazon Elasticsearch Service.","correct":true},{"id":"de19d8eddb68265815d3495f5ee053dd","text":"Use CloudSearch, CloudWatch Logs and CloudKibana managed services to create your ELK stack.","correct":false}]},{"id":"6e252f1f-63d8-43b5-8df5-aaa7d83874d1","domain":"HAFTDR","question":"Your application has a multi-region architecture and database reading and writing is becoming an issue. You are currently storing flat file key-pairs on a shared EFS volume across all of your application servers but it is simply too slow to handle the growth your company is experiencing. Additionally, latency of static files delivered to your customers from S3 has been noted as an issue. Which solution will be not only fast but scalable as you move forward?","explanation":"DynamoDB is the only option that supports multi-region replication and multi-master writes, and it does this using Global Tables.","links":[{"url":"https://aws.amazon.com/dynamodb/global-tables/","title":"Global Tables"}],"answers":[{"id":"cdfa2716b07fec163b72bf966bede22e","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use a multi-region write replica database for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false},{"id":"cdc9c7113a0cd60f29d5d56c49e1819b","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use Amazon DynamoDB Global Tables to create a multi-master, multi-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":true},{"id":"d6880a6902e45e48a98990a1df3f6a3d","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use multi-region rw-replica RDS configuration to create a multi-master, cross-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false},{"id":"e1f2bdaf532480be3c4d53afd2d88645","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use a multi-region read-replica RDS configuration to create a multi-master, cross-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false}]},{"id":"21d1048a-2c33-4d84-af1a-ad6a039e0ddd","domain":"SDLCAutomation","question":"Your organization currently runs a hybrid cloud environment with servers in AWS and in a local data center. You currently use a cronjob and some bash scripts to compile your application and push it out to all of your servers via SSH. It's both difficult to log, maintain and extend to new servers when they are provisioned. You've been considering a move to CodePipeline to manage everything for you. Will it suit your requirements?","explanation":"CodeBuild is able to deploy to any server that can run the CodeDeploy agent, whether in AWS or in your own data centre.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/instances.html","title":"Working with Instances for CodeDeploy"}],"answers":[{"id":"f81f6e25dfbc48337f9ed9f263923302","text":"No, CodeDeploy wont be able to deploy to the non-AWS managed servers in your data center","correct":false},{"id":"2c74ebd4177ccf3f0749b4ecdcd73e7c","text":"No, CodePipeline wont be able to deploy to the non-AWS managed servers in your data center","correct":false},{"id":"beba600b1da886e2a2ad8713309745a2","text":"Yes, CodePipeline will be able to interface with any servers that can run the CodePipeline agent","correct":false},{"id":"299b7a966370e0bfb587d4c2d5ff774b","text":"Yes, CodeDeploy can deploy to any servers that can run the CodeDeploy agent","correct":true}]},{"id":"1e2dcf0c-3e73-455f-a757-d704864b5194","domain":"ConfigMgmtandInfraCode","question":"Your CEO loves serverless. He wont stop talking about how your entire company is built on serverless architecture. He attends Serverlessconf to talk about it. Now, it's up to you to actually build the web application he's been talking about for 6 months. Which AWS services do you look at using to both create the application and orchestrate your components?","explanation":"AWS Lambda is the correct choice for compute in a serverless environment, as is DynamoDB for NoSQL databases and S3 for storage. AWS Step Functions are used to orchestrate your components, such as your lambda functions.","links":[{"url":"https://aws.amazon.com/serverless/","title":"Serverless Computing  Amazon Web Services"}],"answers":[{"id":"a4414e5d2e313708a970e89063df27af","text":"Create your application using AWS Elastic Compute for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":false},{"id":"e9ce4547402d74b7fd336d09bf24e58b","text":"Create your application using AWS Lambda for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Serverless Application Framework can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":false},{"id":"86fe4a20afac7056ea6e0f5dd7f2e70d","text":"Create your application using AWS Elastic Beanstalk for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Cold Store will allow you to archive old files cheaply.","correct":false},{"id":"f3b92e9d4214f817c65085013da23eb8","text":"Create your application using AWS Lambda for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":true}]}]}}}}
