{"data":{"createNewExamAttempt":{"attempt":{"id":"9ed325c1-d7b2-4a11-851a-0e80b67e01a9"},"exam":{"id":"8f0a6714-c363-4079-a182-9997aec22fd1","title":"AWS Certified DevOps Engineer - Professional 2019","duration":10800,"totalQuestions":75,"questions":[{"id":"1e2dcf0c-3e73-455f-a757-d704864b5194","domain":"ConfigMgmtandInfraCode","question":"Your CEO loves serverless. He wont stop talking about how your entire company is built on serverless architecture. He attends Serverlessconf to talk about it. Now, it's up to you to actually build the web application he's been talking about for 6 months. Which AWS services do you look at using to both create the application and orchestrate your components?","explanation":"AWS Lambda is the correct choice for compute in a serverless environment, as is DynamoDB for NoSQL databases and S3 for storage. AWS Step Functions are used to orchestrate your components, such as your lambda functions.","links":[{"url":"https://aws.amazon.com/serverless/","title":"Serverless Computing â€“ Amazon Web Services"}],"answers":[{"id":"a4414e5d2e313708a970e89063df27af","text":"Create your application using AWS Elastic Compute for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":false},{"id":"e9ce4547402d74b7fd336d09bf24e58b","text":"Create your application using AWS Lambda for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Serverless Application Framework can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":false},{"id":"f3b92e9d4214f817c65085013da23eb8","text":"Create your application using AWS Lambda for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":true},{"id":"86fe4a20afac7056ea6e0f5dd7f2e70d","text":"Create your application using AWS Elastic Beanstalk for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Cold Store will allow you to archive old files cheaply.","correct":false}]},{"id":"977b9167-9509-4260-91b8-745ff4a10c16","domain":"ConfigMgmtandInfraCode","question":"You need to quickly test a proof of concept that your boss has given you. He's given you a zip file containing a php web application. You want to get it running in an AWS environment as fast a possible, however there's also a dependency on a library that must be installed as well. The library is available from a yum/apt repository.  Which service do you choose and how do you ensure dependencies are installed?","explanation":"AWS Elastic Beanstalk is a quick way to test the proof of concept, as webserver configuration is not required. Required Libraries can be installed quickly and automatically using ebextensions.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html","title":"Customizing Software on Linux Servers - AWS Elastic Beanstalk"}],"answers":[{"id":"adde411da8e54597432c35347938b2ec","text":"AWS Elastic Beanstalk for deployment, install dependency with ebextensions.","correct":true},{"id":"4f921aa28b46ed7642eb7a3dc8583844","text":"AWS OpsWorks for deployment, install dependency with chef.","correct":false},{"id":"c2f64a6cd63768d866b164a5c278f252","text":"AWS EC2 and apache2, install dependency with apt-get or yum.","correct":false},{"id":"4cc5c190534c4e744d352d96c3ebe2bc","text":"AWS CloudFormation, install dependency with custom resources.","correct":false}]},{"id":"25964f23-34d0-4ed9-9f24-4cee82e76511","domain":"ConfigMgmtandInfraCode","question":"You are building acatguru, which your organization describes as facebook for cats. As part of the sign-up process your users need to upload a full size profile image. You already have these photos being stored in S3, however you would like to also create thumbnails of the same image, which will be used throughout the site. How will you automate this process using AWS resources?","explanation":"S3 triggering Lambda to create thumbnails is a perfect example of how to automate this process","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html","title":"Using AWS Lambda with Amazon S3"}],"answers":[{"id":"a24cc963be9859fff809b1da36be8137","text":"Configure S3 to publish its event stream to an SNS topic. Subscribe a Lambda function to the SNS topic which will trigger when a file is uploaded. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":false},{"id":"62751bd29f8b4728e9393f35cf7bf035","text":"Use CloudTrail to monitor PUT and POST calls sent to S3 and trigger a Lambda function when you identify an upload. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":false},{"id":"9d2bb40a4f2a61f4e70855a070faa693","text":"Create an S3 event trigger to execute a Lambda function when an object is created. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":true},{"id":"30a4f470d1bf4e4d01dab0088e331a6d","text":"Create an S3 bucket notification trigger to execute a Lambda function when an object is created. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":false}]},{"id":"00ff039c-bad1-448c-8d17-e6ad8b1491f6","domain":"SDLCAutomation","question":"You manage a team of developers who currently push all of their code into AWS CodeCommit, and then CodePipeline automatically builds and deploys the application. You think it would be useful if everyone received an email when a pull request is created or updated. How would you achieve this in the simplest way?","explanation":"Notifications in the CodeCommit console is the simplest way to implement this requirement. Triggers only trigger when someone pushes to a repository, not when a pull request is created.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-repository-email.html","title":"Configuring Notifications for Events in an AWS CodeCommit Repository"}],"answers":[{"id":"ee242f6e3c35045661e6306dbd2e0153","text":"Enable notifications in the CodeCommit console. Select 'Pull request update events' as the event type and choose or create a new SNS topic for the notifications. Subscribe everyone to the SNS topic.","correct":true},{"id":"d5af6b2263080d905efb836b99bf8b19","text":"Enable notifications in the CodePipeline console. Create a notification stage after your source stage and select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.","correct":false},{"id":"15c5b59dd67cddfeb10567fef856e7b1","text":"Enable triggers in the CodePipeline console. Create a trigger stage after your source stage and select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.","correct":false},{"id":"439fdcb8b7e144ddacda7b944536f82b","text":"Enable triggers in the CloudCommit console. Select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.","correct":false}]},{"id":"dad35c13-8f81-4aac-a7b9-c2ed4a5f9335","domain":"HAFTDR","question":"With your company moving more internal services into AWS, your colleagues have started to complain about using different credentials to access different applications. Your team has started to plan to implement AWS SSO, connected to the corporate Active Directory system, but are struggling to implement a working solution.  Which of the following are not valid troubleshooting steps to confirm that SSO is enabled and working?","explanation":"The question states which are NOT valid troubleshooting steps, so we need to choose the ones which will not help us troubleshoot the issues. Firstly, you can use the User Principal Name (UPN) or the DOMAIN\\UserName format to authenticate with AD, but you can't use the UPN format if you have two-step verification and Context-aware verification enabled. Secondly, AWS Organisations and the AWS Managed Microsoft AD must be in the same account and the same region.  The answers which suggest the opposite are the ones which should be chosen.  The other answers are correct troubleshooting steps and therefore can not be chosen.","links":[{"url":"https://docs.aws.amazon.com/singlesignon/latest/userguide/prereqs.html","title":"AWS SSO PreRequisites"},{"url":"https://docs.aws.amazon.com/singlesignon/latest/userguide/troubleshooting.html","title":"Troubleshooting AWS SSO Issues"}],"answers":[{"id":"ff24f8f9a780de8999f077e354ae2eef","text":"To allow authentication using a User Principal Name, enable two-step verification in Context-aware verification mode.","correct":true},{"id":"eb8551b7f4293f316685247af6fbe823","text":"Implement AWS Organisations with 'All Features' enabled, deploy the AD Connector residing in your master account.","correct":false},{"id":"0fa6f9fa1ae19cce6cead12014268484","text":"Ensure the number of AWS SSO permission sets are less than 500 and you have no more than 1500 AD groups.","correct":false},{"id":"fe88b553bb4a39e8268dcb243738d505","text":"AWS SSO with Active Directory only allows authentication using the DOMAIN\\UserName format.","correct":true},{"id":"3627bfa9871db5cdd41006b26ec5dbbe","text":"Implement AWS Organisations and deploy AWS Managed Microsoft AD in two separate accounts.  It does not matter which regions they are deployed in.","correct":true}]},{"id":"4c02fcad-e6c2-438b-927c-a343af2e4e89","domain":"SDLCAutomation","question":"About a dozen people collaborate on a company-internal side project using CodeCommit. The developer community is spread across multiple timezones and relies on repository notifications via email. Initially, it was configured for all event types but this resulted in a lot of emails and team members complained about too much 'noise'. Since then, commit comment notifications have been turned off. How can you improve this situation?","explanation":"You can create up to 10 triggers for each CodeCommit repository. The customData field is of type string and is used for information that you want included in the Lambda function such as the IRC channel used by developers to discuss development in the repository. It cannot be used to pass any dynamic parameters. This string is appended as an attribute to the CodeCommit JSON returned in response to the trigger. You can comment on an overall commit, a file in a commit, or a specific line or change in a file. For best results, use commenting when you are signed in as an IAM user. The commenting functionality is not optimized for users who sign in with root account credentials, federated access, or temporary credentials. By default, an Amazon SNS topic subscriber receives every message published to the topic. To receive a subset of the messages, a subscriber must assign a filter policy to the topic subscription","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html","title":"Manage Triggers for an AWS CodeCommit Repository"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda-cc.html","title":"Example: Create a Trigger in AWS CodeCommit for an Existing AWS Lambda Function"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-commit-comment.html","title":"Comment on a Commit in AWS CodeCommit"},{"url":"https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html","title":"Amazon SNS Message Filtering"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-repository-email.html","title":"Configuring Notifications for Events in an AWS CodeCommit Repository"}],"answers":[{"id":"421e56d9693bf5c7a95f304f8be452c8","text":"Create a CodeCommit trigger that invokes a Lambda function when a new comment is added to a commit. Configure it so that its 'Custom data' field is populated with the email address of the user who authored the original commit, i.e. ${commit.author.email}. In the function, use SNS to send the notification to that address.","correct":false},{"id":"bb2ee7c9a6b15ed2c648ce414e7f5501","text":"Ask all team members to sign in to CodeCommit as IAM users.","correct":true},{"id":"a74a895989f0aff5ae55553849a46032","text":"For each team member, create an individual SNS topic and a CodeCommit trigger that uses a Lambda function to filter out notifications not authored by that developer. The remaining ones are send to that SNS topic. This allows users to selectivly subscribe to specific persons to follow their activities.","correct":false},{"id":"a00a4fd86554d04bbf089d9b1f302ca3","text":"Assign Amazon SNS subscription filter policies to the 'commit comment' topic subscriptions so that team members receive only a subset of the messages.","correct":true}]},{"id":"67394e5d-b7ef-4850-b0da-9548411156f3","domain":"SDLCAutomation","question":"Vector Technologies Incorporated wants to implement a CI/CD environment for their online bill payment system. The application runs in a .NET environment on AWS Elastic Beanstalk, and the database is SQL Server. Code is stored in a GitHub repository external to AWS. Changes to the database schema are needed for most code updates, and are done with a script that is also stored on GitHub. The CI/CD process should be initiated every time new source code or a new schema update script is moved to the central repo. Which architecture will provide the capability to automatically retrieve, build, and deploy application components each time changes are made?","explanation":"Code Pipeline can be invoked every time there is a code change or new schema update script in the GitHub repository branch, both of which can be written to S3. CodePipeline can then trigger CodeBuild to use the MSBuild Windows image that has been stored in ECR to build the .NET application code. The resulting package is then placed in a build artifacts S3 folder, which is picked up by CodeDeploy to be implemented on Elastic Beanstalk. CodeDeploy can also invoke a PowerShell script to run the schema update executable. An ECS container is not needed to perform this build, and will incur additional costs. CodeBuild can work directly with the MSBuild container image in ECR. NAnt provides the capability to automate software build processes, but MSBuild is still needed to compile the .NET code. CodeDeploy won't be able to run the database schema update scripts without them being embedded in an executable.","links":[{"url":"https://aws.amazon.com/codepipeline/","title":"AWS CodePipeline"},{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild"},{"url":"https://aws.amazon.com/codedeploy/","title":"AWS CodeDeploy"},{"url":"https://aws.amazon.com/quickstart/architecture/dotnet-cicd-on-aws/","title":".NET CI/CD on the AWS Cloud"},{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html","title":"CodeDeploy AppSpec File Reference"}],"answers":[{"id":"2b48c63fdb564d11a32c45fcc0205c3f","text":"Create an MSBuild container image with the required tools for compiling .NET applications and push it to Amazon Elastic Container Registry (ECR). Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use the MSBuild container image from ECR to compile the source code. Configure CodeBuild to then construct the .NET application. Also have CodeBuild create an executable to run the schema update script. Have CodePipeline trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy invoke a Powershell script to run the schema update executable.","correct":true},{"id":"3833bcf2af22e331e4ba509f55aa1236","text":"Use AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use NAnt to compile the source code and construct the .NET application. Configure CodePipeline to trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy run the schema update scripts against the database.","correct":false},{"id":"a69266d62fd4ec974f90c80873f07508","text":"Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use NAnt to compile the source code and construct the .NET application. Also have CodeBuild create an executable to run the schema update script. Configure CodePipeline to trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy invoke a Powershell script to run the schema update executable.","correct":false},{"id":"0bace657c20367e9692de64ec05c7669","text":"Write an MSBuild container image with the required tools for compiling .NET applications and push it to Amazon Elastic Container Registry (ECR). Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script and write them to S3. Have CodePipeline trigger AWS CodeBuild to create an Amazon Elastic Container Service (ECS) container from the ECR image. Use the ECS container application to retrieve the latest code updates from S3 and construct the .NET application. Have CodePipeline trigger AWS CodeDeploy to implement the .NET application on Elastic Beanstalk. Have CodeDeploy run the schema update scripts against the database.","correct":false}]},{"id":"3e8ee2fb-8946-43b2-ba85-78ce64b74626","domain":"ConfigMgmtandInfraCode","question":"You are discussing error scenarios and possible retry strategies for your Step Functions machine with your colleague. Which of her claims are incorrect?","explanation":"Errors can arise because of state machine definition issues, task failures or because of transient issues. When a state reports an error, the default course of action for AWS Step Functions is to fail the execution entirely.","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-errors.html","title":"AWS Step Functions: Amazon States Language - Errors"}],"answers":[{"id":"8c65f4ebde68eb7bc8a58c7747c9f843","text":"A Retry field with an 'IntervalSeconds' and 'MaxAttempts' value of 3 and 'BackoffRate' value of 1.5 will make three retry attempts after waits of 3, 4.5 and 6.75 seconds.","correct":false},{"id":"c1c080d749a342bf5e9fc2509c11fe40","text":"When a state reports an error, the default course of action for AWS Step Functions is to log the error and perform a single retry after 1 second. If that doesn't succeed, AWS Step Functions will fail the execution entirely.","correct":true},{"id":"51e213b667c0c51b9814eb2430f2d081","text":"A Retrier must contain the 'ErrorEquals' field which is a non-empty array of strings that match Error Names, e.g. 'States.Timeout'. When a state reports an error, Step Functions scans through the Retriers and, when the Error Name appears in this array, it implements the retry policy described in this Retrier.","correct":false},{"id":"5673ef332c854836ed6963374170526a","text":"Any state can encounter runtime errors. Examples are when a Lambda function throws an exception or if a transient network issue exists. AWS Step Functions distinguishes these clearly from Failures such as state machine definition issues that are handled differently.","correct":true}]},{"id":"9404d153-05aa-4c0e-aeb4-5e38aa54f0d9","domain":"MonitoringLogging","question":"Your company has built an app called InstaFaceTube. It's incredibly successful. Your user-base is growing exponentially. Your manager has decided it's time to ensure the web application is as efficient as possible, running with the best possible performance. To achieve this you will have to monitor all aspects of the application and then analyze the issues to determine the root cause of any latencies, errors or issues that might be causing your application to slow down. How do you achieve this using AWS tools?","explanation":"Implementing the AWS X-Ray SDK will assist you in achieving this goal. It will help you understand how your application and its underlying services are performing and give you the information required to analyze issues and determine the root cause of issues.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-gettingstarted.html","title":"Getting Started with AWS X-Ray - AWS X-Ray"}],"answers":[{"id":"d68e824386322cec32abb02c7d311e66","text":"Implement the Amazon Athena SDK, send sectors to Athena, view the service tables and blocklines in the Athena console.","correct":false},{"id":"5dd4c06446d96cea277f4219bafed0c7","text":"Implement the Amazon QuickSight SDK, send partitions to QuickSight, view the service tables and traces in the QuickSight console.","correct":false},{"id":"84b0ed98a0ac96d03ac7f24dbd70d85c","text":"Implement the AWS Macie SDK, send blocks to Macie, view the service graph and blocklines in the Macie console.","correct":false},{"id":"1b1c00f2d92850ce3ef939b7be776c78","text":"Implement the AWS X-Ray SDK, send segments to X-ray, view the service graph and traces in the X-Ray console.","correct":true}]},{"id":"d1e18980-dcfc-4893-afce-cb19efacb6a9","domain":"IncidentEventResponse","question":"Your CEO has heard how an ELK stack would improve your monitoring, troubleshooting and ability to secure your AWS environment. Before letting you explain anything about it, he demands you get one up and running as soon as possible using whatever AWS services you need to use. How do you go about it?","explanation":"The Amazon Elasticsearch service will give you managed Elasticsearch, Logstash and Kibana without the requirement of installing, maintaining or scaling any of them and their associated infrastructure.","links":[{"url":"https://aws.amazon.com/elasticsearch-service/resources/articles/the-benefits-of-the-elk-stack/","title":"The benefits of the ELK stack without the operational overhead"}],"answers":[{"id":"04de12d4521b9dc826f59dc0d42f97f5","text":"Use the Amazon Elasticsearch Service.","correct":true},{"id":"de19d8eddb68265815d3495f5ee053dd","text":"Use CloudSearch, CloudWatch Logs and CloudKibana managed services to create your ELK stack.","correct":false},{"id":"c18a97d462ec5c25526d8376971d37b2","text":"Install CloudSearch, Logstash and Kibana on an EC2 instance.","correct":false},{"id":"2c6e86f82d079bd47ee2f084bbe0fa2d","text":"Install Elasticsearch, Logstash and Kibana on an EC2 instance.","correct":false}]},{"id":"b299b67c-9e64-46b9-8397-b3d92e5a8e9a","domain":"SDLCAutomation","question":"You have multiple teams of developers and at the moment they all have the ability to start and stop any EC2 instance that they can see in the EC2 console, which is all of them. You would really like to implement some security measures so they can only start and stop the instances based on their cost center. What AWS features would you use to achieve this?","explanation":"You can simplify user permissions to resources by using tags and policies attached to roles. the aws:PrincipalTag is a tag that exists on the user or role making the call, and an iam:ResourceTag is a tag that exists on an IAM resource. In this case we want the CostCenter tag on the resource to match the ConstCenter tag assigned to the developer.","links":[{"url":"https://aws.amazon.com/blogs/security/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles/","title":"Simplify granting access to your AWS resources by using tags"}],"answers":[{"id":"1597cc0291688fb61d2e070e6fefc038","text":"Implement tags and restrict access by comparing the aws:PrincipalTag and the iam:ResourceTag in a policy attached to your developer role and seeing if they match.","correct":true},{"id":"3e3f004e8462587e3c0ee0b9692cde3b","text":"Implement EC2 policies which you can assign to each resource which will allow a developer to start or stop the instance if they are also assigned to it.","correct":false},{"id":"d106bcfe8a672539187a3453ec7498d0","text":"Implement roles which you can assign to each resource which will allow a developer to start or stop the instance if they are also assigned to it.","correct":false},{"id":"3c58e079022c3f48db6068b4e061539c","text":"Implement tags and restrict access by comparing the iam:PrincipalTag and the aws:ResourceTag in a policy attached to your developer role and seeing if they match.","correct":false}]},{"id":"b1b39162-3af1-4aa5-9cd4-4e30338d07f6","domain":"HAFTDR","question":"Your CEO wants you to start future proofing your AWS environment, so he's asked you to look into IPv6 compatibility of your existing Load Balanced EC2 stack. You make use of both Application (ALB) and Network (NLB) load balancers in your EC2-VPC. What are your findings?","explanation":"At this moment in time only the Application Load Balancer supports IPv6 in the EC2-VPC environment. Classic Load balancers only support IPv6 if you are using an EC2-Classic environment.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/01/announcing-internet-protocol-version-6-ipv6-support-for-elastic-load-balancing-in-amazon-virtual-private-cloud-vpc/","title":"Announcing Internet Protocol Version 6 (IPv6) support for Elastic Load Balancing in Amazon Virtual Private Cloud (VPC)"}],"answers":[{"id":"2a74b0b6126f559f800f0f5cef044526","text":"No Load Balancers in EC2 support IPv6.","correct":false},{"id":"eda9ef0a0ac30b1db3a5bf8674687603","text":"Application and Network Load Balancers both support IPv6.","correct":false},{"id":"12d596f99a6d191c9c1aaae49901a9e1","text":"Application Load balancers support IPv6, Network Load Balancers do not.","correct":true},{"id":"4aff7beb08ea7fe4fa49ab96903dfc41","text":"Application Load balancers do not support IPv6, Network Load balancers do.","correct":false}]},{"id":"e94bdca3-b267-4c84-8485-4e53c9319db4","domain":"ConfigMgmtandInfraCode","question":"You have an application built on docker. You would like to not only launch your docker instances at scale but load balance them as well. Your director has also enquired as to whether it is possible to build and store the containers programmatically using AWS API calls in future or if additional products are required. Which AWS services will enable you to store and run the containerized application and what do you tell the director?","explanation":"Amazon ECS and ECR are the Amazon Elastic Container Service and Registry, which will meet all of the requirements specified. It also has an API which can be used to implement your requirements.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html","title":"What is Amazon Elastic Container Service? - Amazon Elastic Container Service"}],"answers":[{"id":"361a8c7414d01c019db585e4f410d8c0","text":"Build containers and store them using AWS ECR (Elastic Container Registry). Run the containers using Amazon ECS. Automated container builds are not possible with AWS services and APIs however third-party build engines such as Jenkins and BuildKite will achieve your director's goal.","correct":false},{"id":"2b2965d7b7b01e772fd99e1883745cc6","text":"Build EC2 instances with docker using EC2 and AutoScaling and store them using AWS ECR (Elastic Container Registry). Run the containers on docker desktop on EC2. Yes automated container builds are possible with AWS CodeBuild pushing to ECR and using ECS APIs.","correct":false},{"id":"9d3a19960e725c6a8f4133c71337a88e","text":"Build containers and store them using AWS ECR (Elastic Container Registry). Run the containers using Amazon ECS. Yes automated container builds are possible with AWS CodeBuild pushing to ECR and using ECS APIs.","correct":true},{"id":"2786f753087512c515526c6e2f9909a3","text":"Build EC2 instances with docker using EC2 and AutoScaling and store them using AWS ECR (Elastic Container Registry). Run the containers using Autoscaling EC2. Automated container builds are not possible with AWI services and APIs however third-party build engines such as Jenkins and BuildKite will achieve your director's goal.","correct":false}]},{"id":"6e4406f4-c446-4f58-9533-e04e5b45eb0d","domain":"ConfigMgmtandInfraCode","question":"After many years of running co-located servers, your company has decided to move their services into AWS.  The prime reasons for doing this is to scale elastically and to define their new infrastructure as code using CloudFormation.  Your colleagues have been defining infrastructure in a test account but now they are ready to deploy into production, they are identifying some problems.  They have have attempted to deploy the main CloudFormation Template but they are seeing the following errors; 'Invalid Value or Unsupported Resource Property', 'Resource Failed to Stabilize' and when these errors are encountered, the stack fails to rollback cleanly.  Assist your team by choosing the best ways to troubleshoot these problems.","explanation":"Most of these answers are valid troubleshooting solutions for various CloudFormation issues, but there is only one answer for the problems listed in the question. 'Invalid Value or Unsupported Resource Property' errors appear only if there is a parameter naming mistake or that the property names are unsupported. 'Resource Failed to Stabilize' errors appear because a timeout is exceeded, the AWS service isn't available or is interrupted.  Finally, any update which fails to rollback could be because of a number of reasons, but the most popular is due to the deployment account having permissions to create stacks, but not to modify or delete stacks. The answer which includes all of these pieces of advice is correct.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html","title":"Troubleshooting AWS CloudFormation"}],"answers":[{"id":"9f3b9793c5af6485de0b1b166a4e36be","text":"Ensure that your AMI has the CloudFormation helper scripts installed and that your VPC has a defined route to the Internet.  Also ensure that the user deploying the CloudFormation stack has enough permissions to modify resources.","correct":false},{"id":"a577e14c499181ee450e6306c9e57820","text":"Ensure that you do not have termination protection enabled, that a DependsOn attribute has been included in the Template and that there is no maintenance being undertaken on any of the defined AWS services.","correct":false},{"id":"7114f5881fc7b95daef2e696e207bac9","text":"Check that a DependsOn attribute has been included in the Template, and that you are not waiting for a resource which usually takes longer than the default timeout period. Finally, ensure that the user deploying the CloudFormation stack has enough permissions to create resources.","correct":false},{"id":"c271e5866ef0c9376f5675822c410c2f","text":"Check that resources exist when specified as parameters, ensure that there is no maintenance being undertaken on any of the defined AWS services and that the user deploying the CloudFormation stack has enough permissions to modify resources.","correct":true}]},{"id":"0c1d0c55-7a4a-4f82-b5be-a93cd9be404a","domain":"ConfigMgmtandInfraCode","question":"Your team has been moving various legacy code projects over to AWS in the past few months.  Every application has been analysed to see whether it is best hosted on an EC2 instance or as a Lambda function. One particular software project has been rewritten to store an object in an S3 bucket, and then this action triggers a Lambda function to compress the file as a new object in the same bucket, and then delete the original. Unfortunately, due to the way you have deployed the solution the Lambda function appears to be constantly invoking itself and is stuck in a continuous loop.  How do you temporarily stop this from happening whilst you investigate?","explanation":"When architecting a solution, always ensure that you do not generate a recursive loop.  This is when something in one part of the infrastructure will trigger something elsewhere, and in turn trigger the first part once again.  In particular these scenarios are problematic when one AWS service triggers another AWS service and this in turn triggers the first.  In the example, this can only be rectified by enabling the Throttle option, which sets the reserved concurrency to zero and will throttle all future invocations of this function. This action should only be used in case of emergencies.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/lambda-troubleshooting.html","title":"Troubleshooting AWS Lambda"}],"answers":[{"id":"bdaac855074eb8ac866a362c5e7d6713","text":"Remove the IAM User or Role associated with the Lambda function to cease operation.  The Lambda function will stop and allow you to troubleshoot.","correct":false},{"id":"f709eae24d782eb5222b2128abfc354e","text":"You can constrain the memory or processing power available to the function. The Lambda function will stop and allow you to troubleshoot.","correct":false},{"id":"e2a75904e82f340ee969cbc6d6bdec5b","text":"There is no way to stop a Lambda function once in an infinite loop.  You need to delete the function, re-create it and re-deploy the code.","correct":false},{"id":"e9c4b55a14d04ed64e78c4852549c372","text":"Choose the 'Throttle' option on the function configuration page and then locate and resolve the error which caused the recursive invocation.","correct":true}]},{"id":"9ec0b495-6913-472d-bf6c-7bdfc7a22bef","domain":"MonitoringLogging","question":"You have three EC2 instances running a core application, which has been performing sub-optimally since yesterday.  One of your colleagues said that they remember that the system appeared to perform in a similar way about 18 months ago, but they can't remember what the issue was.  You need to perform an indepth investigation of the current issue and you will need to view graphs of that period, with granular metrics.  Reading the logs from when the issue originally occurred would also help troubleshooting.  Which of the following options would give you the best chance of resolving the issue?","explanation":"You can immediately disregard any option with Cloudtrail as these will only contain API logs and will not record application issues.  For the remaining options, it's important to note that Cloudwatch Logs are available indefinitely by default, so any option stating that logs can't be kept can also be excluded.  Now we have to factor in the length of time in the past we are investigating.  15 months is the maximum amount of time we can retrieve metrics from the Console, this means that we need to retrieve the data from the API and process it locally.","links":[{"url":"https://aws.amazon.com/cloudwatch/faqs/","title":"Amazon CloudWatch FAQs"}],"answers":[{"id":"cb2703cd52452e6b2dd73d35a2beab1d","text":"View the Cloudwatch logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 60 minutes.","correct":false},{"id":"3a9e3e96a4470855c66e7a3f8d0ad2ef","text":"View the Cloudwatch logs from 18 months ago and use the API to retrieve the datapoints and store in a local Database for analysis.","correct":true},{"id":"4adf59b6cbba3d749598c710e3671bdf","text":"View the Cloudwatch logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 15 minutes.","correct":false},{"id":"1c6bee332c95b0f8ec7ed30a6efaa04a","text":"View the Cloudwatch graphs from 18 months ago, setting the granularity at 1 minute. Unfortunately Cloudwatch logs cannot be kept for that length of time.","correct":false},{"id":"aa3088febe99800c543d7921893448b2","text":"View the Cloudtrail logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 1 minute.","correct":false}]},{"id":"5e50532a-5d17-4a6f-a367-db3eb2c87698","domain":"SDLCAutomation","question":"Your organization has been using CodePipeline to deploy software for a few months now and it has been smoothly for the majority of releases, but when something breaks during the build process it requires lots of man hours to determine what the problem is, roll back the deployment and fix it. This is frustrating both management and your customers. Your supervisor would like to assign one developer to test the build works successfully before your CodePipeline proceeds to the deploy stage so you don't encounter this issue again. How would you implement this?","explanation":"CodePipeline allows for manual approval steps to be implemented for exactly this reason","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html","title":"Add a Manual Approval Action to a Pipeline in CodePipeline"}],"answers":[{"id":"ca05b5864508a85819911de3b31a9c02","text":"Ask the assigned developer to run a local build first to test all changes, and then commit it to the repository which is then deployed to production only when they know there are no errors.","correct":false},{"id":"fb383cd098c54e0f94598a5029e3c99e","text":"Configure SES to email the assigned developer when CodePipeline has deployed to production. This will provide an immediate notification so they can check if there are any errors in testing. Then they can push the changes to the production git branch.","correct":false},{"id":"61692576c930a7b4f4bb412881fa1829","text":"Configure SQS to email the assigned developer when CodePipeline has deployed to production. This will provide an immediate notification so they can check if there are any errors in testing. Then they can push the changes to the production git branch.","correct":false},{"id":"adac0bfeeb378f849ee1f8575b660f83","text":"Create a test deploy stage as well as a manual approval stage in CodePipeline. Once the assigned developer checks the testing deploy worked, they can authorize the pipeline to continue and deploy to production.","correct":true}]},{"id":"ee99751c-1258-4f81-adf9-31a8c57b2605","domain":"ConfigMgmtandInfraCode","question":"Your CloudFormation template is becoming quite large, so you have been tasked with reducing its size and coming up with a solution to structure it in a way that works efficiently. You have quite a few resources defined which are almost duplicates of each other, such as multiple load balancers using the same configuration. What CloudFormation features could you use to help clean up your template?","explanation":"Nested stacks should be used to reuse common template patterns.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#nested","title":"AWS CloudFormation Best Practices"}],"answers":[{"id":"86793dc47df7cca5d373cdb3f8fc8f61","text":"Use Intrinsic functions. This allows dedicated functions to be called and parameters passed to generate resources.","correct":false},{"id":"242cb2d9063dee2cfd0a6b0cabacb580","text":"Use goto policies. These allow you to refer to different sections of your template for reuse.","correct":false},{"id":"b8d5d665b7d8b4abfc48daa77a64c021","text":"Use nested stacks. This will allow a dedicated templates to be defined for services that are used multiple times","correct":true},{"id":"8a32bed4f4f47b1c84105ed6173ba8c4","text":"Use custom resources. They can be called by your stack to define resources without having to reuse their code.","correct":false}]},{"id":"0e604476-a66d-48c8-b59d-dbb5efe66d38","domain":"SDLCAutomation","question":"AWS CodeBuild is used for a project that you look after which has a known issue with its build performance. You can trace it back to an environment variable named AWS_CODEBUILD_MAX_MEM_ALLOC that is defined in multiple places. It has a value of '4096' in your build spec declaration, '2048' in the build project definition and a value of '1024' in the start build operation call. Which of the following statements is correct?","explanation":"Its value is '1024'. Do not set any environment variable with a name that starts with 'CODEBUILD_' as this prefix is reserved for internal use.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html","title":"Build Specification Reference for CodeBuild: Build Spec Syntax: env"}],"answers":[{"id":"3e4a9e589e2cb112cdc5aff1387285fc","text":"The variable is undefined as the 'AWS_CODEBUILD_' prefix of its name is reserved for internal use.","correct":false},{"id":"a09a6c2e72b77a064358b20cbf7ffeea","text":"The value in the start build operation call takes highest precedence, followed by the value in the build project definition while the value in the build spec declaration takes lowest precedence. Therefore its value is determined as '1024'.","correct":true},{"id":"7201fd1c5d65552aba9eb7f9f1b7c846","text":"Its value is '4096' because the value in the build spec declaration takes highest precedence, followed by the value in the build project definition while the value in the start build operation call takes lowest precedence.","correct":false},{"id":"6d6b3a85646a3254ab985c43b3a11a20","text":"The value in the build project definition takes highest precedence, followed by the value in the start build operation call while the value in the build spec declaration takes lowest precedence. Because of this, its value is '2048'.","correct":false},{"id":"91e162eb52d2dd94ba6973eae901a468","text":"The value of AWS_CODEBUILD_MAX_MEM_ALLOC  is '2048' as the build project definition takes highest precedence, followed by the value in the build spec declaration while the value in the start build operation call takes lowest precedence.","correct":false}]},{"id":"d330cf75-1b29-4c26-a96b-1bb878b00c5f","domain":"MonitoringLogging","question":"Your security conscious CEO has a strange request. She would like an email every time someone signs in to your organization's AWS Console. What is the simplest way to implement this?","explanation":"The AWS Console Sign-in is a valid event source in CloudWatch Events. This would be the simplest way to implement this requirement.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html","title":"What is Amazon CloudWatch Events?"}],"answers":[{"id":"623afa587682320a753c248c3d41f3c4","text":"Configure a CloudWatch event for the AWS Console Sign-in service and set the target to an SNS queue your CEO is subscribed to.","correct":true},{"id":"0bde2cccb1e8f94a030f2f445adae3fc","text":"Configure an IAM trigger on each user and set the target to an SNS queue your CEO is subscribed to.","correct":false},{"id":"1b70fcf29e1f7aaadbf314e9a69a0dd9","text":"Configure CloudTrail to stream events to CloudWatch Logs and configure a Lambda function to run on a 1 minute schedule to find logins. Send the results to an SNS queue your CEO is subscribed to.","correct":false},{"id":"6497f488d9ea9251133be660f2896779","text":"Configure a CloudWatch logs Insight query to pull login attempts from the CloudTrail log and send the results to an SNS queue your CEO is subscribed to.","correct":false}]},{"id":"2783a2d1-d7aa-42d9-8a75-041a2c73010e","domain":"MonitoringLogging","question":"During a compliance audit, a deficiency is identified stating that insufficient log monitoring and alarming exists for your entire application portfolio on AWS. You have workloads running on Amazon EC2 in ten different AWS accounts and in three AWS regions. Resolving the audit deficiency requires the creation of a centralized log monitoring capability for the AWS-based applications and infrastructure, and for certain on-premises systems that they interface with. You've been tasked with creating an automated solution that will satisfy the auditors. Which architecture will you implement for the solution?","explanation":"Amazon Elasticsearch is a fully managed service that lets you collect and analyze logs and metrics, giving you a comprehensive view into your applications and infrastructure, reducing mean time to detect (MTTD) and resolve (MTTR) issues. Amazon CloudWatch collects API events from CloudTrail, networking events from VPC Flow Logs, and application and system logs from EC2 instances via the CloudWatch Logs agent. Using a single CloudWatch log group ensures that system logs share the same retention, monitoring, and access control settings. Lambda functions in each account can move log data from CloudWatch into Elasticsearch for indexing and visualization with Kibana. CloudWatch log streams are sequences of events from the same source, whereas a log group is a collection of log streams. Logstash works well for collecting logs, but is usually paired with Elasticsearch for log monitoring and analysis. AWS Systems Manager agents send status and execution information back to Systems Manager, but not the application logs, which the CloudWatch Logs agents are capable of sending.","links":[{"url":"https://aws.amazon.com/cloudwatch/","title":"Amazon CloudWatch"},{"url":"https://aws.amazon.com/elasticsearch-service/","title":"Amazon Elasticsearch Service"},{"url":"https://aws.amazon.com/solutions/centralized-logging/","title":"Centralized Logging"}],"answers":[{"id":"08a40cc3203d8aca0e7363189d9cff8e","text":"Install Logstash on an EC2 instance in an Auto Scaling Group in the primary account. Implement AWS Systems Manager agents on all EC2 instances and point them to Systems Manager in the primary account. Also install AWS Systems Manager agents on the on-premises systems and point them to Systems Manager in the primary account. Create an AWS Systems Manager Automation document in the primary account to load AWS CloudTrail, VPC Flow Log, and EC2 log data from all accounts into Logstash. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false},{"id":"9dd2205bea19fc01a45c85453e73a8a0","text":"Deploy CloudWatch Logs agents on all EC2 instances and set up a single log group. Also install CloudWatch Logs agents on the on-premises systems and send their logs to the same log group. Configure Amazon CloudWatch Events to trigger an AWS Lambda function on a regular schedule. Have the Lambda function load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data from CloudWatch into an Amazon Elasticsearch domain in the primary account. Use CloudWatch Events to trigger Lambda functions in each of the other accounts to send CloudWatch data to Elasticsearch in the primary account. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":true},{"id":"b1ea8cd72d3e0e0233c2c1dd38b80375","text":"Configure Logstash on an EC2 instance in an Auto Scaling Group in the primary account. Implement CloudWatch Logs agents on all EC2 instances and set up a single log group. Also install CloudWatch Logs agents on the on-premises systems and send their logs to the same log group. Create an AWS System Manager Automation document in the primary account to load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data in all accounts into Logstash. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false},{"id":"bc1e38ce114c629b8597b80ee4242dc1","text":"Implement CloudWatch Logs agents on all EC2 instances and set up a single log stream. Also deploy CloudWatch Logs agents on the on-premises systems and send their logs to the same log stream. Configure Amazon CloudWatch Events to trigger an AWS Lambda function on a regular schedule. Have the Lambda function load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data from CloudWatch in all accounts into an Amazon Elasticsearch domain in the primary account. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false}]},{"id":"cae0814f-fa70-443b-9c43-1cca04205b54","domain":"SDLCAutomation","question":"Your senior developer wants to be able to access any past version of the binaries that are being built as part of your CI/CD pipeline. You are using CodeBuild with CodePipeline to automate your build process. How will you achieve this?","explanation":"CodeBuild has an optional 'Namespace type', which will insert the build ID into the path to the build output zip file or folder, giving you a unique directory and binary artifact for each build that runs.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/create-project.html","title":"Create a Build Project in CodeBuild"}],"answers":[{"id":"4eed75febec27e9d78dc57a1fbbc70f8","text":"Change the artifact namespace type to Build ID, which will insert the build ID into path of the output folder in S3.","correct":true},{"id":"8c3218ccd025dc9444babadf01808d86","text":"Nothing needs to change, by default all artifacts are given a unique filename in S3.","correct":false},{"id":"baa367f7cda7f088cdd1fbab6a72d39b","text":"Change the artifacts packaging to zip, which will append a version number to each build.","correct":false},{"id":"c52ea2b58f7db8902570649989645ace","text":"Implement a CodeBuild lambda trigger which will copy each build artifact to S3 with a unique ID.","correct":false}]},{"id":"6a6f97fb-45c8-4509-91c7-4507629d60fa","domain":"HAFTDR","question":"Your current workload with DynamoDB is extremely latency bound, and you need it to be as fast as possible. You do not have time to look at other AWS services but instead have been instructed to use features and configuration changes of the services you are currently using. What do you do?","explanation":"Implementing a DAX cluster is the ideal solution here. It meets the requirement of using the existing DynamoDB service feature, while having the ability to reduce latency from milliseconds to microseconds. DynamoDB Scan times can also be optimised by reducing the number of attributes in your table and grouping attributes as JSON blobs within a single attribute.","links":[{"url":"https://aws.amazon.com/dynamodb/dax/","title":"Amazon DynamoDB Accelerator (DAX) â€“ Fully managed in-memory cache for DynamoDB"},{"url":"https://aws.amazon.com/blogs/database/optimizing-amazon-dynamodb-scan-latency-through-schema-design/","title":"Optimize Amazon DynamoDB scan latency through schema design | AWS Database Blog"}],"answers":[{"id":"b5002075c11491df649c4c4f6e960472","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement DynamoDB Cached for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":false},{"id":"3d69fc095075fdb4cc8e97f241e0fb76","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement Elasticache Memcached for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":false},{"id":"357461d5fd66eb609146fe0e65854901","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement Elasticache Redis for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS Insights to make available Insights tracing of their DynamoDB calls.","correct":false},{"id":"4e7fca450cdf45d6ded7827b5ef88c16","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement a DAX cluster for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":true},{"id":"1c8d38560c0975ffecdfff42f01c5965","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement a DAX cluster for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS Insights to make available Insights tracing of their DynamoDB calls.","correct":false}]},{"id":"f6b4a9c9-32fd-433d-93fd-4b61bad8db8a","domain":"PoliciesStandards","question":"You are employee #2 in a new startup, right after the founders and their first developer. You've been given the task of checking over the AWS account that has been used so far and determining what service limits you are currently breaching (if any) and producing a report on the current service limit levels. How do you obtain this information?","explanation":"Trusted advisor makes Service Limit checks free for all users at any support level.","links":[{"url":"https://aws.amazon.com/blogs/mt/monitoring-service-limits-with-trusted-advisor-and-amazon-cloudwatch/","title":"Monitoring Service Limits"}],"answers":[{"id":"0ee62b6775e4ea63a1288cd941f273a5","text":"Sign up for a business support plan to access the detailed billing reports in your billings console.","correct":false},{"id":"6ca7f9bfe1536150652a1e4ecaf6789c","text":"Enable detailed billing and look at the Service Limit reporting page, which is free for all users.","correct":false},{"id":"de4b26655e419de8a97e82d3598b1eda","text":"Enable Trusted Advisor and look at the Service Limit page, which is free for all users.","correct":true},{"id":"1812a1f14ddc45354e11146b11914c56","text":"Sign up for a business support plan to access the Service Limit Checks in Trusted Advisor","correct":false}]},{"id":"bd630b5c-8d55-48ed-9cde-2a0d186b5350","domain":"MonitoringLogging","question":"You have a large amount of infrastructure, and monitoring has been neglected since it was provisioned. You want to monitor your AWS resources, your on-premises resources, applications and services. You would like to be able to retain your system and application logs, graph metrics and be alerted to critical events. Which AWS services and features will assist you in meeting this requirement?","explanation":"Amazon CloudWatch Metrics is a suitable service for graphing, Amazon CloudWatch Logs will allow you to log both AWS and on-premises resources, and Amazon CloudWatch Alarms will be suitable for alerts and notifications.","links":[{"url":"https://aws.amazon.com/cloudwatch/","title":"Amazon CloudWatch - Application and Infrastructure Monitoring"}],"answers":[{"id":"2fb0f1a79c04ca46dfb050e4c7918475","text":"Amazon CloudWatch Metrics for graphing, Amazon CloudWatch Logs for logging, Amazon CloudWatch Alarms for alerts.","correct":true},{"id":"cd727ff9e153593f4a455b61933f1bca","text":"Amazon CloudWatch Metrics for graphing, Amazon CloudLog for logging, Amazon CloudWatch Alarms for alerts.","correct":false},{"id":"572ff814493d6190a6180e49f2deaa1a","text":"Amazon QuickSight for graphing, Amazon CloudWatch Logs for logging, Amazon CloudWatch Alarms for alerts.","correct":false},{"id":"03ee68867429d155452a986cc42efb65","text":"Amazon CloudTrail for graphing and logging, Amazon CloudWatch Alarms for alerts","correct":false}]},{"id":"5c90df95-736b-4b27-a7cf-095c4dc3d4c4","domain":"HAFTDR","question":"You run a load balanced, auto-scaled website in EC2. Your CEO informs you that due to an upcoming public offering, your website must not go down, even if there is a region failure. What's the best way to achieve this?","explanation":"A latency based routing policy will keep your website as fast as possible for your customers, and will act as redundancy should one of the regions go down.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy"}],"answers":[{"id":"230b3ec4e1a08538bc506cca11e9c1a1","text":"Deploy your load balancers and auto-scaled website in two different availability zones. Create a Route53 GeoProximity Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":false},{"id":"f20603c9e3176b9a3bdd585343418443","text":"Deploy your load balancers and auto-scaled website in two different regions. Create a Route53 Latency Based Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":true},{"id":"560436ce37b94afeee434582179ddc95","text":"Deploy CloudFront in front of your instances. It will cache requests even if a region goes down and your users will not notice.","correct":false},{"id":"20703f2de437222f1e63b4b81216e053","text":"Deploy your load balancers and auto-scaled website in two different availability zones. Create a Route53 weighted Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":false}]},{"id":"6313796d-1496-4bfc-b9e4-fe35e00f883b","domain":"HAFTDR","question":"Your team is excited about embarking upon their first greenfield AWS project after months of lift-and-shift migration from your old datacenter into AWS.  This will be your first true infrastructure as code project.  Your team consists of eight people who all will be making changes to infrastructure over time. You would like to use native AWS tooling for writing the infrastructure templates however you are concerned about how team member changes will actually affect the resources you have running already. You don't want to accidentally destroy important AWS resources due to a developer or engineer changing a CloudFormation property whose update property requires replacement. How can you ensure that engineers are aware of the impact of their updates before they implement them, and protect important stateful resources such as EBS volumes and RDS instances against accidental deletion?","explanation":"CloudFormation Change sets will let you submit your modified stack template, it will compare it for you and show you which stack settings and resources will change. You can then execute that change set if you are happy with the changes that will occur.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"a0fa6661560c21c0020855fcc08e5013","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use diff in the Linux terminal to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":false},{"id":"1f2e9869f312d47386abc4a295c498d1","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use CloudFormation Drift Check to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":false},{"id":"3264bb68f07bc74ca58c6288d8f1074e","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use CloudFormation Modify Set to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":false},{"id":"08659d45b30150542b63689978cf172d","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they first create a CloudFormation Change Set. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":true}]},{"id":"80f664b2-633c-4476-ae74-6d249087e79f","domain":"IncidentEventResponse","question":"You are a day trader working from home with a software engineering background. You are considering making use of some cloud services to alert you of stock price changes so you know immediately when the price of certain stocks has risen or fallen more than 1%. You know you can ingest your data to a Kinesis Data stream, but how will you query it?","explanation":"You can query your streams directly from your application using SQL and Kinesis Data Analytics","links":[{"url":"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/continuous-queries-concepts.html","title":"Continuous Queries - Amazon Kinesis Data Analytics for SQL Applications Developer Guide"}],"answers":[{"id":"e010acc5480838d8d1679bd03d0730ba","text":"Use JSON with Kinesis Data Analytics","correct":false},{"id":"1069736ede5f32f059462eb2901e8b93","text":"Use YAML with Kinesis Data Analytics","correct":false},{"id":"ce8028a6137a5e1bb29aa083a15ccd3c","text":"Use RDS with Kinesis Data Analytics","correct":false},{"id":"04efd74e70cd57a041797aa4b35f74e7","text":"Use SQL with Kinesis Data Analytics","correct":true}]},{"id":"860e8b11-9faf-4176-8c11-716aeccf799b","domain":"HAFTDR","question":"You're assisting a developer working on a very large and read-heavy application which uses an Amazon Aurora database cluster. The feature currently being worked on requires reading but no writing, however it will be called by the application frequently and from multiple different servers so the reads need to be load balanced. Additionally your reporting team need to make ad-hoc, expensive queries which need to be isolated so that reads for production are not affected by reporting.  Which Aurora configuration fulfils both needs with minimal extra configuration?","explanation":"The Reader endpoint is appropriate in this situation. The reader endpoint provides load balanced support for read only connections to the database cluster. A custom endpoint can be used to connect to an isolated replica for report generation or ad hoc (one-time) querying,","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html","title":"Amazon Aurora Connection Management - Amazon Aurora"}],"answers":[{"id":"5ff29b324290cc87e7f79502c91662fd","text":"Use the Custom endpoint for your Production system to make its reads against a single high-capacity replica, and create another custom endpoint pointing to a dedicated reporting replica for isolation of ad-hoc reporting.","correct":false},{"id":"2b1242d9bda15b98989935f538280b8f","text":"Use the Cluster endpoint for your Production system to make its reads against high-capacity read replicas, and create an Aurora reporting endpoint pointing to a separate replica for isolation of ad-hoc reporting.","correct":false},{"id":"af531b8d353cecad595506c5239ca5f2","text":"Use the Aurora Instance endpoint for your Production system to make load-balanced reads against a read replicas, and create another custom endpoint pointing to a dedicated reporting replica for isolation of ad-hoc reporting.","correct":false},{"id":"794b7ab3749de5b254f0dd3f2337bc74","text":"Use the Reader endpoint for your Production system to make its load-balanced reads against high-capacity read replicas, and create a custom endpoint pointing to a separate replica for isolation of ad-hoc reporting.","correct":true}]},{"id":"3072e673-774e-4865-a7ed-85fafa597eeb","domain":"SDLCAutomation","question":"Your companies Security Officer just mandated a policy whereby all in use encryption keys within your organisation must be rotated every 120 days. How does that affect your CodePipelines Artifact store?","explanation":"An artifact store for your pipeline, such as an Amazon S3 bucket, is not the source bucket for your source code and is required for each pipeline. When you create or edit a pipeline, you must have an artifact bucket in the pipeline Region, and then you must have one artifact bucket per AWS Region where you are running an action. Unlike the console, running the create-pipeline command in the AWS CLI does not create an Amazon S3 bucket for storing artifacts. The bucket must already exist. When you enable automatic key rotation for a customer managed CMK, AWS KMS generates new cryptographic material for the CMK every year.","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create.html","title":"Create a Pipeline in CodePipeline"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/S3-artifact-encryption.html","title":"Configure Server-Side Encryption for Artifacts Stored in Amazon S3 for CodePipeline"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/S3-view-default-keys.html","title":"View Your Default Amazon S3 SSE-KMS Encryption Keys"},{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html","title":"Rotating Customer Master Keys"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/S3-rotate-customer-key.html","title":"Configure Server-Side Encryption for S3 Buckets When Using AWS CloudFormation or the CLI"}],"answers":[{"id":"83305810e49f3d0e2dc10c7f564a2616","text":"CodePipeline Artifact stores are regional. When you use the CodePipeline console in a specific region to create a pipeline and choose 'Default location' when asked for an Artifact store, a new default one will be created for you in that region if none existed beforehand. CodePipeline creates default AWS-managed SSE-KMS encryption keys when you create a pipeline using the Create Pipeline wizard. The master key is encrypted along with object data and managed by AWS. However, you can also create and manage your own customer-managed SSE-KMS keys in AWS KMS to encrypt or decrypt artifacts in your artifact store and rotate these keys as necessary.","correct":true},{"id":"7cb596ab386cdf5e14c65e9384a96895","text":"Your artifact store can either be a CodeCommit, GitHub or an Amazon ECR repository. It can also be an S3 source bucket where your source code is stored. Every 120 days, you need to generate new access keys and change the connection details to your repository if you are not using S3 as your artifact store. No further action is required if you use S3.","correct":false},{"id":"03be9a17c0b30ac5a6639484c188ed1c","text":"AWS recommends to use the same default global artifact store for all your CodePipelines. This is created for you when you create your first pipeline in any region. By default, CodePipeline uses server-side encryption with the AWS KMS-managed keys (SSE-KMS) using the default key for Amazon S3 (the aws/s3 key). This key is created and stored in your AWS account. When artifacts are retrieved from the artifact store, CodePipeline uses the same SSE-KMS process to decrypt the artifact. Change that key every 120 days.","correct":false},{"id":"30285e906f24c643049a339b0551b048","text":"You can use any already existing artifact store as long as it is in the same Region as your pipeline. When you create a pipeline using AWS CloudFormation or the CLI, you must configure server-side encryption for your artifact store manually. Use an appropriate bucket policy and then create your own customer-managed SSE-KMS encryption keys. Instead of using the default Amazon S3 key, choose to use your own keys so that you can rotate these every 120 days as per your organisations security requirements.","correct":true},{"id":"75c9d50127886728d1cfde275bdb7915","text":"You must have a separate artifact store for each CodePipeline. When using the AWS CLI, these are automatically created during the creation of your pipelines. CodePipeline also configures default AWS-managed SSE-KMS encryption keys for your artifact store. If you enable automatic key rotation and specify a refresh rate of 120 days, AWS KMS generates new cryptographic material and saves the key's older cryptographic material so that it can be used to decrypt data that it encrypted.","correct":false}]},{"id":"dc3d128f-8cce-4541-b6e6-be1ec6cea96d","domain":"SDLCAutomation","question":"Your CI/CD pipeline generally runs well, but your manager would like a report of some CodeBuild metrics, such as how many builds were attempted, how many builds were successful and how many builds failed in an AWS account over a period of time. How would you go about gathering the data for you manager?","explanation":"These are default CloudWatch metrics that come with CodeBuild.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html","title":"Monitoring Builds with CloudWatch Metrics"}],"answers":[{"id":"6149c7ca08a16f697f2988509bb61427","text":"Implement a lambda function to poll the CodeBuild API to gather the data and store it in CloudWatch Logs. Write a metric filter to graph the data and generate your report.","correct":false},{"id":"c5045cc818946fcb9c3f6d71744202b7","text":"Configure CodeBuild to log builds to CloudWatch Logs, and then write a metric filter which will graph the data points your manager requires.","correct":false},{"id":"19e6e35ed627bf15d10ddc88af1ab9be","text":"Configure a CloudWatch custom metric to track the build information, and create custom graphs in the CloudWatch console.","correct":false},{"id":"a1f83b40240e6af928c6cab6eeb010c1","text":"CloudWatch metrics will report these metrics by default. You can view them in the CloudWatch console.","correct":true}]},{"id":"3ab068b8-2849-4c61-8ffd-d4048963f8db","domain":"HAFTDR","question":"Due to the design of your application, your EC2 servers aren't treated as cattle as advised in the cloud world, but as pets. As such, you need DNS entries for each of them. Managing each DNS entry is taking a long time, especially when you have lots of servers, some of which may last a day, a week or a month. You don't want your Route53 records to be messy, and you would prefer some kind of automation to add and remove them. Which method would you choose to solve this in the best way?","explanation":"Tagging your instance with the required DNS record is a great way to help you automate the creation of Route53 records. A Lambda function can be triggered from a CloudWatch Events EC2 start/stop event and can add and remove the Route53 records on your behalf. This will meet your requirements and automate the creation and cleanup of DNS records.","links":[{"url":"https://aws.amazon.com/blogs/compute/building-a-dynamic-dns-for-route-53-using-cloudwatch-events-and-lambda/","title":"Building a Dynamic DNS for Route 53 using CloudWatch Events and Lambda"}],"answers":[{"id":"d6e2786b315c66a437523d7a8fb675c1","text":"Make your instance ID the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use CloudTrail API call logs to detect when an instance is started or stopped and trigger the Lambda function.","correct":false},{"id":"338d60d516c4c48ee9e575f75fe01c98","text":"Tag your instance with the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use a CloudWatch Events rule to monitor when an instance is started or stopped and trigger the Lambda function.","correct":true},{"id":"29aa731344e5dd9aad6885f0b8c7274c","text":"Make your instance ID the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use a CloudWatch Events rule to detect when an instance is started or stopped and trigger the Lambda function.","correct":false},{"id":"b1f933ed5c535439e1e097db065978eb","text":"Tag your instance with the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use CloudTrail API call logs to monitor when an instance is started or stopped and trigger the Lambda function.","correct":false}]},{"id":"82e554d2-f867-4507-974f-04b0b9020cb9","domain":"MonitoringLogging","question":"Your company has a team of Windows Software Engineers which have recently switched from developing on-premise applications, to cloud native micro-services.  The Service Desk has been inundated with questions, but they can't troubleshoot because they don't know enough about Amazon CloudWatch.  The questions they have been receiving mainly revolve around why EC2 logs don't appear in log groups and how they can monitor .NET applications.  Choose the following options which will help troubleshoot the Amazon Cloudwatch issues.","explanation":"The question suggests we are utilising a Windows development environment, so we can discount any answers which have Linux only terms such as running shell scripts.  To run Amazon CloudWatch Application Insights for .NET and SQL Server, we will need to install the SSM Agent with the correct roles, IAM policies and Resource Groups.  We also need to ensure that the CloudWatch agent is running correctly, by starting it using the amazon-cloudwatch-agent-ctl.ps1 script, and as we are assuming defaults, Cloudwatch metrics can be found under the CWAgent namespace.  There are limits for many items in Cloudwatch, utilising Custom Metrics is not one of them.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/troubleshooting-CloudWatch-Agent.html","title":"Troubleshooting the CloudWatch Agent"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.html","title":"Amazon CloudWatch Application Insights for .NET and SQL Server"}],"answers":[{"id":"ff3e53ec436317efcebcc4a0bf3b52a8","text":"In the Amazon Cloudwatch console, select Metrics, AWS Namespaces and then your metrics should appear under 'CWAgent'.","correct":true},{"id":"aa7402fca9b873f6b49d144c5c67440e","text":"For each EC2 instance running .NET code, install the SSM Agent and attach the AmazonEC2RoleforSSM Role.  You will also need to create a Resource Group and IAM Policy.","correct":true},{"id":"815da42a7b90966156b2d66da1a16b9e","text":"Check the common-config.toml file exists and is configured correctly, and then run the following Powershell command; amazon-cloudwatch-agent-ctl.ps1 -m ec2 -a start.","correct":true},{"id":"33668d47f8774c94713045bff628ba97","text":"If you are utilising CloudWatch Custom Metrics, ensure that you have not reached the default limit.","correct":false},{"id":"ffd7b413b965db5c4e66d6dc2c8a73e8","text":"Check the aws-agent.yaml file exists and is configured correctly, and then run the following shell command; aws-cw-agent.sh start.","correct":false}]},{"id":"243b97db-004d-4630-bae5-7020609c97d9","domain":"PoliciesStandards","question":"You have a web application in development which accesses a MySQL database in Amazon RDS. Your organization wants to securely store the credentials to the database so only the developers can retrieve them, and would also like the ability to rotate the passwords on a schedule. How would you best way store the credentials to ensure security and make sure no one else in your organisation has access to them?","explanation":"AWS secrets manager can use IAM policies to restrict access, as well as having the capability to automatically rotate passwords on a schedule.","links":[{"url":"https://aws.amazon.com/secrets-manager/","title":"AWS Secrets Manager"}],"answers":[{"id":"d3f8c3a26e2e90782dd1d1fb76b35173","text":"Store the passwords in AWS Secrets Manager, and use an AWS IAM policy to control the access permissions so only members of the developer group can access the credentials. Ensure your web application is also has permission to use this IAM policy.","correct":true},{"id":"f87078b1bafcac4b31333b4212e2d96d","text":"Store the passwords in Amazon S3 and use an AWS IAM policy to restrict access. Implement a cronjob on an EC2 server which will update the password in MySQL and upload it to S3.","correct":false},{"id":"b24e060733fd54d8805c389d724f62d4","text":"Store the passwords in AWS CodeCommit, and only give the developers the password to the repository. The developers with repository access can then code this into the application, and change it whenever the passwords need to be rotated","correct":false},{"id":"53fe0032f55d7f0c1026328f51bab301","text":"Store the passwords on an encrypted EBS instance attached to an EC2 instance where you can implement a cronjob to rotate the passwords on a schedule. Ensure only the developer SSH key is installed on the EC2 server.","correct":false}]},{"id":"f6f2e5d0-a72e-4d44-aaea-2e2e8fa264c3","domain":"HAFTDR","question":"Your company utilizes EC2 and various other AWS services for its workloads. As the DevOps engineer it is your responsibility to ensure all company policies are implemented. You have noticed that while you are using S3 for data archival and backups, your company policy is that your backups need to reside on company owned servers, such as those you run in your local Equinix data center. You also have another company policy that backup and archival cannot traverse the internet. What do you do?","explanation":"AWS Direct Connect is the only way to access your AWS resources from a Data Center without traversing the internet, despite the encryption offered by the other solutions.","links":[{"url":"https://aws.amazon.com/directconnect/","title":"AWS Direct Connect"}],"answers":[{"id":"f42f4a5866921fe7f0288f667c043aac","text":"Provision an AWS Direct Connect connection to your local router in your data center and your local VPC. Push backups via the Direct Connect connection.","correct":true},{"id":"32f6b2f1cc26546abf7ce835ae778156","text":"Install the EFS agent on your data center backup NAS, mount the volume on an EC2 server and copy the backups to the volume.","correct":false},{"id":"b51577fa11c831eba7efec0c73952602","text":"Implement a Site to site VPN with a company owned server in your data center. Push backups to your data center backup NAS through SSH via vpn.","correct":false},{"id":"008894eae706c0e60a40a59c79342658","text":"Implement an AWS Client VPN with a company owned server in your data center. Push backups to your data center backup NAS through SSH via vpn.","correct":false}]},{"id":"0917c160-74a4-439b-818b-248197d8fd54","domain":"IncidentEventResponse","question":"Your organization has a few million text documents in S3 that are stored in a somewhat random manner, and the amount of files is always growing. The developer that initially wrote the system in use stored everything with a random file name with some attempt at security through obscurity. Now your CEO and CFO both need to be able to search the contents of these documents, and they want to be able to do so quickly at a reasonable cost. What managed AWS services can assist with implementing a solution for your CEO and CFO, and what would the setup process involve?","explanation":"CloudSearch by itself is enough to fulfill the requirements put forward here. CloudSearch is managed, scalable can very quick to configure and get online. In comparison it would take some time to set up EC2 and install ElasticSearch or any other search tool, and would be much more difficult to scale. This involves creating a search domain and configuring the index as required, and then setting up the access policies.","links":[{"url":"https://aws.amazon.com/cloudsearch/","title":"AWS | Amazon CloudSearch - Search Service in the Cloud"}],"answers":[{"id":"562f9538310fcd7c846445ad9768ab7c","text":"Implement Amazon CloudSearch","correct":true},{"id":"e26c5d07fc30848902a99c95897441b4","text":"Set up IAM roles","correct":false},{"id":"8f9d7ad9c2c6d20b41851ce8e3572aaa","text":"Configure your index","correct":true},{"id":"280209643a35ef346f22e051eafca06e","text":"Create a search domain","correct":true},{"id":"d912e8309bb58d3483307e1ea428bf8a","text":"Set up access policies","correct":true},{"id":"ab25d01b99c512f0cb03129afc920a07","text":"Implement ElasticSearch","correct":false},{"id":"705d488d75db0b38eb9811a38a863d57","text":"Implement MongoDB","correct":false},{"id":"a34f38b0356fb4da2ce3a29a66b4b415","text":"Configure your baseline","correct":false},{"id":"38b79d58ed1b5a50d6bbc91173d0c745","text":"Create a search index","correct":false}]},{"id":"57aa5551-ba69-43ce-acd0-daca2c3b1573","domain":"MonitoringLogging","question":"Your CFO wants to know when your estimated AWS charges go over $10,000. Immediately. Without delay. How do you configure this?","explanation":"After enabling billing alerts, you will be able to use the CloudWatch EstimatedCharges metric to track your estimated AWS charges. Creating an alarm to notify your CFO via SNS is also a good idea.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html","title":"Create a Billing Alarm to Monitor Your Estimated AWS Charges"}],"answers":[{"id":"3be9600096bae9c4f9c03bf973014324","text":"Set your Detailed Billing Threshold to $10,000 in the Billing console. Configure the CFO to be the Usage Alert email address.","correct":false},{"id":"311b1ef14cf67f369bb179ff7248b867","text":"Use a Lambda function to poll the CloudWatch API for the EstimatedCharges metric. If it exceeds $10,000 notify an SNS queue your CFO is subscribed to.","correct":false},{"id":"fdc8f5cad0921b3ad23acefc21d3875f","text":"Enable billing alerts. Use the default CloudWatch EstimatedCharges metric and create an alarm when it exceeds $10,000. Set the alarm to notify an SNS queue your CFO is subscribed to.","correct":true},{"id":"544a9a7a06a5adfc0b19d06b964c90d6","text":"Use the default CloudWatch EstimatedCharges metric and create an alarm when it exceeds $10,000. Set the alarm to notify an SNS queue your CFO is subscribed to.","correct":false}]},{"id":"74ff9710-cfb7-46df-a980-0811c45f45f0","domain":"HAFTDR","question":"For a number of years, your company has been running its Billing system using Amazon Aurora for MySQL, using additional Read Replicas to run reports and to generate invoices.  Your Manager has said that RDS is now showing a RDS-EVENT-0045 error after one of the team made a configuration change, then left to go on holiday.  After restarting the replication, you briefly get an RDS-EVENT-0046 notification which quickly returns another RDS-EVENT-0045 notification and replication stops again.  Your Manager wants you to troubleshoot the problem as soon as possible, because no bills can be generated.  What steps could you take to resolve the issue?","explanation":"All of the answers have something to do with failing replication, but we can immediately discount anything which requires a change in the code as the question states it was a configuration change which caused the initial issue.  It can't be a database engine issue, because replication will only run on a transactional engine like InnoDB.  Deleting and recreating the read replicas can resolve an issue, but only if there are data inconsistencies and performing only that operation, means that those errors will come back.  In this case, the only thing it could be, is that your colleague has changed the size of max_allowed_packet.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html","title":"Troubleshooting for Aurora"}],"answers":[{"id":"73f213792c22de894e9255b4caa4c13e","text":"Ensure the max_allowed_packet parameter on a Read Replica is the same as that of the source DB.","correct":true},{"id":"9ad744f1e763f7a5e0ede3939d14a3a3","text":"Switch the database engine from MyISAM to InnoDB and restart replication.","correct":false},{"id":"1ed2f7210119575d1441c76431e4ab4d","text":"Delete and recreate the Read replicas using the same DB instance identifier.","correct":false},{"id":"52956a328038ae53dea8c74318f72c79","text":"Remove any queries using SYSDATE() from the code, redeploy and restart replication.","correct":false},{"id":"da283d585bf5dbb555cb1e9207f19fae","text":"Change and redeploy the code to stop writing to a Read Replica, then set read_only to 1.","correct":false}]},{"id":"9a41a543-3d2a-4090-b97a-16d86d7a7ab0","domain":"SDLCAutomation","question":"CodeBuild has been configured as the Action provider for your Integration Test Action which has been added to your CodePipelines' 'Test' stage. These tests connect to your RDS test database that is isolated on a private subnet and because executing that stage alone takes nearly 2 hours, you want to run it during the night before developers come into the London office in the morning. How might you go about this?","explanation":"Enabling Amazon VPC access in your CodeBuild project requires the ID of the VPC, subnets, and security groups. You cannot use the internet gateway instead of a NAT gateway or a NAT instance because CodeBuild does not support assigning elastic IP addresses to the network interfaces that it creates, and auto-assigning a public IP address is not supported by Amazon EC2 for any network interfaces created outside of Amazon EC2 instance launches. The ordered cron expression fields are: minutes, hours, day of month, month, day of week, year.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/03/aws-codepipeline-adds-support-for-unit-testing/","title":"AWS CodePipeline Adds Support for Unit and Custom Integration Testing with AWS CodeBuild"},{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html","title":"Use CodePipeline with CodeBuild to Test Code and Run Builds"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-schedule-cli.html","title":"Create a CloudWatch Events Rule That Schedules Your Pipeline to Start (CLI)"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html","title":"Schedule Expressions for Rules"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/codebuild/start-build.html","title":"AWS CLI Command Reference: start-build"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_StartPipelineExecution.html","title":"AWS CodePipeline: API Reference: Actions: StartPipelineExecution"},{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/vpc-support.html","title":"Use CodeBuild with Amazon Virtual Private Cloud"}],"answers":[{"id":"f10014d4b7fb41a95a0f4102b36a8460","text":"Amazon VPC access in your CodeBuild project needs to be enabled. Specify your VPC ID, subnets, and security groups in your build project and set up a rule in Amazon CloudWatch Events to start the pipeline on a schedule. Use the following command for that: aws events put-rule --schedule-expression 'cron(10 3 ? * MON-FRI *)' --name IntegrationTests","correct":true},{"id":"680092a0588f13a4230a986961172023","text":"To allow CodeBuild access to your database, you have to specify the VPC ID and the ID of the subnet of your private RDS instance. Then create a CloudWatch events rule to schedule a CodeBuild project build using the following cron expression: 3 10 * * ? *","correct":false},{"id":"e49c56dbee3e27e947c2cda2784975fb","text":"CodeBuild supports assigning elastic IP addresses to the network interfaces that it creates. Therefore, you need to configure your build project with the allocated EIP and VPC details of your RDS instance. Call the AWS CodePipeline 'StartPipelineExecution' API with {'stage': 'Test', 'schedule': '5 23 ? * SUN-THU *' }.","correct":false},{"id":"7a12726d6a6e0d62c071bfa26f1daf46","text":"By default, CodeBuild projects can access VPCs in the same account. Use the AWS CLI CodeBuild command 'start-build' with the --repeat=true, --hours=3, --minutes=10 and --frequency=weekdays option.","correct":false}]},{"id":"26b5e2d7-e00f-4d7b-b086-c09e029d1de9","domain":"HAFTDR","question":"The world wide cat news powerhouse, Meow Jones, has hired you as a DevOps Database consultant. They're currently using legacy in-house PostgreSQL databases which cost a considerable amount to maintain the server fleet, as well as operational costs for staff, and further hardware costs for scaling as the industry grows. You are tasked in finding an AWS solution which will meet their requirements. They require high throughput, push button scaling, storage auto-scaling and low latency read replicas. Any kind of automatic monitoring and repair of databases instances will also be appreciated. Which AWS service(s) do you suggest?","explanation":"Amazon Aurora will fit the needs perfectly, and the Database Migration Service can assist with the migration.","links":[{"url":"https://aws.amazon.com/rds/aurora/details/postgresql-details/","title":"Amazon Aurora Features: PostgreSQL-Compatible Edition"}],"answers":[{"id":"0a663a4b84931c9af9abadcccde84f3f","text":"Keep the current PostgreSQL databases and implement an ElastiCache to cache common queries and reduce load on your in-house databases to save on upgrade costs.","correct":false},{"id":"9802017b44b6d60fc846a356c1fa9e9a","text":"A cluster of Amazon RDS PostgreSQL instances, AWS Database Migration Service.","correct":false},{"id":"e96297fedff7f3062d962bbd0a387cb4","text":"An auto-scaled, load balanced EC2 fleet running PostgreSQL with data shared via EFS volumes.","correct":false},{"id":"6129915cbe4cbc9e1ddb0b74808beded","text":"Amazon Aurora, AWS Database Migration Service","correct":true}]},{"id":"6edec40a-5186-484f-84e3-d7bd3df44eb8","domain":"IncidentEventResponse","question":"Your company has recently switched from an external Email Service Provider to utilising SES to send marketing email.  You have been asked to record all bounce and complaint information from each email sent, in an S3 bucket so that it can be processed later.  You know that Kinesis Firehose can perform this task.  You have also been asked to anonymise the email addresses that are returned, in order to comply with GDPR and you will use Lambda to perform this task.  After a number of attempts, you have failed to receive any data into the S3 bucket.  Which of the following troubleshooting methods are valid in resolving your issues?","explanation":"We can immediately reject any answers that contain RedShift, as there is no need to use it to meet the needs of the question.  All of the remaining answers are possible methods of troubleshooting your issues.","links":[{"url":"https://docs.aws.amazon.com/firehose/latest/dev/troubleshooting.html","title":"Troubleshooting Amazon Kinesis Data Firehose"}],"answers":[{"id":"9c2908c869a796ee883dbbd0f643986a","text":"Make sure the Firehose DeliveryToRedshift.Success metric is available to confirm that Firehose has tried to copy data from your S3 bucket to the Redshift cluster.","correct":false},{"id":"f71cd26ef170d12dafd69c0d3db6ddd9","text":"Make sure the Firehose ExecuteProcessingSuccess metric is available, to confirm that an attempt has been made to invoke your Lambda function.","correct":true},{"id":"bc1a53efea21f6531d0b1a523da1b07f","text":"Make sure the Amazon Redshift STL_LOAD_ERRORS table is available, to verify the reason for the COPY failure.","correct":false},{"id":"196fe528acbe4658755d261eab3182d4","text":"Make sure that the Firehose DeliveryToS3.Success metric is available, to confirm that Firehose has tried putting data into your S3 bucket.","correct":true},{"id":"b0f65e9497f5a0ee31b86e2dfd462e31","text":"Make sure the Firehose IncomingBytes and IncomingRecords metrics are available, to confirm that data is being delivered to your Firehose delivery stream successfully.","correct":true}]},{"id":"279ef48b-4c8c-4cf0-a14a-8144644ac2f1","domain":"MonitoringLogging","question":"You work for a company that uses a serverless architecture to process up to 4,000 events per second for its usage analytics service. At its core, it consists of a multitude of Lambdas that have been given various resource configurations, i.e. memory/CPU settings. For each function invocation, you want to monitor that allocation against the actual memory usage. Select the simplest feasible approach to achieve that.","explanation":"AWS Lambda doesnâ€™t provide a built-in metric for memory usage but you can set up a CloudWatch metric filter. The AWS Lambda console provides monitoring graphs for Invocations, Duration, Error count and success rate (%), Throttles, IteratorAge and DeadLetterErrors.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html","title":"AWS Lambda Function Configuration"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html","title":"Monitoring and troubleshooting Lambda applications using Amazon CloudWatch"},{"url":"https://forums.aws.amazon.com/thread.jspa?threadID=226674","title":"Profiling memory usage for Lambda functions"},{"url":"https://gist.github.com/cgoering/4de674e3e3ca8d6255ea708997cca3b0","title":"AWS Lambda memory usage metric in CloudFormation"}],"answers":[{"id":"2f5f4e15a960c888b86e315b993df292","text":"You can set up a custom CloudWatch metric filter using a pattern that includes 'REPORT', 'MAX', 'Memory' and 'Used:'.","correct":true},{"id":"e2113fd25833a45a179807bc633f635a","text":"Log entries written into the log group associated with a Lambda function don't include profiling info such as memory usage. You need to use AWS X-Ray for that.","correct":false},{"id":"55b98a256995ac4e4c6f7afc1e1a5845","text":"AWS Lambda provides a built-in CloudWatch metric for memory usage.","correct":false},{"id":"65883e3f656c8f5b6f91c895b44649ff","text":"Use the 'Monitoring' tab in the AWS Lambda console and add the 'Resource' monitoring graph to your dashboard.","correct":false}]},{"id":"6e252f1f-63d8-43b5-8df5-aaa7d83874d1","domain":"HAFTDR","question":"Your application has a multi-region architecture and database reading and writing is becoming an issue. You are currently storing flat file key-pairs on a shared EFS volume across all of your application servers but it is simply too slow to handle the growth your company is experiencing. Additionally, latency of static files delivered to your customers from S3 has been noted as an issue. Which solution will be not only fast but scalable as you move forward?","explanation":"DynamoDB is the only option that supports multi-region replication and multi-master writes, and it does this using Global Tables.","links":[{"url":"https://aws.amazon.com/dynamodb/global-tables/","title":"Global Tables"}],"answers":[{"id":"cdfa2716b07fec163b72bf966bede22e","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use a multi-region write replica database for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false},{"id":"cdc9c7113a0cd60f29d5d56c49e1819b","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use Amazon DynamoDB Global Tables to create a multi-master, multi-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":true},{"id":"e1f2bdaf532480be3c4d53afd2d88645","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use a multi-region read-replica RDS configuration to create a multi-master, cross-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false},{"id":"d6880a6902e45e48a98990a1df3f6a3d","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use multi-region rw-replica RDS configuration to create a multi-master, cross-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false}]},{"id":"b2d2d9d6-3ab6-4949-85a3-de49e4783ab9","domain":"ConfigMgmtandInfraCode","question":"In the past, your organisation has had security breaches due to unauthorised changes in infrastructure.  To reduce these, the InfoSec Team have implemented AWS Config so that all changes are recorded.  Any changes to the infrastructure are then monitored by the Service Desk.  However, the following errors; \"We are unable to complete the request at this time. Try again later or contact AWS Support\" are appearing in the console.  The Service Desk Manager has asked you for more information on why these errors are happening.  Choose the correct reasons from below.","explanation":"There are only two issues that would cause this error to appear in AWS Config.  The first is if you make multiple calls to the AWS Config API within a minute and the default rate limiting will then implement a temporary block.  The CloudTrail logs will show the following; \"You have exceeded the maximum request rate. Try again at a later time.\" and you should stop calling the API as frequently.  The second option will be if you exceed the set Aggregator limit (which is 50 by default).  CloudTrail will show; \"The configuration aggregator could not be created because the account already contains '50' configuration aggregators. Consider deleting configuration aggregators or contact AWS Config to increase the limit.\"  No other options listed accurately describe the message.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/config-console-error/","title":"How can I troubleshoot AWS Config console error messages?"},{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"}],"answers":[{"id":"92344dfafeacc6f8c91cc9d5159ce1ff","text":"This error is related to calling the StartConfigRulesEvaluation API more than once per minute.","correct":true},{"id":"8d84028643f27b004e826c24087d49db","text":"This error is related to exceeding the rate limiting if you use the API call GetResourceConfigHistory or ListDiscoveredResources with a Lambda function.","correct":false},{"id":"bb66cca2e29dc015297e99a6c201a347","text":"This error is related to switching to a different region when Remediation is in Progress.","correct":false},{"id":"c2b54e290aa3e1f5ba5b69b19e872a6a","text":"This error is related to the ConformancePackStatusReason, and you should check it to know more about the reason for failure.","correct":false},{"id":"72a656f0e09c8f5382bf1978c403720f","text":"This error is related to the Aggregator limit being reached, and you should contact AWS Support to increase the Configuration Aggregator limit from the default value.","correct":true}]},{"id":"eef970b2-0cae-40bb-a9bc-a2fedbd2bc02","domain":"ConfigMgmtandInfraCode","question":"Your manager wants you to look into including and provisioning a new authentication service in your CloudFormation templates. Generally this would be simple, but the authentication service he has signed a 3 year deal with is not an AWS service at all, which could make things difficult. The authentication service has an API you can use, which will hopefully ease the pain of solving this problem. How do you solve it?","explanation":"This is an excellent use case for a CloudFormation custom resource","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html","title":"Custom Resources"}],"answers":[{"id":"8c7dd37aaf9be0d28c45046384cb9056","text":"Use the API to provision the service with a python script using the boto framework. Run the python script with a CloudWatch Events schedule.","correct":false},{"id":"2bbfc07a49181a40ef8a9ecaefe0c6f9","text":"Use the API to provision the service with an AWS Lambda function. Use a CloudFormation wait conditions to trigger the Lambda function during the provisioning process.","correct":false},{"id":"ccb593ba89acf906fb6a643256d93f44","text":"Use the API to provision the service with an AWS Lambda function. Use a CloudFormation custom resource to trigger the Lambda function.","correct":true},{"id":"6200195512a7053624054d12f6d6d0c7","text":"Provision the authentication service externally to your AWS resources. You will have to maintain them in parallel.","correct":false}]},{"id":"5de9216b-07fe-47be-aed7-41a4da2b44bd","domain":"HAFTDR","question":"One of your colleagues has been asked to investigate increasing the performance of the main corporate Website, for customers in the Asia Pacific Region, using a CDN.  They have decided to use CloudFront to perform this function, but have encountered problems when configuring it.  You can see that CloudFront returns an InvalidViewerCertificate error in the console whenever they attempt to to add an Alternate Domain Name.  What can you suggest to your colleague to assist them in solving the issue?","explanation":"You must ensure that the certificate you wish to associate with your Alternate Domain Name is from a trusted CA, has a valid date and is formatted correctly.  Wildcard certificates do work with Alternate Domain Names providing they match the main domain, and they also work with valid Third Party certificates.  If all of these elements are correct, it may be that there was an internal CloudFront HTTP 500 being generated at the time of configuration, which should be transient and will resolve if you try again.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/troubleshooting-distributions.html","title":"Troubleshooting Distribution Issues"}],"answers":[{"id":"e4f79c0e45af152d4406e7e7123c83a8","text":"There was a temporary, internal issue with CloudFront which meant it couldn't validate certificates.","correct":true},{"id":"2d089a04108e374436b3174f6acaa8e3","text":"The certificate relating to the Alternate Domain Name was imported from a Third Party CA and this will not work.","correct":false},{"id":"6ef625f1455326a96638173bec379999","text":"Ensure that there is a trusted and valid certificate attached to your distribution.","correct":true},{"id":"8eec3b58fb382642048a138370ae92cf","text":"Ensure the certificate is not a Wildcard certificate as these do not work with Alternate Domain Names.","correct":false}]},{"id":"8cd81d91-8288-4830-ae8b-0aaa2c7f706c","domain":"SDLCAutomation","question":"You want to use Jenkins as the build provider in your CI/CD Pipeline. Is this possible, and if so how would you implement it?","explanation":"You can select Jenkins an action provider when creating a build stage in CodePipeline. You cannot select it as a source provider within CodeBuild.","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html","title":"Use CodePipeline with Jenkins"}],"answers":[{"id":"c5a84ac110e8df3c1da4255e362f2066","text":"Yes it's possible. CodeBuild will let you select Jenkins as a source provider when you are creating your build project","correct":false},{"id":"2c74a9018e02367af56b8cd6036ae2a9","text":"No it's not possible.","correct":false},{"id":"80ab8f614bf8d08d188690809f0dc8fa","text":"Yes it's possible. CodePipeline will let you select Jenkins as a destination build provider when you are creating your pipeline.","correct":false},{"id":"d27a4cc08449c797a8fd8b6c6b9d505e","text":"Yes it's possible. You can use a CodePipeline plugin for Jenkins and can configure a build stage which connects to your Jenkins instance.","correct":true}]},{"id":"19cc0253-1dad-4ed2-bc2a-95515248a9ac","domain":"SDLCAutomation","question":"You are developing a completely serverless application and store your code in a git repository. Your CEO has instructed you that under no circumstances are you allowed to spin up an EC2 instance. In fact, he's blocked access to ec2:* company-wide with an IAM policy. He does however still want you to completely automate your development process, you just can't use servers to do so. Which AWS services will you make use of to meet these requirements?","explanation":"CodeDeploy can deploy a serverless lambda function straight from your CodeCommit repository.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html","title":"What Is CodeDeploy?"}],"answers":[{"id":"93bfcd1da129b57627d8753c0b2d65ef","text":"Use AWS Lambda for your compute functions, use CodeDeploy to deploy the functions for you. Store your code in CodeCommit and use a CodePipeline to automatically deploy the functions when you commit your code to the repository.","correct":true},{"id":"d385958d5064373b0d639a8c16f6ab09","text":"Use AWS Lambda for your compute functions, use CodeDeploy to deploy the functions for you. Store your code in GitCommit and use a CodePipeline to automatically deploy the functions when you commit your code to the repository.","correct":false},{"id":"e3ce2b50bb87a7fe18adff65e0320c08","text":"Use AWS Lambda for your compute functions, edit them directly in the Cloud9 console in the browser. Once installed they don't need to redeployed.","correct":false},{"id":"fbc7f2f64b4e3a232325800045b1725f","text":"Move your compute functions into S3, and use CodePipeline to deploy your code from S3 into AWS Lambda.","correct":false}]},{"id":"dd926464-3558-4d65-8dff-ab54005ac1a3","domain":"ConfigMgmtandInfraCode","question":"ADogGuru is developing a new Node.js application which will require some servers, a mySQL database and a load balancer. You would like to deploy and maintain this in the easiest way possible, have the ability to configure it with basic configuration management code, which is a new concept for ADogGuru. You also need to allow non-technical people to deploy software to servers and for managers to administer access. A key component of this setup will also be the ability to rollback deploys should you need to. Which service(s) and configuration management system will you choose to achieve this with  the LEAST operational overhead?","explanation":"OpsWorks can achieve all of stated requirements, deployments are possible with a few clicks in the UI, and can be rolled back. It also supports Chef solo as a built-in configuration management system, which is the recommended solution for those not already using Puppet or Chef. Running a separate Chef Automate instance is unnecessary overhead and cost when Chef Solo will suffice.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/welcome.html","title":"What Is AWS OpsWorks? - AWS OpsWorks"}],"answers":[{"id":"b2721cac2e6b9e28bd6af5140227bde4","text":"Use AWS Elastic Beanstalk to create an application using pre-built layer templates to create your servers, mySQL RDS instances and load balancer. Use the OpsWorks Puppet Enterprise server for configuration management. Grant your non-technical staff the 'deploy' permission level, and the administrator the 'Manage' permission level.","correct":false},{"id":"e570ae2d7902cce0694b0d33367b527d","text":"Use AWS OpsWorks to create an application using pre-built layer templates to create your servers, mySQL RDS instances and load balancer. Use recipes running in Chef solo for configuration management. Grant your non-technical staff the 'deploy' permission level, and the administrator the 'Manage' permission level.","correct":true},{"id":"0df4ca579bacd9555bc9b14d5e3c3441","text":"Use AWS OpsWorks to create an application using pre-built layer templates to create your servers, mySQL RDS instances and load balancer. Use OpsWorks for Puppet Enterprise for configuration management. Grant your non-technical staff the 'deploy' permission level, and the administrator the 'Manage' permission level.","correct":false},{"id":"4b57fb90ae5763059a11fa1dd441ad86","text":"Use AWS OpsWorks to create an application using pre-built layer templates to create your servers, mySQL RDS instances and load balancer. Also deploy OpsWorks for Chef Automate and run recipes for configuration management. Grant your non-technical staff the 'deploy' permission level, and the administrator the 'Manage' permission level.","correct":false}]},{"id":"61e75027-a31b-4647-9a9c-f5cb8c771339","domain":"IncidentEventResponse","question":"Your application uses Kinesis Data Streams to process incoming streaming data.  A colleague has noticed that GetRecords.IteratorAgeMilliseconds metric increases up to 10 minutes when traffic hits its peak during the day, and then reduces again when traffic gets less during the night.  This is a problem as it means data processing is being delayed during the times when the most amount of data is being ingested. What changes could you make to reduce the time lag in the stream?","explanation":"An increased GetRecords.IteratorAgeMilliseconds metric means that either the KCL consumers cannot keep up processing the data from the Kinesis stream or there aren't enough shards in the stream.  Choosing both of these options will satisfy the needs of the question.","links":[{"url":"https://aws.amazon.com/kinesis/data-streams/faqs/","title":"Amazon Kinesis Data Streams FAQs"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html","title":"Troubleshooting Amazon Kinesis Data Streams Consumers"}],"answers":[{"id":"4e638cee5fe6dc1a0c8c05bbfc5d2192","text":"Rewrite code to handle all exceptions within processRecords.","correct":false},{"id":"67a7d4be4c7d2f710e0f6a7ddc4f2086","text":"Increase the amount of KPL producers putting data onto the shards.","correct":false},{"id":"3cef88c0f16832880d58c622e62f99f4","text":"Add more EC2 KCL consumers to allow each to process less shards per instance.","correct":true},{"id":"88282ae94db4da5f7717813516387595","text":"Increase parallelism by adding more shards per stream.","correct":true},{"id":"a3a32610b2f82f78ed6560f077bba3d2","text":"Increase the retention time of the stream from 24 hours to 7 days.","correct":false}]},{"id":"d9e654d8-7cf1-4822-b834-308402b8ea9d","domain":"IncidentEventResponse","question":"You currently host a website from an EC2 instance. Your website is entirely static content. It contains images, static html files and some video files. You would like to make the site as fast as possible for everyone around the world in the most cost effective way. Which solution meets these requirements?","explanation":"S3 and CloudFront is the cheapest solution which will ensure the fastest content delivery around the world, but will also be cheaper due to no ongoing EC2 costs.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/","title":"How do I use CloudFront to serve a static website hosted on Amazon S3?"}],"answers":[{"id":"67c4d281d5d766f7f403fce78108d67d","text":"Move your static assets into S3. Serve the website on your EC2 instance with Amazon CloudFront as your content delivery network.","correct":false},{"id":"08d5998199f3298235e31cd4c01b2e25","text":"Move the website into an S3 bucket and serve it through Amazon CloudFront.","correct":true},{"id":"5851d3ee6cc5b6445f30a454af7d8d08","text":"Keep your website hosting on EC2. Add Amazon CloudFront as your content delivery network.","correct":false},{"id":"f4f5a1b2cd8a58cfae42965d8f070404","text":"Move your dynamic assets into S3. Serve the website on your EC2 instance with Amazon CloudFront as your content delivery network.","correct":false}]},{"id":"97a8a78b-a6d1-4bf7-aaff-a59191881a65","domain":"IncidentEventResponse","question":"Your company runs a popular website for selling cars and its userbase is growing quickly. It's currently sitting on on-premises hardware (IIS web servers and SQL Server backend.)  Your managers would like to make the final push into the cloud. AWS has been chosen, and you need to make use of services that will scale well into the future. Your site is tracking all ad clicks that your customers purchase to sell their cars. The ad impressions must be then consumed by the internal billing system and then be pushed to an Amazon Redshift data warehouse for analysis. Which AWS services will help you get your website up and running in the cloud, and will assist with the consumption and aggregation of data once you go live?","explanation":"Amazon Kinesis Data Firehose is used to reliably load streaming data into data lakes, data stores and analytics tools like Amazon Redshift. Process the incoming data from Firehose with Kinesis Data Analytics in order to provide real-time dashboarding of website activity.","links":[{"url":"https://aws.amazon.com/kinesis/data-firehose/","title":"Streaming Data Firehose - Amazon Kinesis - AWS"},{"url":"https://aws.amazon.com/solutions/real-time-web-analytics-with-kinesis/","title":"Real-Time Web Analytics with Kinesis | AWS Solutions"}],"answers":[{"id":"3e05395168d2e92f31097dc8eaadfb28","text":"Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create a CloudTrail stream to push the data to Amazon Redshift warehouse for analysis.","correct":false},{"id":"5a744ec834a3bed2345133430519b13d","text":"Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Streamer to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.","correct":false},{"id":"ed27547c1d649b6ee7f4c6106499701b","text":"Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Athena Data Stream to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.","correct":false},{"id":"029dca01b35ce55b6a3218b46cb66d8f","text":"Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Aggregator to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.","correct":false},{"id":"ee0e1b18c96ba916c7d61c5f11a173b3","text":"Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.","correct":true}]},{"id":"09effc45-51a5-418a-b1e5-b27430e92d3b","domain":"IncidentEventResponse","question":"Your organisation utilises Kinesis Data Streams to ingest large amounts of streaming data.  Each of the Kinesis Streams is consumed by a Java application running on multiple EC2 instances, utilising the KCL library.  The CloudWatch logs for the KCL application normally only show 'INFO' entries, but you have noticed that some errors have started to appear recently, and you need to investigate.  The errors are all of the form; \"getShard - Cannot find the shard given the shardId\" and include the ID of shard which still appears to exist.  What may have caused these errors to appear in the logs?","explanation":"This error usually appears just after a re-sharding event has completed when the consumer code is using an older version of KCL.  This appears to be caused when the KinesisProxy has a cache of shards which is not updated when leases change in the KCL DynamoDB table.  Although this error appears to have no affect on the data within the shards, it can usually be resolved by restarting the KCL consumers.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/building-consumers.html","title":"Reading Data from Amazon Kinesis Data Streams"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html","title":"Re-sharding, Scaling, and Parallel Processing"}],"answers":[{"id":"45bc1b263334c69f60939ca82c41189a","text":"These errors are caused by the shard iterator expiring before your code is ready to utilise it.","correct":false},{"id":"d8d212ca3330be75fe9cb3a00d49676d","text":"These errors are generated when migrating the Record Processor from version 1.x to version 2.x of the KCL.","correct":false},{"id":"409329597050c414284e9b625163dbfe","text":"These errors are seen when in Enhanced Fan-Out mode and more than 20 consumers are registered per stream.","correct":false},{"id":"54eebd43d1e7d3fcae9de2fcadc3409b","text":"These errors can be seen in the Kinesis Consumer log usually after a re-sharding event and are generated by the KinesisProxy.","correct":true}]},{"id":"e96d9cdb-1a97-4abe-9aea-0f5c2016fd0f","domain":"IncidentEventResponse","question":"You run a cat video website, and have an EC2 instance running a webserver which serves out the video files to your visitors who watch them. On the same EC2 instance, you also perform some transcoding to make the files smaller or to change their resolution based on what the website visitor is requesting. You have found that as your visitors grow, this is getting harder to scale. You would like to add more web servers and some autoscaling, as well as moving the transcoding to its own autoscaling group so it autoscales up when you have a lot of videos to convert. You are also running out of disk space fairly frequently and would like to move to a storage system that will scale too, with the least amount of effort and change in the way your entire system works. What do you do?","explanation":"This is a great case for using EFS. With minimal effort you can move your cat video website to an automatically scaling storage solution which can be used by all of your EC2 instances.","links":[{"url":"https://aws.amazon.com/efs/","title":"EFS"}],"answers":[{"id":"c9acb0ccafd771fdd4739d4cb5146111","text":"Implement S3 Glacier to save on storage costs. Use the S3 Glacier API to retrieve and store video files as required.","correct":false},{"id":"b908baa5c72de0c770b146c5103275e0","text":"Implement S3. Modify your applications to serve videos from S3 as well as downloading files from S3, transcoding them and storing them there.","correct":false},{"id":"0369b9bbb9fc65a7ad7ed7be897b6824","text":"Implement EFS. Mount your volume to each server for serving content and transcoding.","correct":true},{"id":"fc82534fc7c30a5bbc7dd66625b426ae","text":"Implement Storage Gateway on each EC2 instance. Use the gateway to quickly move files between instances as required.","correct":false}]},{"id":"7cd5551a-b9a0-4e59-bb53-d40d81dc8938","domain":"ConfigMgmtandInfraCode","question":"You have been ask to deploy a clustered application on a small number of EC2 instances.  The application must be placed across multiple Availability Zones, have high speed, low latency communication between each of the nodes, and should also minimise the chance of underlying hardware failure.  Which of the following options would provide this solution?","explanation":"Spread Placement Groups are recommended for applications that have a small number of critical instances which need to be kept separate from each other. Launching instances in a Spread Placement Group reduces the risk of simultaneous failures that might occur when instances share the same underlying hardware. Spread Placement Groups provide access to distinct hardware, and are therefore suitable for mixing instance types or launching instances over time. In this case, deploying the EC2 instances in a Spread Placement Group is the only correct option.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html","title":"Placement Groups"}],"answers":[{"id":"112a2330d77c300b357edda7c17dddb6","text":"Deploy the EC2 servers in a Cluster Placement Group","correct":false},{"id":"72371c02b14e73370c1b01dd2523a1c1","text":"deploy the EC2 servers in a Spread Placement Group","correct":true},{"id":"f3d57c381ce0c53f5ff05f7a48d8ae15","text":"The application should deployed as a service in ECS","correct":false},{"id":"b1954190e825c200d890843b70d5cb38","text":"Create a new VPC with the tenancy type of host and deploy the instances in the VPC","correct":false}]},{"id":"781e2a28-ff7a-4712-bf0c-1ae69dec243f","domain":"SDLCAutomation","question":"You are part of a development team that has decided to compile release notes directly out of a CodeCommit repository, the version control system in use. This step is to be automated as much as possible. Standard GitFlow is used as the branching model with a fortnightly production deploy at the end of a sprint and occasional hotfixes. Select the best approach.","explanation":"Following GitFlow's standard release procedures, a release branch is merged into master. That commit on master must be tagged for easy future reference to this historical version. Both release and hotfix branches are temporary branches and would require ongoing updates of the CodeCommit trigger. Feature branches are used to develop new features for the upcoming or a distant future release and might be discarded (e.g. in case of a disappointing experiment). CodeCommit does not provide a generate release notes feature.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html","title":"Manage Triggers for an AWS CodeCommit Repository"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html#codecommit_event_type","title":"CodeCommit Events"},{"url":"https://aws.amazon.com/blogs/devops/build-serverless-aws-codecommit-workflows-using-amazon-cloudwatch-events-and-jgit/","title":"Build Serverless AWS CodeCommit Workflows using Amazon CloudWatch Events and JGit"},{"url":"https://nvie.com/posts/a-successful-git-branching-model/","title":"A successful Git branching model"},{"url":"https://docs.aws.amazon.com/codecommit/latest/APIReference/Welcome.html","title":"AWS CodeCommit API Reference"},{"url":"https://forums.aws.amazon.com/thread.jspa?messageID=756611","title":"CodeCommit Lambda triggers fire off separate events for each commit?"}],"answers":[{"id":"94ff89937241b8005e348ff0550a1947","text":"Setup up an Amazon CloudWatch Event rule to match CodeCommit repository events of type 'CodeCommit Repository State Change'. Look for 'referenceCreated' events with a 'tag' referenceType that are created when a production release is tagged after a merge into 'master'. In a Lambda function, use the CodeCommit API to retrieve that release commit message and store it in a static website hosting enabled S3 bucket.","correct":true},{"id":"625fdc89350c8e0422b2c105b66be63a","text":"Create a trigger for your CodeCommit repository using the 'Push to existing branch' event and apply that to any release and hotfix branch. Add an Amazon SNS topic as the target and have a Lambda listen to it. In that function, filter out specific commit type changes such as style, refactor and test that are not relevant for release notes. Store all other commit messages in a DynamoDB table and, at release time, run a query to collate the release notes.","correct":false},{"id":"daf0713b7db34409528f3e191adf166e","text":"Use the 'generate release notes' feature of CodeCommit by running the 'create-release-notes' command with the --from <datetime> (use the start of the sprint) or --previousTag <tagName> option in the AWS CLI. Create a Lambda to execute this on a regular schedule (i.e. every 2 weeks) using CloudWatch Events with a cron expression.","correct":false},{"id":"601038d6d016fa6b4637092d2a68db75","text":"Configure a trigger by choosing the 'Delete branch or tag' repository event that invokes a Lambda function when development for a sprint is finished, i.e. the last feature-* branch has been deleted. In that Lambda, retrieve the latest git merge commit message before the deletion and append it to the release notes text file stored in an S3 bucket.","correct":false}]},{"id":"d68edb14-2da6-4852-8ca7-ac8f6f9b78fd","domain":"IncidentEventResponse","question":"You have decided to install the AWS Systems Manager agent on both your on-premises servers and your EC2 servers. This means you will be able to conveniently centralize your auditing, access control and provide a consistent and secure way to remotely manage your hybrid workloads. This also results in all your servers appearing in your EC2 console, not just the servers hosted on EC2. How are you able to tell them apart in the console?","explanation":"Hybrid instances with the Systems Manager agent installed and registered to your AWS account will appear with the 'mi-' prefix in EC2.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html","title":"Setting Up AWS Systems Manager for Hybrid Environments"}],"answers":[{"id":"d48ada0514bb41e403b1c5400cbf75a7","text":"The ID of the hybrid instances are prefixed with 'mi-'. The ID of the EC2 instances are prefixed with 'i-'.","correct":true},{"id":"a418f11f7d1488b0abb356ca2f8e070e","text":"The ID of the hybrid instances are prefixed with 'h-'. The ID of the EC2 instances are prefixed with 'i-'.","correct":false},{"id":"62fb5905dab4e1724d15d5ed81dee2d6","text":"The ID of the hybrid instances are prefixed with 'v-'. The ID of the EC2 instances are prefixed with 'i-'.","correct":false},{"id":"86986700437afbb27d1a3deb0c996665","text":"The ID of the hybrid instances are prefixed with 'm-'. The ID of the EC2 instances are prefixed with 'i-'.","correct":false}]},{"id":"353f2086-4a2b-4c7a-b40d-8199509f0f8f","domain":"PoliciesStandards","question":"You have an antiquated piece of software across your Linux virtual machines that you are attempting to move away from. Unfortunately the process of extracting the data from this software is difficult and requires a lot of commands to be run on the command line. The final process of moving away from this software is executing 90 commands on each database server. Which AWS services will make this as painless as possible and easiest to set up?","explanation":"The Systems Manager Run Command on the database servers is the best way to achieve this. You do not need to run commands on all servers.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/rc-console.html","title":"Running Commands from the Console"}],"answers":[{"id":"fdcba29bfa345864d3e47b322ca54389","text":"Create a bash script with the commands and store it in S3. Use a Lambda function to copy the script to all of your database servers and execute the script.","correct":false},{"id":"b524668957c324a9210f76db4fc4995f","text":"Create an EC2 instance and a bash script with the commands. Run the bash script which will ssh to all of your servers and execute the commands.","correct":false},{"id":"16101d7bc0420494d4449f62f5980f41","text":"Use the Systems Manager Run Command feature to run a shell script containing the 90 commands on each database server.","correct":true},{"id":"77c20d947ea3cdba35cde43dc31f265a","text":"Use the scp command to copy the bash script to each server and then execute it manually.","correct":false}]},{"id":"5d8f54bd-8e1d-43f7-ac42-5e95cb9b571d","domain":"PoliciesStandards","question":"Your fleet of Windows EC2 instances are running well. They're automatically built from an AMI and generally don't require much interaction, except for when they need to be patched with the latest Windows updates or have new software installed or updated. Ideally, you'd like to automate the process to install or update applications and apply windows updates and patches during the quiet hours when your customers are sleeping, which means around 3am. How would you best automate this process?","explanation":"Patch Manager combined with Maintenance Windows in AWS Systems manager is the recommended way to automate this requirement.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-scheduletasks.html","title":"Patching Schedules Using Maintenance Windows"}],"answers":[{"id":"b33df3531d79b8884a9ea73b9b3eb281","text":"Implement AWS Systems Manager Patch Manager and AWS Systems Manager Maintenance Windows","correct":true},{"id":"2c14be2bb2789102f0b7f5d920b26df9","text":"Implement AWS Systems Manager Patch manager and AWS Systems Manager Run Commands","correct":false},{"id":"ef3de69841ef1a22eb05e975b235c5ec","text":"Implement a Lambda function to run PowerShell update commands on a schedule using CloudWatch Events rules for each server","correct":false},{"id":"f958a87732086d7eac94980dcc15d1af","text":"Implement AWS Systems Manager Run Commands to execute the PowerShell scripts to run updates at 3am","correct":false}]},{"id":"547f6880-c168-4fa5-91d6-6822a772af6b","domain":"ConfigMgmtandInfraCode","question":"Your team has been working with CloudFormation for a while and have become quite proficient at deploying and updating your stack and their associated resources. However one morning you notice that your RDS MySQL database is completely empty. You track this down to a simple port change request that came through, asking if the default MySQL port could be changed for security reasons. What do you suspect happened?","explanation":"The database was replaced due to the AWS:RDS:DBInstance Port attribute update requirement of 'Replacement', the database will have to be restored from backups.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html","title":"Update Behaviors of Stack Resources"}],"answers":[{"id":"0974d47e810ed46ba7723376856be841","text":"The developer changed the MySQL port to a port that was already in use in your VPC so it can no longer be connected to.","correct":false},{"id":"2cb85ef53a3961bcf3c2798f04263e7e","text":"The 'Update' attribute for AWS:RDS has the update requirement of \"Replacement\".","correct":false},{"id":"aab0bf043065432156b116b7beed04c2","text":"The 'Port' attribute for AWS:RDS:DBInstance has the update requirement of \"Replacement\".","correct":true},{"id":"f3e35605b25b631ab990cfcba037091c","text":"The CloudFormation stack was updated with the Replacement stack value set to true.","correct":false}]},{"id":"1793c7ec-eff4-4f0d-8252-d1a464d5cd02","domain":"ConfigMgmtandInfraCode","question":"A financial services company runs their application portfolio on EC2 Linux instances with Auto Scaling to provide elastic capacity. They need to increase compute resources based on demand during month-end processing, and decrease them after all reporting has completed to avoid unnecessary costs. Each instance must have a secondary network interface in an isolated subnet for administration tasks in order to meet compliance requirements. How will the company ensure that new instances provisioned by Auto Scaling meet the compliance requirement?","explanation":"Auto Scaling supports adding hooks to the launching and terminating stages of the instance lifecycle. These hooks can send an SNS notification and hold the instance in a pending state waiting for a callback to the API. You can trigger a Lambda function from the SNS topic to create an ENI and attach it to the instance. The lifecycle hook will abandon the instance after a timeout period or if the Lambda function fails. Auto Scaling does not allow for specifying a secondary ENI in the launch configuration. The user data shell script option may work, but there will only be a set number of ENIs available in the pool, whereas the lifecycle hook option creates ENIs as needed. The shell script could possibly create the ENIs on demand. aws:createENI is not a valid AWS Systems Manager automation document action.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html","title":"What Is Amazon EC2 Auto Scaling?"},{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html","title":"Amazon EC2 Auto Scaling Lifecycle Hooks"},{"url":"https://aws.amazon.com/blogs/compute/using-aws-lambda-with-auto-scaling-lifecycle-hooks/","title":"Using AWS Lambda with Auto Scaling Lifecycle Hooks"}],"answers":[{"id":"eba70943b29ff601c4b5f75eddbb07eb","text":"Set up the Auto Scaling launch configuration to include a second Elastic Network Interface for EC2 instances. Include the administrative subnet for the secondary ENI as part of the launch configuration. Allow Auto Scaling to create and attach the secondary ENI during the instance launch stage.","correct":false},{"id":"40c2708f1afdec5ad8e8dbf1e224bd2c","text":"Create a pool of Elastic Network Interfaces in the administrative subnet. Pass a shell script as user data to be run when Auto Scaling launches an instance. Have the shell script use the AWS CLI to choose an ENI from the pool in the administrative subnet and attach it to the instance.","correct":false},{"id":"7479f465403b12319be40ec429085a28","text":"Add a lifecycle hook to the Auto Scaling launch stage which publishes to an Amazon Simple Notification Service topic. Configure SNS to trigger an AWS Lambda function that creates an Elastic Network Interface in the administrative subnet. Have the Lambda function attach the ENI to the EC2 instance.","correct":true},{"id":"ad0e0d5cf6e1e64ab9d7089fc43559f7","text":"Include a shell script as user data to be run when Auto Scaling launches an instance. Have the shell script use the AWS CLI to call the AWS Systems Manager StartAutomationExecution API. Pass the aws:createENI action and the administrative subnet id as parameters to the API call.","correct":false}]},{"id":"d498c13f-5b8b-4612-a384-e2d4b6c32014","domain":"PoliciesStandards","question":"Your production website with over 10,000 viewers a day has been ridiculed because your SSL certificate expired, prompting a warning to be shown to every visitor over a period of 8 hours until you woke up and renewed the certificate. Your competitors jumped on this opportunity to claim your company doesn't care about it's customers and you've seen a 20% drop in new sign-ups since then. Obviously, management are furious. What can you do to ensure this never happens again?","explanation":"You will have to generate new certificates to use with AWS Certificate Manager. It will monitor the expiration date of a certificate you import, but it wont be able to automatically replace them when it's close to expiry.","links":[{"url":"https://aws.amazon.com/certificate-manager/features/","title":"AWS Certificate Manager Features"}],"answers":[{"id":"90133a460fe706b9055946a6042f1e33","text":"Implement a python script that checks if your certificates are going to expire","correct":false},{"id":"1940cd7673490afba20e52619195b14a","text":"Import your certificates to AWS Certificate Manager and let it handle auto renewal.","correct":false},{"id":"2011dbaa257893954cd9b6fcc98ffb0b","text":"Generate new certificates to use with AWS Certificate manager and let it handle auto renewal.","correct":true},{"id":"f5ff556c66dcb0eadc9184c5dc9bc8b3","text":"Implement a lambda function to run daily and check the expiry of your SSL certificates. Send an email to an SNS group that the entire System Administration team are signed up to when your certificate is one day out from expiry.","correct":false}]},{"id":"bea8f262-5e3e-48b1-81f5-51f1f6d4d662","domain":"PoliciesStandards","question":"Your accounting team is a bit concerned about your AWS bill. It has been growing steadily over the past 18 months and they would like to know if there's any way you could look into lowering it, either by optimizing the services you are using or removing things that aren't required anymore. You already have a Business support plan with AWS so you could make use of that to see where you could save some money. What would be the best immediate course of action?","explanation":"AWS accounts with a Business level support plan have full access to all Trusted Advisor recommendations, including Cost Optimization. This would be the best thing to look at first.","links":[{"url":"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/","title":"Trusted Advisor"}],"answers":[{"id":"71733007126cf916f3721c71e396c076","text":"Open your Billing console and look at your past bills and cost explorer to find out which resources cost the most and look at scaling them down","correct":false},{"id":"405a81a0638ab334e9f45338e6094730","text":"Submit a business support ticket and ask for recommendations on cost savings from AWS support.","correct":false},{"id":"4e47ecda32f62308a0f47e959c5d6f68","text":"Open your Trusted Advisor dashboard and look at the Cost Optimization suggestions.","correct":true},{"id":"d8610318f35f225ce40f0ff76a2bd0f7","text":"Delete all of your unused AMI's from S3 and your old logs from CloudWatch Logs","correct":false}]},{"id":"4cfee0f4-02a3-43bc-af10-a34d888a0086","domain":"HAFTDR","question":"The Marketing Team of Mega-Widgets Inc. have recently released an advertising campaign in the APAC region, in addition to their home market of Europe.  The CTO has told you that they have started receiving complaints from Australian customers that the site located in eu-west-1 is slow, and they think by utilising the existing Disaster Recovery infrastructure in ap-southeast-2, they can speed up response time for these customers.  As time is of the essence, they have asked you to assist.  You have already confirmed that data is synchronising between both sites.  What is the quickest way to reduce the latency for these customers?","explanation":"As we are not told otherwise, we can assume that the European site is fast for people within Europe and therefore we can assume a high latency is the cause of the problem.  Route 53 Latency-Based Routing sounds like the perfect candidate when utilising the Disaster Recovery site as the main site for Asia Pacific.  CloudFront could offer some help if we only had one site and the Edge locations could cache some content, but in this case the CTO wanted to use the Disaster Recovery infrastructure already in place.","links":[{"url":"https://aws.amazon.com/route53/faqs/","title":"Amazon Route 53 FAQs"},{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Tutorials.html","title":"Tutorials"}],"answers":[{"id":"6752c886fe176d4a7c3fd5d847abf946","text":"In Route 53, create two records for www.mega-widgets.com, a latency record pointing to the European IP and a latency record pointing to the Asia Pacific IP.","correct":true},{"id":"0e6075956527bb40627bfab8871eab13","text":"In Route 53, create eu.mega-widgets.com and ap.mega-widgets.com and point them to the same CloudFront endpoint.","correct":false},{"id":"d718f6cb2c4337528dfde78cbe94dfad","text":"Create a CloudFront endpoint with the origin pointing to the European site and then point www.mega-widgets.com to that endpoint.","correct":false},{"id":"883bf6152c030fbe021fafb5fb55c8d4","text":"In Route 53, create two sub-domain A records of mega-widgets.com, one with the European IP and the other with the Asia Pacific IP. Create two aliases pointing www.mega-widgets.com to both sub-domains.","correct":false}]},{"id":"e6b3fbb9-d87d-4dc7-b056-981294c46bdc","domain":"PoliciesStandards","question":"An error has occurred in one of the applications that your team looks after and you have traced it back to a DB connection issue. There have been no network outages and the database is up and running. A colleague tells you that all credentials are now stored in AWS Secrets Manager and suspects that the problem might be caused by a recent change in that area. Select all possible reasons for this.","explanation":"Although you typically only have one version of the secret active at a time, multiple versions can exist while you rotate a secret on the database or service. GetSecretValue has an optional VersionId parameter that specifies the unique identifier of the version of the secret that you want to retrieve. If you don't specify either a VersionStage or VersionId then the default is to perform the operation on the version with the VersionStage value of AWSCURRENT.","links":[{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/enable-rotation-other.html","title":"Enabling Rotation for a Secret for Another Database or Service"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_retrieve-secret.html","title":"Retrieving the Secret Value"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_overview.html","title":"Overview of Managing Access Permissions to Your Secrets Manager Secrets"}],"answers":[{"id":"8525c502b705804602b85cbe128c73a3","text":"When secret rotation is configured in Secrets Manager, it causes the secret to rotate once as soon as you store the secret. This can lead to a situation where the old credentials are not usable anymore after the initial rotation. It is possible that the team forgot to update the application to retrieve the secret from Secrets Manager.","correct":true},{"id":"ef3ec50fbea476c0443043b89119ef3b","text":"The GetSecretValue API call in the application didn't include the version of the secret to return.","correct":false},{"id":"4add2a29b600d7ccdc5e4633595bd593","text":"The DB credentials and connection details in Secrets Manager have been encrypted using the default Secrets Manager CMK for the account. The application and secret are in different AWS accounts though and no cross-account access has been granted.","correct":true},{"id":"67a9e9da426f34e831b5de27c46e3b56","text":"The AWS credentials that are used for the call to Secrets Manager in the client-side component embedded in the application to retrieve the database password don't have the secretsmanager:DescribeSecret permission on that secret.","correct":true}]},{"id":"f705c5a3-69ca-46c0-8882-58624b8cd458","domain":"SDLCAutomation","question":"In your organization, your recently departed DevOps engineer configured CodeDeploy to use Blue/green deployment in your CodePipeline. This has worked brilliantly in the past, but tighter budgets have unfortunately caused management to direct your department to only run one set of servers and to update everything at the same time, despite your objections. How will you revert to this obviously inferior deployment method in the simplest way?","explanation":"You are able to edit the application deployment group in CodeDeploy, you don't have to create a new one.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html","title":"Working with Deployments in CodeDeploy"}],"answers":[{"id":"9a32906dc072bc7cc4bcbfb82916690e","text":"Edit your CodeDeploy application deployment group and change the deployment type from Blue/Green to In-place.","correct":true},{"id":"a18b1cd7ba9d60ee6d3d8d59614836ec","text":"Edit your CodePipeline deployment group and change the deployment type from Blue/Green to In-place.","correct":false},{"id":"e3d576212de093a8f5f5796f180b6438","text":"Create a new CodePipeline application deployment group with the deployment type set to \"In-place\", and modify your application to use the new deployment group configuration and delete the old one.","correct":false},{"id":"5fb35ab2cfe9fa485ac5d1bf4eae13dc","text":"Create a new CodeDeploy application deployment group with the deployment type set to \"In-place\", and modify your application to use the new deployment group configuration and delete the old one.","correct":false}]},{"id":"9fa6cf61-bd26-4e40-b82b-b21521aaf0fb","domain":"MonitoringLogging","question":"You are developing an augmented reality mobile game. Initially, you want to launch this in California but plan to expand rapidly to other areas if the pilot runs successfully. For operational purposes, you need to keep track of the number of players in each area and decided to use a custom CloudWatch metric called 'Population' for this. To enable a basic level of analytics, you also want to capture more than a dozen mobile device specific values such as battery level and network signal strength. During the pilot, you specify a data collection frequency of 6 times per minute. How could you achieve this? Select all correct statements.","explanation":"Namespace names must contain valid XML characters. Possible characters are: alphanumeric characters (0-9A-Za-z), period (.), hyphen (-), underscore (_), forward slash (/), hash (#), and colon (:). CloudWatch does not aggregate across dimensions for your custom metrics and you can only assign up to 10 dimensions to a metric.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html","title":"Amazon CloudWatch Concepts: Dimensions"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"Publishing Custom Metrics"}],"answers":[{"id":"864ca9273b06458b60886b3cef9bdd16","text":"Every 10 seconds, make a 'PutMetricData' call from within the app to publish the data to a new 'Analytics' CloudWatch custom metric. Specify the individual device values as separate dimensions, e.g. --dimensions BatteryLevel=42,SignalStrength=72, etc.","correct":false},{"id":"e6e3cc57cfb5d4c561244425173d7ecd","text":"For the initial launch, create a separate CloudWatch namespace for your 'Population' metric for each individual area such as 'NewEGamesLtd-ARGame>MountainView'.","correct":false},{"id":"d6192d0e2db11ba00651e36dc06b15df","text":"Standard resolution is for data having a one-minute granularity. When you publish a high-resolution custom metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds. If you choose to use one or more custom CloudWatch metrics for your analytics requirements, define these as high-resolution.","correct":true},{"id":"61c6ab75e1284129ae6c91b415dc5326","text":"Assign CloudWatch dimensions to your 'Population' metric that specify the player's current area, i.e. Country=USA, State=California, City=MountainView, etc. At any time, you can then get the total number of active players on any geographical level by aggregating across the relevant dimensions.","correct":false},{"id":"d30cb8988c9d57590ddfb5479583eac6","text":"If you choose to use one or more custom high-resolution CloudWatch metrics for your analytics requirements, your data points with a period of less than 60 seconds are available for only 3 hours. After that, the data is still available but is aggregated together for long-term storage with a 1-minute resolution. After 15 days the data is aggregated again and is retrievable only with a resolution of 5 minutes.","correct":true}]},{"id":"fc10726c-c2c6-4c3e-a514-ef41e8e8ba30","domain":"IncidentEventResponse","question":"You work for a company who sells various branded products via their popular Website.  Every time a customer completes a purchase, streaming data is immediately sent back to your endpoint and is placed onto a Kinesis Data Stream.  Most of the year, traffic remains static and no scaling of the Stream is necessary.  However, during the Black Friday period, scaling is required and this is accomplished by increasing the KCL instances and also re-sharding the Kinesis Stream.  This year, when sharding some of the streams, it was noticed that an extra shard was left after the operation finished, and this meant that if an even number of shards was requested, the number of open shards became odd.  You have been asked to troubleshoot the issue and find the cause and resolution from the list below.","explanation":"This is an issue which occurs from time to time when re-sharding.  The difference between the StartingHashKey and EndingHashKey values is normally large (depending on the number of shards you have in the stream).  Occasionally, the difference can be a very low value such as 1 and this causes the UpdateShardCount to end up with an extra shard.  This can usually be resolved by finding the ShardID with next adjacent Hash Key value, and merging the small shard with the larger shard.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/introduction.html","title":"What Is Amazon Kinesis Data Streams?"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html","title":"Troubleshooting Amazon Kinesis Data Streams Consumers"}],"answers":[{"id":"4c33b9c64a3047658345b8abf257490c","text":"This occurs when the width of a shard is very small in size in relation to other shards in the stream. This is resolved by merging with any adjacent shard.","correct":true},{"id":"c5d9d335c1cea4d9ab165da89fc4240f","text":"This occurs when an unhandled exception is thrown in the KCL from the processRecords code, and a record is skipped. This is resolved by handling all exceptions within processRecords appropriately.","correct":false},{"id":"8d38887be863e96d47c5353c44733dd0","text":"This occurs when a producer application writes to an encrypted stream without permissions on the KMS master key. This is resolved by assigning the correct permissions to an application to access a KMS key.","correct":false},{"id":"0e5554f90414fbfc45af06379bc77584","text":"This occurs if a shard iterator expires immediately before you can use it.  This may be resolved by ensuring the DynamoDB storing the lease information has enough capacity to store this data, by increasing the write capacity assigned to the shard table.","correct":false}]},{"id":"68f29294-fe1b-46a6-8c84-d5076c7a126e","domain":"MonitoringLogging","question":"Your customers have recently reported that your Java web application stops working sometimes. Your developers have researched the issue and noticed that there appears to be a memory leak which causes the software to eventually crash. They have fixed the issue, but your CEO wants to ensure it never happens again. Which AWS services could help you detect future leaks so you're able to fix the issue before the application crashes?","explanation":"Pushing the custom cloudwatch metric is a good idea, you could add it to a dashboard but that wont alert your developers unless they're actively checking it, which you can't rely on.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"Publishing custom metrics"}],"answers":[{"id":"d4ed3b2231654ab7cfe611a280ed9fa8","text":"Create a CloudWatch dashboard to monitor the memory usage of your app from a custom metric you are pushing to CloudWatch.","correct":false},{"id":"cc97d942d9ce31c3147db1f7555e8fb1","text":"Push your memory usage to a custom CloudWatch metric, set it to alert your developers if it crosses a threshold.","correct":true},{"id":"c2991a8b12bcb9d0e41ed823d43ec6e8","text":"Push your memory usage to CloudWatch logs, have a lambda function monitor it and alert a SNS queue if it crosses a threshold.","correct":false},{"id":"b11a45539b5c62eef7c857b34b391f5e","text":"Push your memory usage to CloudTrail, have a lambda function monitor it and alert a SNS queue if it crosses a threshold.","correct":false}]},{"id":"b8ae52d2-9638-47a9-b2b0-69c471a35eab","domain":"MonitoringLogging","question":"Your organization runs a large amount of workloads in AWS and has automated many aspects of its operation, including logging. As the in-house devops engineer, you've received a ticket asking you to log every time an EC2 instance state changes. Normally you would use CloudWatch events for something like this, but CloudWatch logs aren't a valid target in CloudWatch Events. How will you solve this?","explanation":"CloudWatch Events can use a Lambda function as a target, which will solve this issue.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html","title":"Tutorial: Log the State of an Amazon EC2 Instance Using CloudWatch Events"}],"answers":[{"id":"a68e57ea4a6bd4cfa8f565ef7cbc637e","text":"Create a Lambda function and run it on a schedule with CloudWatch Events. Make the Lambda function parse your CloudTrail logs for EC2 instance state changes and log them to another CloudWatch Logs log.","correct":false},{"id":"1fc1f373fd2b33a9a55daeb93aa03e7b","text":"Parse the EC2 CloudWatch changelog with a Lambda function each minute, and log the results to a separate log for instance state changes","correct":false},{"id":"b482ab43e71b8137c1bb08d0904d0576","text":"Use a CloudWatch dashboard, which will log EC2 state changes to CloudWatch logs if you create a text widget.","correct":false},{"id":"a4e104c9dd0f3e6a03765a0680648eed","text":"Use CloudWatch Events, but use a Lambda function target. Write a Lambda function which will perform the logging for you.","correct":true}]},{"id":"50424d57-02c2-4af2-84ff-10d935637fc6","domain":"IncidentEventResponse","question":"Your company has been producing Internet-enabled Microwave Ovens for two years.  These ovens are constantly sending streaming data back to an on-premises endpoint behind which sit multiple Kafka servers ingesting this data.  The latest Microwave has sold more than expected and your Manager wants to move to Kinesis Data Streams in AWS, in order to make use of its elastic capabilities.  A small team has deployed a Proof of Concept system but are finding throughput lower than expected, they have asked for your advice on how they could put data on the streams quicker.  What information can you give to the team to improve write performance?","explanation":"You should always use the Kinesis Producer Library (KPL) when writing code for Kinesis where possible, due the Performance Benefits, Monitoring and Asynchronous performance.  You should choose any answers which include KPL as a solution.  You should also check all relevant Service Limits to ensure no throttling is occurring.  Only three of these options are shown in the question, but there are many more possibilities.  The remaining options will work, but generally will give slower performance.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-producers.html","title":"Troubleshooting Amazon Kinesis Data Streams Producers"}],"answers":[{"id":"c6a098ae823197ee09ed9de745ce2cc8","text":"Use a Small Producer with the Kinesis Producer Library, but using the PutRecords operation.","correct":true},{"id":"8309467932d5a97196aca6a575a318af","text":"Develop code using the Kinesis Producer Library to put data onto the streams.","correct":true},{"id":"19d55c732ada47508097e33c6e04a120","text":"Check the GetShardIterator, CreateStream and DescribeStream Service Limits.","correct":true},{"id":"7080b30b6deaa847a5edb074ea4be9d8","text":"Use a Large Producer without the Kinesis Producer Library, but using the PutRecord operation.","correct":false},{"id":"95940bef689407d24dcd08ee0da18ca5","text":"Develop code using the SDK to put data onto the streams with the Kinesis Data Streams API.","correct":false}]},{"id":"69875d52-fb7d-47de-b4a9-adaa60e7ff47","domain":"IncidentEventResponse","question":"Your company produces IoT enabled refrigerators and uses Kinesis Data Streams as part of its method to capture the streaming data coming back from all of the installed devices.  Your team has written code using KPL to process, sanitise and enrich the data and then put it onto a Kinesis Stream.  If the data cannot be added onto the stream after five tries, it pushes the data through a Kinesis Firehose and into an S3 bucket.  During the last week they have started to see the following exception appearing:  \"Slow down. Service - AmazonKinesisFirehose; Status Code - 500; Error Code - ServiceUnavailableException\". Why would you be seeing this error? Choose from the options below.","explanation":"Whenever a 'Slow down', 'Status Code 500' or 'ServiceUnavailableException' message is received from Kinesis Firehose, it's safe to assume that the error relates to a limit being reached.  There are a number of limits which could affect why data isn't being successfully sent into Firehose, and you should examine 'Amazon Kinesis Data Firehose Limits' to see which case matches your issue and request an increase from AWS Support.","links":[{"url":"https://docs.aws.amazon.com/firehose/latest/dev/limits.html","title":"Amazon Kinesis Data Firehose Limits"}],"answers":[{"id":"41846b65249adfc479e8f18f544d4245","text":"Firehose does not have the correct permissions to write to the S3 bucket.  Change the permissions allocated to the Role and try again.","correct":false},{"id":"d82d96094ba01a7e79f85057676ceb82","text":"Verify that your Kinesis Data Firehose delivery stream is located in the same Region as your other services, as some services can only send messages to Firehose located in the same Region.","correct":false},{"id":"1ba3381bd83de693b9d3e6d4a0bf0882","text":"This message is shown when throughput limits for the delivery stream have been exceeded, they may be increased by contacting AWS Support.","correct":true},{"id":"a4ba42555f0d61efc6b9531130003027","text":"This message means the data delivery to Firehose is stale.  Check the 'DataFreshness' metric under the Monitoring tab to ensure that it is not increasing over time.","correct":false}]},{"id":"d4166e07-35b5-4196-b355-a04793003f88","domain":"HAFTDR","question":"You currently have a lot of IoT weather data being stored in a DynamoDB database. It stores temperature, humidity, wind speed, rainfall, dew point and air pressure. You would like to be able to take immediate action on some of that data. In this case, you want to trigger a new high or low temperature alert and then send a notification to an interested party. How can you achieve this in the most efficient way?","explanation":"Using a DynamoDB stream is the most efficient way to implement this. It allows you to trigger the lambda function only when a temperature record is created, thus saving Lambda from triggering when other records are created, such as humidity and wind speed.","links":[{"url":"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/","title":"DynamoDB Streams Use Cases and Design Patterns"}],"answers":[{"id":"86d964e70883ad4a58b196f134df686c","text":"Write an application to use a DynamoDB scan and select on your Sort key to determine the maximum and minimum temperatures in the table. Compare them to the existing records and send an SNS notification if they are breached. Run the application every minute.","correct":false},{"id":"bfa4fc6ae9d9c31b4764750964fc2583","text":"Use CloudWatch custom metrics to plot your temperature readings and generate an event alert if it breaches your high and low thresholds.","correct":false},{"id":"3cca31ba4defd9f00540ba540543c0af","text":"Modify your IoT devices to also log their data to Kinesis Data Firehose and trigger a Lambda function which will check for new high or low temperatures. Send a SNS notification.","correct":false},{"id":"295bff0a7cd575c34ecae937a4f12f8e","text":"Use a DynamoDB stream and Lambda trigger only on a new temperature reading. Send a SNS notification if a record is breached.","correct":true}]},{"id":"e7f37856-b31b-4997-872b-99ede9470dda","domain":"SDLCAutomation","question":"Your organization is currently using CodeDeploy to deploy your application to 20 EC2 servers which sit behind a load balancer. It's making use of the CodeDeployDefault.OneAtATime deployment configuration. Your manager has decided to speed up deployment by deploying to as many servers as possible at once, as long as at least five of them remain in service at any one time. How do you achieve this, ensuring that it will scale?","explanation":"Setting the minimum healthy host as a Number and 5 will work as desired. A percentage set to 25% will work for the current 20 servers, but it will not scale down if any servers are removed.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html","title":"Working with Deployment Configurations in CodeDeploy"}],"answers":[{"id":"9cad758b5b1009c5dcdc6df71e58852b","text":"Create a custom deployment configuration, specifying the minimum healthy host as a \"Number\" and set it to 5","correct":true},{"id":"8b33708bd673179fc3d5e2da7d7eed70","text":"Create a custom deployment configuration, specifying the maximum healthy host as a \"Percentage\" and set it to 25%","correct":false},{"id":"c58aab48dd8ec648df3d39b4264533d8","text":"Create a custom deployment configuration, specifying the minimum healthy host as a \"Percentage\" and set it to 25%","correct":false},{"id":"09c0a847baf9482b080d3e333ecca413","text":"Create a custom deployment configuration, specifying the maximum healthy host as a \"Number\" and set it to 5","correct":false}]},{"id":"836be367-5c22-4432-bde3-9be7e7c18889","domain":"SDLCAutomation","question":"Your application is being built with CodeBuild. You would like your artifacts to be stored with the region they were built in added to the filename so you can easily locate them later on. The CodeBuild documentation refers to an environment variable called $(AWS_REGION) that you could use. How and where will you implement this?","explanation":"You can specify the artifact name in the artifacts section of the buildspec.yaml file.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html","title":"Build Specification Reference for CodeBuild"}],"answers":[{"id":"56b7a49bf330e1e6de9009853f296dcb","text":"In the artifacts section of the post_build phase of the buildspec.yaml file. Specify 'name: build-$(AWS_REGION)'","correct":false},{"id":"45f22a55f14531223e9a323369b431d3","text":"In the artifacts section of the buildspec.yaml file. Specify 'name: build-$(AWS_REGION)'","correct":true},{"id":"0cd9fb161ae6c87e00e659ccb22d878e","text":"In the post_build phase in the phases section of the buildspec.yaml file. Specify 'name: build-$(AWS_REGION)'","correct":false},{"id":"63a8a2011e441323f7461896a7d71e16","text":"In the artifacts phase of the buildspec.yaml file. Specify 'name: build-$(AWS_REGION)'","correct":false}]}]}}}}
