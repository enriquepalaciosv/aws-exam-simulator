{"data":{"createNewExamAttempt":{"attempt":{"id":"b3f66922-8d29-47ac-8d08-811163bd811f"},"exam":{"id":"926975c1-b514-4529-8f26-1b89a5567e57","title":"AWS Certified Developer - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"bd3b8069-4ee1-480b-8738-d0683a3de962","domain":"security","question":"A company security team wants to implement a solution for securely storing RDS database credentials.  The solution should provide automatic rotation of database credentials.  What AWS service can the team use to meet these requirements?","explanation":"AWS Secrets Manager is an AWS service that can be used to securely store, retrieve, and automatically rotate database credentials. AWS Secrets Manager has built-in integration for RDS databases. Applications use Secrets Manager API's to retrieve database credentials, enabling secure storage of sensitive information outside of the application code. Systems Manager Parameter Store provides secure storage of sensitive information. However, it does not provide automatic credentials rotation capability specified as a requirement in the question scenario. Key Management Service (KMS) is used for management of cryptographic encryption keys, not for storage of sensitive information. Resource Access Manager is not applicable here as it is used for managing access to AWS resources between multiple accounts.","links":[{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html","title":"What Is AWS Secrets Manager?"}],"answers":[{"id":"fcba7bd474eb1fdf49705827bbb6f28c","text":"AWS Key Management Service","correct":false},{"id":"b77e6ac1dd339ec1c0d94107e6c9d3d2","text":"AWS Systems Manager Parameter Store","correct":false},{"id":"7898cb92c418aeed6974ede9cb146462","text":"AWS Secrets Manager","correct":true},{"id":"7c09430feea9bf5bdf657bd178ce574c","text":"AWS Resource Access Manager","correct":false}]},{"id":"a911b79a-68c9-41f0-af02-0f57c3ccdc66","domain":"mon-trb","question":"A company's services are protected by AWS WAF. The development team would like to enable logging on the WAF to get detailed information about traffic that is analyzed by the web ACLs in order to enhance their troubleshooting efforts. Which service can the team use to collect AWS WAF logs?","explanation":"In order to enable and configure AWS WAF logs, a Kinesis Data Firehose is required for delivery of the logs to the destination.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/logging.html","title":"Logging Web ACL Traffic Information"}],"answers":[{"id":"72e5e39b79c3d4d99d9c68a6a5e4d9f0","text":"Kinesis Data Firehose","correct":true},{"id":"33eb5c1f2566526637e791c925c4c505","text":"VPS Flow Logs","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false}]},{"id":"af4efcf0-4c08-4c5c-ade1-6e8196c9f0b7","domain":"deployment","question":"Your application needs to process large numbers of job requests and you need to ensure that they are processed in order, and that each request is processed only once. How would you deploy SQS to achieve this end?","explanation":"FIFO queues offer FIFO (First-In-First-Out) delivery and exactly-once processing: The order in which messages are sent and received is strictly preserved and a message is delivered once and remains available until a consumer processes and deletes it; duplicates are not introduced into the queue.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html","title":"SQS FIFO Queues"}],"answers":[{"id":"78f49aed62525e2d821207ac91af1b24","text":"Use the SetOrder attribute ensure sequential job processing.","correct":false},{"id":"54bf18f30fcf900dcdce5a9bcd553058","text":"Use an SQS FIFO queue to process the jobs.","correct":true},{"id":"bbd8881e023d2e48310f360e90cc7582","text":"Convert your standard queue to a FIFO queue by renaming your standard queue with the .fifo suffix.","correct":false},{"id":"31ea3ce575bc576b9ed1b36ce7cccbbc","text":"Configure FIFO delivery in a standard SQS queue.","correct":false}]},{"id":"5cf3b6d4-9a0e-4602-8dab-26e687207049","domain":"development","question":"Which of the following platforms are supported in ElasticBeanstalk?","explanation":"Elastic beanstalk supports common platforms like including Tomcat, Passenger, Puma and Docker","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html","title":"Elastic Beanstalk Supported Platforms"}],"answers":[{"id":"bf2528a296adb62d041a7519aa77f248","text":"Passenger","correct":true},{"id":"7345f7045e4668138112c100f25517a4","text":"JBoss","correct":false},{"id":"c5fd214cdd0d2b3b4272e73b022ba5c2","text":"Docker","correct":true},{"id":"8f72e28063c30c7468fb6af4653f4f9c","text":"Tomcat","correct":true}]},{"id":"513198a9-39d6-4e0b-85d2-c774cef841a8","domain":"deployment","question":"Your 'forums' table has a primary key of 'comment_id'. Using DynamoDB, you're able to query the data based on the 'comment_id' primary key. You need to be able to query the forums table by userId. What would you add to the table during table creation time?","explanation":"The creation of a secondary index will allow you to sort by userId.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html","title":"DynamoDB Secondary Indexes"}],"answers":[{"id":"153c557ab1353fc790ad20e9087663c7","text":"A secondary index","correct":true},{"id":"6640a5d74994ccaa1f30daef71496cfe","text":"A partition/sort primary key","correct":false},{"id":"1b924bbbc14b974d21d136baae3e2618","text":"None of these","correct":false},{"id":"bfd6d3cd3f6cac9eb918e1ce84732f98","text":"A new table ordered by userId","correct":false}]},{"id":"2988d8a0-c66e-434f-948b-acee623892fc","domain":"mon-trb","question":"You work for an electric car company that has its front-end website on EC2. Company policy dictates that you must retain a history of all EC2 API calls made on your account for security analysis and operational troubleshooting purposes. Which AWS service should you use to do this?","explanation":"CloudTrail is a web service that records AWS API calls for your AWS account and delivers log files to an Amazon S3 bucket. The recorded information includes the identity of the user, the start time of the AWS API call, the source IP address, the request parameters, and the response elements returned by the service.","links":[{"url":"https://aws.amazon.com/documentation/cloudtrail/","title":"CloudTrail Overview"}],"answers":[{"id":"a907a7338c1fb3821fbe8ed113c64b33","text":"CloudHSM","correct":false},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":true},{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false}]},{"id":"004b0e0e-f5a4-4354-b130-46786f367c11","domain":"security","question":"A developer is running an application on an Amazon EC2 instance that requires access to an Amazon S3 bucket. An administrator creates a role that includes policies that grant read permissions to the bucket and that allow the developer to launch the role with an Amazon EC2 instance. The instance is launched with the created role attached. What additional step is required for the application running on the instance to access the objects in the bucket?","explanation":"Applications that run on an Amazon EC2 instance and that need access to AWS resources such as Amazon S3 buckets or an Amazon DynamoDB table must have security credentials in order to make programmatic requests to AWS. In this case, no other steps are necessary since the application running on the instance will have the necessary permissions by assuming the role attached to the EC2 instance. Since the developer is not using the bucket (the application on the instance is) granting access to the developer will have no impact. There is no need to share credentials with the bucket policy.","links":[{"url":"https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf?refid=em_","title":"AWS Security Best Practices"}],"answers":[{"id":"1bc25b995801c49945fc1175ebfaf95d","text":"The administrator must grant the developer permissions to access the bucket.","correct":false},{"id":"723ec04523ad8f60803dd2ae402f886f","text":"The developer must share his/her credentials with the bucket policy.","correct":false},{"id":"ff86c6c144ca49f130e8a6024c26d48c","text":"Create an IAM policy that allows the developer permissions to access the bucket. Attach the policy to the developer's IAM User.","correct":false},{"id":"d57505894b0e8e826335fc1b6ca1f88c","text":"No other steps are necessary. The application running on the instance will be able to access the bucket.","correct":true}]},{"id":"a94b2c88-352b-4c39-83fc-7edfa8190fca","domain":"development","question":"Your application communicates using messages in an SQS queue. You have noticed recently that you are seeing a large number of empty responses where no messages exist in the queue. You want to make sure that your application is responsive as possible, but the cost of the solution is also a concern. What can you do to ensure your application is both cost-effective and responsive?","explanation":"In almost all cases, Amazon SQS long polling is preferable to short polling. Long-polling requests let your queue consumers receive messages as soon as they arrive in your queue while reducing the number of empty ReceiveMessageResponse instances returned. In general, you should use maximum 20 seconds for a long-poll timeout. Because higher long-poll timeout values reduce the number of empty ReceiveMessageResponse instances returned, try to set your long-poll timeout as high as possible.","links":[{"url":"https://aws.amazon.com/sqs/faqs/","title":"SQS FAQs"}],"answers":[{"id":"cef7badd821b93647928923359f863e4","text":"Use a FIFO queue","correct":false},{"id":"2247660601daf03d12f2fb4a1fccbf55","text":"Use long polling","correct":true},{"id":"66a35e1198bcce83d8654873aab562c6","text":"Configure multiple queues with short polling","correct":false},{"id":"f11916957f37c9cd171f126572d34864","text":"Use short polling","correct":false},{"id":"b2ccbf04a8f468dbcaad76b35f8f076b","text":"Configure multiple queues","correct":false}]},{"id":"2204af3d-2ed7-41c4-9182-2df403ce77df","domain":"security","question":"An organization receives documents from its users, which must be put into a SQS queue, ready for processing. The documents range in size from 3 MB to 20 MB, and must always be encrypted at rest.\n\nWhat is the best was to queue these documents?","explanation":"SQS has a maximum message size of 256 KB, and DynamoDB has a maximum Item size of 400 KB; therefore, neither of these would be suitable for storing such large documents.\n\nGlacier would not be suitable as its use-case is for long term document archiving, not short term document processing.\n\nAll options listed provide encryption at rest.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html","title":"SQS Limits"},{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html","title":"Managing Large Amazon SQS Messages Using Amazon S3"}],"answers":[{"id":"19455cef763d19eace0135946b6c4a0b","text":"Store the document in Glacier. Include a reference to the object in a SQS message.","correct":false},{"id":"f9c2efbd72c96aab06a14e4f3be9b830","text":"Store the document in DynamoDB. Include a reference to the item in a SQS message.","correct":false},{"id":"e0d9c5dbccbb6899d85b6efde57b2929","text":"Base64 encode the document, then attached it to the SQS message as a Message Attribute. ","correct":false},{"id":"23a28d35794f1802b4590c8433d5f0be","text":"Store the document in S3. Include a reference to the object in a SQS message.","correct":true}]},{"id":"943289f7-db1b-4439-b4c2-915168f617f9","domain":"development","question":"You work for a company which facilitates and organizes technical conferences. You ran a large number of events this year with many high profile speakers and would like to enable your customers to access videos of the most popular presentations. You have stored all your content in S3, but you would like to restrict access so that people can only access the videos after logging into your website. How should you configure this?","explanation":"All objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. Anyone who receives the pre-signed URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a pre-signed URL.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html","title":"Serving Private Content"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html","title":"CloudFront and Signed URLs"}],"answers":[{"id":"9d3d0a72e06711a3345d6dd192885795","text":"Use SSE-S3 to generate a signed URL","correct":false},{"id":"90d89af9eb0c2a7b8b70ca4a300d9920","text":"Share the videos by creating a pre-signed URL","correct":true},{"id":"07b930b04afb83395edadd40528e8336","text":"Use web identity federation with temporary credentials allowing access to the videos","correct":false},{"id":"1925533bcb7e305a66391f083dd879d9","text":"Use CloudFront with HTTPS to enable secure access to the videos","correct":false},{"id":"ccfccd796a677ec7b7686f3d8c21a602","text":"Remove public read access from the S3 bucket where the videos are stored","correct":true}]},{"id":"0c156bea-473c-4216-b344-51ad15046bdd","domain":"development","question":"Which of the following platforms are supported in ElasticBeanstalk","explanation":"Elastic Beanstalk provides platforms for programming languages (Java, PHP, Python, Ruby, Go), web containers (Tomcat, Passenger, Puma) and Docker containers, with multiple configurations of each.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html","title":"Elastic Beanstalk Supported Platforms"}],"answers":[{"id":"01fc3141bdcacc23a2e09a5e25ea126b","text":"Single Container Docker","correct":true},{"id":"3b2819dd4c24eda2faf2052eef449551","text":"Node.js","correct":true},{"id":"8fd82b8864d71ed7fa12b59e6e34cd1c","text":"Chef","correct":false},{"id":"10bff31c030c63d11bd1e7ab82759f6d","text":"Java with Tomcat","correct":true}]},{"id":"0d9472d0-587b-4961-9457-e37e75979b7c","domain":"development","question":"You are working on a Serverless application written in Python and running in Lambda. You have uploaded multiple versions of your code to Lambda, but would like to make sure your test environment always utilizes the latest version. How can you configure this?","explanation":"When you create a Lambda function, there is only one version: $LATEST. You can refer to the function using its Amazon Resource Name (ARN). There are two ARNs associated with this initial version, the qualified ARN which is the function ARN plus a version suffix e.g. $LATEST. Or the unqualified ARN which is the function ARN without the version suffix. The function version for an unqualified function always maps to $LATEST, so you can access the latest version using either the qualified ARN with $LATEST, or the unqualified function ARN. Lambda also supports creating aliases for each of your Lambda function versions. An alias is a pointer to a specific Lambda function version, aliases will not be updated automatically when a new version of the function becomes available.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-intro.html","title":"Introduction To Lambda Function Versioning"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda Function Versioning and Aliases"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/how-to-manage-versioning.html","title":"Lambda Versioning"}],"answers":[{"id":"07c1095b47175c42406ba59e91205281","text":"Reference the function using an unqualified ARN","correct":true},{"id":"d483464867986a2d830ac23858944891","text":"Create an alias to point to the correct version of your code","correct":false},{"id":"0311fc386b14ce5afd0843ecf4a6ff16","text":"Create another function to automatically update your function alias to point to the latest version of the code every time it is updated","correct":false},{"id":"cc67dedc36d230ab3574dc07b41b7443","text":"Configure the alias to automatically update to point to the latest version of the code every time it is updated","correct":false},{"id":"02f663ae252fea90c711171a5572d211","text":"Reference the function using a qualified ARN and the $LATEST suffix","correct":true}]},{"id":"36bd2e7a-adf7-4dae-ad6f-5e04d3ca873e","domain":"security","question":"You work for a large government agency which is conducting research for a top secret defense project. You are using SQS to handle messaging between components of a large, distributed application. You need to ensure that confidential data relating to your research is encrypted by the messaging system, which of the following services can you use to centrally manage your encryption keys?","explanation":"You can use a CMK to encrypt and decrypt up to 4 KB (4096 bytes) of data. Typically, you use CMKs to generate, encrypt, and decrypt the data keys that you use outside of AWS KMS to encrypt your data. This strategy is known as envelope encryption. CMKs are created in AWS KMS and never leave AWS KMS un-encrypted. To use or manage your CMK, you access them through AWS KMS.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys","title":"KMS Concepts"}],"answers":[{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":false},{"id":"0e8433f9a404f1f3ba601c14b026d321","text":"HTTPS","correct":false},{"id":"fa9c36d7e57eae51ce84cfd30a7346a3","text":"Systems Manager Parameter Store","correct":false},{"id":"4a4df63c87b4f42081b846d9b9189984","text":"KMS","correct":true},{"id":"ea52c36203c5f99c3ce2442d531b1a22","text":"SSL","correct":false}]},{"id":"58b4c9fa-13b1-4ecf-ad3b-27db3cf299f6","domain":"mon-trb","question":"Which service can you use to analyze and debug distributed applications, identify issues and locate performance bottlenecks?","explanation":"X-Ray allows you to debug distributed, Serverless and micro-services based applications. CloudTrail is used for auditing activity in your AWS account, CloudWatch and Systems Manager do not give an end-to-end view of your application.","links":[{"url":"https://aws.amazon.com/xray/faqs/","title":"X-Ray FAQs"}],"answers":[{"id":"0863526e514d88cd2d00b39f8dda0920","text":"X-Ray","correct":true},{"id":"8e75b153e61c22a8ea4e14aadc7cb4ee","text":"Systems Manager","correct":false},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false}]},{"id":"24137be2-c9b8-4be2-a4f0-0a5476fc15f9","domain":"development","question":"A developer has been tasked with migrating a large legacy web application, written in C++, to AWS. The developer wants to benefit from using Elastic Beanstalk to simplify the management of the infrastructure.\n\nWhich of the following methods would allow the developer to migrate the application with the least amount of work?\n","explanation":"Elastic Beanstalk supports Docker containers and custom AMIs via Packer. Both would allow the legacy application to be wrapped in a layer of abstraction such that Elastic Beanstalk itself would not need to support the specific language of the legacy application.\n\nThe Go platform only supports applications written in Go.\n\nThe application could be re-written in Node.js, but as it's a large application, a full rewrite is unlikely to require the least amount of work.\n\nElastic Beanstalk cannot be used to manage Lambda functions.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.html","title":"Docker on Elastic Beanstalk"},{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/custom-platforms.html","title":"Packer with Elastic Beanstalk"}],"answers":[{"id":"37d73c08ebaf765061f6ad4fe756430a","text":"Create a custom Lambda layer with a C++ runtime, which can be called from within Elastic Beanstalk.","correct":false},{"id":"8711e00909d504f51f9e7cc0aea8ed4d","text":"Use Packer to generate a custom AMI that contains the application, which can then be deployed via Elastic Beanstalk.","correct":true},{"id":"dadd577f7c15163f58854c17403e2a6d","text":"Use Docker to containerize the application, which can then be deployed via Elastic Beanstalk.","correct":true},{"id":"ea130ff77bca2e340acf49c464cdd620","text":"Rewrite the application in Node.js, which can then run natively via Elastic Beanstalk.","correct":false},{"id":"680517836f5d3fd52719bcc51f4f57e9","text":"Use the Go platform, which can support any compiled language such as C++.","correct":false}]},{"id":"a4afc791-647b-40b6-8f2a-29feb6142a0d","domain":"development","question":"ABC corp runs a web application that uses API Gateway to provide their developer customers with access to data. To reduce load on their upstream systems, the ABC corp have enabled API Gateway caching. A small number of developer customers still need access to results directly from the integration endpoint. To prevent all developer customers from bypassing the cache, ABC corp has also enabled the requirement for cache invalidation to require authorization.\n\nWhat must a developer customer do to return a result that is not cached from the API Gateway?","explanation":"Setting a Cache-Control: max-age=0 HTTP header as part of the request tells API Gateway that you want a response directly from the integration endpoint, rather than a cached response. This header can be interpreted as the client stating the maximum age a cached result can be is 0 seconds - equivalent to saying it cannot be cached at all.\n\nAs the cache is configured to require authorization to be invalidated, the request must be signed with a user or role that allows the execute-api:InvalidateCache action to be performed on the API Gateway resource. An example of this policy is found in the documentation for API Gateway Caching. We have also included a link to how to sign a request using AWS Signature Version 4.\n\nIt is recommended that you require authorization to invalidate a cached response; otherwise, if a significant number of requests perform an invalidation, the cache is no longer helping reduce load on upstream systems.\n\nCalling flush-stage-cache is incorrect because this would delete all data in the entire API cache, rather than just for the response the client has requested. Called often, this will likely result in the cache not having sufficient data to be effective.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"API Gateway Cache developer guide"},{"url":"https://docs.aws.amazon.com/apigateway/api-reference/signing-requests/","title":"Signing API Gateway requests"}],"answers":[{"id":"54f0ac8f9360a2dfe63f22a0a503ae7e","text":"Call the /execute-api-invalidate-cache API Gateway endpoint before sending their request.","correct":false},{"id":"cb28453003b461189ca4aec138e2d1bf","text":"Sign their request with a user or role that has the required execute-api:InvalidateCache permissions to invalidate the cache.","correct":true},{"id":"4f0d8679da16e6c12618e0c2b85739be","text":"Include a Cache-Control: max-age=0 HTTP header in their request.","correct":true},{"id":"3dd3c69d480989554365826eaef7be2f","text":"Include a ?execute-api-invalidate-cache query string in their request.","correct":false},{"id":"0f148da68f4442eace5c89fe939ba9ac","text":"Call the flush-stage-cache command before making the request.","correct":false}]},{"id":"08c5bdcb-66a8-486a-91ca-4e201a4797f4","domain":"development","question":"In the CodeDeploy AppSpec file, what are hooks used for?","explanation":"The hooks section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The hooks section for a Lambda deployment specifies Lambda validation functions to run during a deployment lifecycle event. ","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html","title":"AppSpec hooks Section"}],"answers":[{"id":"3b6f1341851de6b2effdba57c78d012c","text":"To specify files that you want to copy during the deployment","correct":false},{"id":"0ade4d62f3b3d4087279596dc342d905","text":"To specify code, scripts or functions that you want to run at set points in the deployment lifecycle","correct":true},{"id":"3c1fd86b9741027161d373f49965fa8c","text":"Hooks are reserved for future use","correct":false},{"id":"b4d79a392ca82974f45ea61ad66c67cc","text":"To reference AWS resources that will be used during the deployment","correct":false}]},{"id":"9548796c-789c-42ea-9e90-3da3a9252c1b","domain":"security","question":"Your Security team have recently reviewed the security standards across your entire AWS environment. They have identified that a number of EC2 instances in your development environment have read and write access to an S3 bucket containing highly confidential production data. You have been asked to help investigate and suggest a way to remedy this. Which of the following can you use to find out what is going on so that you can suggest a solution?","explanation":"With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies attached to IAM users, groups, or roles in your AWS account. You can test which actions are allowed or denied by the selected policies for specific resources.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html","title":"Testing IAM Policies with the IAM Policy Simulator"}],"answers":[{"id":"17d9101067c49b2ec4b35087e24ce8eb","text":"Use the IAM Policy Simulator to identify which role or policy is granting access","correct":true},{"id":"58f20da8b31f9e9da2e5a374608338ef","text":"Use the CLI or console to check the public access permissions of the S3 bucket","correct":false},{"id":"aac02aded2357ef60d7cc0b8f32df947","text":"Use CloudTrail and Athena to identify which role or policy is granting access","correct":false},{"id":"5bc514eb754ea13140481ed2afc68d45","text":"Use the VPC flow logs to identify which EC2 instances are attempting to access the bucket","correct":false}]},{"id":"b419a913-a5f1-4ad2-8c06-3b579643d073","domain":"development","question":"Upon creating your code repository, you remember that you want to receive recommendations on improving the quality of the Java code for all pull requests in the repository. Which of the following services provide this ability?","explanation":"When creating your repository, you have the option of enabling 'Amazon CodeGuru Reviewer for Java.' This will automate reviews of your code to spot problems that can be hard for you to detect, in addition to the recommendations to fix the code. CodeGuru Reviewer would have been the correct answer, but it’s not specific to the type of code (always look for the *most* correct answer in exams.) Creating the repository itself requires the use of CodeCommit, which includes the 'Amazon CodeGuru Reviewer for Java' option; that’s why CodeCommit is the wrong answer. Finally, CodeBuild is for building code, rather than improving it.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/pull-requests.html","title":"Working with Pull Requests in AWS CodeCommit Repositories"}],"answers":[{"id":"f7f128d00b13b324ebd29c42f1037cf8","text":"CodeGuru Reviewer","correct":false},{"id":"56c20df941e069e1a35a1871f662298a","text":"CodeCommit","correct":false},{"id":"f8d1a2a80933c71d9a65e3d825976bda","text":"CodeBuild","correct":false},{"id":"b9a2b9145e469fee4b348e257067e441","text":"CodeGuru Reviewer for Java","correct":true}]},{"id":"d4258c06-e769-46cc-b5ab-1571ea57879b","domain":"development","question":"You are pair programming with another senior developer in your team and you have been tasked with writing a number of different Lambda functions. Your colleague recommends that you separate the Lambda handler from the core business logic of your code. What is the rationale for this?","explanation":"At the time you create a Lambda function, you specify a handler, which is a function in your code, that AWS Lambda can invoke when the service executes your code. Separating the handler from the core business logic is best practice as it enables code re-use as well as making unit testing easier. See the URL below for best practices for developing Lambda functions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html","title":"Best Practices for Working with AWS Lambda Functions"}],"answers":[{"id":"0f56edddb67c69aafb238d531d6abdfb","text":"To reduce the size of your deployment package","correct":false},{"id":"1763988c31a85fbf689902e6d3e1a185","text":"To reduce complexity caused by dependencies","correct":false},{"id":"79c80a290b9838caa5e24e1f96a2a8f7","text":"To make the code easier to re-use","correct":true},{"id":"6a5519ec7f0685c57d611a5c6731c5a4","text":"To improve function performance","correct":false}]},{"id":"8f888c41-8c19-4f1a-8682-7a8f8235182a","domain":"security","question":"A developer is working on a new HR application that must be able to encrypt sensitive documents, each of which is approximately 100 MB in size. The encryption needs to take place within the HR application, and each document must be encrypted using a unique key. The developer has decided to use envelope encryption, and KMS to manage their keys.\n\nWhat KMS operation should be called for each document, to most efficiently meet the requirements of the HR application?","explanation":"generate-data-key returns a plaintext data key, ready to be used to encrypt a document, and a ciphertext version of the key, encrypted using the Customer Master Key. The command should be called for each document, so a different key is used for each. Once the document is encrypted, the plaintext key is securely discarded, and the encrypted data key is stored along with the encrypted document.\n\ngenerate-data-key-without-plaintext is incorrect because it is the plaintext key that is used to encrypt the document within the application.\n\ngenerate-random is incorrect as while the response could be used as a data key; a second step would also be needed to acquire the ciphertext version of the key. It is, therefore, not the best solution.\n\nencrypt is incorrect because it can only encrypt up to 4 kilobytes of data, and because the encryption process itself would take place within KMS, directly using the Customer Master Key, not within the application. To that end, using 'encrypt' directly does not fall under AWS's definition of envelope encryption.","links":[{"url":"https://docs.aws.amazon.com/cli/latest/reference/kms/generate-data-key.html","title":"KMS CLI: generate-data-key"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/kms/encrypt.html","title":"KMS CLI: encrypt"}],"answers":[{"id":"0b6eb26a9e685b2edba22d0b9f8534a3","text":"generate-data-key","correct":true},{"id":"966a36d051e3b8290953bce53c3513bf","text":"generate-random","correct":false},{"id":"f71001650370495dc4bb6bccf8fc3ac9","text":"generate-data-key-without-plaintext","correct":false},{"id":"53c82eba31f6d416f331de9162ebe997","text":"encrypt","correct":false}]},{"id":"d5adb3f2-4ddd-4a5e-be37-47d74da0f6d6","domain":"development","question":"You are working on a Serverless application written in Node.js. You updated the Node.js code and uploaded a new zip file containing your code to Lambda. Your application references the function using the alias \"Prod\", however it not seem to be using the new code. Which of the following is likely to fix this?","explanation":"The problem is that the application is referencing the function using an alias pointing to a previous version of the code. When you use versioning in AWS Lambda, you can publish one or more versions of your function. So that you can use different variations of your Lambda function in your development workflow such as development, beta, and production. Lambda also supports creating aliases for each of your Lambda function versions. Conceptually, an AWS Lambda alias is a pointer to a specific Lambda function version. You can update aliases to point to different versions of functions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda Function Versioning and Aliases"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/how-to-manage-versioning.html","title":"Lambda Versioning"}],"answers":[{"id":"1ab8bb5a6ed0944df7316a0ae848727f","text":"You need to update the alias to reference the new version of your function","correct":true},{"id":"68796527ce97010c4a40777a6462b3fc","text":"You need to call version 2 of the function","correct":false},{"id":"33064ca2d30b93ce4d46769270b3e34e","text":"You need to update your application to use an unqualified ARN","correct":false},{"id":"6a4ac8c4aa2076312300dc09fa5c0f21","text":"You need to call the function using $LATEST","correct":false}]},{"id":"708176eb-71b7-4204-9c7b-3c264fe7b990","domain":"deployment","question":"You are using CloudFormation to build a number of different application environments to host development, test, UAT, pre-production and production stacks. Your application is comprised of web servers, load balancers, application servers and databases each web server, load balancer and database needs to be configured identically across all environments. How can you achieve this with CloudFormation?","explanation":"Nested stacks provide the ability to configure multiple elements within your environment while reducing duplication of code. As your infrastructure grows, common patterns can emerge in which you declare the same components in multiple templates. You can separate out these common components and create dedicated templates for them. Then use the resource in your template to reference other templates, creating nested stacks.","links":[{"url":"https://aws.amazon.com/blogs/devops/use-nested-stacks-to-create-reusable-templates-and-support-role-specialization/","title":"Nested Stacks To Create Reusable Templates"}],"answers":[{"id":"fc8b86a0172250c06d0b47ac00a9581d","text":"Use a CloudFormation Nested Stack","correct":true},{"id":"b37138310fd665c01b87b8a10b3bc912","text":"Use environment variables","correct":false},{"id":"42b52ff9f79cc77411fd5fab9e6f2da7","text":"Use the Mappings section of the template to reference the code you want to re-use","correct":false},{"id":"7787568901c6933adbc971d946dc170a","text":"Copy and paste the configuration code that you want to re-use into the CloudFormation template for each environment","correct":false}]},{"id":"779acf8c-3df3-43fe-9714-3ebaf8e40ef2","domain":"refactoring","question":"You are working for an investment bank and have been asked to help the application support team with their annual Disaster Recovery testing. The main production PostgreSQL database is hosted in RDS Multi-AZ deployment, with multiple applications running on a combination of EC2 and Lambda. You have been asked to help the team to demonstrate the impact that a failed Availability Zone will have on the database. Which of the following do you suggest?","explanation":"If the Amazon RDS instance is configured for Multi-AZ, you can perform the reboot with a failover. An Amazon RDS event is created when the reboot is completed. If your DB instance is a Multi-AZ deployment, you can force a failover from one Availability Zone (AZ) to another when you reboot. When you force a failover of your DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone, and updates the DNS record for the DB instance to point to the standby DB instance. As a result, you need to clean up and re-establish any existing connections to your DB instance. Rebooting with failover is beneficial when you want to simulate a failure of a DB instance for testing, or restore operations to the original AZ after a failover occurs.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RebootInstance.html","title":"RDS - Rebooting a DB Instance"}],"answers":[{"id":"4496cfe7afc55864d488f038430be2b5","text":"Simulate an AZ failure by disconnecting your RDS instance from the network","correct":false},{"id":"428c5da3acc9c6986847b2511e6129f5","text":"Simulate an AZ failure by rebooting the underlying EC2 instance which is running the database","correct":false},{"id":"e8d6fb964d48234750126a73a5fa41f4","text":"Simulate an AZ failure by performing a reboot with forced failover on the RDS instance","correct":true},{"id":"222811da7a574ef8ec4e058ece75fe23","text":"Simulate an AZ failure by deleting the primary RDS instance","correct":false},{"id":"1db313c348c46e6cefc8bf25ce3e0d15","text":"Simulate an AZ failure by moving your RDS instance to a different subnet","correct":false}]},{"id":"d38a2534-f7bb-4cc2-9b9a-0c50dfb7707b","domain":"security","question":"You are attempting to analyse the CloudWatch metrics for a number of your application servers, however when you try to view the metrics you cannot access them, however one of your colleagues is able to access them without any issues. What could be the problem?","explanation":"Access to Amazon CloudWatch Logs requires credentials that AWS can use to authenticate your requests. Those credentials must have permissions to access AWS resources, such as to retrieve CloudWatch Logs data about your cloud resources.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/auth-and-access-control-cw.html","title":"CloudWatch Access Control"}],"answers":[{"id":"691a929a32273fce2328658fc4c41fa6","text":"Your EC2 instance role does not have permission to push the metrics to CloudWatch","correct":false},{"id":"2578745b37e7d610c9f6227f920c4cb5","text":"CloudWatch doesn't have permission to collect the metrics","correct":false},{"id":"dcd1178c83830f41078c6b19fcd33f05","text":"The CloudWatch agent has stopped running","correct":false},{"id":"a0ff1e54243f914c3324d8826543cdd2","text":"Your IAM user doesn't have permission to view CloudWatch metrics","correct":true}]},{"id":"52bc2a05-9de5-46cb-ab80-feb67dadc6d9","domain":"deployment","question":"What is the maximum size of an S3 object?","explanation":"The minimum size of an object is 0 bytes (empty or 'touched' files are permitted) and the maximum size of an object is 5TB.","links":[{"url":"https://aws.amazon.com/s3/faqs/#general","title":"How Much Data Can I Store?"}],"answers":[{"id":"6aaa2ffc817cac34a806a149bee36350","text":"50 GB","correct":false},{"id":"47a5e6e39f64ff5d5dfb25da1aa3f3f2","text":"500 GB","correct":false},{"id":"951b67b6006b1e714e7d8b65b90d56b5","text":"50 TB","correct":false},{"id":"b3dd42c7956751be5d38b5c0cb2e09c0","text":"5 TB","correct":true}]},{"id":"dc3e7895-b954-4fa8-8d4f-faffeca401d2","domain":"security","question":"Which of the following methods will allow you to *securely* upload/download your data to the S3 service? Pick all that apply.","explanation":"You can securely upload/download your data to Amazon S3 via SSL endpoints using the HTTPS protocol.","links":[{"url":"https://aws.amazon.com/s3/faqs/#security","title":"S3 Security"}],"answers":[{"id":"937a8b8e84ad5481f1983a1842154e18","text":"HTTP endpoints using HTTP protocol","correct":false},{"id":"d7ad40729fa333427d4d8c3032d43fdf","text":"SSL endpoints using HTTP protocol","correct":false},{"id":"2b716d646634dd42d3d1ab628b210081","text":"SSL endpoints using the HTTPS protocol","correct":true},{"id":"1019a747b087f11f97ef6a2bf46a1978","text":"HTTP endpoints using HTTPS protocol","correct":true}]},{"id":"6d9f501f-bc25-4802-8cb9-adb7b7cc9724","domain":"deployment","question":"In addition to choosing the correct EBS volume type for your specific task, what else can be done to increase the performance of your volume?","explanation":"There are a number of ways you can optimise performance above that of choosing the correct EBS type.  One of the easiest options is to provide more I/O throughput than you can provision for a single EBS volume.  This can be done by striping using RAID 0.  You can join multiple gp2, io1, st1, or sc1 volumes together in a RAID 0 configuration to provide parallel read/write performance. The second option is to choose an EC2 instance type that supports EBS optimization.  This ensures that network traffic will not contend with traffic between your instance and your EBS volumes.  The final correct choice is only related to HDD based EBS volumes.  When you create a snapshot of a Throughput Optimized HDD (st1) or Cold HDD (sc1) volume, performance may drop as far as the volume's baseline value while the snapshot is in progress. This behaviour is specific to these volume types.  Therefore you should ensure that scheduled snapshots are carried at times of low usage.  The one option on the list which is entirely incorrect is the one that states \"Never use HDD volumes, always ensure that SSDs are used\" as the question first states \"In addition to choosing the correct EBS volume type for your specific task\".  HDDs may well be suitable to certain tasks and therefore they should not be discounted because they may not have have the highest specification on paper.","links":[{"url":"https://aws.amazon.com/ebs/features/","title":"Amazon EBS features"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSPerformance.html","title":"Amazon EBS Volume Performance on Linux Instances"}],"answers":[{"id":"154b9cbae5d3f37e232f1a00c5364547","text":"Schedule snapshots of HDD based volumes for periods of low use","correct":true},{"id":"5f33f064dbad9a3f30cd0395c1ae2106","text":"Ensure that your EC2 instances are types that can be optimized for use with EBS","correct":true},{"id":"060ad726277222bf1fdf1470ea833f1a","text":"Stripe volumes together in a RAID 0 configuration.","correct":true},{"id":"2b3853ec27d2a5f7a8fb114b5fcfc321","text":"Never use HDD volumes, always ensure that SSDs are used","correct":false}]},{"id":"5953c122-dbdd-4d25-a8fe-3e1fd23a6c8f","domain":"mon-trb","question":"An application developer finds that performing a scan operation on a large DynamoDB table is taking a long time to execute.  What can be used to improve the performance and decrease the execution time of the scan operation?","explanation":"Parallel scans can be used by multiple worker threads in an application to perform a scan of a DynamoDB table much faster. Filter expression in a scan operation only filters the results.  Scan operation still performs the same amount of read operations. Projection expression is used to limit the attributes returned by the scan operation.  It reduces the size of the payload of the scan operation, but it does not affect the speed of the scan operation. Pagination can be used to divide the result set into multiple pages, but does not increase the performance of the scan operation.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan","title":"Parallel Scan"}],"answers":[{"id":"e262b00e3caa1a25ec14571777714e9c","text":"Use of projection expression.","correct":false},{"id":"7eeca829a717872bbf53c3ff8a69e41a","text":"Use of pagination.","correct":false},{"id":"62fef50451cb13a49660c69ae13e7932","text":"Use of filter expression.","correct":false},{"id":"b68d0e7d10bb7958f9b4b5fa43f06ac3","text":"Use of parallel scans.","correct":true}]},{"id":"422c80de-8e21-4706-ab03-ce11c4cfa083","domain":"deployment","question":"You are developing a completely serverless application using Lambda and API Gateway. You need a place to persist data as key-value pairs and your application will need low latency access to the data. Which of the following is the best option for storing this data?","explanation":"DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. RDS does not store data as key-value pairs, a JSON document in S3 is not an efficient way to store the data and using an EC2 instance would not be serverless and would not be scalable.","links":[{"url":"https://aws.amazon.com/dynamodb/","title":"DynamoDB Overview"}],"answers":[{"id":"023289eeb117bb428a4288ca492fd5e8","text":"Store the data in a DynamoDB table","correct":true},{"id":"866706615c3cfcbd5b3dfc3bbd116f85","text":"Store the data in JSON format in an S3 bucket","correct":false},{"id":"243a3f875904542a9e0117f28650b59f","text":"Store the data in an RDS database","correct":false},{"id":"5d9c9d9ca04e432986c67415ccfdb791","text":"Store the data in JSON format on an EC2 instance","correct":false}]},{"id":"0f02f4b2-9d11-4efb-b467-19bc558ef33d","domain":"security","question":"Your application on EC2 must write to an Aurora cluster to store user and purchasing data. Your CISO implements a new company-wide policy that requires all AWS credentials are encrypted and rotated monthly. How would you fulfill the new security policy with minimum administrative burden?","explanation":"AWS designed IAM roles so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. IAM roles are based on temporary security tokens, so they are rotated automatically. Credentials embedded in source code cannot be rotated without it being an administrative burden, and is a bad practice. It’s impossible to retrieve credentials from an S3 bucket if you don’t already have credentials for that bucket. IAM users cannot be associated with resources, and Active Directory authorization will not grant access to AWS resources.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html","title":"IAM Roles for Amazon EC2"}],"answers":[{"id":"c3e276ad3302f173e7d92b40534f2074","text":"Allow the application to fetch the credentials from an S3 bucket with SSE-S3. Upload new credentials monthly.","correct":false},{"id":"5fa27e0170ca1f467557d0d8db5502ff","text":"Encrypt the Aurora clusters' credentials using SHA-256 hash function in the application code, and schedule a CRON job to rotate monthly.","correct":false},{"id":"05a57da8fa8c802639a4f49ed0d59116","text":"Associate an IAM user with the application. Enroll that user with your Active Directory domain to use AD authorization.","correct":false},{"id":"fc14a6fe77ae1367c0f776ae96c7379c","text":"Attach an IAM role to the instance with proper credentials.","correct":true}]},{"id":"742534cf-2038-4f0d-b859-216cca30339e","domain":"deployment","question":"Which of the following statements relating are correct in relation to DynamoDB?","explanation":"A local secondary index maintains an alternate sort key for a given partition key value.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html","title":"Local Secondary Indexes"}],"answers":[{"id":"1e76bf9bca3dc7acaf4a05fe6bedabc2","text":"You can add a Local Secondary Index to an existing table","correct":false},{"id":"63fb8d2672eb66b5bcaf5267421cacb2","text":"A Local Secondary Index is an index that has the same partition key as the table, but a different sort key to the table.","correct":true},{"id":"b9f028903456b0bc0b4706cf7c6bea8f","text":"You can add a Global Secondary Index to an existing table","correct":true},{"id":"661c4d893c5ff8ef12919a23394749c8","text":"A Local Secondary Index is an index that has the a different partition key and a different sort key to the table.","correct":false}]},{"id":"313867da-7161-4083-9745-77950b6208dd","domain":"development","question":"You are using CodeBuild to create a Docker image and add the image to your Elastic Container Registry. Which of the following commands should you include in the buildspec.yml?","explanation":"Use the docker push command to add your image to your Elastic Container Registry","links":[{"url":"https://aws.amazon.com/blogs/devops/build-a-continuous-delivery-pipeline-for-your-container-images-with-amazon-ecr-as-source/","title":"Build a Continuous Delivery Pipeline for Your Container Images with Amazon ECR as Source"}],"answers":[{"id":"585a787c56d351517a27b9d64d3db8ae","text":"docker add $REPOSITORY_URI:latest","correct":false},{"id":"87c7975f8fe350f6f133c1f3d3b6b9f2","text":"aws codebuild docker -t $REPOSITORY_URI:latest .","correct":false},{"id":"ec1f85b16af01c1d7ec994f6ba6efa32","text":"docker push $REPOSITORY_URI:latest","correct":true},{"id":"d2d39a1ed4490154f045931eee5d18e4","text":"aws ecr push $REPOSITORY_URI:latest","correct":false},{"id":"c1520af238ed7b988b389f732c0d6287","text":"docker build -t $REPOSITORY_URI:latest .","correct":true}]},{"id":"1c37eac2-4401-11ea-b77f-2e728ce88125","domain":"security","question":"Which of the following protocols are used to set up secure connections to AWS CodeCommit repositories?","explanation":"AWS allows you to use either the HTTPS or the SSH protocol to connect to CodeCommit repositories. There’s no option to select HTTP or RDP connections.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up.html","title":"Setting Up for AWS CodeCommit"}],"answers":[{"id":"293c9ea246ff9985dc6f62a650f78986","text":"HTTP","correct":false},{"id":"0e8433f9a404f1f3ba601c14b026d321","text":"HTTPS","correct":true},{"id":"765553e6c7ac8592c389acb9878a050a","text":"SSH","correct":true},{"id":"a66d0b3ece299ba53eafac86750cfb4a","text":"RDP","correct":false}]},{"id":"17aedf6b-012a-448d-805b-784c7f87ba15","domain":"deployment","question":"What is the CloudFormation helper script cfn-init used for?","explanation":"CloudFormation helper scripts are Python scripts that can be used as part of a CloudFormation template to automate common tasks during stack creation. cfn-init helper script can be used to install packages, create files, and start/stop services. cfn-get-metadata can be used to fetch a metadata block from AWS CloudFormation and print it to standard out.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-init.html","title":"cfn-init"}],"answers":[{"id":"6fe3f8fbab579b6b679da8d9b773b7c2","text":"Fetch a metadata block from AWS CloudFormation template.","correct":false},{"id":"2a5d8dc3f4b9ee91839db7f876a01872","text":"Install packages and start/stop services on EC2 instance.","correct":true},{"id":"ab8e55f1fd87a343e434da332b588142","text":"Initialize CloudFormation IAM Service Role.","correct":false},{"id":"2831dc5c18284a29977a09e8fc481cbb","text":"Fetch required credentials before provisioning AWS resources.","correct":false}]},{"id":"f5072793-928c-4fc3-8ce0-bd18571b6765","domain":"deployment","question":"You are developing a gaming website which scores all players scores in a DynamoDB table. You are using a Partition key of user_ID and a Sort Key of game_ID as well as storing the user_score which is the user's highest score for the game and also a timestamp. You need to find a way get the top scorers for each game, who have scored over 50,000 points. Which of the following will allow to to find this information in the most efficient way?","explanation":"A scan operation would be less efficient than a query, so that is definitely not the most efficient way. The Query operation described won't help you find the top scorers for each game. A local secondary index is an index that has the same partition key as the base table, but a different sort key. A global secondary index is an index with a partition key and a sort key that can be different from those on the base table.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html","title":"Queries and Scans"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html","title":"DynamoDB Indexes"}],"answers":[{"id":"efcb11f92c2fff3dca6935cb74653ad7","text":"Scan the table and order by score","correct":false},{"id":"1e267d3212493fc534c9b2e9de826341","text":"Use a local secondary index with a partition key of user_ID and a sort key of user_score","correct":true},{"id":"7b9684c68079e7ca02fe44efb81fa0eb","text":"Query the table using a partition key of user_ID and sort by game_ID","correct":false},{"id":"eae4d20326b1ecd862aed8669ad068ca","text":"Use a global secondary index with a partition key of game_ID and a sort key of user_ID","correct":false}]},{"id":"7d99fa38-f31e-4527-95a6-a88611f7731c","domain":"mon-trb","question":"A developer deployed a serverless application consisting of an API Gateway and Lambda function using CloudFormation. Testing of the application resulted in a 500 status code and 'Execution failed due to configuration' error. What is a possible cause of the error?","explanation":"When you build an API Gateway API with standard Lambda integration using the API Gateway console, the console automatically adds the required permissions. However, when you set up a stage variable to call a Lambda function through your API, you must manually add these permissions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html","title":"Invoke"},{"url":"https://docs.aws.amazon.com/apigateway/api-reference/handling-errors/#api-error-codes","title":"Error Codes (Client and Server Errors)"}],"answers":[{"id":"eef7bf395052a4cb6bf0286cc9578779","text":"API Gateway is not authorized to invoke the Lambda function.","correct":false},{"id":"dddca3f487612188440bbfe93d1ef829","text":"IAM policy is restricting the user from invoking the API Gateway API endpoint.","correct":false},{"id":"8fa26152f6981113cb014274ea900af1","text":"Too many API Gateway requests were created exceeding the allowed limit.","correct":false},{"id":"d69e5b40405e5538753a24c15bb80a0e","text":"The Lambda function's resource-based policy doesn't include permission for your API to invoke the function.","correct":true}]},{"id":"53899b37-d74d-42b3-9be9-4f9f0d37155d","domain":"security","question":"Your company has a corporate identity store used to authenticate its users. Your company also has resources running on AWS. Your admin has created IAM roles and an identity broker that sits between your corporate users and your AWS resources to manage the authentication and authorization process without needing to re-create all your users as IAM users in AWS. Your CISO asked you to summarize the AWS identity federation process to ensure compliance with your applications' security. Which of the following statements correctly describes the authentication process?","explanation":"Users might already have identities outside of AWS, such as in your corporate directory. However, those users might need to work with AWS resources (or work with applications that access those resources). If so, these users also need AWS security credentials in order to make requests to AWS. The process can be summarized as follows: 1. The enterprise user accesses the identity broker application. 2. The identity broker application authenticates the users against the corporate identity store. 3. The identity broker application has permissions to access the AWS Security Token Service (STS) to request temporary security credentials. 4. Enterprise users can get a temporary URL that gives them access to the AWS APIs or the Management Console. A sample identity broker application for use with Microsoft Active Directory is provided by AWS. Details on page 22 of the URL link.","links":[{"url":"https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf?refid=em_","title":"AWS Security Best Practices"}],"answers":[{"id":"fe4c072b395ee1d0521094c0d1b4a005","text":"The identity broker application has permissions to access the AWS Security Token Service (STS) to request temporary security credentials. The enterprise user accesses the identity broker application. The identity broker application authenticates the users against the corporate identity store. Enterprise users can get a temporary URL that gives them access to the AWS APIs or the Management Console.","correct":false},{"id":"9b6e91665534238e195af79b94bb3233","text":"The enterprise user accesses the identity broker application. The identity broker application authenticates the users against the corporate identity store. Enterprise users can get a temporary URL that gives them access to the AWS APIs or the Management Console. The identity broker application has permissions to access the AWS Security Token Service (STS) to request temporary security credentials.","correct":false},{"id":"26461a0c133d31dd86bb803278256573","text":"The enterprise user accesses the identity broker application. The identity broker application authenticates the users against the corporate identity store. The identity broker application has permissions to access the AWS Security Token Service (STS) to request temporary security credentials. Enterprise users can get a temporary URL that gives them access to the AWS APIs or the Management Console.","correct":true},{"id":"2870fde359fa2a54b4e99faccf44cb32","text":"The enterprise user accesses the identity broker application.  Enterprise users can get a temporary URL that gives them access to the AWS APIs or the Management Console. The identity broker application authenticates the users against the corporate identity store. The identity broker application has permissions to access the AWS Security Token Service (STS) to request temporary security credentials.","correct":false}]},{"id":"6cdfb12d-88c9-4182-8761-892327d0d6e1","domain":"security","question":"You are working on a mobile phone app for an online retailer which stores its customer data in DynamoDB. You would like to enable new users to sign-up using Facebook or Google credentials. What is the recommended approach?","explanation":"Using Cognito is the recommended approach to federating with Web ID Providers for mobile applications","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html","title":"Amazon Cognito for Mobile Apps"}],"answers":[{"id":"f27fa20337c218b3cbc5c69c91585adf","text":"After the user has authenticated with Facebook, allow them to download encrypted AWS credentials to their device so that the mobile app can access DynamoDB","correct":false},{"id":"8a7c438e9d3be3a418abd985f62a5eba","text":"Embed encrypted AWS credentials into the application code, so that the application can access DynamoDB on the user's behalf","correct":false},{"id":"7afe23d8d3d95622b0bc6973be88c599","text":"Write your own custom code which allows the user to log in via a Web Identity Provider and receive an authentication token, then calls the AssumeRoleWithWebIdentity API and exchanges the authentication tokens for temporary access to DynamoDB","correct":false},{"id":"f6b8be7d8470d0f29d80578d719a209e","text":"Once the user has logged in to the Web Identity Provider, use Cognito to exchange the authentication tokens for temporary access to DynamoDB","correct":true}]},{"id":"42ff013e-95f1-4ead-958d-26a843b0b207","domain":"mon-trb","question":"Your security team have brought in an external auditor to review the security standards across your AWS account. They have identified that your development team have elevated privileges across a number of services, which according to company policy, they should not have access to. You have been asked to help work out which of the IAM policies are granting too much access to the team. Which of the following can you use to find out which policies are granting too many privileges?","explanation":"With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies attached to IAM users, groups, or roles in your AWS account. You can test which actions are allowed or denied by the selected policies for specific resources.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html","title":"Testing IAM Policies with the IAM Policy Simulator"}],"answers":[{"id":"0863526e514d88cd2d00b39f8dda0920","text":"X-Ray","correct":false},{"id":"fa535ffb25e1fd20341652f9be21e06e","text":"Config","correct":false},{"id":"df8a1f2103a58209a3008c02a93162b5","text":"Cognito","correct":false},{"id":"b6632fa69795d5fabc908fe75210b177","text":"IAM Policy Simulator","correct":true}]},{"id":"93862748-7e4b-4c90-a475-b52a4d1f6f4c","domain":"development","question":"You are planning to deploy a new version of your application using CodeDeploy. You only have a window of 2 hours to complete the deployment and test it. Your team leader is concerned about the time it could take to roll back the upgrade if it should fail. Which deployment approach would you recommend?","explanation":"Blue / Green is the one to use as this allows you to roll back with minimal disruption. An In-Place upgrade is very disruptive to roll back as it will involve re-deploying the original version of the code and during this time your application will be unavailable. Canary and Rolling updates are not an option for CodeDeploy","links":[{"url":"https://aws.amazon.com/blogs/devops/performing-bluegreen-deployments-with-aws-codedeploy-and-auto-scaling-groups/","title":"Blue/Green Deployments with AWS CodeDeploy"}],"answers":[{"id":"3a27747f75c4e73e94223a9e4065cd9c","text":"Blue / Green","correct":true},{"id":"ecf715d6d79a2698b7fec0357f9d721f","text":"Canary","correct":false},{"id":"ff2713a6181db42fded101c670bbd0dd","text":"Rolling with additional batch","correct":false},{"id":"53b8ba497ea2cdea89f60da12d94b46d","text":"In-Place","correct":false}]},{"id":"3efc0266-4c2e-11ea-b77f-2e728ce88125","domain":"deployment","question":"Which of the following statements are true about the concept of blue/green deployment regarding development and deployment of your application?","explanation":"With blue/green deployment, you can shift traffic between two identical environments that are running different versions of your application. It allows you to easily deploy changes to your application and roll-back on changes very quickly.","links":[{"url":" https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf","title":"Blue/Green Deployments on AWS"}],"answers":[{"id":"545fd2a37ac85b63dad4f92c91401436","text":"The green environment represents the staging environment.","correct":false},{"id":"72f75a1c7558a3b8e4c011917ad6105d","text":"The green environment represents the production environment.","correct":false},{"id":"3dae7ad4a3084f0fa3d2598654de9066","text":"The blue environment represents the current version of your application serving production traffic.","correct":false},{"id":"69da7fc6d495def048f27d73e7e55e82","text":"It allows you to shift traffic between two identical environments that are running different versions of your application.","correct":true}]},{"id":"4e5543f6-fbf3-44cf-ab00-3585ac527a04","domain":"refactoring","question":"You are working on a web application which needs somewhere to store user session state. Which of the following approaches is the best way to deal with user session state?","explanation":"ElastiCache is the best option for storing session state as it is scalable, highly available and can be accessed by multiple web servers. RDS is not optimal, but could be used for storing session state. Since you need to provide 2 answers it is the only other viable answer.","links":[{"url":"https://aws.amazon.com/caching/session-management/","title":"Session Management"}],"answers":[{"id":"be8019a0436605aff98fc69abd0feeec","text":"Store session state locally on the EC2 instance","correct":false},{"id":"88efe13a943c7fa646b910592ecda2f9","text":"Store session state in memory","correct":false},{"id":"b225818943ba4680b8e7dc9d9c376359","text":"Store session state in RDS","correct":true},{"id":"a29448cdc61e8cc9889a72bd45cbd20a","text":"Use an ElastiCache cluster","correct":true}]},{"id":"d91b5d09-a238-441f-b247-d81789372ec1","domain":"development","question":"GetItem operation is used to read data from a DynamoDB table. What strategy can be used to reduce the size of the read operations and increase read efficiency?","explanation":"Projection Expressions are a DynamoDB feature used to limit the attributes returned by the GetItem operation. Thus, this can be used to reduce the size of the payload returned by a read operation. Parallel Scans allows multi-threaded applications to perform DynamoDB Scan operations quicker. It cannot be used with GetItem operations to make them more efficient. Pagination allows developer to perform a Scan operation on a table and divide the result set into multiple pages. It cannot be used to make GetItem operations more efficient. Filter expression can be used with Scan operations to filter the results returned by the scan operation. It is not a GetItem operation feature.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html","title":"Projection Expressions"}],"answers":[{"id":"ba803153fcb13208c9f26c6ae0dedef6","text":"Use Filter Expression.","correct":false},{"id":"30eb0435ad1702227f2d730850a75e93","text":"Use Pagination.","correct":false},{"id":"e37aefa5950be4eef211c98b18690f64","text":"Use Projection Expression.","correct":true},{"id":"5081ca486a2f5aed471c714c6d81489f","text":"Use Parallel Scan.","correct":false}]},{"id":"fe61a353-eb18-40dc-9e65-30f7106d3e6e","domain":"mon-trb","question":"Your application is running on EC2 and on Linux virtual machines in your own data center. You would like to configure your application to send data to X-Ray for troubleshooting and performance analysis. Which of the following steps will you need to complete?","explanation":"You need the X-Ray SDK and the X-Ray daemon on your EC2 instances and on-premises systems, you then need to instrument your application to send the required data to X-Ray","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html","title":"X-Ray Developer Guide"}],"answers":[{"id":"998a1f353d1923ec24b66b6ae427d343","text":"Install the X-Ray daemon, then instrument your application to send data to X-Ray.","correct":false},{"id":"ff31ac1442cb9f28ad8f65a358816a25","text":"Install the AWS SDK and the X-Ray CLI, then instrument your application to send data to X-Ray.","correct":false},{"id":"da1eaa5b340d64b20d5b526dc0de4b85","text":"Install the X-Ray SDK and the X-Ray daemon, then instrument your application to send data to X-Ray.","correct":true},{"id":"6ffb986aa2b49eb856b38668f9a44fc2","text":"Install the AWS CLI, then instrument your application to send data to X-Ray.","correct":false}]},{"id":"3fc48fe5-2386-4e89-ba3e-045136c37de4","domain":"development","question":"You have a legacy application located in your production data centre, which frequently accesses files stored in S3. Due to a significant increase in workload, your application servers are now generating a huge number of requests to your S3 bucket, with many requests now failing. What can you do to improve the situation?","explanation":"Numerous components on a network, such as DNS servers, switches, load balancers, and others can generate errors anywhere in the life of a given request. The usual technique for dealing with these error responses in a networked environment is to implement retries in the client application. Retrying the request with Exponential Backoff technique increases the reliability of the application and reduces operational costs for the developer.","links":[{"url":"https://docs.aws.amazon.com/general/latest/gr/api-retries.html","title":"Error Retries And Exponential Backoff In AWS"}],"answers":[{"id":"b84f4c83038ab1b006eadbd1c591ccaa","text":"Configure your application to read and write to multiple S3 buckets","correct":false},{"id":"40d21c12e8e9a47ba266648c0cd43681","text":"Install a faster network interface in your application servers","correct":false},{"id":"895f33b72cf7d11e0e8f0d9fa8ee793c","text":"Configure your application to use Exponential Backoff","correct":true},{"id":"dc46f742e30fa33e5e1c551a3057eecf","text":"Migrate the data to DynamoDB","correct":false}]},{"id":"098e4897-54dd-493d-ae35-d28374c03576","domain":"security","question":"You application can be accessed using multiple devices, for example, laptop, tablet, iPhone or Android devices. You would like to be able to identify and track when your users access your site using different devices. Which of the following AWS technologies can enable you to do this?","explanation":"Cognito enables developers to remember the devices on which end-users sign in to their application. You can see the remembered devices and associated metadata through the console. In addition, you can build custom functionality using the notion of remembered devices. For example, with a content distribution application (e.g., video streaming), you can limit the number of devices from which an end-user can stream their content.","links":[{"url":"https://aws.amazon.com/blogs/mobile/tracking-and-remembering-devices-using-amazon-cognito-your-user-pools/","title":"Tracking and Remembering Devices Using Amazon Cognito"}],"answers":[{"id":"6cbb574c9488bb59fdd64fa8f509fce8","text":"Use Cognito","correct":true},{"id":"0d6127433a099d6824bfd30c14ad4096","text":"Create a unique user ID and associate it with the device metadata","correct":false},{"id":"95e34ddc7b57bc86318fc7a45f0d2e2d","text":"Use a Lambda function to store a unique device ID in DynamoDB and associate it with the user session ID","correct":false},{"id":"5b46051451752459a684abff24663b39","text":"Store a unique session ID in ElastiCache","correct":false},{"id":"8204565cad3d39f0897a0ce85182016f","text":"Store a unique session ID in DynamoDB","correct":false}]},{"id":"2689a73b-04ed-4719-9cdf-49c4ffe3eb17","domain":"deployment","question":"A developer is deploying a new application to ECS. The application requires permissions to send messages to an SQS queue. \n\nWhich role should the developer apply the policy to so that the application can access the SQS queue?","explanation":"The policy must be attached to the ECS Task's execution role to allow the application running in the container access SQS.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html","title":"Amazon ECS Task Execution IAM Role"}],"answers":[{"id":"29aefbd6a89941ca26d2ef879d25237c","text":"The execution role attached to the ECS Task.","correct":true},{"id":"2d64ba22b9f8e2998aed499099374359","text":"The execution role attached to the ECS Service.","correct":false},{"id":"d89a15f255b7e5a2b6cfe8aa7d628173","text":"The execution role attached to the ECS Cluster.","correct":false},{"id":"2413a4993d7f9395b5ddf84bc7b51b62","text":"The execution role attached to the ECS Container.","correct":false}]},{"id":"68d79e4d-f827-496d-9fc3-606615dd6fe5","domain":"security","question":"When using Web Identity Federation and Cognito to allow a user to access an AWS service (such as an S3 bucket), which of the following is the correct order of steps?","explanation":"A user authenticates with Facebook first. They are then given an ID token by Facebook. An API call, AssumeRoleWithWebIdentity, is then used in conjunction with the ID token. A user is then granted temporary security credentials.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_manual.html","title":"Using Web Identity Federation APIs"}],"answers":[{"id":"e95a8dc9c210c0c1e6e36f8215ab4978","text":"A user makes the AssumeRoleWithWebIdentity API Call. The user is then redirected to Facebook to authenticate. Once authenticated, the user is given an ID token. The user is then granted temporary access to the AWS platform.","correct":false},{"id":"811872e18604a3d8118cc1c9381bf702","text":"Users cannot use Facebook credentials to access the AWS platform.","correct":false},{"id":"13bdd19ca71c9978a4642bd95fcb92c5","text":"A user authenticates with Facebook first. They are then given an ID token by Facebook. An API call, AssumeRoleWithWebIdentity, is then used in conjunction with the ID token. A user is then granted temporary security credentials.","correct":true},{"id":"b66b1e5f0bf94eca60bce37aa0702f6e","text":"A user logs in to the AWS platform using their Facebook credentials. AWS authenticates with Facebook to check the credentials. Temporary Security Access is granted to AWS.","correct":false}]},{"id":"1afa6661-6523-49f7-912c-46b740714117","domain":"deployment","question":"You are deploying a number of Lambda functions using CloudFormation. Which section of the CloudFormation template should you use to define your Lambda resources?","explanation":"Use the Resources section of the CloudFormation template to define the resources you are going to deploy, e.g. EC2 instances, S3 buckets, IAM roles, Lambda functions etc.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"CloudFormation Template Anatomy"},{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-lambda-state-machine-cloudformation.html#lambda-state-machine-cfn-step-2","title":"Example CloudFormation Template"}],"answers":[{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":false},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":true},{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false}]},{"id":"914fca77-6b5e-4b8e-afd1-d6e423835341","domain":"refactoring","question":"Your application is trying to upload a 6TB file to S3 and you receive an error message telling you that your proposed upload exceeds the maximum allowed object size. What is the best way to accomplish this file upload?","explanation":"Amazon S3 allows a maximum object size of 5TB. However, objects 5GB or larger are required to be uploaded using the multipart upload API.","links":[{"url":"https://aws.amazon.com/s3/faqs/","title":"S3 FAQs"},{"url":"https://aws.amazon.com/blogs/aws/amazon-s3-multipart-upload/","title":"Multipart upload"}],"answers":[{"id":"eac600adbe8c5aff9f9bc513cea6c0f0","text":"Contact AWS support to increase the maximum size of your S3 object.","correct":false},{"id":"73d0d8288e849f30fc95922108ef6a15","text":"Use the Multipart Upload API for this object.","correct":false},{"id":"07b09839951d99bc4114d8fdf4c16454","text":"Use the S3 LargeObjectUpload API.","correct":false},{"id":"199f05e3820037e7815b1de805cc4d7d","text":"You cannot fix this, as the maximum size of an S3 object is 5TB.","correct":true}]},{"id":"d32ba0f0-1563-4a30-9e61-b5edf82d628e","domain":"security","question":"You are developing a legacy application which handles confidential healthcare data. The application runs on two EC2 instances behind an Application Load Balancer. Because of the age of the application, you cannot perform TLS encryption on the EC2 instances themselves. What is the least complex way you can ensure data is encrypted in transit between your VPC, and the customer who will be accessing it?","explanation":"Handling the TLS termination process within each EC2 instance adds to the computational load on the instance as well as the operational overhead of installing an X.509 certificate on each instance. You can easily arrange for the entire HTTPS encryption and decryption process, generally known as TLS termination to be handled by an Elastic Load Balancer. Your users can benefit from encrypted communication with very little operational overhead or administrative complexity.","links":[{"url":"https://aws.amazon.com/blogs/aws/elastic-load-balancer-support-for-ssl-termination/","title":"AWS Elastic Load Balancing: Support for SSL Termination"}],"answers":[{"id":"ef68b9b0058c7aa4c84143939156e93b","text":"Perform TLS termination on the ALB","correct":true},{"id":"4567a70530701c962bab095e279dfb3b","text":"Perform TLS termination using Lambda","correct":false},{"id":"f81774e2c858b52c0321762e8489f624","text":"Require customers to connect through a VPN to a virtual private gateway","correct":false},{"id":"18c056fd0ece19fd238f6d245fe85321","text":"Upgrade the application to support TLS on the EC2 Instances","correct":false}]},{"id":"43e8c8e5-d0e3-4502-b7e7-8c236a6625e3","domain":"mon-trb","question":"A company wants to monitor all traffic to a network interface on their bastion host. They wish to be alerted if there are more than 10 attempts to connect to the host via SSH within a one-hour time interval. What solution can the company employ to meet this requirement?","explanation":"VPC flow logs can be sent to CloudWatch Logs. A CloudWatch metric filter and alarm can be configured to send notifications when the specified criteria are satisfied. CloudTrail is not a supported destination for VPC flow logs. Amazon Inspector cannot be used to inspect network traffic in the way specified by the requirements. It performs vulnerability assessments on the host VM. Lambda functions cannot mount EBS volumes.","links":[{"url":"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html","title":"Amazon S3 Glacier Vault Lock"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-cwl.html#flow-logs-cwl-create-flow-log","title":"Creating a Flow Log That Publishes to CloudWatch Logs"}],"answers":[{"id":"6e51df099a849474d4791bce4aa25bb5","text":"Configure a VPC flow log with CloudWatch Logs as the destination. Create a CloudWatch metric filter for destination port 22. Create a CloudWatch Alarm trigger.","correct":true},{"id":"343b93587a12ff88f4e59413fbbd5d96","text":"Install the Amazon Inspector agent on the bastion host. Configure CloudWatch alerts based on Amazon Inspector findings.","correct":false},{"id":"cdab2e49cfa676ebe99e79d8f77f49b3","text":"Create a Lambda function that mounts the bastion host EBS volume and sends logs to CloudWatch logs. Create a CloudWatch metric filter for destination port 22. Create a CloudWatch Alarm trigger.","correct":false},{"id":"ecb0bcaac946feb8b9a9c2abcaace499","text":"Create a VPC flow log for the network interface with CloudTrail as the destination. Create a Lambda function that queries the CloudTrail logs for SSH login attempts. Trigger the Lambda function every 5 minutes with a scheduled CloudWatch event.","correct":false}]},{"id":"f7fd0d7a-5d45-4f72-aa8e-d9a65270360f","domain":"refactoring","question":"You are developing an online hotel booking application which makes an number of requests to different back end applications to get quotes for travel related add-on services. You are using API gateway handle all the API calls and you notice that the majority of requests are for the same 5 or 6 services. How can you optimize the configuration to ensure the best performance for your application?","explanation":"You can enable API caching to cache your endpoint's responses, this reduces the number of calls made to your endpoint and improves the latency of requests to your API.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"API Caching"}],"answers":[{"id":"9a86454fefc71c7430655ce2e18ac716","text":"Configure a CloudFront CDN in front of the API Gateway to cache the most frequent HTTP requests","correct":false},{"id":"336a729744a0b4682222e3a4a1cdc750","text":"Implement API Caching to cache the endpoint's response for the most popular requests","correct":true},{"id":"b255bca6117b1096664ab7b1415fae80","text":"Add an ElastiCache cluster in front of your database to cache the most frequently accessed data","correct":false},{"id":"59a69a5bb20a3dbe9214ef93c38041f7","text":"Configure auto-scaling for the API Gateway","correct":false}]},{"id":"d8b8e087-28cf-41f5-b882-7c4d8e237e4b","domain":"refactoring","question":"You are working on a project to migrate an on-premises website to AWS, your CTO has mandated that wherever possible, Serverless technologies should be used. Which of the following services would you consider for this project?","explanation":"S3, Lambda and DynamoDB are all serverless technologies that could be used to build a website. EC2 and RDS are not considered to be serverless because they are backed by a virtual server.","links":[{"url":"https://aws.amazon.com/serverless/","title":"What is Serverless?"}],"answers":[{"id":"81b22456f78954c460ce2f531b5e048f","text":"EC2","correct":false},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":true},{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true}]},{"id":"0394976d-f5c9-489a-8411-0dca671a3f67","domain":"deployment","question":"An application connects to an external third-party service with API keys being managed by AWS Secrets Manager. The development team uses CodeBuild for source code compilation activities in their CI/CD process.   Where should the reference to the third-party service API keys be specified?","explanation":"CodeBuild uses the BuildSpec file as a specification of build commands and settings.  “secrets-manager” syntax can be used to retrieve API Keys stored in AWS Secrets Manager. Build Environment variables should NOT be used for storing sensitive information as they are displayed in plain text. AppSpec file is used by CodeDeploy to specify and manage deployments. CloudFormation templates are used by CloudFormation and not by CodeBuild.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html","title":"Build Specification Reference for CodeBuild"}],"answers":[{"id":"47d4255c389a3bcdca135b3871c5238d","text":"CodeBuild Environment Variable","correct":false},{"id":"ef56db4cbf113c83bf357241b3fe4418","text":"BuildSpec file","correct":true},{"id":"3e1115820889c27bfd8aca3774b9c1f0","text":"AppSpec file","correct":false},{"id":"5bd169bcf0b77c844c2f4bf1bf3eac9d","text":"CloudFormation Template","correct":false}]},{"id":"03a312db-34cd-4c47-9f7e-da02470c8c03","domain":"mon-trb","question":"You work in the security industry for a large consultancy. One of your customers uses Lambda extensively in their production environment and they require a log of all API calls made to and from their Lambda functions. How can you achieve this?","explanation":"Enabling CloudTrail for Lambda will allow you to log all API calls to an S3 bucket.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/logging-using-cloudtrail.html","title":"Logging Lambda API Calls Using AWS CloudTrail"}],"answers":[{"id":"02ef0d69dfa15f0bee0148eeee8c3936","text":"Enable Detailed Monitoring on the Lambda function","correct":false},{"id":"2365a4464b0b2502654aeb4ddf868a14","text":"Enable CloudTrail for Lambda","correct":true},{"id":"e2cd3f939eef3b06ec217db11988c978","text":"Enable CloudWatch for Lambda","correct":false},{"id":"77bca249739dbbe611e40d1dd5f2dc7e","text":"Enable Access Logs for Lambda","correct":false}]},{"id":"00544151-fdaf-446d-ade3-7d6988c9133d","domain":"deployment","question":"You are building a distributed application, which is made up of a number of Docker instances running on an ECS cluster. You would like to configure your application to send data to X-Ray. Where should you install the X-Ray daemon?","explanation":"In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. X-Ray provides an official Docker container image that you can deploy alongside your application.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html","title":"Running the X-Ray daemon on Amazon ECS"}],"answers":[{"id":"bfdfc3a1dc6179ede8ebcc926faacd94","text":"Install the X-Ray daemon on the EC2 instance where your Docker containers are running.","correct":false},{"id":"5844e2a52b5fd5bfeeab3e714e68ba83","text":"Update the Docker image to include the X-Ray daemon and provision the new version of the application.","correct":false},{"id":"981d352ae2b571b99295e2880457f235","text":"Create a separate Docker image to run the X-Ray daemon.","correct":true},{"id":"63a693207215523784e22cf504e4579b","text":"Install the X-Ray daemon on the same Docker container where the application code is running.","correct":false}]},{"id":"cd8831e1-d079-43e5-a45d-4d0770135faa","domain":"refactoring","question":"You are working on a flight booking application which runs on a number of EC2 instances. Recently one of your servers crashed which meant all of your users lost their sessions and had to log in again. Many of your customers have complained that they had to start their session again from the beginning because your application does not store session state anywhere. Which of the following could you use to persist session state and stop this from happening?","explanation":"Many applications store session state data in memory. However, this approach doesn't scale well. After the application grows beyond a single web server, the session state must be shared between servers. DynamoDB provides an effective and scalable solution for sharing session state across web servers.","links":[{"url":"https://docs.aws.amazon.com/sdk-for-net/v3/ndg/web-dynamodb-session.html","title":"Managing Session State with Amazon DynamoDB"}],"answers":[{"id":"248c1b0bcb74394beb4a08030c4a6847","text":"EBS","correct":false},{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":false}]},{"id":"4216cacb-ee8a-42e5-acff-9b1ea39357e6","domain":"development","question":"A developer is implementing an IoT application using DynamoDB as the data store for device event data. An application requirement is to automatically purge all event data older than 30 days. What is the optimal option to implement this requirement?","explanation":"Time to Live (TTL) for Amazon DynamoDB is functionality that enables automatic deletion of items after a specified expiration time. The expiration time is defined by a timestamp in the TTL attribute. When Time to Live (TTL) is enabled on a table in Amazon DynamoDB, a background job checks the TTL attribute of items. The background job compares the current time in epoch time format to the time stored in the Time to Live attribute of an item to determine whether the item is expired. If the epoch time value stored in the attribute is less than the current time, the item is marked as expired and then deleted.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html","title":"Time to Live: How It Works"}],"answers":[{"id":"89ff2a65dcb0d5ab07793957e16ed650","text":"Enable TTL on the DynamoDB table and store the expiration timestamp in the TTL attribute in the epoch time format.","correct":true},{"id":"ac0b210d52da3eacb34d48327798c599","text":"Create a new DynamoDB table every 30 days. Delete the old DynamoDB table.","correct":false},{"id":"0a4018a4dc09457f483a0fb75a49f140","text":"Enable DynamoDB streams on the table. Implement Lambda function to read events from the stream and delete expired items.","correct":false},{"id":"c408a205dcdc64e12ad635afc658fa61","text":"Implement a Lambda function to perform a query on the table and delete items with timestamp greater than 30 days. Use CloudWatch events to trigger Lambda function.","correct":false}]},{"id":"7e589557-fe47-4b17-8c0a-f4afe1a9c764","domain":"mon-trb","question":"Your application servers are behind an Application Load Balancer with sticky sessions configured. However during busy times you are occasionally finding that one of your application servers is becoming overloaded. Which of the following options could help avoid this from happening?","explanation":"The use of Sticky Sessions means that requests which are part of the same session get routed to the same target, which may cause the host to become throttled. ElastiCache can be accessed by multiple servers, allowing the load to be distributed more evenly.","links":[{"url":"https://aws.amazon.com/caching/session-management/","title":"Session Management"}],"answers":[{"id":"be8019a0436605aff98fc69abd0feeec","text":"Store session state locally on the EC2 instance","correct":false},{"id":"2aa6ae3b5971bedd1a05b4d5636afbac","text":"Store session state in an ElastiCache cluster","correct":true},{"id":"b225818943ba4680b8e7dc9d9c376359","text":"Store session state in RDS","correct":false},{"id":"88efe13a943c7fa646b910592ecda2f9","text":"Store session state in memory","correct":false}]},{"id":"f4f9e915-de73-4bcb-b2ad-dc9f35db47b8","domain":"development","question":"A document management application stores a catalogue of documents, each uniquely identified by its Document Number. Each document is also described by additional attributes: Document Title, Publication Date, Publisher Name, Country of Origin, and Length.  Functional requirements specify that the application should be able to produce a listing of all documents for each country of origin.  What would be the optimal DynamoDB data model for this application?","explanation":"Document Number is unique for each item thus making it a good choice for the table partition key.  Using a random prefix for the GSI partition key and County of Origin as the sort key enables us to have high cardinality for the partition key (thus avoiding any hot partitions) while still allowing for fast querying based on the Country of Origin. Publication Date is a poor choice for the partition key as it is a low cardinality attribute. It results in a hot partition for all items published on that date. Country of Origin is a low cardinality attribute and makes a poor choice for the partition key of the GSI.  This would result in a hot partition for the GSI.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html","title":"Designing Partition Keys to Distribute Your Workload Evenly"},{"url":"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/","title":"Choosing the Right DynamoDB Partition Key"}],"answers":[{"id":"5758693e8b3c98551e39e76dc9a454f3","text":"Table Partition Key=Document Number; Table Sort Key=Document Type; GSI Partition Key=Publication Date; GSI Sort Key=Country of Origin;","correct":false},{"id":"6885e1c4312042be0a8e712ed8803a35","text":"Table Partition Key=Document Number; Table Sort Key=Document Type; GSI Partition Key=Country of Origin; GSI Sort Key=Publisher Name;","correct":false},{"id":"a164aeb879c69b453a5da649dcf8644e","text":"Table Partition Key=Publication Date; Table Sort Key=Document Number; GSI Partition Key=Publisher Name; GSI Sort Key=Country of Origin;","correct":false},{"id":"c564e5a2356a715aed2326166cf46f04","text":"Table Partition Key=Document Number; Table Sort Key=Document Title; GSI Partition Key=Random Prefix; GSI Sort Key=Country of Origin;","correct":true}]},{"id":"42bcba21-0656-4e34-a234-64c5a04098e5","domain":"development","question":"Which of the following approaches can improve the performance of your Lambda function?","explanation":"Establishing connections within the execution environment allows them to be reused next time the function is invoked which saves time. Only including the libraries you need will minimise the time taken for Lambda to unpack the deployment package.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html","title":"Lambda Best Practices"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html","title":"Lambda Execution Model"}],"answers":[{"id":"a48e2c99a71dd50095f8f61a4c1ea957","text":"Establish your database connections from within the Lambda execution environment to enable connection reuse","correct":true},{"id":"a9cca095324da14b8dcbc1b0e8c29b7c","text":"Store environment variables outside the function","correct":false},{"id":"e775953bf1c59c14203112df2cbefb91","text":"Only include the libraries you need to minimize the size of your deployment package","correct":true},{"id":"9164293eddfbca67a6d4536c763ed565","text":"Package all dependencies with your deployment package","correct":false}]},{"id":"1256624a-ca55-4279-94ef-95dd891413b0","domain":"development","question":"Which of the following AWS services would you recommend using to store session state data for a scalable web application?","explanation":"Storing session state locally is not a good idea for a scalable application, so it doesn't make sense to store the session state on the EC2 instance. Lambda is generally for short-lived functions which do not persist, so is not suitable for managing session state. Glacier is designed for archiving infrequently used data so is not suitable for session data which could be frequently used for the lifetime of the session and then no longer required. In order to address scalability and to provide a shared data storage for sessions that can be accessed from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached, and in AWS the service to use is ElastiCache.","links":[{"url":"https://aws.amazon.com/caching/session-management/","title":"Session Management"}],"answers":[{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":false},{"id":"50848259480914860b338e7baf94c29a","text":"ElastiCache","correct":true},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":false},{"id":"04f055bdce7b135b43773f720d011d7c","text":"EC2 instance memory","correct":false},{"id":"18a293ade3b7503fb165b9b2805ed819","text":"EC2 instance EBS volume","correct":false}]},{"id":"7ff30792-25db-4803-a4db-867473e5ca8a","domain":"security","question":"In order to enable encryption at rest using EC2 and Elastic Block Store,  you must ________.","explanation":"To enable encryption, you must specify encryption when creating the EBS volume.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html","title":"About EBS Encryption"}],"answers":[{"id":"4bd8d581e477097d1396901aab8b3cf4","text":"Mount the EBS volume in to S3 and then encrypt the bucket using a bucket policy","correct":false},{"id":"aaed5dee4871b8256093659e8bbeba4a","text":"Configure encryption using the appropriate Operating Systems file system","correct":false},{"id":"64237cc9809266ebc9fb12b6cede9aa8","text":"Configure encryption using X.509 certificates","correct":false},{"id":"d8e355c7726e173ad29951d3461865d6","text":"Configure encryption when creating the EBS volume","correct":true}]}]}}}}
