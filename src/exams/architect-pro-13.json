{"data":{"createNewExamAttempt":{"attempt":{"id":"d6c2781c-40f4-4488-b3ae-9a1d6889912d"},"exam":{"id":"c3885b88-656d-4f0f-9305-47d1c70fa497","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"b303f8e0-2c68-44aa-93bb-45b987b17d95","domain":"awscsapro-domain3","question":"You are helping a client build some internal training documentation to serve as architectural guidelines for their in-house Solutions Architects.  You suggest creating something inspired by the AWS Well-Architected Framework.  The client agrees and wants you to come up with some examples of each pillar.  Which of the following are examples of the Reliability pillar?","explanation":"The Reliability pillar includes five design principles:  Test recovery procedures, Automatically recovering from failure, Scaling horizontally to increase aggregate system availability, Manage change in automation and Stop guessing capacity.  By being able to closely monitor resource utilization, we can increase the Reliability and efficiency to right-size capacity.","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"3eb456756ecb767ab18179d87ec49a6b","text":"We can drive improvement through lessons learned from all operational events and failures. Share what is learned across teams and through the entire organization.","correct":false},{"id":"c57c8249b3885f2078d8dac40d695dec","text":"We can design workloads to allow components to be updated regularly, making changes in small increments that can be reversed if they fail.","correct":false},{"id":"c257f1eec7a5a7f1eae9338f4de45cb0","text":"On AWS, we'll be able to monitor demand and system utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over or under-provisioning.  We can stop guessing on capacity needs.","correct":true},{"id":"e82a19c63c4c9fedccc997eece7eccdc","text":"With virtual and automatable resources, we can quickly carry out comparative testing using different types of instances, storage, or configurations.  This will allow us to experiment more often.","correct":false}]},{"id":"d8bfc54e-024a-4fbb-9daa-9a218a10b738","domain":"awscsapro-domain5","question":"Your company has an Inventory Control database running on Amazon Aurora deployed as a single Writer role. Over the years more departments have started querying the database and you have scaled up when necessary.  Now the Aurora instance cannot be scaled vertically any longer, but demand is still growing.  The traffic is 90% Read based.  Choose an option from below which would meet the needs of the company in the future.","explanation":"This question is about scaling, and if you have scaled up to the maximum level (db.r4.16xlarge) the next step is to consider scaling out.  In this case the application is Read heavy, which lends itself perfectly to adding extra Read replicas and using Read-Write splitting to help future growth.  Changing the max_connections value or using Query plan optimisation may make performance more efficient, but they are not long term solutions. Adding Multi-AZ simply adds High Availability.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html","title":"Managing Performance and Scaling for Aurora DB Clusters"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-replicas-adding.html","title":"Adding Aurora Replicas to a DB Cluster"}],"answers":[{"id":"15cd367246a8991991de371856908334","text":"Create multiple additional Readers within the Aurora cluster and alter the application to make use of Read-Write splitting","correct":true},{"id":"c6229282cfe5ed1024d22e629709b94d","text":"Increase the maximum number of connections into the database by changing the max_connections parameter","correct":false},{"id":"d1fe9035612273c2d0c6b9f4229e1d3d","text":"Convert Aurora to a Multi-AZ Deployment in three or more zones","correct":false},{"id":"ce644fa65dde55c85ae051c5081ba860","text":"Use Query plan management to allow the optimizer to choose the most efficient plan for each job and make transactions quicker","correct":false}]},{"id":"c3ac5de9-a343-4cde-af1b-6c9f89824d2f","domain":"awscsapro-domain5","question":"An external auditor is reviewing your process documentation for a Payment Card Industry (PCI) audit.  The scope of this audit will extend to your immediate vendors where you store, transmit or process cardholder data.  Because you do store cardholder data in the AWS Cloud, the auditor would like to review AWS's PCI DSS Attestation of Compliance and Responsibility.  How would you go about getting this document? ","explanation":"AWS Artifact provides on-demand downloads of AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use. You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls.","links":[{"url":"https://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html?icmpid=docs_artifact_console","title":"What Is AWS Artifact? - AWS Artifact"}],"answers":[{"id":"09e838e873f25f954fef911d50b3d1ab","text":"AWS Pinpoint","correct":false},{"id":"d9208942349d1c6f7dbaba3661069bc1","text":"AWS WorkDocs","correct":false},{"id":"1d16d307ee989a80e421198a01993a9c","text":"AWS IAM Console","correct":false},{"id":"d7cb47dd1f374d3ed079b14cc6f2cd75","text":"Submit a Support Case requesting the document","correct":false},{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":true},{"id":"63df0d05cd43af35c95cf04d92aaf685","text":"AWS Legal Services website","correct":false},{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false}]},{"id":"90c0c2e2-4a39-4731-80b8-5f4d64c3d896","domain":"awscsapro-domain2","question":"You have been asked to design a landscape that can facilitate the upload very high resolution photos from mobile devices, gather metadata on objects in the photos and store that metadata for analysis.  Which of the following components would you use for this use-case for quickest implementation and best scalability?","explanation":"DynamoDB and S3 represent the most reasonable and scalable choices in this list for metadata storage (DynamoDB) and file upload (S3).  Kinesis has size limits on the inbound object so it would not be appropriate for use cases that involve potentially large files like photos.  Amazon Rekognition is image processing service that can extract metadata on objects in a photograph.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition â€“ Video and Image - AWS"}],"answers":[{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true},{"id":"803b0d23fbee2cf79d83376ef09a3eee","text":"Polly","correct":false},{"id":"6fa977a31b941055e5cc04cc2000fb84","text":"Rekognition","correct":true},{"id":"8d4c0b2cef256d21ab680366c8b1c6bf","text":"EMR","correct":false},{"id":"594025cae6dfa6b9073dc25de93ddb56","text":"Kinesis","correct":false},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true}]},{"id":"ba6576ca-a3da-4742-8917-cf5852a133bc","domain":"awscsapro-domain2","question":"You are in the process of porting over a Java application to Lambda.  You find that one Java application's code exceeds the size limit Lambda allows--even when compressed.  What can you do?","explanation":"If your code is too large for Lambda, it might indicate the need to break the code down into more atomic elements to support microservice best practices.  If breaking the code down is not possible, you should consider deploying in a different way like ECS or Elastic BeanStalk.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/limits.html","title":"AWS Lambda Limits - AWS Lambda"}],"answers":[{"id":"2c12b2cfb5b150801f171e15a1e6bfb7","text":"Consider containerization and deploy using Elastic Beanstalk.","correct":true},{"id":"91f37922d4b3b7e70c1f900cb95370e8","text":"Enable Extended Storage in the Lambda console to permit a larger codebase to be deployed.","correct":false},{"id":"0d95df1e6f5e72a068c3fbcbc598b16d","text":"Use AWS CodeBuild to identify unused libraries and remove them from the package. ","correct":false},{"id":"7c5f7ca2bf3fe981064a0883ba4d7158","text":"Change the Java Runtime Version in the Lambda function to one that supports BIGINT. ","correct":false},{"id":"a6bb1edf368a254d9604ca8e6419be1d","text":"Consider using API Gateway to offload some of the I/O your code requirements.","correct":false},{"id":"5fd2567315fb3689d9b56ea422cba642","text":"Evaluate the structure of the program and break it into more modular components.","correct":true}]},{"id":"93badb2c-68ab-4715-b29e-1209af0c7b27","domain":"awscsapro-domain2","question":"You are a developer for a Aerospace company.  As part of an outreach and education program, the company has financed the construction of a free public service that provides weather forecasts for the sun.  Anyone can make a call to this REST service and receive up-to-date information on forecasted sun flare or sun spots that might have an electromagnetic impact here on Earth.  You are in the final stages of developing this new serverless application based on DynamoDB, Lambda and API Gateway.  During performance testing, you notice inconsistent response times for the service.  You had expected the API to be relatively consistent since its just retrieving data from DynamoDB and returning it as JSON via the API Gateway.  What might account for this variation in response time?","explanation":"Inconsistent response times can have a few different causes.  The exact nature of the testing is not explained but we can anticipate a few causes.  If you have enabled API Gateway caching, the gateway can return a result from its cache without having to go back to a supplying service or database.  This can result in various response rates depending on if an item is in the cache or not.  (The question did not specify we had slow response...just inconsistent response which could be a response faster than we expected.)  When a Lambda function is run for the first time or after an update, AWS must provision the Lambda environment and pull in any external dependencies.  This can result in a slower response time at first but faster later.  Also, if we do not have sufficient RCU for our DynamoDB table, we could run into throttling of the reads which could appear as inconsistent response times.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html","title":"AWS Lambda Execution Context - AWS Lambda"},{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"Enable API Caching to Enhance Responsiveness - Amazon API Gateway"}],"answers":[{"id":"3bd10d778b04bfa32a7338a04e832d38","text":"The CloudFront distribution used by API Gateway is not deployed fully yet.","correct":false},{"id":"b03fe2e16e6904dc4b5fa34c91c1f855","text":"You are using HTTP rather than HTTPS.","correct":false},{"id":"43fb818a88f47b6b3b136abdd40a15a6","text":"There are not enough open inbound ports in your VPC.","correct":false},{"id":"4922e0fbbf140cd0dbb6458bbadbf268","text":"The data is being updated on DynamoDB at the exact same time you are trying to read it.","correct":false},{"id":"f13e59f5ef22ace9340e24f434eb09cb","text":"You are experiencing a cold start.","correct":true},{"id":"4c3254ba1ddb0fb7a3ce3824707a7105","text":"You have enabled caching on the API Gateway.","correct":true},{"id":"158838df2d14636ed1429fffbcc6825e","text":"Your DynamoDB RCUs are underprovisioned.","correct":true}]},{"id":"4c49e888-8f76-4b15-b267-7f6ec35579ca","domain":"awscsapro-domain5","question":"A client has asked you to review their system architecture in advance of a compliance audit.  Their production environment is setup in a single AWS account that can only be accessed through a monitored and audited bastion host. Their EC2 Linux instances currently use AWS-encrypted EBS volumes and the web server instances sit in a private subnet behind an ALB that terminates TLS using a certificate from ACM. All their web servers share a single Security Group, and their application and data layer servers similarly share one Security Group each. Their S3 objects are stored with SSE-S3.  The auditors will require all data to be encrypted at rest and will expect the system to secure against the possibility that TLS certificates might be stolen by would-be spoofers.  How would you help this client pass their audit in a cost effective way? ","explanation":"All the measures they have taken with Certificate Manager, S3 encryption and the EBS volumes meet the audit requirements.  There is no need for LUKS, CloudHSM or client-side encryption.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html","title":"Amazon EBS Encryption - Amazon Elastic Compute Cloud"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"Protecting Data Using Server-Side Encryption with Amazon S3-Managed  Encryption Keys (SSE-S3) - Amazon Simple Storage Service"}],"answers":[{"id":"bdddd09d0832e16504afd5c88136cf7e","text":"Leave the S3 objects alone.","correct":true},{"id":"ab9ad7bfd57b97954a6f861d872c6137","text":"Continue to use the ACM for the TLS certificate.","correct":true},{"id":"93113c2b6ca9be67acbd3561eef56481","text":"Reconfigure the EC2 EBS volumes to use LUKS OS-Level encryption.","correct":false},{"id":"92194f6603feb3f83a46b00bda37de5a","text":"Deploy CloudHSM and migrate the TLS keys to that service.","correct":false},{"id":"d9447af4853ab8736e49349138cac8fb","text":"Make no changes to the EBS volumes.","correct":true},{"id":"0c31fb48e65443ba5bfa312a7dcc117c","text":"Encrypt the S3 objects with OpenPGP locally before re-uploading them to S3.","correct":false}]},{"id":"663fbd6a-87bd-4fa6-a0ea-428ba2de5b51","domain":"awscsapro-domain5","question":"You manage a relatively complex landscape across multiple AZs.  You notice that the incoming requests vary mostly depending on the time of day but also there is a more unpredictable component resulting in smaller spikes and valleys for your resources.  Fortunately, you manage this landscape via OpsWorks Stacks.  What options, if any, are available to you as part of the OpsWorks featureset.","explanation":"OpsWorks Stacks offers three types of scaling: 24/7 for instances that remain on all the time; time-based for instances that can be scheduled for a certain time of day and on certain days of the week; and load-based scaling which will add instances based on metrics.  All this can be configured from within the OpsWorks Stack console.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/best-practices-autoscale.html","title":"Best Practices: Optimizing the Number of Application Servers - AWS OpsWorks"}],"answers":[{"id":"3622d494dceb973760a46dea038d1dc2","text":"If you need the ability to dynamically scale, you will need to use OpsWorks for Chef Automate.  OpsWorks Stacks does not support scaling.","correct":false},{"id":"b7ff5b06f51facca179494cb2bb00e55","text":"You can enabled CloudFormation Anticipated Scaling that uses past CloudWatch metrics and machine learning to automatically design a scaling policy optimized for the incoming request patterns.","correct":false},{"id":"216c997091da6e24174ad1b83d0be8b9","text":"You would define a baseline level of resources within the OpsWorks Stack Console to cover the average load.  But for the periodic load, that requires a scheduled auto-scaling policy.  Similarly, for the volatile spikes, you must use a stepped auto-scaling policy defined in an auto scaling group. ","correct":false},{"id":"75ab4de4ea42c1971b0ee09ae04ca591","text":"You would define a baseline level of resources and configure them for 24/7 instances.  Then you could define a time-based instances to cover certain times of day.  Finally, you could cover the volatile spikes with a load-based instances.  All this can be done within OpsWorks Stacks.","correct":true}]},{"id":"79f2f5be-b591-44e6-957b-eb0383640d7d","domain":"awscsapro-domain5","question":"You have a standard SQS queue to receive messages from the frontend application. The backend application is JAVA based and the AWS SDK is used to get the messages from the queue for processing. The SQS queue is not busy most of the time. According to the backend application logs, there is a high number of empty ReceiveMessageResponse instances returned. You want to adjust the settings to minimize the number of empty responses and reduce the cost. How would you implement this? ","explanation":"Amazon SQS long polling is preferable to short polling in most of the cases. Long polling requests let the consumers receive messages as soon as they arrive in the queue. It can help to reduce the number of empty responses. In order to enable long polling, the attribute ReceiveMessageWaitTimeSeconds should be more than 0. Short polling is incorrect. Visibility timeout and delivery delay do not address the problem of empty responses.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html","title":"Amazon SQS short and long polling"}],"answers":[{"id":"203fa6faf2e6bf53939b43300ec6dac2","text":"Increase the default visibility timeout of the queue to reduce the possibilities that the messages become visible to consumers again. The application can also use the ChangeMessageVisibility API to specify a suitable timeout value.","correct":false},{"id":"c3cbf51c591cb7fa17bd023ab814f95c","text":"Consume the messages in the SQS queue using long polling. Set the queue attribute ReceiveMessageWaitTimeSeconds to be more than 0. Amazon SQS will wait until there is an available message in a queue before sending a response.","correct":true},{"id":"220352e5b3779c1f2030cfd4b391b19e","text":"Modify AWS SDK to get the messages in the SQS queue by short polling. The ReceiveMessage call from the consumer sets the WaitTimeSeconds attribute to 0. As a result, the empty responses are eliminated.","correct":false},{"id":"a5cdcd2968c3566cbb7fc7bcd5fef01a","text":"Add a delivery delay in the SQS queue such as 1 minute. The delay helps to postpone the delivery of new messages to the queue for some time. When the JAVA application polls the messages from the queue, there will be a lower chance to get an empty response.","correct":false}]},{"id":"b401741c-5b37-4b47-8e61-7802fbc9d7d6","domain":"awscsapro-domain1","question":"You are helping a client consolidate several separate accounts into a single account.  This consolidation will result in approximately 50 new VPCs in their one account.  They want to continue to use Route 53 for DNS but only want it accessible privately. How can you accomplish this most efficiently?","explanation":"Private Hosted Zones provide DNS services to VPCs but cannot be access from the internet.  They can be associated with VPCs either by the console, CLI or programmatically via SDK.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html","title":"Associating More VPCs with a Private Hosted Zone - Amazon Route 53"}],"answers":[{"id":"cbffb64b6b43e6fe45496b6e77ce17b8","text":"Create a Private Hosted Zone within Route 53 for each respective VPC.  Configure replication between the private hosted zones to keep records in sync.","correct":false},{"id":"3cc28b12f45b3dee8f7f16a0f93d00ce","text":"Install BIND on an EC2 instance in a single VPC.  Create VPC peering connections between the DNS VPC and any new VPCs.  Configure a DHCP Option Set to assign a DNS and link that to each VPC.","correct":false},{"id":"82d5ef7e7176200aa4350ef90dd4c354","text":"Create a Public Hosted Zone within Route 53 and associate it to each VPC.  Configure a NACL on each VPC to deny inbound DNS queries (UDP port 53).","correct":false},{"id":"315372936e7ffba65896da15d0f45c2d","text":"Create a Private Hosted Zone within Route 53.  As the new VPCs are created, associate them with the Private Hosted Zone.","correct":true},{"id":"f74daa2300ac594c111ca9fce198f19c","text":"Create a central DNS server using EC2 and BIND.  Configure Route 53 to reference this DNS server as a resolver.  Update DNS records at the registrar to point to the central DNS.","correct":false}]},{"id":"4b00251a-a278-4d88-b715-955b4752a79a","domain":"awscsapro-domain2","question":"You'd like to create a more efficient process for your company employees to book a meeting room.  Which of the following is the most efficient path to enabling this improved business experience?","explanation":"With Alexa for Business, you can enlist Alexa-enabled devices to perform tasks for employees like retrieve information, start conference calls and book meeting rooms.","links":[{"url":"https://aws.amazon.com/blogs/business-productivity/announcing-room-booking-for-alexa-for-business/","title":"Announcing Room Booking for Alexa for Business | Business Productivity"}],"answers":[{"id":"5445641c568edaf760839d9f5bb7169c","text":"Configure an Alexa device with a custom skill backed by a Lambda function.  Use Amazon Lex to convert the audio sent to the Lambda function into an actionable skill.  ","correct":false},{"id":"a723a332c2ed052968becb9b6824e2a4","text":"Sign-up for AWS Alexa for Business. Create conference rooms in the console and place an Alexa device in each conference room.","correct":true},{"id":"12cb35ea295a9a96bfba94741cb4f0df","text":"Sign-up for Amazon Chime.  Create conference rooms in the console and place speakerphones in each conference room.","correct":false},{"id":"e43eb2c75536fac2d714b862f4fec490","text":"Invest in a voice-to-text API from the AWS Marketplace.  Create a custom Lambda function that calls the API and books a conference room.  Equip each conference room with Amazon Dash buttons and configure them to invoke the Lambda function.","correct":false}]},{"id":"91e4ebb5-18ac-45d6-867e-4c2eddefc075","domain":"awscsapro-domain4","question":"Your company has come under some hard times resulting in downsizing and cuts in operating budgets.  You have been asked to create a process that will increase expense awareness and your enhance your team's ability to contain costs.  Given the reduction in staff, any sort of manual analysis would not be popular so you need to leverage the AWS platform itself for automation.  What is the best design for this objective?","explanation":"AWS Budgets is specifically designed for creating awareness and transparency in your AWS spending rate and trends.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing Your Costs with Budgets - AWS Billing and Cost Management"}],"answers":[{"id":"8e9e7154ed9b1f92015c5fde5ca7a2a3","text":"Use Kinesis Streams to Ingest CloudTrail log streams.  Create a Lambda function to parse the log stream and insert a record into DynamoDB whenever a pay-per-use activity happens.  Use DynamoDB streams to alert the team when a threshold has been exceeded.","correct":false},{"id":"ce7a1fa4194d00b76a1491e66b2a10ec","text":"Use AWS Budgets to create a budget.  Choose to be notified when monthly costs are forecasted to exceed your updated monthly target.","correct":true},{"id":"aadb36f869250c141ae2eeec088d3bb3","text":"Implement a Cost Allocation Tagging strategy.  Create a periodic aggregation process in AWS Glue to read costs based on the tags.  Configure Glue to send the report to SNS where it can be either emailed or texted to the team depending on their own preference.","correct":false},{"id":"be1b52e799fe44ef89f78b167401c067","text":"Export AWS bill data into Redshift.  Use Quicksight to create reports per cost center.  Provide access for users to QuickSight to monitor their usage.","correct":false},{"id":"0e5c5be553400912c2517757d9aba6b2","text":"Provide access to Cost Explorer for your team for transparency.  Allow them to periodically review the accrued costs for the month and take the appropriate action.","correct":false}]},{"id":"4ebf4191-8562-4f67-945d-ea5aae2f9f26","domain":"awscsapro-domain3","question":"You are consulting for a client who is trying to define a comprehensive cloud migration roadmap.  They have a legacy custom ERP system written in RPG running on an AS400 system.  RPG programmers are becoming rare so support is an issue.  They run Lotus Notes email which has not been upgraded in years and thus out of support.  They do have a web application that serves as their CRM created several years ago by a consulting group.  It is a Java and JSP-based application running on Tomcat with MySQL as the data layer hosted on a Red Hat Linux server. The company is in a real growth cycle and realizes their current platforms cannot sustain them.  So, they are about to launch a project to implement SAP as a replacement for their legacy ERP system over the next year.  What migration strategy would you recommend for their landscape that would allow them to modernize as soon as possible?","explanation":"In this case, retiring Lotus Notes is the better move because it would just prolong the inevitable by simply migrating to EC2.  The CRM system is fairly new and can be re-platformed on Elastic Beanstalk.  Due to the impending ERP upgrade, it makes no sense to do anything with the legacy ERP.  It would take lots of work to port over an RPG application to run on AWS--if it's even possible.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud | AWS Cloud Enterprise  Strategy Blog"}],"answers":[{"id":"c7be9ce93a4a7a74b44e281cb697dcc4","text":"Begin a product search for a new CRM system that is cloud-ready.  Once identified, migrate the existing CRM into the new CRM system.  Migrate Lotus Notes to Workmail using the AWS Migration Hub.  Invest in training the IT staff about AWS through a Certified AWS Training Partner.  Provision and run the various SAP environments from scratch using EKS.  Do nothing to the legacy ERP until the SAP implementation is complete.","correct":false},{"id":"7579c185856bdbbcffbe49a649866488","text":"Rehost Lotus Notes mail on EC2 instances.  Refactor the CRM application to make use of Lambda and DynamoDB.  Use a third-party RPG to Java conversion tool to create Java versions of the legacy ERP to make it more supportable. Invest time in training developers continuous integration and continuous deployment concepts.  Because SAP implementations always take longer than estimated, rehost the legacy ERP system on EC2 instances so the AS400 can be retired.","correct":false},{"id":"20ef21af4ab4ecda70e90e90861b154c","text":"Retire the Lotus Notes email and implement AWS Workmail.  Replatform the CRM application Tomcat portion to Elastic Beanstalk and the data store to MySQL RDS.  Invest time in training Operations staff CloudFormation and spend time architecting the landscape for the new SAP platform.  Do nothing to the legacy ERP platform until the SAP implementation is complete.  ","correct":true},{"id":"480496ebe4100958e2f466291752ae2d","text":"Retire the CRM application and migrate the MySQL data over to Aurora.  Use QuickSight to provide access to the application for users.  Pay back support agreements to bring Lotus Notes back into support so it can be upgraded.  Migrate Notes email to EC2 instances.  Invest time in training Operations staff CloudFormation.  Create the complete SAP landscape as scriptable elements.  Do nothing to the legacy ERP platform until the SAP implementation is complete.","correct":false}]},{"id":"66d28221-31ce-4cf0-aca3-2b5a69535bb5","domain":"awscsapro-domain3","question":"You are consulting with a small Engineering firm that wants to move to a Bring-Your-Own-Device policy where employees are given some money to buy whatever computer they want (within certain standards).  Because of device management and security concerns, along with this policy is the need to create a virtualized desktop concept.  The only problem is that the specialized engineering applications used by the employees only run on Linux.  Considering current platform limitations, what is the best way to deliver a desktop-as-a-service for this client?","explanation":"AWS Workspaces added support for Linux desktops the middle of 2018.  BYOD scenarios work together well with a DaaS concept to provide security, manageability and cost-effectiveness.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/create-custom-bundle.html","title":"Create a Custom WorkSpaces Bundle - Amazon WorkSpaces"}],"answers":[{"id":"3480305b106307937ed81bba73d294ab","text":"Package the required apps as WAM packages.  When launching new Windows Workspaces, instruct users to allow WAM to auto-install the suite of applications prior to using the Workspace.","correct":false},{"id":"4018dbf7b4646b285f5ceaa7b49a5934","text":"Launch a Linux Workspace in AWS WorkSpaces and customized it with the required software.  Then, create a custom bundle from that image and use that bundle when you launch subsequent Workspaces.","correct":true},{"id":"e8a33593afbd82697d0ab168304265ed","text":"Launch an EC2 Linux instance and install XWindows and Gnome as the GUI.  Configure VNC to allow remote login via GUI and load the required software.  Create an AMI and use that to launch subsequent desktops.","correct":false},{"id":"bf3d66705af4677c9ade8605aa6bd89a","text":"Given current limitations, running Linux GUI applications remotely on AWS is not feasible.  They should reconsider their BYOD policy decision.","correct":false},{"id":"0cbf457eccae17779144d4e64e92a43e","text":"Launch a Windows Workspace and install VirtualBox along with a minimal Linux image.  Within that Linux image, install the required software.  Create an image of the Windows Workspace and create a custom bundle from that image.  Use that bundle when launching subsequent Workspaces.","correct":false}]},{"id":"6b6689f4-b150-482a-aa96-eab1674cb232","domain":"awscsapro-domain5","question":"Quality Auto Parts, Inc. has installed IoT sensors across all of their manufacturing lines. The devices send data to both AWS IoT Core and Amazon Kinesis Data Streams. Kinesis Data Streams triggers a Lambda function to format the data, and then forwards it to AWS IoT Analytics to perform monitoring and time-series analyses, and to take actions based on business processes. After an equipment failure on one of the manufacturing lines causes tens of thousands of dollars in revenue losses, it's determined that alarms for a specific piece of equipment where received seventy-five seconds after the issue originated, and that automated corrective action within a few seconds of the problem could have avoided the financial losses altogether. What changes should be made to the architecture to improve the latency of device alerts?","explanation":"AWS IoT Analytics is useful for understanding long-term device performance, performing business reporting, and identifying predictive fleet maintenance needs, but common latencies run from seconds to minutes. If you need to analyze IoT data in real-time for device monitoring, use Kinesis Data Analytics, which provides latencies in the millisecond to seconds range. A Lambda function can be used as the destination for Kinesis Data Analytics to perform corrective actions. IoT Core rules can write messages to a Kinesis stream, but not directly to Kinesis Data Analytics. Having a Lambda function perform anomaly detection will work, but will require more logic to be written for query setup and execution than using a specialized service like Kinesis Data Analytics. With Amazon CloudWatch Alarms, an alarm will watch a single metric over a period time, but will not provide the capabilities of SQL to detect complex anomaly conditions.","links":[{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/aws-reference-architecture-time-series-processing.pdf?did=wp_card&trk=wp_card","title":"Processing IoT Time Series Data on AWS"},{"url":"https://aws.amazon.com/iot-analytics/faq/","title":"AWS IoT Analytics FAQs"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/05/introducing-real-time-iot-device-monitoring-with-kinesis-data-analytics/","title":"Introducing Real-Time IoT Device Monitoring with Kinesis Data Analytics"}],"answers":[{"id":"71ade679994c0c1e6e55b3853194e4c5","text":"Add another AWS Lambda function as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Have the Lambda function write the anomalies to Amazon DynamoDB and perform device corrective action when needed.","correct":false},{"id":"522af3cd7d520d3e94e97c02d19c0672","text":"Create an AWS IoT Core rule to write the message to Amazon CloudWatch Alarms to detect anomalies in the data. Invoke another AWS Lambda function from CloudWatch Alarms to perform device corrective action when needed.","correct":false},{"id":"7e2eb8f3a96a390aab88e66000821c26","text":"Create an AWS IoT Core rule to write the message to Amazon Kinesis Data Analytics to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":false},{"id":"fe8ff20697982ca413f81ea14472e603","text":"Add Amazon Kinesis Data Analytics as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":true}]},{"id":"f3fef147-7b9d-45b6-8b2b-d943c90e8920","domain":"awscsapro-domain5","question":"You are assisting a company in the migration of their container-based web landscape over to Amazon.  They have a total of 21 containers which comprise their DEV, QA and Production environments.  All environment are identical in design and size.  Each environment consists of 3 web servers, 3 app servers and 1 datastore server.  Given the landscape, which of the provided options would be best for them to minimize maintenance?","explanation":"Deploying containers via ECS is a good option but we would want to use the EC2 hosted path.  Fargate is generally used for transient workloads and our datastore would be something we'd want to persist.  We might be able to deploy the data store with RDS, but the question does not make it clear if the data store is an RDS-supported database.  It could be a NoSQL data store or some other database unsupported by RDS.  Similarly, a MEAN stack under Elastic Beanstalk might not be compatible with our landscape either.","links":[{"url":"https://aws.amazon.com/ecs/resources/","title":"Resources for Amazon ECS - run containers in production"}],"answers":[{"id":"c1354e6d48fedccbf7b4e9c18854d980","text":"Redeploy the web landscape on a MEAN stack under Elastic Beanstalk, making use of auto-scaling groups to right-size the respective environments.  ","correct":false},{"id":"e0ea997f77cb156d35ec716cf772c49c","text":"Deploy the web, app and database containers using ECS.  Make use of Fargate for the underlying ECS infrastructure.","correct":false},{"id":"f05469f4c7578263f4271e7514c338ef","text":"Deploy the web and app servers in each environment using ECS.  Provision an RDS instance for each environment.  Use AWS Systems Manager to provide a common management console.","correct":false},{"id":"619957021a43a829fbb6228467323ca1","text":"Deploy the web, app and database servers using ECS on EC2.  Purchase 1-year reserved instance contracts for the required EC2 instances.","correct":true}]},{"id":"02a9611c-591c-4280-bb83-6c65c7c4921f","domain":"awscsapro-domain5","question":"A sporting goods retailer runs WordPress on Amazon EC2 Linux instances to host their customer-facing website. An ELB Application Load Balancer sits in front of the EC2 instances in Auto Scaling Groups in two different Availability Zones of a single AWS region. The load balancer serves as an origin for Amazon CloudFront. Amazon Aurora provides the database for WordPress with the master instance in one of the Availability Zones and a read replica in the other. Many custom and downloaded WordPress plugins have been installed. Much of the DevOps teams' time is spent manually updating plugins across the EC2 instances in the two Availability Zones. The website suffers from poor performance between the Thanksgiving and Christmas holidays due to a high occurrence of product catalog lookups. What should be done to increase ongoing operational efficiency and performance during high-volume periods?","explanation":"ElastiCache Memcached will provide in-memory access speeds for the catalog read transactions. A WordPress plugin is required to leverage caching. WordPress can access an EFS Mount Target for file sharing across all instances. Aurora offers a MySQL option, and WordPress requires MySQL, so the solution would have been set up that way already. CodeDeploy could update plugins on all instances, and will work well for the custom in-house code, but triggering the updates of downloaded plugins will need to be orchestrated. Aurora Auto Scaling will distribute catalog reads across multiple replicas for increased performance, but not to the extent of in-memory caching. Elastic File System is a managed service providing operational advantages over NFS file shares. ElastiCache Redis will provide the in-memory read performance desired, but changing the wp-config.php file won't provide access to it, as a plugin is needed for that. WordPress does work with S3, but a shared file system is easier to implement.","links":[{"url":"https://aws.amazon.com/getting-started/projects/build-wordpress-website/","title":"Build a WordPress Website"},{"url":"https://github.com/aws-samples/aws-refarch-wordpress?did=wp_card&trk=wp_card","title":"Hosting WordPress on AWS"}],"answers":[{"id":"f060160b20c4408b2442010d3ea4d387","text":"Use Amazon ElastiCache Redis as a caching layer between the EC2 instances and the database. Change wp-config.php to point to the Redis caching layer, and have Redis point to Aurora. Move the WordPress files to S3 and have WordPress access them there.","correct":false},{"id":"17b12e57cd85610e888cda82b5a8a145","text":"Migrate the WordPress database to RDS MySQL since MySQL is WordPress's native database and WordPress is performance optimized for MySQL. Implement AWS CodeDeploy to update WordPress plugins on all EC2 instances.","correct":false},{"id":"aeb27370afc3b672eb0a1afcb28e9176","text":"Implement Aurora Auto Scaling to increase the number of replicas automatically as demand increases. Create an NFS file share to hold the WordPress files. Access the file share from the EC2 instances in both Availability Zones.","correct":false},{"id":"bc643d3342a5a675e65e5baed00e88b9","text":"Deploy Amazon ElastiCache Memcached as a caching layer between the EC2 instances and the database. Install a WordPress plugin to read from Memcached. Implement Amazon Elastic File System to store the WordPress files and create mount targets in each EC2 subnet.","correct":true}]},{"id":"338ce579-a236-4282-9670-8da3b0baf2e9","domain":"awscsapro-domain3","question":"You are helping a Retail client migrate some of their assets over to AWS.  Presently, they are in the process of moving their Enterprise Data Warehouse.  They are planning to re-host their very large Oracle data warehouse on EC2 in a high availability configuration across AZs.  They presently have several Scala scripts that process some detailed Point of Sale data that is collected each day.  The scripts perform some aggregation on the data and import the aggregate into their Oracle database.  They want to move this process to AWS as well.  Which option would be the most cost-effective way for them to do this?","explanation":"AWS Glue is a fully managed extract, translate and loading service and is compatible with Scala.  EMR could do this but represents more overhead than necessary.  Lambda is not compatible with Scala and migrating to Redshift does not bring anything in this case if the customer wants to retain their Oracle database.","links":[{"url":"https://aws.amazon.com/glue/faqs/","title":"AWS Glue Features - Amazon Web Services"}],"answers":[{"id":"8b01d948d5ad2f4b1c8e817c2d98e7c2","text":"Migrate the processing to AWS Glue.","correct":true},{"id":"4f1ff8b853c3ba363bdd2bda53538ab4","text":"Migrate from Oracle to Redshift and use Kinesis Firehose.","correct":false},{"id":"1a4de6676c8c078310e08aad71d9dce6","text":"Migrate the processing to AWS EMR.","correct":false},{"id":"04578ae8419780f9dc441d01fe11582d","text":"Create Lambda functions using your Scala scripts.","correct":false},{"id":"a445a1a877009cd7c31858687a818116","text":"Import your Scala scripts into AWS SCT for processing.","correct":false}]},{"id":"e24f7f76-6908-4ad9-820a-11790bfdcec6","domain":"awscsapro-domain3","question":"You are helping a client migrate over an internal application from on-prem to AWS.  The application landscape on AWS will consist of a fleet of EC2 instances behind an Application Load Balancer.  The application client is an in-house custom application that communicates to the server via HTTPS and is used by around 40,000 users globally across several business units.  The same exact application and landscape will be deployed in US-WEST-2 as well as EU-CENTRAL-1.  Route 53 will then be used to redirect users to the closest region.  When the application was originally built, they chose to use a self-signed 2048-bit RSA X.509 certificate (SSL/TLS server certificate) and embedded the self-signed certificate information into the in-house custom client application.  Regarding the SSL certificate, which activities are both feasible and minimize extra administrative work?","explanation":"You can import private certificates into Certificate Manager and assign them to all the same resources you can with generated certificates, including an ALB.  Also note that Certificate Manager is a regional service so certificates must be imported in each region where they will be used.  The other options in this question would either require you to update the certificate on the client or requires unnecessary steps to resolve the challenge.","links":[{"url":"https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html","title":"Importing Certificates into AWS Certificate Manager - AWS Certificate  Manager"}],"answers":[{"id":"28694bd7f280694a43741563f6933ad6","text":"Create a new public SSL/TLS certificate using Certificate Manager and configure the common name and OU to match the existing certificate.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false},{"id":"111997579381183b07a22fad8574e76c","text":"Create a new Certificate Authority within Certificate Manager and import the existing certificate.  Generate a new certificate, CA chain and private key and push an update for the application.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false},{"id":"0c4320d1dd787a5bae2b43479b645d94","text":"Use Service Catalog to push an update of the in-house app which includes an updated certificate and CA chain.  Generate a new private certificate using OpenSSL. Import the new certificate to Certificate Manager in US-EAST-1.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false},{"id":"0663551d15f5b15af587ac8bf75a2566","text":"Purchase a new public SSL/TLS certificate from a third-party CA.  Upload the certificate to Certificate Manager and assign that certificate to the Application Load Balancers.","correct":false},{"id":"d4976a6c33ee189e6b681dffc83cbac5","text":"Import the existing certificate and private key into Certificate Manager in both regions.  Assign that imported certificate to the Application Load Balancers using their respective regionally imported certificate.","correct":true}]},{"id":"254eeafd-1183-4117-936e-f6f7ffa9d88a","domain":"awscsapro-domain2","question":"A client is having a challenge with performance of a custom data collection application.  The application collects data from machines on their factory floor at up to 1000 records per second.  It uses a Python script to collect data from the machines and write records to a DynamoDB table.  Unfortunately, under times of peak data generation, which only last 1-2 minutes at a time, the Python application has timeouts when trying to write to DynamoDB.  They don't do any analytics on the data but only have to keep it for potential warranty issues.  They are willing to re-architect the whole solution if it will mean a more reliable process.  Which of the following options would you recommend to give them the most scalable and cost-efficient solution?","explanation":"The application is likely running into throttling when writing to DynamoDB.  Kinesis Firehose makes for a good option in this case to accommodate streaming records.  Since we do not have to perform any analytics, we can simply store it on S3 using Firehose.","links":[{"url":"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html","title":"What Is Amazon Kinesis Data Firehose? - Amazon Kinesis Data Firehose"}],"answers":[{"id":"b30cd1d2aae071493c960798a422ade4","text":"Change the application design to use SWF to take in the data.  Use Amazon Elasticache in front of the DynamoDB database as a buffer to throttle the writes.","correct":false},{"id":"61f03e55a88efffb8060d5d3e07f247d","text":"Turn on DynamoDB Auto Scaling and configure appropriate upper and lower limits.","correct":false},{"id":"80999a665ea6597ad65e91f9d4a84b3f","text":"Change the application design to use Kinesis Streams to take in the data.  Provision at least 5 shards to ensure enough peak capacity.  Configure the Kinesis Streams to load the data into DynamoDB.  Increase the RCU and WCU for the DynamoDB table to match peak needs.","correct":false},{"id":"7e2676bbdbc658bd6a68274495e2376f","text":"Change the application design to use Kinesis to take in the data.  Use Kinesis Firehose to spool the data files out to S3.  Use S3 Lifecycle to transition the files to Glacier after a few days.","correct":true},{"id":"6ac1d0de8c41824a325523f050a3784e","text":"Change the application design to write the data records to EMR.  Use a Pig script to transfer the data from EMR to DynamoDB periodically.","correct":false},{"id":"0bd59f25e091570f7b171c3dd76b0206","text":"Change the application design to use SQS and a custom process on an EC2 spot fleet to throttle inbound messages into DynamoDB.","correct":false}]},{"id":"9624e171-081f-43e5-a74c-b4b792676b63","domain":"awscsapro-domain2","question":"A connected home solutions company is creating a central console to manage smart home environments. Some of the capabilities needed include capturing device telemetry data, making that data available to query from client applications, alerting on undesirable device situations, and providing device analytics. The company has decided to run the console's application backend on AWS. Devices will communicate with the console via MQTT protocol. Which architecture will provide the best scalability and the highest operational efficiency for the smart home console?","explanation":"AWS IoT Core provides all the capability needed to create a smart home console. IoT rules can be configured to send device messages to DynamoDB for client application querying, and to send device messages to IoT Events for alerting and notification. IoT Analytics is fully integrated with IoT Core to receive messages from connected devices as they stream in. These are all managed services which maximize operational efficiency. AWS IoT Greengrass can perform all the required functions, but must run on a local device, which may not be desirable to customers since another piece of hardware would need to be acquired. Amazon Athena cannot query from DynamoDB directly. IoT Events and IoT Analytics are IoT-centric services which will be better solutions than CloudWatch Events and Amazon Redshift.","links":[{"url":"https://aws.amazon.com/iot-core/","title":"AWS IoT Core"},{"url":"https://docs.aws.amazon.com/iotevents/latest/developerguide/what-is-iotevents.html","title":"What is AWS IoT Events?"},{"url":"https://docs.aws.amazon.com/iotanalytics/latest/userguide/welcome.html","title":"What Is AWS IoT Analytics"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/connected_home_telemetry_ra.pdf?did=wp_card&trk=wp_card","title":"Measuring and collecting data from smart home devices"}],"answers":[{"id":"678fa8ab5656d7f8a5fd360c0cbb3586","text":"Implement AWS IoT Core to receive MQTT messages from the devices. Create an IoT rule to store the device messages in Amazon DynamoDB. Have client applications use Amazon Athena to query DynamoDB. Create a second IoT rule in IoT Core to forward telemetry information to AWS IoT Events. Publish to Amazon Simple Notification Service topics when device issues are identified by IoT Events. Have AWS IoT Analytics receive device messages from IoT Core for visualization with Amazon QuickSight.","correct":false},{"id":"d70e3ad71c36d95078725805fcfaebf8","text":"Implement AWS IoT Greengrass to receive MQTT messages from the devices. Create Lambda functions on IoT Greengrass to send device information to Amazon DynamoDB and Amazon CloudWatch Events. Have client applications use Amazon Athena to query DynamoDB. Publish to Amazon Simple Notification Service topics when device issues are identified by CloudWatch Events. Use an AWS Glue job to aggregate the device information in DynamoDB and write summaries to Amazon Redshift. Create visualizations of the Redshift summaries in Amazon QuickSight.","correct":false},{"id":"518de0ee58995da859fef3f8b0108750","text":"Implement AWS IoT Greengrass to receive MQTT messages from the devices. Create Lambda functions on IoT Greengrass to send device information to Amazon DynamoDB, Amazon CloudWatch Events, and AWS IoT Analytics. Make an Amazon API Gateway API available to client applications. Have API Gateway invoke a Lambda function to query DynamoDB. Publish to Amazon Simple Notification Service topics when device issues are identified by CloudWatch Events. Send IoT Analytics summaries to Amazon QuickSight for visualization.","correct":false},{"id":"1e93269720ca34bcc18e1d67c5ccee18","text":"Implement AWS IoT Core to receive MQTT messages from the devices. Create an IoT rule to store the device messages in Amazon DynamoDB. Make an Amazon API Gateway API available to client applications. Have API Gateway invoke an AWS Lambda function to query DynamoDB. Create a second IoT rule in IoT Core to forward telemetry information to AWS IoT Events. Publish to Amazon Simple Notification Service topics when device issues are identified by IoT Events. Have AWS IoT Analytics receive device messages from IoT Core for visualization with Amazon QuickSight.","correct":true}]},{"id":"58fa6440-b521-4dff-b6be-b1f8818c718d","domain":"awscsapro-domain2","question":"You currently manage a website that consists of two web servers behind an Application Load Balancer.  You currently use Route 53 as a DNS service.  Going with the current trend of websites doing away with the need to enter \"www\" in front of the domain, you want to allow your users to simply enter your domain name. What is required to allow this?","explanation":"Route 53 allows you to create a record for a zone apex.  In this case, we have created an alias record for the ALB.","links":[{"url":"https://docs.aws.amazon.com/govcloud-us/latest/ug-west/setting-up-route53-zoneapex-elb.html","title":"Setting Up Amazon Route 53 Zone Apex Support with an AWS GovCloud (US-West)  Elastic Load Balancing Load Balancer - AWS GovCloud (US-West) User Guide"}],"answers":[{"id":"9e87834a987d5b3556ce3e9f35ac6a82","text":"Create an A record for your top-level domain name as an alias for the ALB.","correct":true},{"id":"f70aff68786523bc13fd44cb89feb502","text":"Create an A record for your top-level domain name using the public IP of your ALB.","correct":false},{"id":"0dc522eb50b1c1320900ac02fcfb4dd6","text":"Create an S3 bucket as a static web host.  Create simple HTML file that redirects to the www subdomain.  Use CloudFront custom origins as a front-end for your top-level domain name.","correct":false},{"id":"5c48fe59299d582ae6074d269711de20","text":"Route 53 does not currently support zone apex records.  You must use a third-party DNS provider.","correct":false},{"id":"935c6d54a7555f482da11c1ded5dfc9c","text":"Create a CNAME record for the root domain and configure it to resolve to www subdomain name.","correct":false}]},{"id":"7e090e30-00fe-4ea9-85e7-502b055a9537","domain":"awscsapro-domain2","question":"You have configured your VPC CIDR as 10.0.0.128/25.  What IP address would you expect is assigned to the DNS server?","explanation":"The DNS is a reserved address in the VPC CIDR and will be the second usable address.  In this example,  .128 is the network address, .129 is reserved for the router, .130 is reserved for the  DNS.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html","title":"VPCs and Subnets - Amazon Virtual Private Cloud"}],"answers":[{"id":"38fbcb4c4667561b2f5a0fca9574d615","text":"/25 is an invalid netmask for VPCs.","correct":false},{"id":"2d13502142d78cd771686cc0ba0ebdd8","text":"10.0.0.255","correct":false},{"id":"f3677d1e4c9be060a85b0f539eabfb97","text":"10.0.0.130","correct":true},{"id":"cb09135ab9a635aa9719248fe6e3634d","text":"10.0.0.128","correct":false},{"id":"98a2cd7efa1ba950d42e42af495c4248","text":"10.0.0.2","correct":false}]},{"id":"0ee4566a-508e-472d-9789-3318e3284aca","domain":"awscsapro-domain5","question":"You are an AWS Solutions Architect and you maintain a CloudFormation stack that includes the resources of a network load balancer and an Auto Scaling group. The ASG has one running instance. A developer uses the instance for feature development and testing. However, after he adds some configurations and restarts an application process, the instance is terminated by the Auto Scaling group and a new instance is created. The new configurations are lost in the new instance. You need to modify the resource settings to make sure that the instance is not terminated by the ASG when application processes are restarted. Which of the following methods would best achieve this?","explanation":"The instance fails the health check in the ELB target group and is then terminated by ASG whenever the application processes are restarted. The prevent the ASG from terminating the EC2 instance you need to modify the health check type from ELB to EC2. As a result, even if the instance fails the health check in the ELB target group, it will not be terminated by the Auto Scaling group. You do not need to create an AMI or a new launch configuration to address the issue. And the custom health check script that runs every minute cannot prevent the instance from being terminated.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health checks for Auto Scaling instances"}],"answers":[{"id":"745390da2b4433786bf4cdf17df3d2d3","text":"Edit a health check shell script that performs some sanity checks in the EC2 instance. If the sanity checks pass, the shell script uses AWS CLI â€œaws autoscaling set-instance-healthâ€ to set its status to be healthy. Run the script every minute.","correct":false},{"id":"c66bf36d77a2aa4cf5ef75a4ac494df8","text":"Store all custom configuration scripts in an S3 bucket and create a new launch configuration. In its user data section, download the scripts from the S3 bucket and execute them. Whenever a new instance is launched, the configurations can be installed automatically. ","correct":false},{"id":"1584209c598c68a956227b3770a97fb2","text":"Create an AMI and configure a new launch configuration with the AMI. Then modify the Auto Scaling group to use the new launch configuration and launch a new instance.","correct":false},{"id":"8fad414c494de200ee3990b334d22b13","text":"Update the CloudFormation script and modify the health check type from ELB to EC2.","correct":true}]},{"id":"a7c939f1-277e-469f-a209-9b290e8136c9","domain":"awscsapro-domain5","question":"Your company has contracted with a third-party Security Consulting company to perform some risk assessments on existing AWS resources.  As part of a routine list of activities, they inform you that they will be launching a simulated attack on one of your EC2 instances.  After the Security Group performed all their activities, they issue their report.  In their report, they claim that they were successful at taking the EC2 instance offline because it stopped responding soon after the simulated attack began.  However, you're quite certain that machine did not go offline and have the logs prove it.  What might explain the Security company's experience?","explanation":"AWS Shield and other counter-measure technologies work to protect all AWS customers from DDoS attacks.  Unless AWS was aware of the test time and expected duration, its likely the traffic was blocked as suspicious.  AWS Firewall Manager is used to manage WAF ACLs and not dynamically blacklist IPs.  Similarly, VPC Flow Logs cannot automatically implement NACL changes as described here. Despite being a permitted service, traffic suspected of being malicious will still be blocked","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/penetration-testing/","title":"Submit a Penetration Testing Request"}],"answers":[{"id":"f3f963b71e307f8c28109631df115418","text":"The EC2 instance is using an ENI and the Security Company temporarily exceeded the throughput limit resulting in a throttling of their connection.","correct":false},{"id":"82a05cb45adab6d248655e827de16c6f","text":"AWS Firewall Manager is dynamically adding a blacklist entry for the Security Company's testing machine because it sees the traffic as a threat.","correct":false},{"id":"7d4826f179b8dc854c9cfb6e43678373","text":"The VPC Flow Logs record the spike in suspicious traffic and implement an update to the inbound NACL to block the remote IP address.","correct":false},{"id":"e3facacbe52b6423f9cf2e700d8e0b81","text":"The Security Company's traffic was seen as a threat and blocked dynamically by AWS.  AWS must grant permission before any penetration testing is done.","correct":true}]},{"id":"d8b88385-e15f-4313-bc53-e9fb82f89cc3","domain":"awscsapro-domain2","question":"You are developing an application to be hosted on EC2. The application uses some environmental configuration data and other necessary parameters when running. For example, the application needs to get the correct username and password in order to communicate with a RDS database. You want to find a free AWS service to store these parameters. To meet security requirements, these stored parameters must be encrypted by the AWS Key Management Service. Which of the following methods is the best?","explanation":"You can manage parameters in Systems Manager Parameter Store. The type of the parameters must be SecureString so that they are encrypted by KMS. Parameter Store has standard tier and advanced tier. In this scenario, standard tier is enough and advanced tier is not a free service. AWS Secrets Manager does not have the concept of standard or secure parameter. It also charges you per secret per month.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html","title":"How AWS Systems Manager Parameter Store uses AWS KMS"}],"answers":[{"id":"c08527f2eeacd9f56133d1dfed701f1a","text":"Create secure parameters in AWS Secrets Manager. Secrets Manager protects secrets by integrating with KMS and all stored parameters are automatically encrypted by the AWS managed key \"aws/secretsmanager\". You can also configure Secrets Manager to rotate the secrets.","correct":false},{"id":"3295f034025fe6d21155d228ee3dd0a2","text":"Store secure string parameters in AWS Systems Manager Parameter Store so that the parameters are encrypted by KMS. Use the standard tier as there is no additional charge for it. Use AWS Encryption SDK in the application to get the parameters.","correct":true},{"id":"4f357d94d3f20ee0d0c3c7c892ef0d70","text":"Create standard parameters in AWS Secrets Manager. Use the advanced tier as it uses envelope encryption to encrypt the stored parameters with KMS. You can also configure Secrets Manager to rotate the stored secrets or API keys automatically.","correct":false},{"id":"41e6f55938cd5c32278c6f4b6708641e","text":"Create standard string parameters in AWS Systems Manager Parameter Store as it is a free service. The parameters are automatically encrypted with envelope encryption by the default AWS managed key (aws/ssm) in KMS. Use AWS Encryption SDK in the application to fetch the parameters.","correct":false}]},{"id":"2afb0db9-a43f-4e97-8272-5ce423ded162","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They notice on their RDS console that one of their mission-critical production databases has an \"Available\" listed under the Maintenance column.  They are extremely concerned that any sort of updates to the database will negatively impact their DB-intensive mission-critical application.  They at least want to review the update before it gets applied, but they are not sure when they will get around to that.  What do you suggest they do?","explanation":"For RDS, certain OS updates are marked as Required. If you defer a required update, you receive a notice from Amazon RDS indicating when the update will be performed. Other updates are marked as Available, and these you can defer indefinitely.  You can also apply the maintenance items immediately or schedule the maintenance for your next maintenance window.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html","title":"Maintaining a DB Instance - Amazon Relational Database Service"}],"answers":[{"id":"b9f81cfac73d5c4d2871a7f969ecb9f3","text":"Disable the Maintenance Window so the updates will not be applied.","correct":false},{"id":"93df9eb71cb48a0c821fe555e35f5b62","text":"Defer the updates indefinitely until they are comfortable.","correct":true},{"id":"35c3b3d1cb8d9866e1abc96dec8bfa3f","text":"Apply the maintenance items immediately.  AWS validates each update with each customer's RDS instances using a shadow image so there is little risk here.","correct":false},{"id":"66ed278812c2a52913954afa52952b97","text":"The maintenance will be automatically performed during the next maintenance window.  They have no choice in the matter.","correct":false},{"id":"6aeada3a9046319bc858239b15031f66","text":"Backup the database immediate because the updates could come at any time.  If possible, create a Read Replica to act as a standby in case problems are introduced with the update.","correct":false}]},{"id":"4e6b1423-41e3-4d39-93b1-c5c47705477b","domain":"awscsapro-domain2","question":"Given an IP CIDR block of 56.23.0.0/24 assigned to a VPC and the single subnet within that VPC for that whole range, how many usable addresses will you have?","explanation":"For VPCs and subnets, you can use IP addresses that are in RFC1918 or not.  If you choose addresses not in the RFC1918 ranges, you will not be able to route traffic to the internet directly with those addresses.  You would have to use a NAT.  For a /24 netmask, you can expect 251 usable addresses because of the 5 reserved addresses.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html","title":"IP Addressing in Your VPC - Amazon Virtual Private Cloud"}],"answers":[{"id":"f7efa4f864ae9b88d43527f4b14f750f","text":"4096","correct":false},{"id":"19f3cd308f1455b3fa09a282e0d496f4","text":"251","correct":true},{"id":"c52f1bd66cc19d05628bd8bf27af3ad6","text":"254","correct":false},{"id":"6aa49d907a3637314f53838d83286e5d","text":"Zero.  You must use a private IP range as defined in RFC1918 for VPCs.","correct":false},{"id":"3873a2be62d17c29ac441293ab21e143","text":"You cannot assign the entire CIDR range of a VPC to a single subnet.","correct":false}]},{"id":"b06ef2a9-b122-4b47-b5be-b6d604e78405","domain":"awscsapro-domain2","question":"You are working with a customer to implement some better security policies.  They have a group of remote employees working on a confidential project that uses some proprietary Windows software and stores data in S3.  The Chief Information Security Officer is concerned about the threat of the desktop software or confidential data being smuggled out to a competitor.  What architecture would you recommend to best address this concern? ","explanation":"Using a locked down virtual desktop concept would be the best way to manage this.  AWS WorkSpaces provides this complete with client software to log into the desktops.  These Workspaces can be walled off from the Internet.  Using policies, you could allow access from only those in the Workspaces VPC.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html","title":"What Is Amazon WorkSpaces? - Amazon WorkSpaces"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html","title":"Endpoints for Amazon S3 - Amazon Virtual Private Cloud"}],"answers":[{"id":"b5b5a93dca37986129c444b7654d600f","text":"Provision Amazon Workspaces in a secured private VPC.  Do not enable Internet access for the Workspaces.  Create a VPC Gateway Endpoint to S3 and implement an endpoint policy that explicitly allows access to the required bucket.  Assign an S3 bucket policy that denies access unless the sourceVpce matches the VPC endpoint.  Supply the users with instructions on downloading and login into the Workspaces instances.","correct":true},{"id":"36e6b9fb8ba6dd4b59c9f032ba6f78f7","text":"Provision Windows 2016 instances in a private subnet.  Create a specific security group for the Windows machines permitting only SSH inbound.  Create a NACL which allows traffic to S3 services and explicitly deny all other network traffic to and from the subnet.  Assign an S3 bucket policy that only allows access for members of the Windows machine security group.","correct":false},{"id":"21d0f58369770fb84d6bff8bfcf9c265","text":"Use Service Catalog to deploy and manage the proprietary Windows software to the remote employees.  Create an OpenVPN server instances within a VPC.  Create an VPC Interface Endpoint to S3 and use a security group to only permit traffic from the OpenVPN server security group.  Supply the remote employees with instructions to install and login using OpenVPN client software.","correct":false},{"id":"f8c297565cec662fd215e6c551daca36","text":"Create a bucket policy using the sourceIP condition to only allow access from a specific VPC CIDR.  Apply a NACL which only permits inbound port 22 and outbound ephemeral ports.  Deploy Amazon Workspaces in the VPC and disable internet access.  Supply the users with instructions on downloading and login into the Workspaces instances.","correct":false}]},{"id":"3e23b8a9-ce64-430f-936f-9a318a85da75","domain":"awscsapro-domain4","question":"To be sure costs of AWS resources are allocated to the proper budgets, you are trying to come up with a way to allocate the AWS bill to the proper cost centers.  Which of the following would be most effective for your organization?","explanation":"Using Service Catalog is a good way to automatically enforce and apply a tagging strategy and it requires no special effort from the product consumers.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/tagoptions-launching.html","title":"Launching a Product with TagOptions - AWS Service Catalog"}],"answers":[{"id":"683fce4e1eed553dd4cfa34806ffcbc6","text":"Use API Gateway to create a proxy for the API of the resources your users will deploy.  Insert some custom logic using VTL to automatically append a cost center tag to the request based on the cost center of the IAM user making the request.","correct":false},{"id":"e2f5a49c87b9eac8673cfe95968f124d","text":"Use AWS Batch to periodically run a custom Lambda function that scans all resources and deletes any without proper tagging for cost center.","correct":false},{"id":"874d7ac46d1adeb7650db0db8dc9d2c1","text":"Use an SCP at the organizational level to require a cost center tag be applied to every resource.  Activate the cost center tag in the Billing Console and allocate costs based on that.","correct":false},{"id":"37c28a1741830444dec6469aa71f605c","text":"Deploy products within AWS Service Catalog and only allow users to deploy resources using the catalog.  Use TagOptions to provide the users a list from which they can select their cost center.  Activate the cost center tag in the Billing Console.","correct":true},{"id":"f065124beaeb3f8cde1976516f7fb97a","text":"Make use of AWS Artifact to analyse the spending pattern over the month and identify the IAM users responsible for the most costs.  Cross-reference that with the cost centers to which IAM users belong.","correct":false}]},{"id":"2c034786-9b7e-4933-aad2-d0c4b1d89ca8","domain":"awscsapro-domain2","question":"A beach apparel company has begun an initiative to improve their sales analytics capabilities using AWS services. They'll need to be able to visualize summary sales data by product line, territory, and sales channel for each day, month, and year, and they'll need to be able to drill-down with ad-hoc queries on individual sales records. There are multiple data sources that provide transactional information in different formats. The company has chosen Amazon QuickSight as their visualization tool for the summary information. Visualizations and drill-down queries will require three years of rolling sales history, which estimates to seven petabytes of data. Which architecture will provide the best performance and cost efficiency?","explanation":"Using S3 to store the detailed sales transaction data and using Lambda to standardize data formats is the most cost effective option. Storing the summary data in Redshift provides a high performance option for reads from QuickSight, and keeping the detailed transaction data out of Redshift allows for smaller node sizes and lower cost. Amazon Redshift Spectrum can be used for drill-down queries that join tables from both Redshift and S3. For answer number two, Redshift will be a better option than Aurora for OLAP query performance due to it's columnar organization. Answer number four provides no simple way to perform ad-hoc drill down queries.","links":[{"url":"https://aws.amazon.com/redshift/","title":"Amazon Redshift"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"4740b70569f15040edf0916e47386757","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Streams and write them to Amazon Elastic Block Store on EC2 instances in Auto Scaling Groups. Perform data format standardization and summary aggregation on EC2, and write the summary results to Amazon Redshift tables","correct":false},{"id":"ebf4a6c248510d05a046c8f0ea4298b7","text":"Use Amazon Kinesis Data Analytics to format the data source transactions in a standard way and load it into Amazon Aurora. Invoke Lambda functions to aggregate the data and write it into summary tables in Aurora","correct":false},{"id":"b3deac265c2195cb988c345d096800fd","text":"Ingest individual sales transactions from each data source into Amazon S3 with Amazon Kineses Data Firehose. Trigger an AWS Lambda function to format the transaction data in a standard way and redeposit the results in S3. Run AWS Glue jobs to aggregate the summary data into Amazon Redshift","correct":true},{"id":"a08e38f138d23c0f1759ab2d1801f67e","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Firehose and load them into Amazon Redshift. Run AWS Glue jobs to format the transaction data in a standard way and perform aggregate functions to write the data into summary tables in Redshift","correct":false}]},{"id":"230f422f-7118-4096-8dce-59c642fb55c8","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a new Direct Connect connection.  The connection is up and you can ping the AWS peer IP address, but the BGP peering session cannot be established.  What should be your next logical troubleshooting steps?","explanation":"Because the connection is up and we can ping the AWS peer, the problem must be at a higher level on the OSI model than the Physical or Data layers.  BGP uses TCP port 179 to communicate routes so we should check that no NACL or SG is blocking it.  Additionally, we should make sure the ASNs are properly configured in the proper ranges.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/Troubleshooting.html","title":"Troubleshooting AWS Direct Connect - AWS Direct Connect"}],"answers":[{"id":"3d2a55832b90f19a2137e8715525d717","text":"Make sure no firewalls or ACLs are blocking TCP port 179 or any high-numbered ephemeral ports.","correct":true},{"id":"8fc27418eee2ce07b64bc672007d2c1b","text":"Contact the co-location provider and request a written report for the Tx/Rx optical signal across the cross connect.","correct":false},{"id":"81977d7a1eb5714746851077b93f44d6","text":"Power cycle all the equipment to clear ARP table cache.","correct":false},{"id":"45d4c1753395277878b9a17343628c52","text":"Ensure that the VLAN is configured properly between your on-prem router the provider. ","correct":false},{"id":"edd3f9408cecbbf9182678ccc51d7981","text":"Ask your network provider to provide you with a cross connect completion notice and compare the ports with those listed on your LOA-CFA","correct":false},{"id":"16e5aea88df69cc18f99e3f066ec99c1","text":"Ensure that the local ASNs and AWS-side ASNs are properly configured.","correct":true}]},{"id":"42a413d2-b7c0-4f63-ab1c-37b8ec9b724a","domain":"awscsapro-domain2","question":"You have been contracted by a small start-up to help them get ready for their new product release--a web-based application that lets users browse through detailed photographs of the world's most famous paintings.  The company is expecting a huge debut with very heavy traffic so the solution should be robust and scalable with the least amount of hands-on management.  A key feature of their app is that they have created separate web sites specifically optimized for three different form factors: mobile phone, tablet and desktop.  As such, they need the ability to detect the device and direct the requester to the proper version of the site.  Which architecture will do this and meet the requirements? ","explanation":"A common use case for Lambda@Edge is to implement some front-end logic for incoming requests as close to the requester as possible.  In this case, we can use a custom Lambda function to attempt to determine the device type in the HTTP request.  We can then direct them to the CloudFront distribution that is optimized for their form-factor.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/11/lambda-at-edge-now-supports-content-based-dynamic-origin-selection-network-calls-from-viewer-events-and-advanced-response-generation/","title":"Lambda@Edge Now Supports Content-Based Dynamic Origin Selection, Network  Calls from Viewer Events, and Advanced Response Generation"}],"answers":[{"id":"ea4ee05f24a93c3b1188072a0f4777cc","text":"Configure a Network Load Balancer to use SNI to direct the request to different EC2 web servers based on device type.  Configure multiple auto scaling groups to maintain a minimum number of servers for each device type.","correct":false},{"id":"dbb8c65d7e02773f600d4e87fecc45b5","text":"Build a custom Lambda function to dynamically redirect the requester to the proper S3 origin based on device type.  Associate a CloudFront distribution with a Lambda@Edge function.","correct":true},{"id":"f28bf27096a077030c408fe2417a1d0e","text":"Create multiple distributions in CloudFront for each needed origin.  Use Route 53 to dynamically direct the requester to the appropriate alias based on device type.","correct":false},{"id":"2e04372e48d55855d33ef7c797110044","text":"Use Amazon AppSync to detect the type of device issuing the inbound request.  Use a Lambda function to redirect to the proper CloudFront distribution based on device type returned from AppSync.","correct":false},{"id":"d6b70dc60b5f361f7b92d61ac7964558","text":"Configure the Requester Interrogation feature on CloudFront to identify the device used by the requester.  Redirect the requester to the desired S3 origin based on the device type.  ","correct":false}]},{"id":"95f1d7a8-c3d4-4fec-952a-72385aa8b4c8","domain":"awscsapro-domain5","question":"You are consulting for a company that performs specialized customer data analytics.  Their customers can upload raw customer data to a website and receive back demographic statistics.  Their application consists of a REST API created using PHP and Apache.  The application is self-contained and works in real-time to return results as a JSON response to the REST API call.  Because there is customer data involved, company policy states that data must be encrypted in transit and at rest.  Sometimes, there are data quality issues and the PHP application will throw an error.  The company wants to be notified immediately when this occurs so they can proactively reach out to the customer.  Additionally, many of the company's customers use very old mainframe systems that can only access internet resources using IP address rather than a FQDN.  Which architecture will meet these requirements fully?","explanation":"The requirement of a static IP leads us to a Network Load Balancer with an EIP.","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/","title":"Elastic Load Balancing Features"}],"answers":[{"id":"0c8e5c32f081b0484e86b71651ae3642","text":"Provision a Network Load Balancer in front of your EC2 target group and terminate SSL at the load balancer using Certificate Manager.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"a9ed133e35b8332aea2bf603521b891a","text":"Provision an Application Load Balancer in front of your EC2 target group and offload SSL to CloudHSM.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch and configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from CloudHSM.","correct":false},{"id":"e53df806e37b325d7f61be27772875f1","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Enabled SSL termination on the API Gateway using Certificate Manager.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"569eec0061a1a97be77e3bdab43a1756","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Offload SSL termination using AWS KMS.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"5e4c5230c7e08202a0ea0575d5412d57","text":"Provision a Network Load Balancer with an EIP in front of your EC2 target group.  Install the CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.  Terminate SSL on the EC2 instances.","correct":true},{"id":"434ce04b2c3a4d2e679d37df43de2585","text":"Provision an Application Load Balancer with an EIP in front of your EC2 target group and terminate SSL at the ALB.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false}]},{"id":"580790f0-3491-4b4c-a9b8-42b36a787cf5","domain":"awscsapro-domain2","question":"A new project needs a simple, scalable Amazon Elastic File System (EFS) to be used by several Amazon EC2 instances located in different availability zones. The EFS file system has a mount target in each availability zone within a customized VPC. You have already attached a security group to EC2 instances that allows all outbound traffic. In the meantime, how would you configure the EFS volume to allow the ingress traffic from these EC2 instances?","explanation":"EC2 instances connect to the Amazon EFS file system through the mount targets using the Network File System (NFS) port. The mount targets use a security group to control the access from EC2 instances. It should have an inbound rule to allow the NFS port (TCP:2049). The source of the security group would be the EC2 security group rather than the VPC CIDR. Either IAM role or network ACL cannot be used to control inbound access to an EFS system.","links":[{"url":"https://docs.aws.amazon.com/efs/latest/ug/network-access.html","title":"Security Groups for Amazon EC2 Instances and EFS Mount Targets"}],"answers":[{"id":"0fb49dc49facb19b17db23bdff392a1d","text":"Configure a new security group to allow inbound traffic from the VPC CIDR IP range such as 10.10.0.0/16. Associate the security group with the EFS file system directly to allow the ingress traffic from EC2 instances in the VPC.","correct":false},{"id":"10fa29df072c2421e53a66048a92cbe3","text":"Make sure the attached IAM role in EC2 instances has the \"elasticfilesystem:*\" permission. The ingress traffic in the EFS volume is automatically allowed if the IAM role has enough permissions to interact with Elastic File Systems.","correct":false},{"id":"b346d4a177db41a1cfb51b80ae95708a","text":"Add an inbound rule in the network ACL to allow the ingress traffic from the NFS TCP port 2049. The source of the rule should be 0.0.0.0/0. Apply the network ACL in all the subnets where the EFS mount targets exist.","correct":false},{"id":"070a2bfc6b40165751997ed4d5d4e295","text":"Attach a security group to the mount targets in all availability zones. Allow the NFS port 2049 for its inbound rule. Identify the EC2 security group name as the source in the rule of the security group.","correct":true}]},{"id":"945797fb-5147-44fc-a50c-aaa636c3705b","domain":"awscsapro-domain3","question":"Your organisation currently runs an on-premise Windows file server.  Your manager has requested that you utilise the existing Direct Connect connection into AWS, to provide a method of storing and accessing these files securely in the Cloud.  The method should be simple to configure, appear as a standard file share on the existing servers, use native Windows technology and also have an SLA.  Choose an option which meets these needs.","explanation":"To choose the correct option, we can start by eliminating services which don't have an SLA, in this case only Storage Gateway doesn't have an SLA so we can remove that as an option.  Next we can rule out EFS and S3 as they don't use native Windows technology or provide a standard Windows file share, therefore the only correct answer is to use Amazon FSx for Windows File Server.","links":[{"url":"https://aws.amazon.com/fsx/windows/faqs/","title":"Amazon FSx for Windows File Server FAQs"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs"},{"url":"https://aws.amazon.com/efs/faq/","title":"Amazon EFS FAQs"}],"answers":[{"id":"af082c9581ff0bb5e1c9cc6daf7d72e0","text":"Write a Powershell script which uses the CLI to synchronise the files into an S3 bucket","correct":false},{"id":"437caccc8e39a279a232207ec3ca741a","text":"Map an SMB share to the Windows file server using Amazon FSx for Windows File Server and use RoboCopy to copy the files across","correct":true},{"id":"5a76242c7735a5846218b183930c3a41","text":"Map an Amazon Elastic File System (EFS) share to the Windows file server and use RoboCopy to copy files across","correct":false},{"id":"e953bc59adceffe1274c4bc66d83b365","text":"Create an AWS Storage Gateway for Files server and map the generated SMB share to the Windows file server, then synchronise the files","correct":false}]},{"id":"e3a59454-94fa-4b98-8d8a-80882a7d0e30","domain":"awscsapro-domain5","question":"You are setting up a new EC2 instance for an ERP upgrade project.  You have taken a snapshot and built an AMI from your production landscape and will be creating a duplicate of that system for testing purposes in a different VPC and AZ.  Because you will only be testing an upgrade process on this new landscape and it will not have the user volume of your production landscape, you select an EC2 instance that is smaller than the size of your production instance.  You create some EBS volumes from your snapshots but when you go to mount those on the EC2 instances, you notice they are not available.  What is the most likely cause?","explanation":"In order to mount an EBS volume on an EC2 instance, both must be in the same AZ.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html","title":"Amazon EBS Volumes - Amazon Elastic Compute Cloud"}],"answers":[{"id":"7a11f6b6797c8dd005c9ce25c77a37fe","text":"An SCP applied to the account you are in has restricted you from attaching EBS volumes to instances outside the original VPC","correct":false},{"id":"c2f8c791750ad6ce4c8c41ac45b246a0","text":"You have reached your account limit for EBS volumes.  You will need to create a support ticket to request an increase to the limit.","correct":false},{"id":"5a9ffc3875f9e3f50c9fc2684a6006b2","text":"The original volume is encrypted and you failed to check the encryption flag when creating the new volume.","correct":false},{"id":"a9e147ffe118e514fd5069020e86cca6","text":"The instance that you selected for your testing landscape is too small.  It must be equal to or larger than the source of the AMI.","correct":false},{"id":"134c64ea3d25d70a667400e778a13c1a","text":"You created them in a different availability zone than your testing EC2 instance.","correct":true}]},{"id":"489eea6e-bdaa-42a9-a80c-bfd209129fda","domain":"awscsapro-domain5","question":"You are working in a site reliability engineering team. There are dozens of EC2 instances in production that your team needs to maintain. When issues happen and online troubleshooting is required, the team needs to connect to a bastion host in order to login into an EC2 instance. You want to use the AWS Session Manager in Systems Manager to bypass the bastion host when accessing the instances. Which benefits can Session Manager bring to the team?","explanation":"Session Manager is a service in AWS Systems Manager that lets you access and manage your Amazon EC2 instances. Session Manager offers several benefits to the organization. With Session Manager, no SSH ports need to be open and no SSH keys are required. CloudTrail can also capture information about the Session Manager activities. However, the System Manager agent must be installed for Session Manager. For the Session Manager connections, you can choose to encrypt them with a CMK in KMS instead of an SSH key. And you should associate the IAM policies with IAM entities as an IAM policy cannot be attached in Session Manager.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html","title":"AWS Systems Manager Session Manager"}],"answers":[{"id":"8660dde753cd22508fc49e487f99b4dd","text":"Session Manager provides highly secure connections via encryptions in transit. You can choose to create an SSH key pair and use the private key to encrypt the connections.","correct":false},{"id":"1a123e85219205112eef2029de78d167","text":"AWS Session Manager handles the connections to EC2 instances and provides an interactive on-click shell. No agents need to be installed in the EC2 instances.","correct":false},{"id":"77650e8a40264f564a70afe948a6b602","text":"You do not need to open inbound SSH ports or PowerShell ports for the remote connections and no SSH keys are required for the connections with Session Manager.","correct":true},{"id":"3166b29d289b61b2a127864f52105929","text":"Session Manager integrates with IAM and you can attach an IAM policy in Session Manager to control who can initiate sessions to instances.","correct":false},{"id":"e097db56681740bcf0785fb6555a2a2d","text":"The Session Manager API calls made in the AWS account can be tracked by AWS CloudTrail. You can have a record of the connections made to the instances.","correct":true}]},{"id":"312b233b-ecc8-4e70-ae91-665159c7f77b","domain":"awscsapro-domain2","question":"You work for an automotive parts manufacturer as a Cloud Solutions Architect and you are in the middle of a design project for a new quality vision system.  To \"help out\", your parent company has insisted on contracting with a very expensive consultant to review your application design.  (You suspect that the consultant has more theoretical knowledge than practical knowledge however.)  You explain that the system uses video cameras and special polarizing filters to identify defects on fuel injectors.  As the part passes each station, an embedded RFID serial number is read and included with the PASS/FAIL vision test result in a JSON record written to DynamoDB.  The DynamoDB table is exported to Redshift on a monthly basis.  If a flaw is detected, the part can sometimes be reworked and sent back through the process--but it does retain its unique RFID tag.  Only the latest tests need to be kept for the part.  The consultant reviews your design and seems slightly frustrated that he is unable to recommend any improvement.  Then, he smiles and asks \"How are you ensuring idempotency?  In case a part is reprocessed?\"    ","explanation":"Idempotency or idempotent capability is a design pattern that allows your application to deal with the potential of duplicate records.  This can happen when interfaces fail and some records need to be reprocessed.  In this case, we are using a unique RFID serial number as our identifier for the part.  In DynamoDB, we would just overwrite the record with the latest record using a UpdateItem SDK method.  For Redshift, an UPSERT function allows us to either insert as a new record or update if a record of the same key already exists.  Redshift can do this using a merge operation with a staging table.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateItem.html","title":"UpdateItem - Amazon DynamoDB"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-upsert.html","title":"Use a Staging Table to Perform a Merge (Upsert) - Amazon Redshift"}],"answers":[{"id":"d87a9f08720c0fd91dee999f81f6f0ed","text":"You could change your design to write the message first to an SQS queue with FIFO enabled.  The records would then be guaranteed to process in the order they arrived.","correct":false},{"id":"8f594d180a595027ddef4e33f2784b0f","text":"You will be using CloudWatch to monitor the DynamoDB tables for capacity concerns.  If needed, you can enable DynamoDB auto scaling to accommodate the extra volume that reprocessing might introduce.","correct":false},{"id":"5a65801823c8e80af868a9ca05e34e18","text":"You will be using API Gateway and Lambda for the insert into DynamoDB so scaling is not a concern. The part can be processed as many times as needed and Lambda will scale as needed.","correct":false},{"id":"9dc68479e32f89bfe0afde00f41ae6c3","text":"For the target DynamoDB table, you have defined the unique RFID string as the partition key.  When copying to Redshift, you use table merge method to perform a type of UPSERT operation.","correct":true}]},{"id":"3f7fa126-1155-4aa3-802d-e9eeb75f5e5a","domain":"awscsapro-domain3","question":"You work for a Clothing Retailer and have just been informed the company is planning a huge promotional sale in the coming weeks.  You are very concerned about the performance of your eCommerce site because you have reached capacity in your data center.  Just normal day-to-day traffic pushes your web servers to their limit.  Even your on-prem load balancer is maxed out, mostly because that's where you terminate SSL and use sticky sessions.  You have evaluated various options including buying new hardware but there just isn't enough time.  Your company is a current AWS customer with a nice large Direct Connect pipe between your data center and AWS.  You already use Route 53 to manage your public domains.  You currently use VMware to run your on-prem web servers and sadly, the decision was made long ago to move the eCommerce site over to AWS last.  Your eCommerce site can scale easily by just adding VMs, but you just don't have the capacity.  Given this scenario, what is the best choice that would leverage as much of your current infrastructure as possible but also allow the landscape to scale in a cost-effective manner?","explanation":"A Target Group for an ALB can contain instances or IP addresses.  In this case, we can define the private IP addresses of our on-prem web servers along side the private IP addresses of any EC2 instances we spin up.  The caveat is that we can only use private IP addresses when defining a target group in this way.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancing-via-ip-address-to-aws-on-premises-resources/","title":"New â€“ Application Load Balancing via IP Address to AWS & On-Premises  Resources | AWS News Blog"}],"answers":[{"id":"4df24111c113846bfe0505ad0c84d9a3","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define two target groups:  one containing the public IP addresses of your on-prem load balancer and one including an auto scaling group of additional EC2 instances created from the imported AMI.  Assign both target groups to the ALB using the same listener port.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"6d3db4c52e96931f925f17fe8e9fd50f","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using public IP addresses of your on-prem web servers and additional EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"3e47f65e4524f53faba23e6995b592f5","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":true},{"id":"77592781918fa63474b5efbd5cc9555f","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an NLB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the NLB as an alias record.","correct":false}]},{"id":"512696b6-6160-45f7-8000-d664f78a86aa","domain":"awscsapro-domain2","question":"You have been contracted by a Security Company to build a face recognition service for its customer, Department of Corrections, in the AWS Cloud. Whenever a new inmate or personnel joins a facility, their facial image will be taken by an application running on a laptop, and stored centrally, along with metadata like their name. They will have a second application getting a live image feed from cameras installed throughout the secure areas of the facility. Whenever the second application receives an image from a camera, it needs to check against the pool of images stored centrally to check if there is a match. If a match is found, the location must be saved along with the timestamp and name in a database which can later be used to query the location of a person at or near a given time-period.\nHow will you, as the AWS Architect, design this suite of applications?","explanation":"This question tests the knowledge of the various Machine Learning technologies and services offered by AWS. Amazon Rekognition is the AWS AI service for image and video analysis. The question also offers AWS Sagemaker, AWS Personalize and AWS Comprehend as alternatives. AWS Sagemaker is used to build and train Machine Learning models. Hence, it is not relevant in this scenario, as the use case is not about training a model. Amazon Personalize is an ML service that enables developers to create individualized recommendations for customers using their applications. This use case is not related to recommendations, hence we can eliminate AWS Personalize. Amazon Comprehend is a natural language processing (NLP) service that uses ML to find relationships and insights in text. We can eliminate Comprehend as this is an image analysis scenario, as opposed to text analysis.","links":[{"url":"https://aws.amazon.com/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/","title":"Build Your Own Face Recognition Service Using Amazon Rekognition"},{"url":"https://aws.amazon.com/machine-learning/","title":"Various components of Machine Learning on AWS"}],"answers":[{"id":"3c168275751e0bed48552554f05bad14","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Personalize from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in a Dynamodb database. Later, when a match is required, query Amazon Personalize with the image taken by security cameras. Personalize will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the Dynamodb database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to an RDS database for later querying","correct":false},{"id":"3fac11da1f4ec1911a5b1c4262c16bb4","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Comprehend from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in an RDS Postgresql database. Later, when a match is required, query Amazon Comprehend with the image taken by security cameras. Comprehend will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the RDS database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to a second RDS database for later querying","correct":false},{"id":"cc5d32350e93736f1e3ec5e359029c80","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Sagemaker Image Classification Algorithm from the Lambda function to index and classify the face in the image, returning a Face id. Store this face id and Name of the person in an RDS Postgresql database. Later, when a match is required, query Amazon Sagemaker with the image taken by security cameras. Sagemaker will return face ids with confidence values for the match. If a face id is found whose confidence value is higher than a predefined set value, query the RDS database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to a second RDS database for later querying","correct":false},{"id":"a24bc02aa584b134a2abecb810d5b32c","text":"Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda function to be triggered on putObject event of the bucket. Invoke Amazon Rekognition from the Lambda function to index the face in the image as a Collection, returning a Face id. Store this face id and Name of the person in a Dynamodb database. Later, when a match is required, query the Amazon Rekognition Collection with the image taken by security cameras. Rekognition will return a set of face ids that have potentially matched. If a face id is found whose confidence value is higher than a predefined set value, query the Dynamodb database for the name belonging to the face id. Then write a record containing the name, timestamp and the location id of the camera to an RDS database for later querying","correct":true}]},{"id":"431e43bc-ccbc-480f-9915-210bc7773d2b","domain":"awscsapro-domain5","question":"You are in the process of migrating a large quantity of small log files to S3 for long-term storage.  To accelerate the process and just because you can, you have created quite sophisticated multi-threaded distributed process deployed across 100 VMs which can load hundreds of thousands of files at one time.  For some reason, the process seems to be throttled somewhere along the chain.  You try many things to try to uncover the source of the throttling but nothing works.  Reluctantly, you decide to turn off the KMS encryption setting for your S3 bucket and the throttling goes away.  You turn AMS-KMS back on and the throttling is back. Given the troubleshooting steps, what is the most likely cause of the throttling and how can you correct it?","explanation":"Through a process of elimination, it seems you have identified the variable that is causing the throttling.  KMS, like other AWS services, does have rate limiters which can be increased via Support Case.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/limits.html","title":"Limits - AWS Key Management Service"}],"answers":[{"id":"05aeb0bc36d7b53aa30bf9e22b6cd120","text":"You are maxing out your PUT requests to S3.  You need to change over to multi-part upload as a workaround.","correct":false},{"id":"01148eae3319190a0b228c6d02c9572c","text":"You are maxing out your network connection.  You must split the traffic over multiple interfaces.","correct":false},{"id":"60ff77ab365c15bb11771e94e3dc271d","text":"You have exceeded the number of API calls for your account.  You must create a new account.","correct":false},{"id":"fe629daf7473efc279d7d8ee6f5a5806","text":"You are maxing out your SYNC requests to S3.  You need to request a limit increase via a Support Case.","correct":false},{"id":"1dd4f25e52404e18ddec0b8711a82a13","text":"You are hitting the KMS encrypt request account limit.  You must request a limit increase via a Support Case.","correct":true}]},{"id":"edf9ffa9-02ec-4341-a179-577cd590543e","domain":"awscsapro-domain1","question":"You work for a technology product company that owns two AWS Accounts - Prod and DevTest, both belonging to the same Organizational Unit (OU) under AWS Organizations Root. There are three different teams in your company - Dev Team, Testing Team and Ops Team. While Dev and Testing Team members have IAM Users created in the DevTest account, the Ops Team members have IAM Users created in the Prod account. There is an S3 bucket created in the Prod account that Testing Team members need access to - they need both read and write access. What is the best way to give the Testing Team members access to the Prod account S3 bucket?","explanation":"Cross-Account Access is best achieved using IAM Cross-Account Roles. The solution that suggests that the Testing Team members have to sign in to a different AWS account every time they need to access the S3 bucket is not correct as it is inefficient and unproductive.\nThere is no such AWS Organization feature that can directly enable Cross-Account Access from the console. There no way to select or deselect groups or users in this manner. Hence, the option that suggests using these is eliminated.\nThe remaining two options are a play in words. Carefully read both options. The Role must be created in the Trusting Account, in this case Prod Account because it has the Resource (S3 bucket) that needs to be accessed by the someone from another AWS Account, i.e., the Trusted Account.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html","title":"Tutorial - Delegate Access Across AWS Accounts Using IAM Roles"},{"url":"https://aws.amazon.com/organizations/","title":"AWS Organizations"},{"url":"https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/","title":"How to Enable Cross-Account Access to the AWS Management Console"}],"answers":[{"id":"87953f71b8915388e4ecb3f7b2378374","text":"Create an IAM Role in the DevTest Account that allows access to the Prod Account, thus establishing trust between the Accounts. Attach a Trust Policy to the Role that grants the Testing Team members permission to assume the Role. Attach an Access Policy to the Testing Team's IAM Users in the DevTest Account that allows them to call STS AssumeRole for the specific Resource whose value is the ARN of the Role in Prod Account. Attach a Bucket Policy to the S3 Bucket that specifies the IAM Users of Testing Team members as Principal.","correct":false},{"id":"56e4fe89e8a8ffb36bfcd67126afa0d5","text":"Create an IAM Role in the Prod Account that allows the DevTest Account to assume it, thus establishing trust between the Accounts. Attach an Access Policy to the Role that allows access to the S3 bucket. Attach a Bucket Policy to the S3 Bucket that specifies the ARN of the Role as Principal. Attach an Access Policy to the Testing Team's IAM Users in the DevTest Account that allows them to call STS AssumeRole for the specific Resource whose value is the ARN of the Role in Prod Account.","correct":true},{"id":"e661ebc45484fb5a715bc96c884b4024","text":"Enable Cross-Account Access at the AWS Organizational Unit (OU) level from the console. Deselect the Dev Team UAM Users from Cross-Account Access Setup Wizard. This will allow only the Testing Team members to be able to access the Prod account. Write a bucket policy for the S3 bucket that lists the Testing Team members as Principals who are allowed to access the bucket.","correct":false},{"id":"e91ff3381d499ccd06e7891c37a77891","text":"Create IAM Users for the Testing Team members in the Prod account. Create a Testing IAM Group in the Prod account and add the IAM Users of the Testing team members to the Group. Assign an access policy to the Testing Group in the Prod account that grants Read and Write access to the correct S3 bucket. Testing team members will sign into the Prod account to access the S3 bucket.","correct":false}]},{"id":"08a68d51-48ba-43b7-b0c3-c24e04bb33a8","domain":"awscsapro-domain3","question":"You have just completed the move of a Microsoft SQL Server database over to a Windows Server EC2 instance.  Rather than logging in periodically to check for patches, you want something more proactive.  Which of the following would be the most appropriate for this?","explanation":"The default predefined patch baseline for Windows servers in Patch Manager is AWS-DefaultPatchBaseline.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html","title":"Default and Custom Patch Baselines - AWS Systems Manager"}],"answers":[{"id":"78beead90b3e8deb4bf1ee7e3544a309","text":"Make use of AWS Batch to apply patches as they appear on the RSS feed from Microsoft","correct":false},{"id":"5ee836bac8680ff00d457a8d7f90fad6","text":"Make use of Patch Manager and the AWS-WindowsDefaultPatchBaseline pre-defined baseline","correct":false},{"id":"b40084bb5d5139b353505e5f942afc34","text":"Make use of Patch Manager to apply patches as you have defined in the Patch Groups","correct":false},{"id":"4b5715f588231d7445bb512181ea13a2","text":"Make use of Server Manager and the AWS-LinuxWindowsDefaultPatchBaseline pre-defined baseline","correct":false},{"id":"42c46b2fcd3034cf79b83f0d5dc37f7d","text":"Make use of Patch Manager and the AWS-DefaultPatchBaseline pre-defined baseline","correct":true}]},{"id":"0abe2292-3f6e-47e1-93d9-6af24d5ea4c2","domain":"awscsapro-domain4","question":"A graphic design company has purchased eighteen m5.xlarge regional Reserved Instances and sixteen c5.xlarge zonal Reserved Instances. They receive their monthly AWS bill and find the invoice amount to be significantly higher than expected. Upon investigation, they discover RI discounted and non-discounted charges for nine m5.xlarge instances, nine m5.2xlarge instances, eight c5.xlarge instances, and eight c5.2xlarge instances. The business will need all of this capacity for at least the next twelve months. As their consultant, what would you advise them to do to maximize and monitor their RI discounts?","explanation":"Regional Reserved Instances allow for application of RI discounts within instance families, so all of the m5 instances are covered. Zonal Reserved Instances only provide discounts for specific instance types and sizes. So purchase of additional RIs would lower costs on the eight c5.2xlarge instances. Unused Reserved Instances are contractual and cannot be cancelled, so looking for another place to use them is the right approach. They could possibly be sold on the Reserved Instance Marketplace. AWS Budgets reservation budgets provide visibility and alerting on RI coverage specifically. Cost budgets and usage budgets may be useful, but they won't target RI coverage specifically.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html","title":"Regional and Zonal Reserved Instances (Scope)"},{"url":"https://aws.amazon.com/blogs/aws-cost-management/launch-instance-family-coverage-budgets/","title":"Launch: Instance Family Coverage Budgets"}],"answers":[{"id":"8c1f6db4cd04a86fd43efa1bc97640ea","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances and eight c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":false},{"id":"8e0e716570fad7bcaf6fe8bebe4f9d49","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Cancel the Reserved Instances for the nine unused m5.xlarge instances and eight unused c5.xlarge instances. Create an AWS Budgets cost budget that sends notification whenever costs exceed 80% of usage expectation","correct":false},{"id":"1c87a2204ad8da355c8ca0890aa06785","text":"Purchase an additional nine m5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances. Create an AWS Budgets usage budget that sends notification whenever RI coverage drops below 60%","correct":false},{"id":"4b6d77d8423cd16b8e711b482dbcafbf","text":"Purchase an additional nine c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":true}]},{"id":"3ca7c5e0-432a-4d40-afee-ab996819b429","domain":"awscsapro-domain3","question":"You are consulting with a client to guide them on migration of an in-house data center to AWS.  The client has stipulated in the contract that the migration cannot require any more than 1 hour downtime at a time and that there is always a fallback path.  Additionally, they want an overall increase in business continuity capabilities when the migration is done.  Their landscape is as follows:  (1) Several databases with about 1TB of data combined which are heavily used 24x7 and considered mission critical; (2) About 40TB of historic files which are read sometimes but almost never updated; (3) About 150 web servers on VMware in various states of customization of which there is a current project underway to standardize them.  The client's team has suggested some next steps but because they aren't yet familiar with AWS, they are not using equivalent AWS terms.  Translating their suggestions, which of the following activities would you choose to meet the requirements, reducing costs and management where possible?","explanation":"The database migration suggestion aligns well with DMS as it can keep the databases in sync until cutover.  SAN replication sounds a lot like Storage Gateway which is a reasonable way to migrate data to AWS.  However, simply using K8s does not convert your VMs into containers or make them serverless.  We can't restore tapes to AWS.  Creating the same VM landscape on AWS just adds an additional layer of complexity that's not needed.","links":[{"url":"https://aws.amazon.com/dms/faqs/","title":"AWS Database Migration Service FAQs - Amazon Web Services"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"}],"answers":[{"id":"31ea0eccdcea45ea4fce3b9459de52d4","text":"Use some block-level SAN replication tool to gradually migrate the on-prem historic files to AWS.","correct":true},{"id":"801ce55cfc2a125e7d17c729ca3e2e93","text":"Create new high powered stand-alone database instances in AWS and migrate data from on-prem database.  Use log shipping to keep the databases in sync.  Once we better understand AWS, we'll rebuild the servers and repartition the tables. ","correct":true},{"id":"75d528d2ec243c60d1478ae605c89f40","text":"Build a matching VMware environment on AWS and use third-party tools to backup and restore the VMs there.","correct":false},{"id":"d2dde578790a34d9e740015474ea23e4","text":"Migrate the majority of the 150 web servers to a serverless concept by moving the VMs to a Kubernetes cluster.","correct":false},{"id":"3a02ebcd33fe18255e4ce43e8babb730","text":"Over several months, at end of business on Friday, backup all the servers and data to tape and restore to new instances in AWS to prove out AWS capabilities and reliability.","correct":false}]},{"id":"54d12a9a-149b-42a7-8491-300583d5c2b8","domain":"awscsapro-domain5","question":"A client is trying to setup a new VPC from scratch.  They are not able to reach the Amazon Linux web server instance launched in their VPC from their on-prem network using a web browser.  You have verified the internet gateway is attached and the main route table is configured to route 0.0.0.0/0 to the internet gateway properly.  The instance also is being assigned a public IP address.  Which of the following would be another potential cause of the problem?","explanation":"For an HTTP connection to be successful, you need to allow port 80 inbound and allow the ephemeral ports outbound.  Additionally, it is possible that the subnet is not associate with the route table containing the default route to the internet.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html","title":"Troubleshooting Connecting to Your Instance - Amazon Elastic Compute Cloud"}],"answers":[{"id":"32cdd854c9d71059eac396dd1249830e","text":"The subnet of the instance is not associated with the main route table.","correct":true},{"id":"2bde109ce87f4a4f513679f31116184d","text":"The instance does not have an elastic IP address assigned. ","correct":false},{"id":"25d3393c550b8dbc2179367832573598","text":"The outbound network ACL allows port 80 and 22 only.","correct":true},{"id":"01b7463243eb231b840fcd4b737e044b","text":"The default route to the internet gateway is incorrect.","correct":false},{"id":"c9f886542d1dabe99bc64dd39c5e1615","text":"The inbound security group allows port 80 and 22 only.","correct":false},{"id":"c7ca1b6a8fe855bda71123163488960b","text":"The customer has disabled the ec2-user account on the Amazon Linux instance.","correct":false},{"id":"d0f5e76f9fc753305b11c4c3a11e97ef","text":"The IAM role assigned to the LAMP instances does not have any policies assigned.","correct":false}]},{"id":"374fde7d-232a-4cfe-b5d6-7755d564c6ca","domain":"awscsapro-domain1","question":"You are consulting for a company that has decided to partially migrate some resources to AWS from their two data centers (DC1 and DC2).  Their first order of business is to design a robust, redundant and cost-effective network connection between their data centers and AWS.  They already have redundant links between DC1 and DC2.  Which of the following architectures provides the highest availability at the least cost?","explanation":"A common and cost effective way to provide a redundant link to AWS with Direct Connect is a VPN connection.  In the event that the Direct Connect path fails at DC1, your on-prem router can redirect traffic over the VPN at DC2 via the DC1-DC2 link.  Having dual Direct Connect links is definitely redundant but more expensive than a VPN.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/","title":"Multiple Data Center HA Network Connectivity â€“ AWS Answers"}],"answers":[{"id":"abd8b38f393b6a0d0b42f68dca5ec24d","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure BGP to dynamically route traffic across the nearest Direct Connect link.","correct":false},{"id":"425adb8435a7c170eef3698fa729f5ea","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure a default route in both DC1 and DC2 to route traffic to the local Direct Connect link.","correct":false},{"id":"77c9fb7459e1b19f6f655d63556016e8","text":"Configure a Direct Connect connection from DC1 to a Virtual Private Gateway on AWS.  Setup a VPN connection from DC2 to a Virtual Private Gateway on AWS.  Configure a dynamic route across DC1 and DC2 for both paths with a route priority favoring the Direct Connect path to AWS.","correct":true},{"id":"bcac2f1d84c5f367ede3342f0adda492","text":"Ensure that DC1 and DC2 have separate ISPs.  Setup VPN connections from DC1 and DC2 to a Virtual Private Gateway on AWS.  Create static routes at each DC to use the local VPN to AWS.  Use CloudTrail to monitor traffic on the Virtual Private Gateway and trigger a script to update the static route if one of the VPN connections goes down.","correct":false}]},{"id":"33803c8a-b588-4dca-8067-e500383254f3","domain":"awscsapro-domain4","question":"You work for a retail services company that has 8 S3 buckets in us-east-1 region. Some of the buckets have a lot of objects in them. There are Lambda functions and EC2-hosted custom application code where the names of these buckets are hardcoded. Your manager is worried about disaster recovery. As part of her business continuity plan, she has requested you to set up Cross-Region Replication of these S3 buckets to us-west-1, ensuring that the replicated objects are using a less expensive Storage Class because they would not be accessed unless disaster strikes. You are worried that in the event of failover due to the entire us-east-1 region being unavailable, the application code, once deployed in us-west-1, must continue to work while trying to access the S3 buckets in the new region. She has also requested you to start taking periodic snapshots of EBS Volumes and make these snapshots available in the us-west-1 region so that EC2 instances can be launched in us-west-1 using these snapshots if needed. How would you ensure that (a) the launching of EC2 instances works in us-west-1 and (b) your application code works with the us-west-1 S3 buckets?","explanation":"This question presents two problems - (1) how to ensure that EBS snapshots are created periodically and are also made available in a different region for launching required EC2 instances in case of failure of the primary region (2) how to deal with application code where S3 bucket names are hardcoded and whether this hardcoding will impact disaster recovery while trying to run in a different region. Both of these problems are real-life issues AWS customers face when designing and planning their disaster recovery solutions.\n(1)Remember that Data Lifecycle Manager can only schedule snapshot creation in the same Region. If we want to copy that snapshot into a different region, we must write our own scripts or Lambda functions for doing that. Hence, the choices that state that DLM can be used to directly create the snapshot into different regions are eliminated. Additionally, only root volume snapshots can be used to create an AMI. Non-root EBS Volume snapshots cannot be used to generate an AMI. Hence, the choices that specify using non-root volume snapshots are eliminated.\n(2)Remember that S3 bucket names are globally unique. Hence, one cannot create a second S3 bucket in the DR Region with the same name as the bucket in the primary region. Hence, the options that hint the creation of S3 buckets by the same name are eliminated. This results in a problem if S3 names are hardcoded in the application - that application will simply not run in a new region, it will fail. Hence, it is best to avoid hardcoding, and fetch the S3 bucket name from a key-value storage service like AWS Systems Manager Parameter Store at runtime. Creating this Parameter Store in each region and storing the correct bucket names in them can help in designing this non-hardcoded solution. Additionally, enabling Cross-Region Replication does not copy pre-existing content. Hence, the choices that suggest that pre-existing content will be automatically copied are eliminated.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-launch-snapshot.html","title":"Launching an Instance from a Backup"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/ec2/copy-snapshot.html","title":"Copy-snapshot documentation"}],"answers":[{"id":"fc1a2efe00ef610249a55dadb0dd64fe","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false},{"id":"fae496411f2d461e0926abfdf8ad8b64","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":false},{"id":"824dc187ab89444593b50521f60b8ff3","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":true},{"id":"02e8aed40d51e4bf9256f1ed436aa069","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false}]},{"id":"9cab3dd3-61ad-44c6-b27a-2999e7aedf61","domain":"awscsapro-domain1","question":"For large organizationally complex AWS landscapes, it is considered a best practice to combine a tagging strategy with lifecycle tracking of various projects to identify orphaned resources that are no longer generating value for the organization and should be decommissioned.  With which AWS Well-Architected Framework Pillar is this best practice most aligned?","explanation":"Tagging has many uses but one strong use-case is in being able to tie resources that incur costs with cost centers or projects to create a direct line of sight to actual AWS expenses.  If this visibility does not exist, costs tend to increase because \"someone else is paying.\"  A Best Practice of the Cost Optimization Pillar is to maintain expenditure awareness.","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"2fae32629d4ef4fc6341f1751b405e45","text":"Security","correct":false},{"id":"820b8f74ad843d1574106ec4cadfc07e","text":"Reliability","correct":false},{"id":"2fa2e9f495df0cdcfa992171784f89b7","text":"Operational Excellence","correct":false},{"id":"d0bf6ab153eaa104c878925472ff129a","text":"Cost Optimization","correct":true},{"id":"bc368b371822f4e7df28c5768582dde8","text":"Performance Efficiency","correct":false}]},{"id":"07f91ae7-094b-48a9-8924-a4d142cbbcb6","domain":"awscsapro-domain5","question":"On your last Security Penetration Test Audit, the auditors noticed that you were not effectively protecting against SQL injection attacks.  Even though you don't have any resources that are vulnerable to that type of attack, your Chief Information Security Officer insists you do something.  Your organization consists of approximately 30 AWS accounts.  Which steps will allow you to most efficiently protect against SQL injection attacks?","explanation":"Firewall Manager is a very effective way of managing WAF rules across many WAF instances and accounts.  It does require that the accounts be linked as an AWS Organization.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"9a5fa8e9a1a9be9149138c6307abde19","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service.  Use Firewall Manager to create an ACL rule to deny requests that contain SQL code.  Apply the ACL to WAF instances across all organizational accounts.","correct":true},{"id":"3927b32bfb85cc040d2b7dcb21015fc5","text":"Use AWS WAF to create an ACL that denies requests that include SQL code.  Assign the ACL to Firewall Manager instances in each account using AWS OpsWorks.","correct":false},{"id":"e072f443d93c569cce94eb4946a912af","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service and use Consolidated Billing. Subscribe to AWS Shield Advanced to automatically enable SQL injection protection across all sub-accounts.","correct":false},{"id":"2238e9ae7489214f767fa479d013cd23","text":"Create a custom NACL filter using Lambda@Edge to check requests for SQL code.  Use OpsWorks to apply the NACL across all public subnets across the organization. ","correct":false},{"id":"ee9c067e3dea2fa4cd2e3968abaf86de","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations.  Use CloudFormation to implement request restrictions for SQL code on the CloudFront distributions across all accounts.  Setup a CloudWatch event to notify administrators if requests with SQL code are seen.","correct":false}]},{"id":"91006a08-8658-479c-9f96-4f9cd9c770c6","domain":"awscsapro-domain2","question":"Your customer is a commercial real-estate company who owns parking lots in all major cities in the world. They are building a website on AWS that will be globally accessed by international travellers. The website will provide near-real-time information on available parking lot spaces for all cities. You need a back-end comprising a multi-region multi-master data storage solution, with writes happening in all regions. Writes must be replicated between regions in near real-time so that different regional website compute instances can query the database instance in its own region and retrieve the data pertaining to all other regions. In addition, all users should be able to access the website using the same domain name, but they need to be routed to the Elastic Load Balancer that responds most quickly to their request (which should be the one closest to them most of the time). The website must also serve locally cached static content and have protection against malicious attacks. Which AWS-based architecture should you choose?","explanation":"The only AWS managed database service that offers multi-region multi-master is DynamoDB Global Tables. AWS Aurora does offer multi-master clusters, but all nodes of an Aurora multi-master must be in the same region. Aurora also offers cross-region replication, but the nodes, in that case, are Read Replicas, they are not Masters (in the database world, a master is an instance that accepts write requests). Similarly, AWS RDS does not currently have a multi-master multi-region solution to leverage. Thus, the only choice that can satisfy the multi-region multi-master nature of the requirement is DynamoDB Global Tables with DynamoDB Streams enabled for Replication. Additionally, the question tests knowledge in other areas. However, just focusing on the above fact about the database tier is enough to eliminate all incorrect answers.\nRoute 53 latency-based multi-region routing is the correct Routing solution here. Multi-value answering records results in randomly picking one, which is not the requirement here (in this scenario, the region must be picked based on latency, not randomly). Similarly, geolocation-based routing honours country boundaries, not latency. If we use geolocation-based routing, an end-user in country A will always go to the AWS Region in country A, but that may not be the one with least latency, especially if the user is close to an international boundary, and the AWS data centre happens to be on the other end of his country.\nLambda@Edge can be used with Cloudfront to re-write origins for requests. This is an important usage of Lambda@Edge. Similarly, WAF can be activated with a Cloudfront web distribution for protection against malicious traffic.","links":[{"url":"https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/","title":"How to use Amazon DynamoDB global tables to power multiregion architectures"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html","title":"Search for - all DB instances in a multi-master cluster must be in the same AWS Region"},{"url":"https://aws.amazon.com/blogs/aws/latency-based-multi-region-routing-now-available-for-aws/","title":"Multi-Region Latency Based Routing now Available for AWS"}],"answers":[{"id":"25c4e1f274a66d3e2b5d67c75702ef4b","text":"In AWS, all cross-region replication solutions treat secondary instances as read-only, so multiple masters are not possible in a managed multi-region database solution. Use Cassandra on EC2 instances in different regions that are peered to each other and configure Cassandra in multi-master mode. For routing, use Route 53 multi-region latency based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":false},{"id":"c74e4146c5eb59b1a892f52ac51bfbd7","text":"For the database, use Amazon Aurora Cross-Region Replication, with instances in each AWS Region configured as master. For routing, use Route 53 geolocation-based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":false},{"id":"32667dba126e63f9c8c6adb288419974","text":"For the database, use DynamoDB Global Tables, with instances in each AWS Region configured as master, and replication enabled using DynamoDB Streams. For routing, use Route 53 multi-region latency based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":true},{"id":"9faf11728364108db71381c81e091523","text":"For the database, use Amazon RDS with Cross Region Replication, with instances in each AWS Region configured as master. For routing, use Route 53 multi-region multi-value answering based routing. Use Cloudfront to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF with Cloudfront geo restriction feature.","correct":false}]},{"id":"482e75c9-071e-4a10-83f4-575f9c15b885","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They have just accidentally deleted the private key portion of their EC2 key pair.  Now, they are unable to SSH into their Amazon Linux servers.  Unfortunately the keys were not backed up and are considered gone for good.  What can this customer do to regain access to their instances?","explanation":"The two methods that AWS recommends if you lose a private key for an EC2 key pair are using Systems Manager Automation or using a secondary instance to edit the authorized_keys file.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2reset.html","title":"Reset Passwords and SSH Keys on Amazon EC2 Instances - AWS Systems Manager"}],"answers":[{"id":"79457a1b908d4a36cfeba625be909d40","text":"Use AWS Systems Manager Automation with the AWSSupport-ResetAccess document to create a new SSH key for your current instance.","correct":true},{"id":"f17aa014620843abd81fa849982566b0","text":"Create a new key pair in KMS then assign the new public key to the required EC2 instance.","correct":false},{"id":"492c38d2fe2c3b96608bb8436592fe26","text":"Use the AWS CLI with the EC2 ModifyInstance action to enable SSH password-only access for the ec2-user account.  Attach using a password rather than an SSH key.  Modify the authorized_key file for the new public key.","correct":false},{"id":"be45655cb1d64dff71a97aa729bc4e4a","text":"Open the TELNET port (port 23) on the Security Group for the server.  Use a TELNET client to attach to the instances using the root account and password.  Modify the authorized_key file with the new public key.","correct":false},{"id":"bec3d01a56c851f61a1a09c852635db7","text":"Generate and upload a new key pair.  Stop the instances and select the new key pair from the dropdown on the Instance Settings sub-menu in the Console.","correct":false},{"id":"509a77ae9827c5fcb60ccecc62fc9853","text":"Stop the instances, detach its root volume and attach it as a data volume to another instances.  Modify the authorized_keys file, move the volume back to the original instance and restart the instances.","correct":true}]},{"id":"05e085a9-4de3-46fe-9470-10c7f2faba57","domain":"awscsapro-domain5","question":"You are consulting with a client who is in the process of migrating over to AWS.  Their current on-prem Linux servers use RAID1 to provide redundancy.  One of the big benefits they are looking forward to with moving to AWS is the ability to create snapshots of EBS volumes without downtime.  Right now, they intend on migrating the servers over to AWS and retaining the same disk configuration.  What is your advice for them?","explanation":"Because RAID is based upon multiple volumes being in sync, taking snapshots of an individual volume that's part of a active and mounted RAID array would not create a proper backup.  You must first unmount the RAID volume and then create the snapshots of the component volumes.  This of course means any data on the RAID volume would be unavailable.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html","title":"RAID Configuration on Linux - Amazon Elastic Compute Cloud"}],"answers":[{"id":"21a6a5218cf9778e0184eed7897c54ce","text":"Consider using RAID6 rather than RAID1 on AWS for performance reasons.","correct":false},{"id":"73b207bc7a947de1eed26bc058b4b67b","text":"If snapshots without downtime are the priority, do not use RAID.","correct":true},{"id":"99a9d29ef15a0ced996c1510ff6d8f6a","text":"EC2 does not support RAID configurations.","correct":false},{"id":"ead9938f5d4fc4d2df30763406b6a8e5","text":"Consider using RAID10 when on AWS because it offers the best of both RAID0 and RAID1.","correct":false},{"id":"bee3a756d6bfddce4e9917e171a4b0e2","text":"Consider using RAID0 when on AWS for performance reasons.","correct":false}]},{"id":"768271e9-9fd0-4921-a473-49ec465a0b34","domain":"awscsapro-domain4","question":"Your company is preparing for a large sales promotion coming up in a few weeks.  This promotion is going to increase the load on your web server landscape substantially.  In past promotions, you've run into scaling issues because the region and AZ of your web landscape is very heavily used.  Being unable to scale due to lack of resources is a very real possibility.  You need some way to absolutely guarantee that resources will be available for this one-time event.  Which of the following would be the most cost-effective in this scenario.","explanation":"If we only need a short-term resource availability guarantee, it does not make sense to contract for a whole year worth of Reserved Instance.  We can instead use On-Demand Capacity Reservations.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html","title":"On-Demand Capacity Reservations - Amazon Elastic Compute Cloud"}],"answers":[{"id":"08d50c618757e489c5ac563be91f25f7","text":"Purchase a regional Reserved Instance.","correct":false},{"id":"f10307b8e644773b68b43dade41382f1","text":"Purchase Dedicated Instances.","correct":false},{"id":"9dc64814f005254de6e1a269849ae0b1","text":"Purchase zonal Reserved Instance.","correct":false},{"id":"0862f1bd2009c6ca8b52898831991642","text":"Use an On-Demand Capacity Reservation.","correct":true},{"id":"0b48767c7357e407fa6f1e958fe0f051","text":"Purchase a Dedicated Host.","correct":false}]},{"id":"0c3d85ee-ff65-46e7-86d4-9fb7dcd21176","domain":"awscsapro-domain4","question":"Per the requirements of a government contract your company recently won, you must encrypt all data at rest.  Additionally, the material used to generate the encryption key cannot be produced by a third-party because that could result in a vulnerability.  You are making use of S3, EBS and RDS as data stores, so these must be encrypted.  Which of the following will meet the requirements at the least cost?","explanation":"When possible, making use of KMS is much more cost-effective than CloudHSM.  We can import our own key material into KMS for creating Customer Master Keys.  Because KMS works natively with the services we will be using, we save on any sort of custom integration that CloudHSM would have required.","links":[{"url":"https://aws.amazon.com/kms/faqs/","title":"FAQs | AWS Key Management Service (KMS) | Amazon Web Services (AWS)"},{"url":"https://aws.amazon.com/cloudhsm/faqs/","title":"AWS CloudHSM FAQs - Amazon Web Services"}],"answers":[{"id":"9052cdd68b68935da91384397a18351c","text":"Use AMS KMS to create a 256-bit encryption key.  Use a grant to only allow access by S3, RDS and EBS.  When creating an S3 bucket, select the SSE-KMS option and pick the key from the dropdown.  For EBS and RDS, use the CLI to assign the KMS key when creating those instances.","correct":false},{"id":"66a66f8efdd420438db68069645b1ae7","text":"Initialize a CloudHSM instance.  Use it to generate custom encryption keys for each service you will use.  When creating an S3 bucket, EBS volume or RDS instance, select the custom CloudHSM key from the dropdown in the setup wizard.","correct":false},{"id":"473891e603ed3e2091e789a13094399c","text":"Use AMS KMS to create a customer-managed CMK.  Create a random 256-bit key and encrypt it with the wrapping key.  Import the encrypted key with the import token.  When creating S3 buckets, EBS volumes or RDS instances, select the CMK from the dropdown list.","correct":true},{"id":"a4e27f3b361ede72810f71b15168297e","text":"Generate a public and private key pair.  Upload the public key via the EC2 dashboard.  When creating EBS volumes, select encryption and select this public key.  When creating S3 buckets, implement a bucket policy which requires encryption at rest only, rejecting other files.  Create an RDS instance and select the public key from the dropdown in the setup wizard.","correct":false}]},{"id":"6da286f8-23a6-4e8a-a3a4-c7b496a06523","domain":"awscsapro-domain5","question":"An online health foods retailer stores its product catalog in an Amazon Aurora database. The catalog contains over 6,000 products. They'd like to offer a product search engine on the website using Amazon Elasticsearch Service. They'll use AWS Database Migration Service (DMS) to perform the initial load of the Elasticsearch indexes, and to handle change data capture (CDC) going forward. During the initial load of the indexes, the DMS job terminates with an Elasticsearch return code of 429 and a message stating 'Too many requests'. What must be done to load the Elasticsearch indexes successfully?","explanation":"When the ElasticSearch indexing queue is full, a 429 response code is returned and an es_rejected_execution_exception is thrown. The DMS load task then terminates. Throttling the DMS input stream based on the number of Elasticsearch indexes, shards, and replicas to be loaded will result in a successfully completed job. The DMS MaxFullLoadSubTasks parameter indicates how many source tables to load in parallel, and the ParallelLoadThreads parameter determines the number of threads that can be allocated for a given table. Increasing Elasticsearch shards without modifying DMS subtask and thread parameters could still overrun the request queue. Changing the DMS stream buffer count won't help with this issue. Amazon Elasticsearch currently doesn't provide support for AWS Glue as a source, so integration would require significant effort. Increasing Elasticsearch EBS volume IOPS won't solve an ingress queue overrun problem. The DMS batch split size parameter sets the maximum number of changes applied in a single batch, but doesn't reduce the total number of requests.","links":[{"url":"https://aws.amazon.com/dms/","title":"Amazon Database Migration Service"},{"url":"https://aws.amazon.com/elasticsearch-service/","title":"Amazon Elasticsearch Service"},{"url":"https://aws.amazon.com/blogs/database/scale-amazon-elasticsearch-service-for-aws-database-migration-service-migrations/","title":"Scale Amazon Elasticsearch Service for AWS Database Migration Service migrations"}],"answers":[{"id":"ee750f72f1e70b83f6d83819f2d504f5","text":"Raise the baseline IOPS performance of the Elasticsearch cluster EBS volumes to enable more throughput. Increase the DMS batch split size parameter to send more data in each request and reduce the number of total requests","correct":false},{"id":"157e8733386382289486b3592774442f","text":"Increase the number of Elasticsearch shards for each index to increase distribution of the load. Change the DMS stream buffer count parameter to match the number of Elasticsearch shards","correct":false},{"id":"c1e1a85b26a30433a6af5d30c8bb8d76","text":"Calculate the number of queue slots required for the Elasticsearch bulk request as a product of the number of indexes, shards, and replicas. Adjust DMS subtask and thread parameters accordingly","correct":true},{"id":"62ac117860ebe5397e04bad8ea29a5fb","text":"Replace DMS with AWS Glue for the initial index load and ongoing change data capture. Enable parallel reads when the ETL methods are called in the Glue jobs","correct":false}]},{"id":"9038ce91-1730-47e6-b804-571614ac4752","domain":"awscsapro-domain5","question":"For your production web farm, you have configured an auto scaling group behind a Network Load Balancer.  Your auto-scaling group is defined to have a core number of reserved instances and to scale with spot instances.  Because of differences in spot pricing across AZs, sometimes you end up with many more instances in one AZ over another.  During times of peak load, you notice that AZs with fewer instances are averaging 70% CPU utilization while the AZ with more instances average barely above 10% CPU utilization.  What is the most likely cause of this behavior? ","explanation":"Cross-zone load balancing ensures that requests are equally spread across all available instances, regardless of AZ.  When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. When cross-zone load balancing is disabled, each load balancer node distributes traffic across the registered targets in its Availability Zone only. ","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html","title":"How Elastic Load Balancing Works - Elastic Load Balancing"}],"answers":[{"id":"19fc4224d11c7dbb028fcb29fc93ce38","text":"Cross-zone load balancing is disabled on the Network Load Balancer.","correct":true},{"id":"61e42ddbbd5b30ec8e73f350f7782986","text":"At the time a scale event is triggered, there are no more available resources in the AZ or region of the instance type you have configured in your auto scaling group.","correct":false},{"id":"733135f9c29b6aef570b5c4f6748df2f","text":"CloudWatch is not accurately reflecting the true CPU load due to mesh processing on Nginx.  The real CPU load might in fact not reach the threshold which explains why the group is not scaling.","correct":false},{"id":"dbec1ca528c5fed82acd6704fa187842","text":"The cooldown time is too short in the launch configuration.","correct":false},{"id":"1d1ef2b9e2525e667cc97a8dd5e91429","text":"The TTL for sticky sessions is set too high and therefore are blocking a scale-out event until some connections are dropped.","correct":false}]},{"id":"3d98c88b-ccae-4d7a-804a-b329b8afbdb5","domain":"awscsapro-domain1","question":"You work for a freight truck operating company that operates a website for tracking the realtime location of trucks. The website has a 3-tier architecture with web and application tiers communicating with a PostgreSQL database. Under average load, the website requires 8 web servers and 3 application servers, with average CPU consumption at 85%. Though the experience will suffer, the application can still operate with 6 web servers and 2 application servers; however, in that case, the CPU usage is close to 100%. You are deploying this application in us-west-2 region which has four AZs (Availability Zones). Select the architecture that provisions maximum availability and the ability to withstand the loss of up to two Availability Zones at the same time, being cost-efficient as well.","explanation":"The key to answering this kind of question is using a simple mathematical formula. If the requirement states that the least number of Availability Zones which will still be functioning after a catastrophic loss is X, then the servers I need per AZ is n such that n * X = minimum number of EC2 Instances required by the application.\nLet us apply this formula to the web-tier. The minimum number needed for the application to function is 6. If I am spreading my instances across 4 AZ-s, then X = 2 because I must be able to withstand the loss of two AZ-s. Therefore, n * 2 = 6. Thus, n = 3. Which means I need 3 web servers in each of my AZ-s. Let us reverse calculate with that number to be sure. If I have 3 web servers per AZ, and I have 4 AZ-s, I have 12 web servers running. Now if 2 AZ-s fail, I will still have 6 web servers running. Thus, my application will still perform, which is the requirement.\nThere are a few additional interesting aspects to this kind of problems. First, note that some of the answer choices use 3 AZ-s instead of all 4. This is because the exam wants to penalize inattentive reading, in case you miss that detail. Second, using the above formula sometimes results in multiple correct answers. In that case, we should choose the one with the lower total number of EC2 instances because that will be the lower-cost option. Thirdly, some of the answer choices would be correct if you are trying to make the architecture withstand the failure of 1 AZ (and not 2). This again is trying to penalize inattentive reading in case you are in a hurry, read the question partially, and try to find the correct answer based on 1 AZ going down instead of two, which is the stated requirement.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html","title":"AWS Regions and Availability Zones"},{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html#arch-AutoScalingMultiAZ","title":"Distributing Instances Across Availability Zones"}],"answers":[{"id":"3bff2d20d9b822da03cdfc315586449e","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning all 4 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 2 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":false},{"id":"2a80c20c82cfb8e8bd358b60ee1952a5","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning all 4 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 3 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":true},{"id":"0d286ae78f17320b7244d3c53be500ba","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning 3 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 3 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":false},{"id":"698838baa59961738cfcdc08cbdcd653","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning 3 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 6 EC2 Instances in each Availability Zone for the web-tier. Deploy 2 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS with Read Replicas.","correct":false}]},{"id":"0a4b2449-9275-4c2f-af02-0f8c51614f3a","domain":"awscsapro-domain2","question":"You are part of a business continuity team at a consumer products manufacturer.  In scope for the current project is the company web server which serves up static content like product manuals and specification sheets which customers can download.  This landscape consists only of a single NGINX web server and 5TB of local attached storage for the static content.  In the case of a failover, RTO has been defined as 15 minutes with RPO as 24 hours as the content is only updated a few times a year.  Staff reductions and budget constraints for the year mean that you need to carefully evaluate and choose the most cost-effective and most automated solution in the case of a failover.  Which of the following would be the most appropriate given the situation?","explanation":"In this case, the most cost-effective and most automated way to ensure the reliability statistics would be to migrate the static content to S3.  This option has built-in robustness and will cost less than any other option presented.","links":[{"url":"http://d36cz9buwru1tt.cloudfront.net/AWS_Disaster_Recovery.pdf","title":"Using Amazon Web Services for Disaster Recovery"}],"answers":[{"id":"925fb4f48c2c90011e7e1f92d3412dcd","text":"Migrate the static content to an EFS share.  Mount the EFS share via NFS from on-prem to serve up the web content.  Configure another EC2 instances with NGINX to also mount the same share.  Upon fail-over, manually redirect the Route 53 record for the web server to the IP address of the EC2 instance.","correct":false},{"id":"69fd92fc5be4948bfc0128d02ed2f392","text":"Install the CloudWatch agent on the web server and configure an alarm based on a health check.  Create an EC2 replica installation of the web server and stop the instances.  Create a Lambda function that is triggered by the health check alarm which starts the dormant EC2 instance and updates a DNS entry in Route 53 pointing to the new server.","correct":false},{"id":"bceaccaea071754d3724eaf31f0f6189","text":"Migrate the website content to an S3 bucket configured for static web hosting.  Create a Route 53 alias record for the web server domain.  End-of-life the on-prem web server.","correct":true},{"id":"46b84866da301243767946743c6024a1","text":"Download and configure the AWS Storage Gateway, creating a volume which can be replicated to AWS S3.  Attach that volume to the web server via iSCSI and migrate the content to that Storage Gateway volume.  Locate an AMI from the AWS Marketplace for NGINX.  If a failover is required, manually launch the AMI and run an RSYNC between the on-prem server and the EC2 server to migrate the content.","correct":false},{"id":"2d14eb477a2f2c9dc3605ea5740297cd","text":"Create a small pilot-light EC2 instance and configure with NGINX. Configure a CRON job to run every 24 hours that syncs the data from the on-prem web server to the pilot-light EC2 EBS volumes.  Configure an Application Load Balancer to direct traffic to the on-prem web server until a health check fails.  Then, the ALB will redirect traffic to the pilot light EC2 instances. ","correct":false}]},{"id":"0bfd631e-c08c-406a-acf5-a07416aab129","domain":"awscsapro-domain5","question":"Your team uses a CloudFormation stack to manage AWS infrastructure resources in production. As the AWS resources are used by a large number of customers, the update to the CloudFormation stack should be very cautious. Your manager asks for additional insight into the changes that CloudFormation is planning to perform when it updates the stack with a new template. The change needs to be reviewed before being applied by a DevOps engineer. What is the best method to achieve this requirement?","explanation":"CloudFormation Change Sets are able to provide the information on how the running resources are affected by a stack update. The outputs can be reviewed before being executed. Users can view the Change Set through AWS Console or CLI. The Retain option in the DeletionPolicy, CloudFormation stack policy or termination protection helps on protecting the stack resources. However, they cannot provide a summary of  changes in a stack update.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"4657e544cc1daf4315865e230d92dd00","text":"For key AWS resources in the CloudFormation stack, add a Retain option in the DeletionPolicy attribute, which prevents the resources from being accidentally deleted by a stack update. Add a Delete option for the resources that you want to delete along with the stack.","correct":false},{"id":"550c80eaf15eeb89d49aa2a86eb747a6","text":"Enable termination protection in the CloudFormation stack so that the AWS resources cannot be accidentally deleted or modified. Disable the protection only if the changes are approved. Execute the changes in a maintenance window.","correct":false},{"id":"700aa7cb0f0e8cfb417b67ae5d49e962","text":"Add a CloudFormation stack policy to prevent updates to stack resources. Only after the changes are reviewed and approved, change the stack policy to allow the stack update. Revert the stack policy after the change.","correct":false},{"id":"935df51ce2c7f07994c2b8a257489e00","text":"Create a CloudFormation Change Set using AWS Management Console or CLI, review the changes to see if the modifications are as expected and execute the changes to update the stack.","correct":true}]},{"id":"1141835f-8419-4642-8777-81c354976178","domain":"awscsapro-domain2","question":"You are implementing a new eCommerce system for your organization.  It requires Red Hat Linux and uses either multicast or external cache (Redis or Memcached) to share sessions.  You need to implement SSL but do not want to manage individual certificates on each EC2 instance.  Additionally, you want to be sure all parts of the landscape are setup for high availability.  Which of the following architectures best fits the situation at the least cost?","explanation":"Because multicast is not supported in VPCs, we have to use a cache.  Redis supports more high availability configurations than Memcached.  Exclusive use of a spot fleet could leave us with no running instances, so we avoid that option.","links":[{"url":"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/SelectEngine.html","title":"Comparing Memcached and Redis - Amazon ElastiCache for Redis"}],"answers":[{"id":"2cfd86b2c2bf79a4de540a7db3b9949b","text":"Use ElastiCache for Memcache as a session cache.  Use an Application Load Balancer attached to an auto scaling group of RHEL instances.  Use Certificate Manager to assign a certificate to the load balancer and terminate SSL there.","correct":false},{"id":"38b528a48d4729c06200bead282115fb","text":"Use an Application Load Balancer attached to an auto scaling group of RHEL instances.  Enable multicast support in the VPC containing the web servers.  Use Certificate Manager to assign a certificate to the ALB and terminate SSL there.","correct":false},{"id":"f046d563a942b8afa2bb70fa9daf2b13","text":"Use an Application Load Balancer attached to a spot fleet of RHEL.  Use ElastiCache for Redis as a session cache.  Use Certificate Manager to assign a certificate to the ALB and terminate SSL there.","correct":false},{"id":"f792a5026bd6de7e4734492c311889f2","text":"Use ElastiCache for Memcached as a session cache.  Use a Network Load Balancer attached to an auto scaling group of RHEL instances.  Use Certificate Manager to assign a certificate to the load balancer and terminate SSL there.","correct":false},{"id":"55247c98d5a84134fd3e58f8b41b977f","text":"Use ElastiCache for Redis as a session cache.  Use an Application Load Balancer attached to an auto scaling group of RHEL instances.  Use Certificate Manager to assign a certificate to the ALB and terminate SSL there.","correct":true}]},{"id":"35fc536d-d968-472d-84d5-a7ae5d343564","domain":"awscsapro-domain1","question":"You work for a genetics company that has extremely large datasets stored in S3. You need to minimize storage costs, while maintaining mandated restore times that depend on the age of the data. Data 30-59 days old must be available immediately, and data â‰¥ 60 days old must be available within 12 hours. Which of the following options below should you consider?","explanation":"You should use S3 - IA for the data that needs to be accessed immediately, and you should use Glacier for the data that must be recovered within 12 hours. S3 - RRS and 1Zone-IA would not be suitable solution for irreplaceable data or data that required immediate access (reduced Durability or Availability), and CloudFront is a CDN service, not a storage solution.  The use of absolute words like 'Must' is an important clue as it will eliminate options where the case may not be possible such as with OneZone-IA.","links":[{"url":"https://aws.amazon.com/s3/faqs/#sia","title":"S3 - Infrequent Access"},{"url":"https://aws.amazon.com/s3/faqs/#glacier","title":"About Glacier"}],"answers":[{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false},{"id":"31e831ec49678aed7f467f791d1f8704","text":"S3 - RRS","correct":false},{"id":"e9a5105fa288ef2b71c037e42d665d91","text":"S3 - OneZone-IA","correct":false},{"id":"4340570ba672bfa48cd45e3f026c01d1","text":"S3 - IA","correct":true},{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":true}]},{"id":"edc23d5d-e9ea-4713-b8c2-e35a1aa13626","domain":"awscsapro-domain2","question":"You are helping a company design a fully cloud-based Customer Service application.  Over 50% of their Customer Service Representatives are remote and that number increases and decreases seasonally.  They need the ability to handle inbound and outbound calls as well as chatbot capabilities.  Additionally, they want to provide a self-service option using interactive voice response to customers who do not need to speak to a person.  Which design is feasible and makes most efficient use of AWS services?","explanation":"AWS Connect is Amazon's \"call center in a box\" solution that enabled interactive voice response with Lex and inbound and outbound calling.  Additionally, you can use Lex to build a chatbot.  AWS Workspaces is a managed DaaS that is we suited for deploying to remote workers.","links":[{"url":"https://aws.amazon.com/connect/","title":"Amazon Connect Overview"},{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html","title":"What Is Amazon WorkSpaces? - Amazon WorkSpaces"}],"answers":[{"id":"29d3b6d296574b52ab6aa78419ca5aad","text":"Create a standardized Customer Service Rep desktop and deploy via CloudFront.  Use Translate and AWS Connect to create a chatbot component.  Leverage Polly to create an interactive voice response component.  Use Alexa for Business for the inbound and outbound calling.","correct":false},{"id":"22128c4eb6db1f8d7e92b3fbb7155565","text":"Use AWS Comprehend to create the chatbot and interactive voice response components.  Use Asterisk PBX from AWS Marketplace to handle the inbound and outbound calling.  Create a standardized Customer Service Rep desktop and deploy using Service Catalog.","correct":false},{"id":"da552d1c23cba5e3e05a5dca7bdc0ca5","text":"Setup Twilio with Lambda to manage inbound and outbound calling.  Create a standard Customer Service Rep desktop Windows AMI and deploy via Service Catalog.  Leverage Polly for creating a chatbot and Translate for an interactive voice response system.","correct":false},{"id":"8ed07bd636bec143bc1f7e2ce888bb03","text":"Create a standard Customer Service Rep desktop and deploy using AWS Workspaces. Setup AWS Connect for inbound and outbound calling.  Leverage Alexa for Business to create chatbot and interactive voice response components.  Store call logs in Redshift and analyze using Quicksight.","correct":false},{"id":"4086f40e467d8d2fadaac8b5b9a2d49b","text":"Setup AWS Connect for inbound and outbound calling.  Make use of Polly and Lex for interactive voice response components.  Create a standard Customer Service Rep desktop and deploy using AWS Workspaces.  Leverage Lex to create a chatbot component.","correct":true}]},{"id":"49107f33-5b31-4d7e-a2cb-95f3ce8a2d75","domain":"awscsapro-domain1","question":"Your customer has setup AWS Organizations to help manage a collection of AWS Accounts.  They are running into a problem though and need your help.  They have created accounts for each business unit and applied SCPs to those OUs. However, they notice that root accounts in in those sub-accounts can still change root access keys and disable MFA.  How do you instruct your customer?","explanation":"Service Control Policies can control many aspects but they cannot restrict root account actions of changing root access keys or disabling MFA.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"94a2f948d3d2f9c317a6ebb1f5a24ea5","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the specific sub-accounts.","correct":false},{"id":"3d722952f024bcec9174a311c17dcc14","text":"You can not use SCPs to restrict root account activities of changing the root password or managing MFA settings.","correct":true},{"id":"194cc2d07b5c378b62b1e090f0aea956","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the entire OU in the root account.","correct":false},{"id":"77df34553819fdc2e31fb79762948993","text":"You can establish a trust with the top-level account and use the \"organizations:ServicePrinciple\" condition key to restrict root access at the sub-account level.","correct":false}]},{"id":"1239c235-107c-4f5e-8bac-9dc824c00680","domain":"awscsapro-domain5","question":"You are helping a client with some process automation.  They have managed to get their website landscape and deployment process encapsulated in a large CloudFormation template.  They have recently contracted with a third-party service to provide some automated UI testing.  To initiate the test scripts, they need to make a call out to an external REST API.  They would like to integrate this into their existing CloudFormation template but not quite sure of the best way to do that.  Help them decide which of the following ideas is feasible and incurs the least extra cost.","explanation":"To integrate external services into a CloudFormation template, we can use a custom resource.  Lambda makes a very good choice for this scenario because it can handle some logic if needed and make a call out to an external API.  Using an EC2 instances to make this call is excessive and we likely would not have the ability to configure the third-party API to poll an SQS queue.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html","title":"AWS Lambda-backed Custom Resources - AWS CloudFormation"}],"answers":[{"id":"6b3b26e17f2323a91f04f792f0c2d20c","text":"Create a Lambda function which issues a call out to the external REST API using the POST method.  Define a custom resources in the CloudFormation template and associate the Lambda function and execution role with the custom resource.  Include DependsOn to ensure that the function is only called after the other instances are ready.","correct":true},{"id":"43569df3ec4b7db0265dea4051c04644","text":"Add a small EC2 instance definition to the CloudFormation template.  Define a User Script for that instance which will install a custom application from S3 to call out to the external REST API endpoint using the POST method to trigger the testing process.  Add a CleanUp parameter to the EC2 instance definition that will shut down the instance once the activity has completed.","correct":false},{"id":"97a123d11bbf3be0e3e1788e2f0874ac","text":"Add an API Gateway deployment to the CloudFormation template.  Add the DependsOn parameter to the API Gateway resource to ensure that the call to the external API only happens after all the other resources have been created.  Create a POST method and define it as a proxy for the external REST API endpoint.  Using SWF, call the API Gateway endpoint to trigger the testing process.","correct":false},{"id":"f761e84ee0cd0f689465458a41b69fae","text":"Include an SQS queue definition in the CloudFormation template.  Define a User Script on the deployed EC2 instance which will insert a message into the SQS queue only once it has fully booted.  Configure the external REST API to use long polling to check the queue for new messages in order to initiate the testing process.","correct":false}]},{"id":"73708d6f-e6cb-4b8f-90d9-723a2961496e","domain":"awscsapro-domain2","question":"Your team is architecting an application for an insurance company.  The application will use a series of machine learning methods encapsulated in an API call to evaluate claims submitted by customers.  Whenever possible, the claim is approved automatically but in some cases were the ML API is unable to determine approval, the claim is routed to a human for evaluation.  Given this scenario, which of the following architectures would most aligned with current AWS best practices?","explanation":"Formerly, AWS recommended SWF for human-involved workflows.  Now AWS recommends Step Functions be used as it requires less programmatic work to build workflows and is more tightly integrated into other AWS services.","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html","title":"Starting a State Machine Execution in Response to Amazon S3 Events - AWS  Step Functions"}],"answers":[{"id":"837fa3e7c6b347eddfa7fefd2a092017","text":"Create a workflow using Simple Workflow Service and an EC2 fleet to host worker and decider programs.  Create worker programs for each processing step and ML API call.  Create decider program to receive the output of the API and decide if the claim is approved.  For unapproved claims, create a worker program use the WorkMail SDK to place the unapproved claim into a mailbox to be reviewed by a human.","correct":false},{"id":"dfc86c259de4881886ffdac5b8106777","text":"Create a State Machine using Step Functions and a Lambda function for calling the API.  Intake the claims into an S3 bucket configured with a CloudWatch Event.  Trigger the Step Function from the CloudWatch Event.  Create an Activity Task after the API check to email an unapproved claim to a human.","correct":true},{"id":"fa669ed2f0666420f19ad9c8836509f6","text":"Take in the claims into an SQS queue.  Create a Lambda function to poll the SQS queue, fetch the claim and submit the API call.  Use another Lambda function to evaluate the API results and if the claim is not approved, place the claim in a dead letter queue.  Train a person to periodically log into the SQS console and read the dead letter queue for review.","correct":false},{"id":"5298caaea3a55734efa8c62e637d0d40","text":"Use Kinesis to take in the claims and save them on S3 using Firehose.  Use Sagemaker to analyze the claims on S3 as a training set and devise a decider function.  Save the approved claims to another S3 bucket setup with an Event to trigger an SES message to a reviewer.","correct":false}]},{"id":"edb30172-3f76-4423-a6bb-78a3d2fdeb42","domain":"awscsapro-domain2","question":"Your team is managing hundreds of Linux and Windows EC2 instances in different environments such as development, QA, staging and production. You need a tool to help you automate the process of patching instances so that the operating systems have latest patches and meet the compliance policies. You want to manage the patching in different groups depending on the environment. For example, patches should be deployed and tested in the QA environment first before the production environment. How would you achieve this requirement through an AWS service?","explanation":"AWS Systems Manager Patch Manager is the most appropriate tool to manage the patching for large groups of EC2 or on-premises instances. For different environments, users can configure patch groups using the \"Patch Group\" tag and then establish a patch baseline for each patch group. It is better to manage instances with the \"Patch Group\" tag rather than other customized tags. AWS SSM Session Manager and AWS SSM Run Command are not suitable to deploy patches across a large number of instances. The AWS-RunRemoteScript command is also incorrect as it is used to execute scripts stored in a remote location.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-patchgroups.html","title":"Patch Manager Patch Groups"}],"answers":[{"id":"6c4763d45c6cb24622b0db9533f95e0c","text":"Create environmental tags in EC2 instances such as a tag key named \"env\". In AWS SSM Patch Manager, configure patching activities by selecting the instances using the tag. Patch on the QA environment first and perform the necessary testing.","correct":false},{"id":"08b9feba2a12ba67141a0ffe02938798","text":"Add patch group tags in the EC2 instances. Perform the patching using the command AWS-RunRemoteScript in AWS SSM Run Command. Patch on the QA environment first by selecting the QA patch group tag.","correct":false},{"id":"32ac8d27221ec8e699eb4e872d3f6ed0","text":"Centrally manage the instances in AWS SSM Managed Instances and divide them into different categories. Perform the patching activities from AWS SSM Session Manager in a maintenance window.","correct":false},{"id":"b99f4271e073e1e030f3c26c383c5959","text":"In AWS Systems Manager Patch Manager, create different patch groups using the tag key \"Patch Group\" and configure a patch baseline for each patch group. Schedule the patching in a maintenance window by selecting a patch group.","correct":true}]},{"id":"2eee6f1c-96d7-4d2b-821f-4ce8acaf3de3","domain":"awscsapro-domain5","question":"You've deployed a mobile app for a dance competition television show's viewers to vote on performances. The app's backend leverages Amazon API Gateway, AWS Lambda, and Amazon RDS Oracle, with voting activity going from devices directly to API Gateway. In the middle of the broadcast, you begin receiving errors in CloudWatch indicating that the database connection pool has been exhausted. You also see log entries in CloudWatch with a 429 status code. After the show concludes, ratings for the app indicate a very poor user experience, with multiple retries needed to cast a vote. What would be the best way to increase the scalability of the app going forward?","explanation":"Placing Kineses between API Gateway and Lambda decouples the architecture, making use of an intermediary service to buffer incoming requests. The 429 status code indicates a Lambda concurrency throttling error, which you can resolve by controlling the Kinesis batch size per batch delivery. Database sharding will increase scalability, but will still have an upper limit of capacity. Increasing available Lambda memory will have no effect. Inserting a Lambda traffic manager doesn't address the database scalability issues, nor does increasing the regional Lambda concurrency limit. Modifying RDS DB Parameter Group values will require a database restart to take effect, which won't be feasible during live voting activity.","links":[{"url":"https://aws.amazon.com/blogs/architecture/how-to-design-your-serverless-apps-for-massive-scale/","title":"How to Design Your Serverless Apps for Massive Scale"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/scaling.html","title":"AWS Lambda Function Scaling"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","title":"Using AWS Lambda with Amazon Kinesis"}],"answers":[{"id":"2f66b519925956d6a91d0026056904d9","text":"Have API Gateway route requests to a new Lambda function that manages traffic and retries for the voting logic Lambda function. Request that the regional function concurrency limit be increased based on volume projections","correct":false},{"id":"8f962935878f87ebc9523dc54f505886","text":"Scale the database horizontally by creating additional instances and use sharding to distribute the data across them. Provide the Lambda function with a mapping of the sharding scheme in DynamoDB. Increase the amount of memory available to the Lambda function during execution","correct":false},{"id":"a4051078cf0289e689e79fe70eab1f14","text":"Create a separate Lambda function to increase the maximum DB connection value in the RDS DB Parameter Group when a CloudWatch Metrics DB connection threshold is exceeded. Invoke Lambda functions with an 'event' invocation type to retry failed events automatically","correct":false},{"id":"6b6e6fb8b54db42df4d343376ccd8c60","text":"Insert Amazon Kinesis between API Gateway and Lambda, and configure Kinesis as an event source for Lambda. Set the number of records to be read from a Kinesis shard to an optimal value based on volume projections","correct":true}]},{"id":"c9f44641-660b-4c42-9380-9e7f6b0a9ba4","domain":"awscsapro-domain4","question":"As the solution architect, you are assisting your customer design and develop a mobile application using API Gateway, Lambda and DynamoDB. S3 buckets are being used to serve static content. The API created using API Gateway is protected by WAF. The development team has just staged all components to the QA environment. They are using a load testing tool to generate short bursts of a high number of concurrent requests sent to the API Gateway method. During the load testing, some requests are failing with a response of 504 Endpoint Request Timed-out Exception.\nWhat is one possible reason for this error response from API Gateway endpoint?","explanation":"The SA-P exam sometimes focuses on knowledge of response codes from API Gateway and what each distinct HTTP response code could mean.\nThe key to answering this question correctly is being able to distinguish between 4XX and 5XX HTTP error response codes. Though AWS has not been entirely consistent in their error code assignment philosophy, 4XX usually happens any time throttling kicks in because the request in that case never makes to an instance of Lambda function. 5XX happens when a Lambda function is actually instantiated, but some error (like time out) happened inside the Lambda function. One sneaky way to remember this is the fact that 5XX errors are called server errors in HTTP-land, so to generate a 5XX a server process must exist (and must have failed). Of course, in this context, the HTTP server process is a Lambda function - so in scenarios where throttling prevented a Lambda function from getting spawned, the response code cannot be 5XX. This is not consistently followed by AWS API Gateway error design, though, as we can see that AUTHORIZER_CONFIGURATION_ERROR and AUTHORIZER_FAILURE are both 500, though no Lambda function is actually spawned in either case. However, the candidate must remember that throttling always results in 4XX codes. An Endpoint Request Timed-out Exception (504) suggests that the requests in question actually made its way past the API Gateway into a Lambda function instance.\nFor the scenario where request rate exceeds API Gateway limits, the request would be blocked by API Gateway itself. The response would be 429. The exact knowledge of the code 429, however, is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe scenario where 1000 Lambda functions are already running is a similar example of throttling - the 1001st Lambda function will not even be spawned. The response, again, will be 429. However, the exact knowledge of the code 429 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe WAF scenario is yet another example of the request not even crossing the protections placed at the gateway level. If WAF is activated on API Gateway, it will block requests when the rate exceeds the HTTP flood rate-based rule (provided all such requests come from a single client IP address). However, the response, again, will be in the 4XX area (specifically, 403 Forbidden) - however, the exact knowledge of the code 403 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThis leaves Lambda time-out as the only correct answer. The mention of 30 seconds or more is a diversion tactic, in case candidate believes that the relevant Lambda time-out is 5 minutes. A given Lambda function instance may have a time-out limit of 5 minutes, but when it is invoked from API Gateway, the timeout imposed by API Gateway is 29 seconds. If a Lambda function runs for longer than 29 seconds, API Gateway will stop waiting for it and return 504 Endpoint Request Timed-out Exception.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html","title":"Amazon API Gateway Limits and Important Notes"},{"url":"https://aws.amazon.com/blogs/compute/amazon-api-gateway-adds-support-for-aws-waf/","title":"Amazon API Gateway adds support for AWS WAF"}],"answers":[{"id":"82f6a6eb1a4aeef5dd34f21fcd2069ef","text":"The number of requests generated by the load testing framework has exceeded the threshold for the HTTP flood rate-based rule set in the WAF settings for the stage in question","correct":false},{"id":"98daaa701198f7e1c541fe8051799129","text":"The Lambda function is sometimes taking 30 seconds or more to finish executing","correct":true},{"id":"370770015087b4ff70656089fc9e3316","text":"The test is triggering too many Lambda functions concurrently. AWS imposes a soft limit of 1000 concurrent Lambda functions per region","correct":false},{"id":"8090c6b0fe9036ec84fce24b16a7dc10","text":"The load testing tool has exceeded the soft limit for request rate allowed by API Gateway","correct":false}]},{"id":"6dc7fe81-03aa-45d6-b8e1-6dc3b70914e0","domain":"awscsapro-domain1","question":"A company owns multiple AWS accounts managed in an AWS Organization. You need to generate daily cost and usage reports that include the activities of all the member accounts. The reports should track the AWS usage for each resource type and provide estimated charges. The report files also need to be delivered to an Amazon S3 bucket for storage. How would you create the required reports?","explanation":"The consolidated billing feature in AWS Organization does not generate billing reports automatically. You need to configure the AWS Cost and Usage Reports in the master account and use an S3 bucket to store the reports. The generated reports include activities for all the member accounts and it is not required to create a report in each member's account. The option of CloudWatch Event rule and Lambda function may work however it is not a straightforward solution.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-reports-costusage.html","title":"AWS Cost and Usage Report"}],"answers":[{"id":"149237c5674e21794204a4a0ca00bee2","text":"Create a CloudWatch Event rule that runs every day. Register a Lambda function target which calls the PutReportDefinition API to get cost reports of all AWS accounts and store them in an S3 bucket.","correct":false},{"id":"967b7874a080c033470777ce955a4550","text":"In the master account of the AWS Organization, generate the AWS Cost and Usage Reports and save the reports in an S3 bucket. Modify the bucket policy to allow the billing reports service to put objects.","correct":true},{"id":"033f176fcb8f66b1ee9fb950c8741cda","text":"Enable the consolidated billing feature in the AWS Organization which automatically generates a daily billing report. Predefine an S3 bucket to store the reports. Make sure the S3 bucket has a bucket policy to allow the AWS Organization service to write files.","correct":false},{"id":"b66bf1016802f0edb247437b5fda31cb","text":"Login in each AWS account using the root IAM user, configure the daily Cost and Usage Report and set up a central S3 bucket to save the reports from all AWS accounts. Store the reports in different folders in the S3 bucket.","correct":false}]},{"id":"1aa1b2b9-bb81-44d6-bcee-ec9408859b70","domain":"awscsapro-domain4","question":"A small business owner owns two businesses - a restaurant and a pet grooming facility. Each of her businesses has its own website, respectively www.xyz-restaurant.com and www.pqr-pet-grooming.com, both supporting only HTTP (no HTTPS). One single EC2 instance on AWS serves both the websites. This Ec2 instance has an Elastic IP Address attached to it, with two Route 53 A records pointing at it. The owner wants to add credit card payment processing to both her websites. As a result, she wants to ensure that both sites are served on HTTPS. Also, due to the popularity of her businesses, she needs to run a second EC2 instance which will be an exact copy of the existing one. She plans to create the second instance from the AMI snapshot of the first one, hoping to not needing to make any website configuration or code changes on any of the instances. She wants the traffic for each site to be equally or randomly split between the two servers though all requests from the same user-session must reach the same server.\nAs an AWS architect, what approach would you recommend for functionality as well as cost-effectiveness?","explanation":"This question tests several aspects of Load Balancers, knowledge of SNI and SAN, Sticky Sessions and Route 53.\nApplication Load Balancer supports SNI - hence it can deal with multiple SSL certificates per Listener. Thus, the option that says that a single ALB can handle a single SSL certificate only is incorrect. Also, as the question states that no code or website configuration changes can be done, we cannot terminate SSL at the EC2 instances - as doing so would surely need changes to the website configuration if not both code and website configuration. This eliminates the options that propose the Classic Load Balancer or a completely Load-Balancer-less solution.\nThe Classic Load Balancer option would technically work as long as the website configuration can be updated to terminate SSL at the EC2 instance. In fact, that would be the lowest cost solution from a Load Balancer perspective as CLB costs less than ALB - however, terminating SSL at the EC2 instances usually requires greater compute capacity, so the cost of EC2 instance could go up slightly. Note that the CLB option correctly states that SAN must be used with CLB, as CLB does not support SNI. Also, when SAN is used with CLB, it correctly states that the listener must be configured as TCP instead of HTTPS as the SSL termination must occur at the EC2 instance in that case. However, the only reason this option cannot be selected as correct is the restriction imposed on not changing the AMI at all. We cannot terminate SSL on the EC2 instance without changing the AMI or configuring the website.\nThe Load-Balancer-less solution that achieves routing via Route 53 has two problems - one is terminating SSL at the EC2 instance. The other is the fact that Route 53 does not provide Sticky Sessions.\nHence, the right answer is to use SNI with both SSL certificates bound to the same ALB listener, use Sticky Sessions, and use Route 53 to CNAME the website domains to the ALB","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/","title":"SNI and ALB"},{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-https-load-balancers.html","title":"Classic Load Balancer Listener Configuration for SAN"}],"answers":[{"id":"d1e651efb4dab3eecdbcda26841cba22","text":"Procure an SSL certificate for one of the domains and add a Subject Alternative Name (SAN) for the other domain to the same certificate. Deploy a Classic Load Balancer with a single TCP Listener. Deploy the SAN certificate to the Classic Load Balancer. Add both the instances as Targets to the CLB. Turn on Session Affinity on the CLB. Terminate SSL on the EC2 instances. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the CLB","correct":false},{"id":"676aa985e464f22918d04ea588f87ae4","text":"Procure SSL certificates for both the domains. Attach a second Elastic IP Address to the second EC2 instance. Add two Multivalue-answer records to Route53, one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, each of type A Record, and each with both the Elastic IP addresses - one for each domain name, one domain per hosted zone. This will ensure traffic being equally split between the two instances. Terminate SSL at the EC2 instances. Turn on Session Affinity for both the Route 53 Hosted Zones","correct":false},{"id":"8b618d3847d47090857df2e1dd20b574","text":"Procure SSL certificates for both the domains. Deploy an Application Load Balancer with a single HTTPS Listener. Bind both certificates to the Listener on the Load Balancer. Add both the instances as Targets belonging to the same Target Group to the ALB, assigning the Target Group to the Listener. Turn on Sticky Sessions for this Target Group. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the ALB","correct":true},{"id":"5344d22fe688e9bd0ec7d0ff5a2ca246","text":"Procure SSL certificates for both the domains. Deploy an Application Load Balancer. As a single ALB can only handle a single SSL certificate, SSL termination must be now done at the EC2 instance. Hence, create a TCP/443 Listener on the ALB. Add both the instances as Targets belonging to the same Target Group to the ALB, assigning the Target Group to the Listener. Turn on Sticky Sessions for this Target Group. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the ALB","correct":false}]},{"id":"f679d23d-14d4-4021-9749-481bbe11046c","domain":"awscsapro-domain3","question":"A telecommunications company has decided to migrate their entire application portfolio to AWS. They host their customer database and billing application on IBM mainframes. IBM AIX servers running WebSphere provide an API layer into the mainframes. Customer-facing online applications are hosted on Linux systems. The customer service backend application resides on Oracle Solaris and makes use of gigabytes of persistent information. Their ERP and CRM systems also run on Solaris boxes. Telecom switches send call records to Linux-based applications, and their employee productivity suite runs on Windows. They need to complete the project in twelve months to satisfy budgetary constraints. Which migration strategy will provide them with the most resilient, scalable, and operationally efficient cloud environment within the project time frame?","explanation":"Since the company has twelve months to complete the project, they can plan for a highly cloud-centric migration. Refactoring the mainframe billing application to EC2 and the customer database to Aurora will require significant cost and effort, but will result in significant intermediate to long-term business value for most companies. A number of AWS Partner Network (APN) solutions are available to assist with this. The WebSphere layer can be replaced by API Gateway with HTTP, REST, or WebSocket APIs that call modules on the EC2 instances. Refactoring the customer-facing online apps to Lambda serverless and Step Functions will provide high operational efficiency. Performing a replatform of the Solaris customer service application to EC2 with Auto Scaling will achieve elasticity to avoid the excess capacity inefficiencies that were most-likely present in the on-premises environment. Many robust ERP, CRM, and employee productivity SaaS solutions exist and should be leveraged rather than trying to manage these applications with in-house staff. The call record processing Linux system can simply be rehosted to EC2. Repurchasing mainframe capacity from a third party provider only extends the rigidness of making mainframe changes whenever new business requirements arise. A Lambda/Step Functions solution will provide all the functionality needed for the online apps, and will be more economical than Elastic Beanstalk. Refactoring the customer service application to Lambda presents issues with processing the gigabytes of persistent information, so replatforming to EC2 is a better choice.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud"},{"url":"https://aws.amazon.com/blogs/apn/automated-refactoring-of-a-new-york-times-mainframe-to-aws-with-modern-systems/","title":"Automated Refactoring of a New York Times Mainframe to AWS with Modern Systems"},{"url":"https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/","title":"Build a Serverless Web Application"}],"answers":[{"id":"290da60b3bdd11f28e175fa86972386a","text":"Repurchase mainframe capacity from a third party provider and run the customer database and billing application there. Replace the API layer with Amazon API Gateway. Rehost the customer-facing online applications to Amazon Elastic Beanstalk. Refactor the customer service application to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the ERP and CRM applications to EC2 Linux instances with Auto Scaling. Rehost the call record processing applications, and repurchase SaaS applications for the employee productivity suite.","correct":false},{"id":"57d6d888b4cee16e22852910fa09cdb8","text":"Refactor the mainframe applications onto Amazon EC2 Linux instances, and migrate the customer database to Amazon Aurora. Replace the API layer with Amazon API Gateway. Rehost the customer-facing online applications to Amazon Elastic Beanstalk. Refactor the customer service application to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Repurchase SaaS solutions for the ERP and CRM systems. Rehost the call record processing applications and the employee productivity suite onto EC2.","correct":false},{"id":"c875a6eb6d74f098c971af4a638f288f","text":"Repurchase mainframe capacity from a third party provider and run the customer database and billing application there. Replatform the API layer onto EC2 Linux instances. Refactor the customer-facing online applications to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the customer service, ERP, and CRM applications to EC2 Linux instances with Auto Scaling. Rehost the call record processing applications onto EC2, and repurchase SaaS applications for the employee productivity suite.","correct":false},{"id":"5a404284de876ce2a054dcd916ed80ec","text":"Refactor the mainframe applications onto Amazon EC2 Linux instances, and migrate the customer database to Amazon Aurora. Replace the API layer with Amazon API Gateway. Refactor the customer-facing online applications to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the customer service applications to EC2 Linux with Auto Scaling. Repurchase SaaS solutions for the ERP and CRM systems. Rehost the call record processing applications onto EC2, and repurchase SaaS applications for the employee productivity suite.","correct":true}]},{"id":"0e57f592-af77-4ff7-b32a-752702278ab5","domain":"awscsapro-domain2","question":"Several years ago, the company you are consulting for started an SOA concept to enable more modularity.  At that time, they chose to deploy each microservice as separate LAMP stacks launched in Elastic Beanstalk instances due to the ease of deployment and scalability.  They are now in the process of migrating all the services over to Docker containers.  Which of the following options would make the most efficient use of AWS resources?","explanation":"ECS might be a possible choice, but pods are a concept with Kubernetes and not ECS.  Elastic Beanstalk deployments run in their own self-contained environment and don't share resources.  Of the choices presented, the only feasible choice is K8s on EC2.","links":[{"url":"https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html","title":"Pod Networking - Amazon EKS"},{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html","title":"What Is AWS Elastic Beanstalk? - AWS Elastic Beanstalk"}],"answers":[{"id":"661aa72d0ab709e1931e20010a6e3ad7","text":"Use the Elastic Container Repository and AWS Spectrum to fully automate the deployment and scaling of the Docker containers.","correct":false},{"id":"d93d65de15f35d86079a7a18bda05be2","text":"Package the containers as JAR files and deploy them with Lambda to take advantage of the pay-per-use model","correct":false},{"id":"a473fc9c398c6c24b4c0514eb9fb546a","text":"Configure Elastic Beanstalk to more efficiently run the existing landscapes across a single auto-scaled group rather than separate instances.","correct":false},{"id":"25ffdcfeb30ada0ea8a578689b8d0a49","text":"Create an ECS cluster and configure each container to exist in its own pod.  Use the Calico add-on to manage access to each service.","correct":false},{"id":"f9f924e7f824f3e085294f8e97e7b362","text":"Create an auto-scaled group of EC2 instances and run Kubernetes across them to orchestrate the containers.","correct":true}]},{"id":"9fc0785a-d5cb-47e3-bc2f-829b5a36ba26","domain":"awscsapro-domain3","question":"You work for a Genomics company which has decided to migrate its DNA Sequencing application to the AWS Cloud. The application is containerized. Currently, container image A works on genomics data residing on an on-premises file server, validating the data and updating the metadata in a local database. When it is done, engineers manually trigger 100 or more instances of container image B that process this data in parallel by reading the metadata, creating output files. When all these container instances have done their job, engineers manually trigger container image C that validates the results, cleans up and sends notifications.\nThe CTO has decided to use S3 for storing the input and output data files. She has also mandated that the parallel processing phase should run on a fleet of Spot EC2 instances to reduce compute costs. She also wants to automate the workflow, so that engineers do not have to manually trigger the next set of actions. The requirement is to minimize administrative overhead and custom development for the migration.\nAs the AWS Architect, which of the following approaches should you recommend?","explanation":"AWS ECS does not natively provide workflow management. In an ECS service definition file, you cannot specify a sequence of tasks with execution dependencies such that one will be run only after the previous one completes. Hence, the two ECS choices are ruled out.\nDistraction warning - Fargate does not allow you to specify Spot instances as it is serverless in nature (it absolves you from specifying server details). This effectively creates a distraction - when the candidate rules out ECS Fargate due to this reason, they may be relieved to see the ECS EC2 choice and jump to a conclusion because it is relatively easy to remember that EC2 launch type actually lets you select Spot instances. However, this distraction is designed to take focus away from the fact that neither of these two choices is correct. Both of the choices require service definition files to set up execution workflows. Task instances mentioned in an ECS service definition file are executed in parallel - ECS does not control the sequence of tasks.\nAWS SWF does not let you specify Spot instances either. Also, SWF is usually used in cases where human intervention is needed in the workflow.\nThis leaves AWS Batch as the correct answer. AWS Batch is indeed the most suitable AWS service for this scenario as it meets all requirements.","links":[{"url":"https://docs.aws.amazon.com/batch/latest/userguide/create-compute-environment.html","title":"How to create a compute environment for AWS Batch"},{"url":"https://docs.aws.amazon.com/batch/latest/userguide/example_array_job.html","title":"Example AWS Batch Array Job Workflow"},{"url":"https://aws.amazon.com/ec2/spot/containers-for-less/get-started/","title":"How to run ECS clusters in EC2 Spot Instances"}],"answers":[{"id":"49755d6c34da495b8c91964f52946d29","text":"Use AWS SWF workers and deciders to manage the workflow. Configure the workers to use EC2 Spot Instances","correct":false},{"id":"ceb4c03a526e8ddb01ada7a40bb60001","text":"Use AWS Batch, setting up an array job with 100 or more copies preceded by pre-requisite and follow-up jobs where the workflow is controlled by dependencies between jobs. Also, use Spot as the Provisioning Model for compute environment","correct":true},{"id":"757ddde350053553e44844d066c91386","text":"Use AWS ECS with Fargate Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"e46ada36d33a9e5b23aa37ee94c4c5d6","text":"Use AWS ECS with EC2 Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false}]},{"id":"5af539b7-b132-4a3a-bc80-406c620e7325","domain":"awscsapro-domain1","question":"A food service business has begun an initiative to migrate all applications and data to the AWS cloud. Governance needs to be established before any migrations can occur. Business units such as sales, marketing, and product management have fluctuating infrastructure capacity and security requirements, while other business units like finance, operations, and human resources have more static demand. Security policies and compliance needs vary by project group within each business units. Each business unit is responsible for it's own cost center, and the finance group would like cost reporting to be as streamlined as possible. Which AWS account structure will best satisfy the company's governance needs?","explanation":"Leveraging AWS Organizations to manage an account structure with a core Organizational Unit and Organizational Units for each business unit provides flexibility for future organizational changes. Creating an account for each project group facilitates security policy differences within business units, and limits the exposure of a single security event. Managing differing security requirements by project group in a single account will require more governance maintenance. Creating billing, shared services, and log archive accounts in multiple Organizational Units will result in duplication of services, and can be done at the core level.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-laying-the-foundation/introduction.html","title":"Laying the Foundation: Setting Up Your Environment for Cost Optimization"},{"url":"https://aws.amazon.com/solutions/aws-landing-zone/?did=sl_card&trk=sl_card","title":"AWS Landing Zone"}],"answers":[{"id":"a66d8391267460b5800c5c3d07921767","text":"Use AWS Organizations to create Organizational Units for each business unit. Create a billing account, a shared services account, and a log archive account in each Organizational Unit. Create accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"a8f7d8fbb7c6c3a1a14c91577dff42e1","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Place business units with similar security requirements in shared Organizational Units. Create accounts for each business unit in the shared Organizational Units. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"03705913700b8d76205d4203c58dc5e1","text":"Use AWS Organizations with a single Organizational Unit to consolidate costs. Create a billing account, a shared services account, and a log archive account in the Organizational Unit. Create individual accounts for each business unit. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs","correct":false},{"id":"bd400ff0d22480599228a0442d2bb8d4","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Create an Organizational Unit for each business unit that contains accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":true}]},{"id":"d58acd29-561d-4017-a55d-bcff8eb40f90","domain":"awscsapro-domain4","question":"Last month's AWS service cost is much higher than the previous months. You check the billing information and find that the used hours of Elastic Load Balancer (ELB) increases dramatically. You manager asks you to plan and control the ELB usage. When the ELB service has been used for over 5000 hours in a month, the team should get an email notification immediately and further actions will be taken accordingly. Which of the following options is the easiest one for you to choose?","explanation":"AWS Budgets include the types of cost budget, usage budget, reservation budget and savings plan budget. The usage budget enables you to plan the usage of ELB service and receive budget alerts when the actual usage becomes more than a threshold (5000 hours in this scenario). The cost budget type is incorrect as it evaluates the cost instead of usage and you cannot receive budget alerts from either Cost Explorer or AWS Config.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing your costs with Budgets"}],"answers":[{"id":"15fe48e7010a90e06774d9aa9668704a","text":"Create a usage budget in AWS Budgets. Check the ELB running hours for every month and set the budgeted amount to be 5000 hours. Configure an email alert or an Amazon Simple Notification Service (SNS) notification when the actual usage is more than the threshold.","correct":true},{"id":"39f1073c8a5f51031cb743e7cf45dce2","text":"Calculate the estimated ELB cost when the total ELB usage is 5000 hours in a month. Configure a cost budget in AWS Budgets for the EC2 ELB service and set the number as the threshold. When the cost is greater than the user-defined threshold, send an email alert to the team.","correct":false},{"id":"baf09736ecf556311eb6e77579877ccd","text":"In AWS Config, monitor the usage of all ELB resources within the AWS account. Create a custom Config rule via a Lambda function that calculates the ELB usage and sends an alert message to an SNS topic when the usage is over 5000 hours.","correct":false},{"id":"0f88bd3ee9fd3724922aa89baa9f656d","text":"Launch the Cost Explorer in AWS billing dashboard, filter the EC2 ELB service and configure a CloudWatch alert to track its actual monthly usage. When the monthly ELB usage grows more than 5000 hours, raise the CloudWatch alert and notify an SNS topic.","correct":false}]}]}}}}
