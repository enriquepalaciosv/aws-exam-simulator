{"data":{"createNewExamAttempt":{"attempt":{"id":"2221a8ac-aaf3-45dc-8c0f-ba1a144a301e"},"exam":{"id":"0ef24e49-7fe5-4914-997e-9d9ec3b6beee","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"af6501cb-5a69-4af7-a5e7-cf03d6ce09c1","domain":"awscsapro-domain1","question":"As an AWS Solutions Architect, you are responsible for the configuration of your company's Organization in AWS. In the Organization, the Root is connected with two Organizational Units (OUs) called Monitor_OU and Project_OU. Monitor_OU has AWS accounts to manage and monitor AWS services. Project_OU has another two OUs as its children named Team1_OU and Team2_OU. Both Team1_OU and Team2_OU have invited several AWS accounts as their members. To simplify management, all the AWS accounts under Team1_OU and Team2_OU were added with a common administrative IAM role which is supposed to be used by EC2 instances in their accounts. For security concerns, this role should not be deleted or modified by users in these AWS accounts. How would you implement this?","explanation":"The requirement of this scenario is that the IAM role should not be modified by IAM principals in Team1_OU and Team2_OU. The best place to implement this is in SCP as it provides central management. Since Team1_OU and Team2_OU can inherit the SCP policy from Project_OU, only Project_OU needs to attach the SCP that denies the action. In the meantime, the Root should have a default full access policy. It is improper to use IAM policies or Permissions Boundary to achieve this requirement as they may be easily changed at a later stage by other IAM users, and it is also complicated to implement if there is a large number of IAM principals.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"How SCP works in AWS Organization?"}],"answers":[{"id":"f868081ae3429a7ed8318636537ec834","text":"Make sure that Root node has a SCP policy that allows all actions. Create a SCP policy that restricts IAM principals from changing this particular IAM role. Attach the SCP policy to Project_OU.","correct":true},{"id":"e5acb8782282566185828b0ca39813fc","text":"Configure a new SCP policy to prevent IAM principals from deleting or modifying the IAM role. Attach the SCP policy to Project_OU, Team1_OU and Team2_OU. Configure a default full access SCP policy to Monitor_OU.","correct":false},{"id":"77abf7e62575051a27444694461acccd","text":"The nodes of Root, Project_OU and Monitor_OU in the Organization should allow all actions. For AWS accounts in Team1_OU and Team2_OU, attach an IAM policy that denies modifying the IAM role to IAM users, groups and roles.","correct":false},{"id":"46f9090f957b494e2e0075ceef9bac8b","text":"For the AWS accounts in Team1_OU and Team2_OU, configure a Permissions Boundary in IAM principals to prevent them from making modifications to this particular IAM role. As a result, all IAM principals will not have IAM policies to potentially change the role.","correct":false}]},{"id":"4e6b1423-41e3-4d39-93b1-c5c47705477b","domain":"awscsapro-domain2","question":"Given an IP CIDR block of 56.23.0.0/24 assigned to a VPC and the single subnet within that VPC for that whole range, how many usable addresses will you have?","explanation":"For VPCs and subnets, you can use IP addresses that are in RFC1918 or not.  If you choose addresses not in the RFC1918 ranges, you will not be able to route traffic to the internet directly with those addresses.  You would have to use a NAT.  For a /24 netmask, you can expect 251 usable addresses because of the 5 reserved addresses.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html","title":"IP Addressing in Your VPC - Amazon Virtual Private Cloud"}],"answers":[{"id":"6aa49d907a3637314f53838d83286e5d","text":"Zero.  You must use a private IP range as defined in RFC1918 for VPCs.","correct":false},{"id":"c52f1bd66cc19d05628bd8bf27af3ad6","text":"254","correct":false},{"id":"3873a2be62d17c29ac441293ab21e143","text":"You cannot assign the entire CIDR range of a VPC to a single subnet.","correct":false},{"id":"19f3cd308f1455b3fa09a282e0d496f4","text":"251","correct":true},{"id":"f7efa4f864ae9b88d43527f4b14f750f","text":"4096","correct":false}]},{"id":"dc82c397-347d-4f69-bb06-03822238c7a0","domain":"awscsapro-domain1","question":"You are consulting for a large multi-national company that is designing their AWS account structure.  The company policy says that they must maintain a centralized logging repository but localized security management.  For economic efficiency, they also require all sub-account charges to roll up under one invoice.  Which of the following solutions most efficiently addresses these requirements?","explanation":"Service Control Policies are an effective way to broadly restrict access to certain features of sub-accounts.  Use of a single separate logging account is an effective way to create a secure logging repository.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"74a6c6df518100b16da3f16e870b5d5c","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Create localized IAM policies to restrict modification of CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create a centralized security account and establish trust relationships between each sub-account.","correct":false},{"id":"871870fa49beedfb95106595e4a1c9f4","text":"Configure billing for each account to load into a consolidated RedShift instance.  Create a centralized security account and establish trust relationships between each sub-account.  Configure admin roles within IAM of each sub-account for local administrators.  Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  ","correct":false},{"id":"0c9b5a803a99a3d2ef53869b6857c0e0","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use ACLs to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.  Establish trust relationships between the Consolidated Billing account and all sub-accounts.","correct":false},{"id":"cbec34b5388f7f183659e82c20fb3abf","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use an SCP to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.","correct":true}]},{"id":"401cbed4-e977-4303-9344-586af01a4180","domain":"awscsapro-domain2","question":"You have been contracted by a manufacturing company to create an application that uses DynamoDB to store data collected in a automotive part machining process.  Sometimes this data will be used to replay a process for a given serial number but that's always done within 7 days or so of the manufacture date.  The record consists of a MACHINE_ID (partition key) and a SERIAL_NUMBER (sort key).  Additionally, there is a CREATE_TIMESTAMP attribute that contains the creation timestamp of the record and a DATA attribute that contains a BASE64 encoded stream of machine data.  To keep the DynamoDB table as small as possible, the industrial engineers have agreed that records older than 30 days can be purged on a continual basis.  Given this, what is the best way to implement this with the least impact on provisioned throughput.","explanation":"Using DynamoDB Time to Live feature is a perfect way to purge out old data and not consume any WCU or RCU.  Other methods of deleting records would impact the provisioned capacity units.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","title":"Time To Live - Amazon DynamoDB"}],"answers":[{"id":"41f68b5f48ef97cc437ebfe50ae10882","text":"Use AWS Batch to execute a daily custom script which queries the DynamoDB table and deletes those records where CREATE_TIMESTAMP is older than 30 days.  ","correct":false},{"id":"0a945c4865c940ffaddaeade6f6bbdaf","text":"Use Step Functions to track the lifecycle of DynamoDB records.  Once 30 days has elapsed, branch to a Delete step and trigger a Lambda function to remove the record.","correct":false},{"id":"ef62d6ec88d117a0ac0cb7c99cd1abbd","text":"Use DynamoDB Streams to trigger a Lambda function when the record ages past 30 days.  Use the DynamoDB SDK in the Lambda function to delete the record.","correct":false},{"id":"7c29d6c8effb51be4df54033ce45d01f","text":"Enabled Lifecycle Management on the DynamoDB table.  Create a rule that deletes any records where CREATE_TIMESTAMP attribute is greater than 30 days old.","correct":false},{"id":"081f76fb4967f9d10a3799ae400ad898","text":"Update the table to add a attribute called EXPIRE  Change the application to store EXPIRE as CREATE_TIMESTAMP + 30 days.  Enable Time to Live on the DynamoDB table for the EXPIRE attribute.","correct":true}]},{"id":"ef90df82-b36a-41f5-9294-904edfb5830e","domain":"awscsapro-domain1","question":"You are helping a client design their AWS network for the first time.  They have a fleet of servers that run a very precise and proprietary data analysis program.  It is highly dependent on keeping the system time across the servers in sync.  As a result, the company has invested in a high-precision stratum-0 atomic clock and network appliance which all servers sync to using NTP.  They would like any new AWS-based EC2 instances to also be in sync as close as possible to the on-prem atomic clock as well.  What is the most cost-effective, lowest maintenance way to design for this requirement?","explanation":"DHCP Option Sets provide a way to customize certain parameters that are issued to clients upon a DHCP request.  Setting the NTP server is one of those parameters.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_DHCP_Options.html","title":"DHCP Options Sets - Amazon Virtual Private Cloud"}],"answers":[{"id":"40efb671ff85878c28d1fa99329f15c7","text":"Configure your Golden AMI to use Amazon Time Sync Server at 169.254.169.123 and require this AMI to be used.  Use AWS Config to periodically audit the NTP configuration of all AWS assets.","correct":false},{"id":"d94eaf4f8670e4d3f42fbae47250e3e6","text":"Create a dedicated host instance on AWS and place it within a transit VPC.  Configure the server to run NTP as a stratum-2 server.  Ensure NTP (UDP port 123) is allowed inbound and outbound in the Security Groups local to the stratum-2 server.","correct":false},{"id":"2b2197df0ae72d4d462432d7c2f65c98","text":"Create a bridged network tunnel from the on-prem time server to the VPCs on AWS.  Configure the VPC route tables to route NTP (UDP 123) over the tunnel.","correct":false},{"id":"c950359dab3520d6c8c8f4e16a925860","text":"Deploy a third-party time server from the AWS Marketplace.  Configure it to sync from the on-prem time server.  Ensure NTP (UDP port 123) is allow inbound in the NACLs for the VPC containing the third-party server.","correct":false},{"id":"d5ea0e4895105bc1137c03c8bfb28ebe","text":"Configure a DHCP Option Set with the on-prem NTP server address and assign it to each VPC.  Ensure NTP (UDP port 123) is allowed between AWS and your on-prem network.","correct":true}]},{"id":"0e57f592-af77-4ff7-b32a-752702278ab5","domain":"awscsapro-domain2","question":"Several years ago, the company you are consulting for started an SOA concept to enable more modularity.  At that time, they chose to deploy each microservice as separate LAMP stacks launched in Elastic Beanstalk instances due to the ease of deployment and scalability.  They are now in the process of migrating all the services over to Docker containers.  Which of the following options would make the most efficient use of AWS resources?","explanation":"ECS might be a possible choice, but pods are a concept with Kubernetes and not ECS.  Elastic Beanstalk deployments run in their own self-contained environment and don't share resources.  Of the choices presented, the only feasible choice is K8s on EC2.","links":[{"url":"https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html","title":"Pod Networking - Amazon EKS"},{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html","title":"What Is AWS Elastic Beanstalk? - AWS Elastic Beanstalk"}],"answers":[{"id":"661aa72d0ab709e1931e20010a6e3ad7","text":"Use the Elastic Container Repository and AWS Spectrum to fully automate the deployment and scaling of the Docker containers.","correct":false},{"id":"25ffdcfeb30ada0ea8a578689b8d0a49","text":"Create an ECS cluster and configure each container to exist in its own pod.  Use the Calico add-on to manage access to each service.","correct":false},{"id":"a473fc9c398c6c24b4c0514eb9fb546a","text":"Configure Elastic Beanstalk to more efficiently run the existing landscapes across a single auto-scaled group rather than separate instances.","correct":false},{"id":"d93d65de15f35d86079a7a18bda05be2","text":"Package the containers as JAR files and deploy them with Lambda to take advantage of the pay-per-use model","correct":false},{"id":"f9f924e7f824f3e085294f8e97e7b362","text":"Create an auto-scaled group of EC2 instances and run Kubernetes across them to orchestrate the containers.","correct":true}]},{"id":"f02ba751-479b-4ff0-a09b-8f18a63177b5","domain":"awscsapro-domain3","question":"An automotive supply company has decided to migrate their online ordering application to AWS. The application leverages a Model-View-Controller architecture with the user interface handled by a Tomcat server and twenty thousand lines of Java Servlet code. Business logic also resides in two thousand lines of PL/SQL stored procedure code in an Oracle database. The company's technology leadership has directed your team to move the database to a more cost-effective offering, and to adopt a more cloud-native architecture. Business objectives dictate that the application must be live in the AWS cloud in sixty days. Which migration approach will provide the most scalable architecture and meet the schedule objectives?","explanation":"This solution will require trade-offs between schedule requirements and architectural desires. Converting twenty thousand lines of Model-View-Controller code to a serverless architecture in sixty days is unreasonable, so moving the Tomcat MVC as-is to EC2 for the initial migration is the best approach. We can migrate to a serverless user interface in a later phase. Database Migration Service will suit our needs well for moving the application data to Aurora, but the most scalable architecture strategy is to migrate the stored procedure code out of the database so that database nodes won't need to be resized when the business logic needs more compute resources. Under normal circumstances, recoding two thousand lines of PL/SQL code to Python Lambda functions within a sixty day time frame will not be a problem.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/blogs/database/migrate-your-procedural-sql-code-with-the-aws-schema-conversion-tool/","title":"Migrate Your Procedural SQL Code with the AWS Schema Conversion Tool"},{"url":"https://aws.amazon.com/lambda/","title":"AWS Lambda"}],"answers":[{"id":"97a348ed01424c357958b49bcc030935","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"db222d8a15bda541fc4147908131cfd6","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service to move the application data into Amazon Aurora. Convert the stored procedure code to AWS Lambda Python functions, and modify the Servlet code to invoke them","correct":true},{"id":"2cf84f7f8daee7548143ad181423c7cb","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"5ab40b2a54c4e82a7aafa05c8fc9a458","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service to migrate the application data and stored procedures to an Amazon RDS Oracle instance","correct":false}]},{"id":"6da286f8-23a6-4e8a-a3a4-c7b496a06523","domain":"awscsapro-domain5","question":"An online health foods retailer stores its product catalog in an Amazon Aurora database. The catalog contains over 6,000 products. They'd like to offer a product search engine on the website using Amazon Elasticsearch Service. They'll use AWS Database Migration Service (DMS) to perform the initial load of the Elasticsearch indexes, and to handle change data capture (CDC) going forward. During the initial load of the indexes, the DMS job terminates with an Elasticsearch return code of 429 and a message stating 'Too many requests'. What must be done to load the Elasticsearch indexes successfully?","explanation":"When the ElasticSearch indexing queue is full, a 429 response code is returned and an es_rejected_execution_exception is thrown. The DMS load task then terminates. Throttling the DMS input stream based on the number of Elasticsearch indexes, shards, and replicas to be loaded will result in a successfully completed job. The DMS MaxFullLoadSubTasks parameter indicates how many source tables to load in parallel, and the ParallelLoadThreads parameter determines the number of threads that can be allocated for a given table. Increasing Elasticsearch shards without modifying DMS subtask and thread parameters could still overrun the request queue. Changing the DMS stream buffer count won't help with this issue. Amazon Elasticsearch currently doesn't provide support for AWS Glue as a source, so integration would require significant effort. Increasing Elasticsearch EBS volume IOPS won't solve an ingress queue overrun problem. The DMS batch split size parameter sets the maximum number of changes applied in a single batch, but doesn't reduce the total number of requests.","links":[{"url":"https://aws.amazon.com/dms/","title":"Amazon Database Migration Service"},{"url":"https://aws.amazon.com/elasticsearch-service/","title":"Amazon Elasticsearch Service"},{"url":"https://aws.amazon.com/blogs/database/scale-amazon-elasticsearch-service-for-aws-database-migration-service-migrations/","title":"Scale Amazon Elasticsearch Service for AWS Database Migration Service migrations"}],"answers":[{"id":"c1e1a85b26a30433a6af5d30c8bb8d76","text":"Calculate the number of queue slots required for the Elasticsearch bulk request as a product of the number of indexes, shards, and replicas. Adjust DMS subtask and thread parameters accordingly","correct":true},{"id":"ee750f72f1e70b83f6d83819f2d504f5","text":"Raise the baseline IOPS performance of the Elasticsearch cluster EBS volumes to enable more throughput. Increase the DMS batch split size parameter to send more data in each request and reduce the number of total requests","correct":false},{"id":"157e8733386382289486b3592774442f","text":"Increase the number of Elasticsearch shards for each index to increase distribution of the load. Change the DMS stream buffer count parameter to match the number of Elasticsearch shards","correct":false},{"id":"62ac117860ebe5397e04bad8ea29a5fb","text":"Replace DMS with AWS Glue for the initial index load and ongoing change data capture. Enable parallel reads when the ETL methods are called in the Glue jobs","correct":false}]},{"id":"78111d9b-922f-435f-8e91-4ae84e990761","domain":"awscsapro-domain3","question":"A hotel chain has decided to migrate their business analytics functions to AWS to achieve higher agility when future analytics needs change, and to lower their costs. The primary data sources for their current on-premises solution are CSV downloads from Adobe Analytics and transactional records from an Oracle database. They've entered into a multi-year agreement with Tableau to be their visualization platform. For the time being, they will not be migrating their transactional systems to AWS. Which architecture will provide them with the most flexible analytics capability at the lowest cost?","explanation":"AWS Database Migration Service can be configured with an on-premises Oracle database as a source and S3 as a target. It can provide continuous replication between the two. AWS Glue can aggregate the data from S3 according to desired reporting dimensions and store the summaries in Redshift. Keeping the transactional detail in S3 and only keeping the aggregate information in Redshift will save on costs. The same is true for keeping transactional detail in S3 instead of RDS Oracle. AWS Glue is a great solution for transforming the Adobe Analytics CSV files to Parquet format in S3. Parquet's columnar organization will provide excellent performance for Redshift Spectrum queries that join between Redshift tables and S3. Tableau's Redshift connector supports Redshift Spectrum queries. For this use case, using Amazon QuickSight would not make sense since the company has already committed payments to Tableau via their multi-year agreement.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/glue/","title":"AWS Glue"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"1bc635be85a059a6135751bf21fd3550","text":"Use Oracle Data Guard to continuously replicate Oracle transactional data to an Oracle instance on Amazon EC2. Configure AWS Glue to aggregate the transactional data from the Oracle instance for each dimension into Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Redshift. Use Amazon QuickSight to query the data for visualization.","correct":false},{"id":"e1f7ec66baca2c4e27cf072a8ca91424","text":"Employ AWS Database Migration Service to continuously replicate Oracle transactional data to Amazon S3. Configure AWS Glue to aggregate the transactional data from S3 for each dimension into Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Amazon S3 in Parquet format. Install Tableau on Amazon EC2 and write queries to Amazon Redshift Spectrum.","correct":true},{"id":"dc33d336682223190f2d8cb22449cf81","text":"Implement AWS Database Migration Service to continuously replicate Oracle transactional data to an Amazon RDS Oracle instance. Use AWS Glue to write the Adobe Analytics data to the RDS Oracle instance. Install Tableau on Amazon EC2 and write queries against the RDS Oracle database.","correct":false},{"id":"86c10f6cca438461e60f5c04886f57c9","text":"Configure AWS Database Migration Service to continuously replicate Oracle transactional data to Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Redshift. Use Amazon QuickSight to query the data for visualization.","correct":false}]},{"id":"663fbd6a-87bd-4fa6-a0ea-428ba2de5b51","domain":"awscsapro-domain5","question":"You manage a relatively complex landscape across multiple AZs.  You notice that the incoming requests vary mostly depending on the time of day but also there is a more unpredictable component resulting in smaller spikes and valleys for your resources.  Fortunately, you manage this landscape via OpsWorks Stacks.  What options, if any, are available to you as part of the OpsWorks featureset.","explanation":"OpsWorks Stacks offers three types of scaling: 24/7 for instances that remain on all the time; time-based for instances that can be scheduled for a certain time of day and on certain days of the week; and load-based scaling which will add instances based on metrics.  All this can be configured from within the OpsWorks Stack console.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/best-practices-autoscale.html","title":"Best Practices: Optimizing the Number of Application Servers - AWS OpsWorks"}],"answers":[{"id":"3622d494dceb973760a46dea038d1dc2","text":"If you need the ability to dynamically scale, you will need to use OpsWorks for Chef Automate.  OpsWorks Stacks does not support scaling.","correct":false},{"id":"b7ff5b06f51facca179494cb2bb00e55","text":"You can enabled CloudFormation Anticipated Scaling that uses past CloudWatch metrics and machine learning to automatically design a scaling policy optimized for the incoming request patterns.","correct":false},{"id":"216c997091da6e24174ad1b83d0be8b9","text":"You would define a baseline level of resources within the OpsWorks Stack Console to cover the average load.  But for the periodic load, that requires a scheduled auto-scaling policy.  Similarly, for the volatile spikes, you must use a stepped auto-scaling policy defined in an auto scaling group. ","correct":false},{"id":"75ab4de4ea42c1971b0ee09ae04ca591","text":"You would define a baseline level of resources and configure them for 24/7 instances.  Then you could define a time-based instances to cover certain times of day.  Finally, you could cover the volatile spikes with a load-based instances.  All this can be done within OpsWorks Stacks.","correct":true}]},{"id":"b00cb57f-7191-4f17-aa6d-ac687c418332","domain":"awscsapro-domain5","question":"You have a running EC2 instance and the name of its SSH key pair is \"adminKey\". The SSH private key file was accidentally put into a GitHub public repository by a junior developer and may get leaked. After you find this security issue, you immediately remove the file from the repository and also delete the SSH key pair in AWS EC2 Management Console. Which actions do you still need to do to prevent the running EC2 instance from unexpected SSH access?","explanation":"Although the SSH key pair is deleted in EC2, the public key content is still placed on the instance in an entry within ~/.ssh/authorized_keys. Someone can SSH to the instance if he has a copy of the leaked SSH private key. Users should not configure the instance to support another key pair as the old key pair still works. The correct method is deleting the instance immediately to prevent it from being compromised and launching another instance with a new SSH key pair. There is no need to use the AWS CLI command delete-key-pair as the key is already deleted from AWS EC2.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html","title":"Amazon EC2 Key Pairs"}],"answers":[{"id":"a928b9732684f81d9ce046842965f1f6","text":"No action is required as the SSH key pair \"adminKey\" is already deleted from AWS EC2. Even if someone has the SSH private key, he still cannot use the key to access the instance.","correct":false},{"id":"bc01758fc758191425928382b18697ca","text":"Create another SSH key pair via AWS EC2 or a third party tool such as ssh-keygen. Stop the instance and configure the instance with this new key pair in AWS Management Console. Restart the instance to activate the key pair.","correct":false},{"id":"4f9620d4caefa25fafabc98f00c6b192","text":"Use AWS CLI delete-key-pair to completely delete the key pair so that no one can use it to SSH to the instance. Configure CloudWatch logs to monitor the SSH logging events and filter the logs with the SSH key ID to see if the key pair is still used by someone.","correct":false},{"id":"ee7464becbe88068a5f848419e621bba","text":"Stop and terminate the instance immediately as someone can still SSH to the instance using the key. Launch a new instance with another SSH key pair. SSH to the EC2 instance using the new key.","correct":true}]},{"id":"9b8fcbdc-35da-425a-a77a-d5d7b2f6682b","domain":"awscsapro-domain5","question":"You have been asked to help develop a process for monitoring and alerting staff when malicious or unauthorized activity occurs.  Your Chief Security Officer is asking for a solution that is both fast to implement but also very low maintenance.  Which option best fits these requirements?","explanation":"AWS GuardDuty is a managed service that can watch CloudTrail, VPC Flow Logs and DNS Logs, watching for malicious activity.  It has a build-in list of suspect IP addresses and you can also upload your own lists of IPs.  GuardDuty can trigger CloudWatch events which can then be used for a variety of activities like notifications or automatically responding to a threat.  AWS Macie is a service to discovery and classify potentially sensitive information.  CloudWatch alone lacks the business rules that are provided with GuardDuty.","links":[{"url":"https://aws.amazon.com/guardduty/faqs/","title":"Amazon GuardDuty FAQs – Amazon Web Services (AWS)"}],"answers":[{"id":"8a07e363b2fe9a6ac84afe772979dec2","text":"Configure CloudWatch to create an event whenever malicious or unauthorized behavior is observed.  Trigger an SMS message via SNS to the Security Officer whenever the event happens. ","correct":false},{"id":"0809c23cd3f14044c6f063646502e2e7","text":"Use AWS SageMaker to implement a Linear Learner algorithm that periodically reviews CloudFront logs for malicious and unauthorized behavior.  When the ML model finds something suspicious, trigger an SES email to the Security Officer.","correct":false},{"id":"256dea4919c6b38545573ed3fe0fee5c","text":"Configure VPC Flow Logs to capture all traffic going in and out of the VPC.  Use ElastiSearch to process the logs and trigger a Lambda function whenever malicious or unauthorized behavior is found.","correct":false},{"id":"759d9a95c918d18810232d19f92f6d79","text":"Enable AWS Macie to monitor for malicious and unauthorized behavior.  Configure a custom whitelist for the IPs that were wrongly flagged.  Setup a Lambda function triggered from a CloudWatch event when anomalies are detected. ","correct":false},{"id":"069bd8db7abe860a4270a1b362abf0f0","text":"Enable AWS GuardDuty to monitor for malicious and unauthorized behavior.  Configure a custom blacklist for the IPs which you have seen suspect activity in the past.  Setup a Lambda function triggered from a CloudWatch event when anomalies are detected. ","correct":true},{"id":"68a4943d35341bc4e1fae5493607eaa0","text":"Use AWS Glue to direct all CloudTrail logs into Redshift.  Use QuickSight as a presentation layer with custom reports for visualizing malicious and unauthorized behavior.  Run the reports periodically and email them to the Security Officer. ","correct":false}]},{"id":"cb61ecae-afb0-4437-970e-72ddfe908e6b","domain":"awscsapro-domain1","question":"You are an AWS architect working for a B2B Merger and Acquisitions consulting firm, which has 15 business units spread across several US cities. Each business unit has its own AWS account. For administrative ease and standardization of AWS Usage patterns, corporate headquarters have decided to use AWS Organizations to manage the individual accounts by grouping them into relevant Organization Units (OU-s).\nYou have assisted the Organization Administrator to write and attach Service Control Policies (SCP-s) to the OU-s. SCP-s have been configured as the default Deny list, and they are written to explicitly deny actions wherever required.\nData Scientists in one of the Business Units are complaining that they are unable to spin up or access Sagemaker Clusters for building, training and deploying Machine Learning models. Which of the following can be a possible cause and how can this be fixed?","explanation":"This question tests the conceptual knowledge of Service Control Policies (SCP-s) in AWS Organizations.\nThe choice that requires the SCP to be modified is incorrect because there is no need to grant explicit allows from SCP, especially when it is configured in the default mode (Deny List mode). In this mode, everything is allowed by default. We only need to specify what we want to deny.\nThe choice that requires the IAM Policy to be modified is correct because SCPs do not actually grant any permission. The permission that is missing in this case must be granted via IAM Roles and Policies at the Account level.\nThe choice mentioning Service Linked Roles is incorrect as Trust Policies on Service Linked Roles cannot be modified to let an IAM user assume that role. Service Linked Roles are for AWS Services.\nThe choice that requires re-configuration of SCP as Allow List is incorrect because configuring SCP as Allow List is usually a messy idea. In that case, all permissions will need to be explicitly granted, and it can easily defeat the purpose of streamlining management and reducing administrative overhead by using AWS Organizations. Allow Lists have very specific use cases. In addition, no change in the SCP grants or allows any permission. Permission needs to be granted using IAM Roles and Policies at the Account level.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_denylist","title":"Using SCPs as a Deny List"},{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_allowlist","title":"Using SCPs as an Allow List"},{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Documentation for Service Control Policies"}],"answers":[{"id":"cd63be86b6c62d032d40d6408a54bda9","text":"The SCP for the OU to which the Business Unit Account belongs does not explicitly allow granting Sagemaker access. To fix this, add the following to the attached policy Statement of the SCP - Effect set to Allow, Action set to Everything starting with SageMaker, Resource set to All","correct":false},{"id":"f64b0d2d0b81fd2aa0badced0c59abfe","text":"The IAM Policy attached to the IAM Role that the data scientists are assuming in the Business Unit Account does not grant them Sagemaker access. To fix this, add the following to the IAM Policy Statement for that Role - Effect set to Allow, Action set to Everything starting with SageMaker, Resource set to All","correct":true},{"id":"1b1615b479c5aa7e1e9a3547b926aaef","text":"The Service Linked Role associated with AWS Sagemaker does not allow the data scientists to assume the Role. To fix this, add a Trust Policy to the Sagemaker Service Linked Role that lists the IAM user ids of the data scientists as Principal, with the value of Action is AssumeRole and Effect is set to Allow","correct":false},{"id":"ad6186009ae4678d814923c4929282a5","text":"SCP is configured as a Deny List. To fix this, SCP must be configured as an Allow List instead of a Deny List for the OU. Then, Sagemaker access should be added explicitly","correct":false}]},{"id":"6b6689f4-b150-482a-aa96-eab1674cb232","domain":"awscsapro-domain5","question":"Quality Auto Parts, Inc. has installed IoT sensors across all of their manufacturing lines. The devices send data to both AWS IoT Core and Amazon Kinesis Data Streams. Kinesis Data Streams triggers a Lambda function to format the data, and then forwards it to AWS IoT Analytics to perform monitoring and time-series analyses, and to take actions based on business processes. After an equipment failure on one of the manufacturing lines causes tens of thousands of dollars in revenue losses, it's determined that alarms for a specific piece of equipment where received seventy-five seconds after the issue originated, and that automated corrective action within a few seconds of the problem could have avoided the financial losses altogether. What changes should be made to the architecture to improve the latency of device alerts?","explanation":"AWS IoT Analytics is useful for understanding long-term device performance, performing business reporting, and identifying predictive fleet maintenance needs, but common latencies run from seconds to minutes. If you need to analyze IoT data in real-time for device monitoring, use Kinesis Data Analytics, which provides latencies in the millisecond to seconds range. A Lambda function can be used as the destination for Kinesis Data Analytics to perform corrective actions. IoT Core rules can write messages to a Kinesis stream, but not directly to Kinesis Data Analytics. Having a Lambda function perform anomaly detection will work, but will require more logic to be written for query setup and execution than using a specialized service like Kinesis Data Analytics. With Amazon CloudWatch Alarms, an alarm will watch a single metric over a period time, but will not provide the capabilities of SQL to detect complex anomaly conditions.","links":[{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/aws-reference-architecture-time-series-processing.pdf?did=wp_card&trk=wp_card","title":"Processing IoT Time Series Data on AWS"},{"url":"https://aws.amazon.com/iot-analytics/faq/","title":"AWS IoT Analytics FAQs"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/05/introducing-real-time-iot-device-monitoring-with-kinesis-data-analytics/","title":"Introducing Real-Time IoT Device Monitoring with Kinesis Data Analytics"}],"answers":[{"id":"7e2eb8f3a96a390aab88e66000821c26","text":"Create an AWS IoT Core rule to write the message to Amazon Kinesis Data Analytics to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":false},{"id":"71ade679994c0c1e6e55b3853194e4c5","text":"Add another AWS Lambda function as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Have the Lambda function write the anomalies to Amazon DynamoDB and perform device corrective action when needed.","correct":false},{"id":"522af3cd7d520d3e94e97c02d19c0672","text":"Create an AWS IoT Core rule to write the message to Amazon CloudWatch Alarms to detect anomalies in the data. Invoke another AWS Lambda function from CloudWatch Alarms to perform device corrective action when needed.","correct":false},{"id":"fe8ff20697982ca413f81ea14472e603","text":"Add Amazon Kinesis Data Analytics as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":true}]},{"id":"ebcf9ff3-82a1-48f8-b2fc-5d2aeb1d018c","domain":"awscsapro-domain3","question":"You are consulting with a company who is at the very early stages of their cloud journey.  As a framework to help work through the process, you introduce them to the Cloud Adoption Framework.  They read over the CAF and come back with a list of activities as next steps.  They are asking you to validate these activities to keep them focused.  Of these activities, which would you recommend delaying until later in the project?","explanation":"External communication usually comes much later in the process once project plans are defined and specific customer impact is better understood.","links":[{"url":"https://aws.amazon.com/professional-services/CAF/","title":"The AWS Cloud Adoption Framework"}],"answers":[{"id":"153eadc71676701cd67fdf00dc6c4723","text":"Work with Marketing business partners to design an external communications strategy to be used during potential outages during the migration.","correct":true},{"id":"2f063bd85b14ae1bbeec72bf0f6c06f5","text":"Work with the Human Resources business partners to create new job roles, titles and compensation/remuneration scales.","correct":false},{"id":"199907bba5306a10dbadf5a330d5f1f6","text":"Hold a workshop with IT business partners about the creation of an IT Service Catalog concept.","correct":false},{"id":"937d0c376f475ce2eac7a3356601b8fb","text":"Investigate the need for training for Program and Project Management staff around agile project management.","correct":false},{"id":"525fde29088ab22597d7d8063c7dadf6","text":"Work with internal Finance business partners to design a transparent chargeback model.","correct":false}]},{"id":"a2a38759-7505-4ece-bd74-965f9d589a08","domain":"awscsapro-domain4","question":"You have been asked to help a company with optimizing cost on AWS.  You notice in reviewing documentation that they have constructed a transit network to link around 30 VPCs in different regions.  When you review traffic logs, most of the traffic is across regions.  Given this information, what might you recommend to reduce costs?","explanation":"By smartly consolidating resources into fewer regions and AZs, you are able to reduce or potentially eliminate data transfer and thus lower your overall costs.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-global-transit-network/","title":"AWS Global Transit Network – AWS Answers"}],"answers":[{"id":"94085d779db3a238c9dabf1d52624a3c","text":"Consolidate resources into as few regions and AZs as necessary.","correct":true},{"id":"0e3b0d145e1202cb34b4947f2f6ed8ef","text":"Create VPC Gateway Endpoints within each of the 30 VPCs and add the necessary prefix lists.","correct":false},{"id":"d053c2a500bfc32b54da5688d6f230a4","text":"Implement NAT rules to compensate for overlapping networks and permit more direct routes.","correct":false},{"id":"9eba5c209cd53cc1ac849178ebf5fa05","text":"Implement a host-based router in the Transit VPC to intelligently route traffic based on least cost.","correct":false}]},{"id":"d2b0a9d5-1875-4a55-968d-3a2858601296","domain":"awscsapro-domain2","question":"You currently are using several CloudFormation templates. They are used to create stacks that include the resources of VPC subnets, Elastic Load Balancers, Auto Scaling groups, etc. You want to deploy all the stacks with a root stack so that all the resources can be configured at one time. Meanwhile, you need to isolate information sharing to within this stack group, which means other stacks outside of the stack group can not import its resources. For example, one stack creates a VPC subnet resource and this subnet can only be referenced by the stack group. What is the best way to implement this?","explanation":"As the stack outputs should be limited within the stack group, nested stacks should be chosen. The export stack outputs cannot prevent other stacks to use them. The AWS::CloudFormation::Stack resource type is used in nested stacks to provide dependencies. The DependsOn attribute is not used for configuring nested stacks.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html","title":"Exporting stack output values"}],"answers":[{"id":"e208251ba3e5646edab96df0da85794c","text":"Upload stack templates to an S3 bucket. Create a root CloudFormation stack to use the uploaded templates with the resource type of \"AWS::CloudFormation::Template\". Configure the \"TemplateURL\" field with the template location in S3.","correct":false},{"id":"fe3bf9eee211c59e9c64ad17a2d32e27","text":"Export output values for each child stack if needed. Create a parent stack to use the exported values from child stacks to deploy and manage all resources at one time.","correct":false},{"id":"e44006dac54e63b93a8804a4e632eeb5","text":"Upload the root and all child stack templates to an S3 bucket under the same directory. Use the \"DependsOn\" attribute in the root template to add dependencies. When the root stack is created, all the child stacks are created first. ","correct":false},{"id":"aca679ff150a8a8176faf99cc057e825","text":"Create nested stacks with the \"AWS::CloudFormation::Stack\" resources. Use the outputs from one stack in the nested stack group as inputs to another stack in the group if needed.","correct":true}]},{"id":"2d9a7fba-ee40-4128-8f36-b32ef55ce662","domain":"awscsapro-domain3","question":"You have just been informed that your company's data center has been struck by a meteor and it is a total loss.  Your company's applications were not capable of being deployed with high availability so everything is currently offline.  You do have a recent VM images and DB backup stored off-site.  Your CTO has made a crisis decision to migrate to AWS as soon as possible since it would take months to rebuild the data center.  Which of the following options will get your company's applications up and running again in the fastest way possible?","explanation":"The Server Migration Service uses the Server Migration Service Connector which is an appliance VM that needs to be loaded locally in vCenter.  We don't have a VMware system...only a backup of an image so this won't work.  The best thing we can do is import the VM and restore the database.","links":[{"url":"https://docs.aws.amazon.com/server-migration-service/latest/userguide/prereqs.html","title":"Server Migration Service (SMS) Requirements - AWS Server Migration Service"},{"url":"https://aws.amazon.com/ec2/vm-import/","title":"VM Import/Export"}],"answers":[{"id":"e50f3fc14c0feac5ff24b30bf605d687","text":"Use Server Migration Service to import the VM into EC2.  Use DMS to restore the backup to an RDS instance on AWS.","correct":false},{"id":"af2dfb68e123596c40e5014f3e8dc491","text":"Copy the VMs into AWS and create new AMIs from them.  Create a clustered auto scaling group across multiple AZs for your application servers.  Provision a multi-AZ RDS instance to eliminate the single-point-of-failure problem.  Restore the data from the backups using the database admin tools.","correct":false},{"id":"b8015467f854fe29575bd8fd26c819a5","text":"Use VM Import to upload the VM image to S3 and create the AMI of key servers.  Manually start them in a single AZ.  Stand-up a single AZ RDS instance and use the backup files to restore the database data.","correct":true},{"id":"364a256b0572da0bd43823d35b22e01d","text":"Call your data communications provider and order a Direct Connect link to your main office.  Order a Snowball Edge to serve as a mobile data center.  Restore the VM image to the Snowball Edge device as an EC2 instance.  Restore the backup to an RDS instance on the Edge device.  When the Direct Connect link is installed, use that to smoothly migrate to AWS.","correct":false},{"id":"c3692f609e87c0dc0416f0ad05897b3f","text":"Explain to company stakeholders that it is not possible to migrate from the backups directly to AWS.  Recommend that we first find a co-location site, procure similar hardware as before the disaster and restore everything there.  Then, we can carefully migrate to AWS.","correct":false}]},{"id":"66d28221-31ce-4cf0-aca3-2b5a69535bb5","domain":"awscsapro-domain3","question":"You are consulting with a small Engineering firm that wants to move to a Bring-Your-Own-Device policy where employees are given some money to buy whatever computer they want (within certain standards).  Because of device management and security concerns, along with this policy is the need to create a virtualized desktop concept.  The only problem is that the specialized engineering applications used by the employees only run on Linux.  Considering current platform limitations, what is the best way to deliver a desktop-as-a-service for this client?","explanation":"AWS Workspaces added support for Linux desktops the middle of 2018.  BYOD scenarios work together well with a DaaS concept to provide security, manageability and cost-effectiveness.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/create-custom-bundle.html","title":"Create a Custom WorkSpaces Bundle - Amazon WorkSpaces"}],"answers":[{"id":"bf3d66705af4677c9ade8605aa6bd89a","text":"Given current limitations, running Linux GUI applications remotely on AWS is not feasible.  They should reconsider their BYOD policy decision.","correct":false},{"id":"0cbf457eccae17779144d4e64e92a43e","text":"Launch a Windows Workspace and install VirtualBox along with a minimal Linux image.  Within that Linux image, install the required software.  Create an image of the Windows Workspace and create a custom bundle from that image.  Use that bundle when launching subsequent Workspaces.","correct":false},{"id":"e8a33593afbd82697d0ab168304265ed","text":"Launch an EC2 Linux instance and install XWindows and Gnome as the GUI.  Configure VNC to allow remote login via GUI and load the required software.  Create an AMI and use that to launch subsequent desktops.","correct":false},{"id":"3480305b106307937ed81bba73d294ab","text":"Package the required apps as WAM packages.  When launching new Windows Workspaces, instruct users to allow WAM to auto-install the suite of applications prior to using the Workspace.","correct":false},{"id":"4018dbf7b4646b285f5ceaa7b49a5934","text":"Launch a Linux Workspace in AWS WorkSpaces and customized it with the required software.  Then, create a custom bundle from that image and use that bundle when you launch subsequent Workspaces.","correct":true}]},{"id":"b533b3c1-222f-4f33-99da-2c828e98ff91","domain":"awscsapro-domain5","question":"You have run out of root disk space on your Windows EC2 instance.  What is the most efficient way to solve this?","explanation":"We can easily increase the size of an EBS from the console or the CLI (using modify-volume) but then we also need to allow the OS to expand the resized volume so we can use it.  For Windows Server, we could use Disk Manager.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/expand-ebs-root-volume-windows/","title":"Expand the EBS Root Volume of Your EC2 Windows Instance"}],"answers":[{"id":"346ac05b4353d34c630eb6d233f8a35d","text":"Compress all files on the root volume using the built-in zip utility.  Modern versions of Windows will automatically unzip the files when they are accessed.","correct":false},{"id":"b893490015da5047b17b1220d43f4a1c","text":"From the AWS CLI, use the \"modify-instance\" command for EC2 to resize the volume to a larger size.  Using RDP, connect to the Windows instances and use Disk Manager to expand the volume.","correct":false},{"id":"38862a719074689f75df6a20a42f7df7","text":"Use AWS System Manager Run service to remotely execute a PowerShell script using AWS Tools for PowerShell to expand the volume using the ModifyInstance command.","correct":false},{"id":"229b013eff2d5f53df7b9c3a60bd2418","text":"From the AWS Console, select Modify Volume for the EBS volume.  Enter the new size and confirm the change.  Connect to your Windows instance and use Disk Manager to extend the newly resized volume.","correct":true}]},{"id":"c01346c8-d230-4b52-b53d-78cdbfbc7794","domain":"awscsapro-domain4","question":"You work for an Insurance Company as an IT Architect. The development team for a microservices-based claim-processing system has created containerized applications to run on ECS. They have spun up a Fargate ECS cluster in their development VPC inside a private subnet. The containerized application uses awslogs driver to send logs to Cloudwatch. The ECS task definition files use private ECR images that are pulled down to ensure that the latest image is running always. The cluster is having connectivity problems as it cannot seem to connect with ECR to pull the latest images and neither can it connect with Cloudwatch to log. The development team has approached you to help troubleshoot the issue. What is a possible reason for this and what is the best way to fix it?","explanation":"ECS Fargate clusters can be deployed in a private subnet. Hence, we can safely eliminate the choice that says that ECS Fargate clusters must be deployed in a public subnet only.\nECS Fargate clusters do not need the user to control the ECS Agent Version on the nodes, as Fargate is serverless by design. Hence, we can safely eliminate the choice that deals with ECS Agent Version.\nThis leaves two options. One proposes using NAT Gateway. The other proposes using ECS Interface VPC Endpoint. Both are working solutions. However, one of them makes a false claim - it states that ECS Fargate clusters connect to ECR or Cloudwatch only over the internet. That is not true, as it can connect either using a public or a private network. Hence, the only fully correct choice is the one that uses ECS Interface VPC Endpoint","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/vpc-endpoints.html","title":"Amazon ECS Interface VPC Endpoints (AWS PrivateLink)"},{"url":"https://aws.amazon.com/blogs/compute/setting-up-aws-privatelink-for-amazon-ecs-and-amazon-ecr/","title":"Setting up AWS PrivateLink for Amazon ECR"}],"answers":[{"id":"f54cb63446f49ab92455024f17ae67b8","text":"ECS Fargate clusters must be deployed in a public subnet so that it can use the Internet Gateway to communicate with ECR or Cloudwatch. To fix this, redeploy the cluster in a public subnet","correct":false},{"id":"d73a8a48aaef0ddbe725f06264c3c6a3","text":"ECS Fargate clusters can connect to ECR (to pull down latest private images) or Cloudwatch (to log) only over the internet. Hence, if it is deployed in a private subnet, it needs a route to a NAT Gateway which must be connected to an Internet Gateway. To fix this issue, deploy a NAT Gateway in a public subnet of the VPC and add appropriate routes to the Routing Table","correct":false},{"id":"43901acf8308ae39f9cf7006afa72751","text":"ECS Fargate clusters can connect to ECR (to pull down latest private images) or Cloudwatch (to log) using either private or public network. Hence, if it is deployed in a private subnet, deploy an ECS Interface VPC Endpoint in the same subnet for connecting to internal services. Add appropriate routes to the Routing Table","correct":true},{"id":"d103c9eb11e007e2a9410cfb2e1bc1ba","text":"The version of the ECS agent may be too old. To fix this, upgrade the ECS Agent version in the cluster nodes to be compatible with connectivity requirements","correct":false}]},{"id":"6fcb65b9-6213-4b86-b7d8-48feb4dee16d","domain":"awscsapro-domain2","question":"You have registered a domain name (example.com) and created a hosted zone in Route 53. The network team configures several record sets that route the traffic to AWS resources including CloudFront distributions and Elastic Load Balancers. You need to collect the information of DNS queries that Route 53 receives for this particular hosted zone and save the data in a secure and highly available place. Which option would you choose to record the required information?","explanation":"You can configure Amazon Route 53 to log the query information to a CloudWatch Logs log group in the US East Region. Then you can use CloudWatch Logs tools to access the query logs. The logs from Route 53 cannot be directly forwarded to an S3 bucket or CloudWatch Metrics. VPC Flow Logs can capture IP traffic information going to and from network interfaces but the logs do not contain the DNS query data.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/query-logs.html","title":"Logging DNS Queries"}],"answers":[{"id":"e5ce673695e34b533c1aa4dea4673838","text":"In Amazon Route 53 management console, forward the Route 53 query logs to CloudWatch Metrics. Save the metrics data in CloudWatch Logs and ensure the log events in the log group never expire so that the logs are not deleted by CloudWatch.","correct":false},{"id":"4d3fcf4269f935cb4ecc5e67b1c8777d","text":"Create a CloudWatch Log group in US East (N. Virginia) Region and publish the DNS query logs to the log group. Configure an IAM policy to grant Route 53 permission to publish query logs to the log group.","correct":true},{"id":"b329ad4dfc36a45f3f0e594f9b27dfd9","text":"Select the Route 53 hosted zone in each AWS region and configure it to forward its query logs to an S3 bucket. The S3 bucket should have a bucket policy that allows the Route 53 service to put objects. Each AWS region should be assigned a folder in the S3 bucket.","correct":false},{"id":"2fdbd173f2e3d6aef24c02df40c9dcb8","text":"Find out the AWS resources including the origins of CloudFront distributions and the targets of Elastic Load Balancers. Locate the VPCs of these resources and enable VPC Flow Logs. Save the logs in an S3 bucket that contains the DNS query records from Route 53.","correct":false}]},{"id":"0abe2292-3f6e-47e1-93d9-6af24d5ea4c2","domain":"awscsapro-domain4","question":"A graphic design company has purchased eighteen m5.xlarge regional Reserved Instances and sixteen c5.xlarge zonal Reserved Instances. They receive their monthly AWS bill and find the invoice amount to be significantly higher than expected. Upon investigation, they discover RI discounted and non-discounted charges for nine m5.xlarge instances, nine m5.2xlarge instances, eight c5.xlarge instances, and eight c5.2xlarge instances. The business will need all of this capacity for at least the next twelve months. As their consultant, what would you advise them to do to maximize and monitor their RI discounts?","explanation":"Regional Reserved Instances allow for application of RI discounts within instance families, so all of the m5 instances are covered. Zonal Reserved Instances only provide discounts for specific instance types and sizes. So purchase of additional RIs would lower costs on the eight c5.2xlarge instances. Unused Reserved Instances are contractual and cannot be cancelled, so looking for another place to use them is the right approach. They could possibly be sold on the Reserved Instance Marketplace. AWS Budgets reservation budgets provide visibility and alerting on RI coverage specifically. Cost budgets and usage budgets may be useful, but they won't target RI coverage specifically.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html","title":"Regional and Zonal Reserved Instances (Scope)"},{"url":"https://aws.amazon.com/blogs/aws-cost-management/launch-instance-family-coverage-budgets/","title":"Launch: Instance Family Coverage Budgets"}],"answers":[{"id":"8e0e716570fad7bcaf6fe8bebe4f9d49","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Cancel the Reserved Instances for the nine unused m5.xlarge instances and eight unused c5.xlarge instances. Create an AWS Budgets cost budget that sends notification whenever costs exceed 80% of usage expectation","correct":false},{"id":"4b6d77d8423cd16b8e711b482dbcafbf","text":"Purchase an additional nine c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":true},{"id":"1c87a2204ad8da355c8ca0890aa06785","text":"Purchase an additional nine m5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances. Create an AWS Budgets usage budget that sends notification whenever RI coverage drops below 60%","correct":false},{"id":"8c1f6db4cd04a86fd43efa1bc97640ea","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances and eight c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":false}]},{"id":"872cd65b-287b-4fdb-b59f-f07f7bff707f","domain":"awscsapro-domain5","question":"Your client is a small engineering firm which has decided to migrate their engineering CAD files to the cloud.  They currently have an on-prem SAN with 30TB of CAD files and growing at about 1TB a month as they take on new projects.  Their engineering workstations are Windows-based and mount the SAN via SMB shares.  Propose a design solution that will make the best use of AWS services, be easy to manage and reduce costs where possible. ","explanation":"At present, EFS doesn't support Windows-based clients.  Storage Gateway-File Gateway does support SMB mount points.  The other options introduce additional unneeded costs.","links":[{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/limits.html","title":"Amazon EFS Limits - Amazon Elastic File System"}],"answers":[{"id":"f52e177dc6a594ed2c0852c91b6133d3","text":"Order a Snowmobile to migrate the bulk of the data.  Setup S3 buckets on AWS to store the data.  Use AWS WorkDocs to mount the S3 buckets from the engineering workstations.","correct":false},{"id":"c15496feac5aa7ce58d0c5d4813a5a29","text":"Use AWS CLI to sync the CAD files to S3.  Setup Storage Gateway-File Gateway locally and configure the CAD workstations to mount as SMB.","correct":true},{"id":"9c4821e0d9178e80636a5d4c7d0c6441","text":"Setup Storage Gateway-File Gateway and configure the CAD workstations to mount as iSCSI.  Use a Snowball appliance to sync data daily to S3 buckets at AWS.","correct":false},{"id":"7cad520a624f4c3b174014f339f732df","text":"Use AWS CLI to sync the CAD files to S3.  Use EC2 and EBS to create an SMB file server.  Configure the CAD workstations to mount the EC2 instances.  Setup Direct Connect to ensure performance is acceptable.","correct":false},{"id":"340da8cc24330ac5b143b526c880e4f7","text":"Order a Snowball appliance to migrate the bulk of the data.  Setup an EFS share on AWS and configure the CAD workstations to mount via SMB.  ","correct":false}]},{"id":"a2fb4f91-4c73-4080-bbf3-6d07a1b2ce03","domain":"awscsapro-domain2","question":"You have been asked to investigate creating a production Oracle server in RDS.  You need to choose the correct options that will allow you to run the latest version of Oracle 12c with High Availability.  You do not currently have any Oracle licenses. Which of the below are valid options?","explanation":"To get to the correct answer, you must first disregard any option with Oracle Data Guard as this is not available in RDS, then remove any answer containing the editions SE or SE1 as they only allow version 11g to be deployed, not 12c.  The remaining two options are correct as they allow High Availability, 12c and either a Bring-You-Own or licence included option, so you can ensure you get the best value.","links":[{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"Amazon RDS for Oracle FAQs"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Oracle.html","title":"Oracle on Amazon RDS"}],"answers":[{"id":"5d1af18d1bfbdb6d990f138d844f75dc","text":"Choose either the SE or SE1 editions, Bring-Your-Own-Licence and Oracle Data Guard","correct":false},{"id":"1ed8c7e073924060dbe648fbdf8a9c17","text":"Choose the SE2 edition with licence included and enable Multi-AZ","correct":true},{"id":"0cb21536be9ea582178c7e54e365bd33","text":"Choose either the SE or SE1 editions, purchase your own licenses from Oracle and enable Multi-AZ","correct":false},{"id":"0d9a63f8cb9a74845f363b450fa5cf11","text":"Purchase your own licenses from Oracle, Choose either the Enterprise or SE2 editions, Bring-Your-Own-Licence and enable Multi-AZ","correct":true}]},{"id":"b5f80e7e-7b92-402f-b74d-7397f0778eaf","domain":"awscsapro-domain4","question":"You are an AWS administrator and you want to enforce a company policy to reduce the cost of AWS usage. For the AWS accounts of testing environments, all instances in Auto Scaling groups should be terminated at 10:00PM every night and one instance in ASG should be launched at 6:00AM every morning so that the team can resume their work. At 10:00PM and 6:00AM, the team should get an email alert for the scaling activities. Which of the following methods would you use to implement this?","explanation":"A Lambda function can be used to modify the desired capacity at 10:00PM and 6:00AM. Only the desired capacity should be changed and the maximum capacity should be kept. If the maximum capacity is 1 during the day, the number of instances in ASG cannot be over 1. AWS CLI \"terminate-instances\" is inappropriate to terminate all instances in an ASG because ASG will automatically create new ones to maintain the desired capacity.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-manual-scaling.html","title":"Manual Scaling for Amazon EC2 Auto Scaling"}],"answers":[{"id":"5e1b982a6d6e29c160572d2430d8e3bb","text":"Use AWS CLI \"aws autoscaling put-scheduled-update-group-action\" to modify the desired capacity and maximum capacity to be 0 at 10:00PM and 1 at 6:00AM. Notify the team by configuring a CloudWatch Event rule.","correct":false},{"id":"32fa65e299613094d47b0e1a67a22636","text":"Create a cron job running in an EC2 instance. The cron job modifies the minimum, desired and maximum capacities to 0 at 10:00PM and changes the minimum, desired and maximum capacities back to 1 at 6:00AM. Configure a CloudWatch Event rule to notify the team.","correct":false},{"id":"0344a7b1df36d0f6e904c191acd31686","text":"Create a Lambda function that runs at 10:00PM and 6:00AM every day. The function terminates all EC2 instances using AWS CLI \"terminate-instances\" at 10:00PM and launches a new instance using AWS CLI \"run-instances\" at 6:00AM. Notify the team by publishing a message to an SNS topic.","correct":false},{"id":"8bbb4a7f1fc80cbb616df074b2984bd7","text":"Create a Lambda function that loops through all Auto Scaling groups, modifies the desired capacity to be 0 every 10:00PM and increases the desired capacity to be 1 every 6:00AM. Notify the team through an SNS notification in the Lambda function.","correct":true}]},{"id":"e50e5c98-f9f8-48fd-80fc-6a8741d17482","domain":"awscsapro-domain1","question":"You have just completed setup of Direct Connect from your data center to VPCs in us-east-1.  You would also like to leverage that Direct Connect for communication between your data center and other VPCs in other regions.  What is the simplest way to do this?","explanation":"You can use an AWS Direct Connect gateway to connect your AWS Direct Connect connection over a private virtual interface to one or more VPCs in your account that are located in the same or different regions. You associate a Direct Connect gateway with the virtual private gateway for the VPC, and then create a private virtual interface for your AWS Direct Connect connection to the Direct Connect gateway. You can attach multiple private virtual interfaces to your Direct Connect gateway.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html","title":"Direct Connect Gateways - AWS Direct Connect"}],"answers":[{"id":"15955693a61b06cc43a38a2ac5a2ede9","text":"Setup a Transitive VPC and configure virtual private gateways between the us-east-1 VPC and other VPCs in other regions.","correct":false},{"id":"38573789b2722bdd38529167996196d1","text":"You cannot use a Direct Connect gateway in one region to reach another region.  You must order a second Direct Connect link from a Partner in the desired regions.","correct":false},{"id":"d9271b66eb35f235c0b61fd11f3af41d","text":"Associate the Direct Connect gateway with the virtual private gateway for the VPC, and then create a private virtual interface for your Direct Connect connection to the Direct Connect gateway.","correct":true},{"id":"17077e5e04411e1e4e4b8ab017ae6c18","text":"Add a new route to the VPC peer in the desired region in the Customer Gateway.  Ensure that BGP routing is enable to propagate the route.","correct":false}]},{"id":"599dee9a-6ae7-4c85-a7c6-49edc6ae7d6b","domain":"awscsapro-domain5","question":"A development team is comprised of 20 different developers working remotely around the globe all in different timezones.  They are currently practicing Continuous Delivery and desperately want to mature to true Continuous Deployment.  Given a very large codebase and distributed nature of the team, enforcing consistent coding standards has become the top priority.  Which of the following would be the most effective to address this problem and get them closer to Continuous Deployment?","explanation":"Including an automated style check prior to the build can move them closer to a fully automated Continuous Deployment process.  A style check only before UI testing is too far in the SDLC.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"e60e97c4fb6cbf6c1dcf3e806624762f","text":"Require all developers to use the Pair Programming feature of Cloud9.  The commits must be signed by both developers before merging.","correct":false},{"id":"ec61b60c7eeb3bf9ca9c4149c09c5f3d","text":"Issue a department directive that standards must be followed and require the developers to sign the document.","correct":false},{"id":"db4ecdbd1c7c8fda5d3e0792a15411ab","text":"Include code style check in the build stage of the deployment pipeline using a linting tool.  ","correct":true},{"id":"f4cd7f15eb32d8ddd77234b38d0b35b8","text":"Incorporate a code style check right before user interface testing to ensure standards are being followed.","correct":false},{"id":"628453003287afe2200912bb38d0456b","text":"After integrating and load testing, run a code compliance check against the binary created during the build.","correct":false},{"id":"1f40591b9d9dbe7a2371e5e82ec05997","text":"Introduce a peer review step into their deployment pipeline during the daily stand-up, requiring sign off for each commit.","correct":false}]},{"id":"e3a59454-94fa-4b98-8d8a-80882a7d0e30","domain":"awscsapro-domain5","question":"You are setting up a new EC2 instance for an ERP upgrade project.  You have taken a snapshot and built an AMI from your production landscape and will be creating a duplicate of that system for testing purposes in a different VPC and AZ.  Because you will only be testing an upgrade process on this new landscape and it will not have the user volume of your production landscape, you select an EC2 instance that is smaller than the size of your production instance.  You create some EBS volumes from your snapshots but when you go to mount those on the EC2 instances, you notice they are not available.  What is the most likely cause?","explanation":"In order to mount an EBS volume on an EC2 instance, both must be in the same AZ.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html","title":"Amazon EBS Volumes - Amazon Elastic Compute Cloud"}],"answers":[{"id":"134c64ea3d25d70a667400e778a13c1a","text":"You created them in a different availability zone than your testing EC2 instance.","correct":true},{"id":"c2f8c791750ad6ce4c8c41ac45b246a0","text":"You have reached your account limit for EBS volumes.  You will need to create a support ticket to request an increase to the limit.","correct":false},{"id":"7a11f6b6797c8dd005c9ce25c77a37fe","text":"An SCP applied to the account you are in has restricted you from attaching EBS volumes to instances outside the original VPC","correct":false},{"id":"a9e147ffe118e514fd5069020e86cca6","text":"The instance that you selected for your testing landscape is too small.  It must be equal to or larger than the source of the AMI.","correct":false},{"id":"5a9ffc3875f9e3f50c9fc2684a6006b2","text":"The original volume is encrypted and you failed to check the encryption flag when creating the new volume.","correct":false}]},{"id":"a21ff5b0-e658-4466-9b30-ad1292dde65d","domain":"awscsapro-domain2","question":"A composite materials company is implementing a new monitoring solution on their manufacturing floor. Wi-Fi enabled IoT devices will be registered with AWS IoT Core to read data from numerous control systems. Dashboards will be created in Amazon QuickSight to present aggregate metrics to users (average, min, max, standard deviation, variance, and percentile). Drill down capabilities will also be needed for deeper analyses of exception scenarios. Which architecture will provide the most reliable and performance efficient solution for the company's monitoring needs?","explanation":"The MQTT protocol is a publish/subscribe protocol that provides clients with independent existence from one another, enhancing the reliability of the solution. HTTP is a document-centric ,request-response protocol, requiring more processing and storage overhead for IoT devices. There is no need to use Kinesis Data Analytics in this case because QuickSight can perform all of the aggregate functions required for this use case. Answer number four won't allow for data drill down because the device messages are not written to any persistent storage service.","links":[{"url":"https://aws.amazon.com/iot-core/","title":"AWS IoT Core"},{"url":"https://aws.amazon.com/blogs/compute/visualizing-sensor-data-in-amazon-quicksight/","title":"Visualizing Sensor Data in Amazon QuickSight"},{"url":"https://aws.amazon.com/quicksight/","title":"Amazon QuickSight"},{"url":"https://aws.amazon.com/athena/","title":"Amazon Athena"}],"answers":[{"id":"35c4c115504645bedabfaccb200fee7f","text":"Install MQTT libraries on the IoT devices. Create an IoT Core rule that forwards the MQTT messages to an Amazon Kineses Data Analytics stream, which writes aggregate data to an Amazon Kinesis Data Streams stream. Have an AWS Lambda function trigger to read the aggregate data and deposit it into Amazon DynamoDB tables","correct":false},{"id":"48b8bb005b2b6181fa6fc7161df04e9d","text":"Install HTTP libraries on the IoT devices. Create an IoT Core rule that forwards the HTTP messages to an AWS Lambda function. Have the Lambda function write the messages to S3, and to an Amazon Kinesis Data Analytics stream to aggregate the data. Have an AWS Lambda function trigger to read the aggregate data and deposit it into Amazon DynamoDb tables","correct":false},{"id":"2287d42764ee3a22b9bdcb617461dbc6","text":"Install MQTT libraries on the IoT devices. Create an IoT Core rule that forwards the MQTT messages to an AWS Lambda function. Have the Lambda function write the messages to an Amazon Kinesis Data Firehose stream, which deposits them into S3","correct":true},{"id":"d3578dd1f9b95b2daf716b5298605a4d","text":"Install HTTP libraries on the IoT devices. Create an IoT Core rule that forwards the HTTP messages to an Amazon Kineses Data Firehose stream, which deposits the data into S3, and writes the data to an Amazon Kinesis Data Analytics stream to aggregate the data. Have an AWS Lambda function trigger to read the aggregate data and deposit it into S3.","correct":false}]},{"id":"758841a6-0db9-4a02-a85b-699092182451","domain":"awscsapro-domain4","question":"You have just finished a contract with your client where you have helped them fully migrate to AWS.  As you are preparing to transition out of the account, they would like to integrate their current help desk software, Jira, with the AWS support platform to be able to create and track tickets in one place.  Which of the following do you recommend?","explanation":"You must have subscribe to at least the Business Support Plan to gain access to the AWS Support API. ","links":[{"url":"https://aws.amazon.com/premiumsupport/compare-plans/","title":"AWS Support - Compare all support plans"}],"answers":[{"id":"6c97334691882e8f846aad300e6a7811","text":"Subscribe to the Business Support Plan and direct them to the AWS Support API documentation","correct":true},{"id":"7e007e2c40ad802633ef945f34b23230","text":"It is currently not possible to integrate third-party products into the AWS Support system.  Offer to contract with them to perform manual updates between Jira and AWS Support cases.","correct":false},{"id":"55d01f0a938121dd662f6d3068168eec","text":"Subscribe to the Developer Support Plan and direct them to the AWS Support API documentation","correct":false},{"id":"8b3759a4758ab061f15798717c42c8e0","text":"Subscribe to the Platinum Support Plan and direct them to the AWS Support API documentation","correct":false},{"id":"0846705e92c2fcc07e439ddc2698dd15","text":"Use API Gateway to create a proxy service for the AWS Support API to allow third-party access.  Direct them to the AWS Support API documentation.","correct":false}]},{"id":"945797fb-5147-44fc-a50c-aaa636c3705b","domain":"awscsapro-domain3","question":"Your organisation currently runs an on-premise Windows file server.  Your manager has requested that you utilise the existing Direct Connect connection into AWS, to provide a method of storing and accessing these files securely in the Cloud.  The method should be simple to configure, appear as a standard file share on the existing servers, use native Windows technology and also have an SLA.  Choose an option which meets these needs.","explanation":"To choose the correct option, we can start by eliminating services which don't have an SLA, in this case only Storage Gateway doesn't have an SLA so we can remove that as an option.  Next we can rule out EFS and S3 as they don't use native Windows technology or provide a standard Windows file share, therefore the only correct answer is to use Amazon FSx for Windows File Server.","links":[{"url":"https://aws.amazon.com/fsx/windows/faqs/","title":"Amazon FSx for Windows File Server FAQs"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs"},{"url":"https://aws.amazon.com/efs/faq/","title":"Amazon EFS FAQs"}],"answers":[{"id":"437caccc8e39a279a232207ec3ca741a","text":"Map an SMB share to the Windows file server using Amazon FSx for Windows File Server and use RoboCopy to copy the files across","correct":true},{"id":"5a76242c7735a5846218b183930c3a41","text":"Map an Amazon Elastic File System (EFS) share to the Windows file server and use RoboCopy to copy files across","correct":false},{"id":"e953bc59adceffe1274c4bc66d83b365","text":"Create an AWS Storage Gateway for Files server and map the generated SMB share to the Windows file server, then synchronise the files","correct":false},{"id":"af082c9581ff0bb5e1c9cc6daf7d72e0","text":"Write a Powershell script which uses the CLI to synchronise the files into an S3 bucket","correct":false}]},{"id":"071d48ba-80e7-420e-969e-98cb2bcfbaa3","domain":"awscsapro-domain2","question":"Across your industry, there has been a rise in activist hackers launching attacks on companies like yours.  You want to be prepared in case some group turns its attention toward you.  The most common attack, based on forensic work security researchers have done after other attacks, seems to be the TCP Syn Flood attack.  To better protect yourself from that style of attack, what is the least cost measure you can take?","explanation":"AWS Shield Standard is offered to all AWS customers automatically at no charge and will protect against TCP Syn Flood attacks without you having to do anything - this meets the requirements of protecting TCP Syn Flood attacks at the lowest cost possible, as described in the question. A more robust solution which is better aligned to best practice would involve a load balancer in the data path, however as this would provide more functionality than required at a higher cost, is not the correct option for this question.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"6f87c7b3eda4188070a6635a49710939","text":"Implement AWS WAF and configure filters to block cross-site scripting match conditions.","correct":false},{"id":"aec18df10393205d72a60b52cf05bab9","text":"This type of attack is automatically addressed by AWS.  You do not need to take additional action.","correct":true},{"id":"7d8280921ead2af66cf214b774f94306","text":"Re-architect your landscape to use an application load balancer in front of any public facing services.","correct":false},{"id":"c50dd258b1fac18bd9368b07bf0fbc11","text":"Implement AWS Shield Advanced and configure it to generate CloudWatch alarms when malicious activity is detected.","correct":false},{"id":"82f7ef9f4b3e8e05aef157162915fecf","text":"Subscribe to a Business or Enterprise Support Plan.  Engage AWS DDoS Response Team and arrange for a custom mitigation.","correct":false}]},{"id":"b303f8e0-2c68-44aa-93bb-45b987b17d95","domain":"awscsapro-domain3","question":"You are helping a client build some internal training documentation to serve as architectural guidelines for their in-house Solutions Architects.  You suggest creating something inspired by the AWS Well-Architected Framework.  The client agrees and wants you to come up with some examples of each pillar.  Which of the following are examples of the Reliability pillar?","explanation":"The Reliability pillar includes five design principles:  Test recovery procedures, Automatically recovering from failure, Scaling horizontally to increase aggregate system availability, Manage change in automation and Stop guessing capacity.  By being able to closely monitor resource utilization, we can increase the Reliability and efficiency to right-size capacity.","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"e82a19c63c4c9fedccc997eece7eccdc","text":"With virtual and automatable resources, we can quickly carry out comparative testing using different types of instances, storage, or configurations.  This will allow us to experiment more often.","correct":false},{"id":"c57c8249b3885f2078d8dac40d695dec","text":"We can design workloads to allow components to be updated regularly, making changes in small increments that can be reversed if they fail.","correct":false},{"id":"3eb456756ecb767ab18179d87ec49a6b","text":"We can drive improvement through lessons learned from all operational events and failures. Share what is learned across teams and through the entire organization.","correct":false},{"id":"c257f1eec7a5a7f1eae9338f4de45cb0","text":"On AWS, we'll be able to monitor demand and system utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over or under-provisioning.  We can stop guessing on capacity needs.","correct":true}]},{"id":"a976aaea-e25f-4b84-879b-4f4843809b1f","domain":"awscsapro-domain2","question":"You need to create several file systems in Amazon Elastic File System (Amazon EFS) for an application. The storage class of the EFS file systems is standard. The Amazon EFS mount helper has already been installed in all Amazon EC2 instances and you are going to mount EFS file systems using the mount helper. According to the company policy, all file systems should be encrypted in transit. How would you achieve this requirement for the EFS file systems?","explanation":"Amazon EFS supports two forms of encryption, encryption in transit and encryption at rest. Both types of encryption are not enabled by default. When mounting the Amazon EFS file systems with the mount helper in EC2 instances, you need to explicitly add a TLS option \"-o tls\" to enable the encryption in transit. A customer managed key (CMK) is required for the encryption at rest. However, no CMK is needed for the encryption in transit.","links":[{"url":"https://docs.aws.amazon.com/efs/latest/ug/encryption.html","title":"Encrypting Data and Metadata in EFS"}],"answers":[{"id":"05ca59a3b9132178fb264eeebdaeac45","text":"EFS enables the encryption both in transit and at rest by default if the EFS file systems are mounted by the Amazon EFS mount helper. No extra actions are required.","correct":false},{"id":"d54fdbebc155feccf6b1a973a118e9f2","text":"Encryption in transit is not enabled by default. Users can enable the encryption in transit in the AWS Management Console by selecting a customer managed key in KMS. The CMK must have a key policy to allow the AWS EFS service to fetch the key.","correct":false},{"id":"26cc1a439bf3ff5489f933a0268b20f1","text":"When mounting the file systems using the Amazon EFS mount helper, enable Transport Layer Security (TLS) by adding the  \"-o tls\" option such as \"sudo mount -t efs -o tls fs-12345678:/ /mnt/efs\".","correct":true},{"id":"164503f2e48af52368a3dc0ba046e275","text":"EFS file systems only support the encryption at rest with a KMS AWS managed key or customer managed key. Encryption in transit is not supported so that EFS file systems should not be used in this scenario.","correct":false}]},{"id":"6a0e9756-1b9e-495c-965b-a8c715843d4f","domain":"awscsapro-domain1","question":"A client has asked you to help troubleshoot a Service Control Policy.  Upon reviewing the policy, you notice that they have used multiple \"Statement\" elements for each Effect/Action/Resource object but the policy is not working. What would you suggest next?  ","explanation":"The syntax for an SCP requires only one Statement element.  You can have multiple objects within a single Statement element though. ","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/org_troubleshoot_policies.html","title":"Troubleshooting AWS Organizations Policies - AWS Organizations"}],"answers":[{"id":"4b06a8d83f7ea11c2700b91cae578fbb","text":"Split the SCP out into multiple policies and apply in a cascading manner to higher level OUs.","correct":false},{"id":"3410d9f58eea1a807fdc3d0f6194c3ce","text":"Change the policy to combine the multiple Statement elements into one element with an object array.","correct":true},{"id":"6d5becfe4962c177196b4dce486c4e58","text":"Look elsewhere as multiple Statement elements are used when multiple conditions are specified in SCPs.","correct":false},{"id":"c88df8df089061b0ce17c8aba3a63305","text":"Have them apply the same policy on another OU to eliminate any localized conflicts.","correct":false}]},{"id":"8765bd56-057b-488c-9a0a-f5bd413dd240","domain":"awscsapro-domain5","question":"Due to new corporate policies on data security, you are now required to use encryption at rest for all data.  You have some EC2 Linux instances on AWS that were created without encryption for the root EBS volume.  What can you do that meet the requirement and reduce administrative overhead?","explanation":"AWS does support encrypted root volumes but conversion from unencrypted root to an encrypted root requires a bit of a process. You must first create an AMI then copy that newly created AMI to the same region, specifying that you want to encrypt the EBS volumes during the copy.  You can then create a new instance with an encrypted root volume from the copied AMI.  You can use either a generated key from KMS or your own CMK imported into KMS.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html","title":"AMIs with Encrypted Snapshots - Amazon Elastic Compute Cloud"},{"url":"https://aws.amazon.com/blogs/aws/new-encrypted-ebs-boot-volumes/","title":"New – Encrypted EBS Boot Volumes | AWS News Blog"}],"answers":[{"id":"9fbbb2e71386cb7f7a7ed77129a1a960","text":"Create an encrypted EFS instance and mount-points in the respective subnets.  Log into the instance and mount an encrypted EFS mount-point.  Copy all the root files over to the EFS mount point.  Edit the FSTAB file to mount the EFS mount point as the root volume instead of the root EBS device and reboot.","correct":false},{"id":"1e30ccf0b75e9e70fd76c6e041510c75","text":"At present, EC2 does not support encrypted root volumes.  Create new encrypted EBS data volumes and attach the new volumes to the existing instances.  Use RSYNC to migrate all the non-OS data over to the encrypted data volumes.","correct":false},{"id":"4430b7492a6c058c3574ce4e8ea43955","text":"Stop the instances and temporarily detach the EBS volumes.  Attach the root volumes to another EC2 instance and mount them a data volume.  Use a encryption tool like GPG or OpenPGP to recursively encrypt all the files on the mounted root volumes.  Detach and reattach the encrypted EBS volumes to the original instances and restart.  Import the encryption keys in KMS as a CMK.","correct":false},{"id":"50c17b27f0bd0390ae321943f7db5c3d","text":"Create a certificate in CMS for the encryption key.  Stop the instances and temporarily detach the root volumes.  Via the AWS CLI, enable encryption on the root volumes using the \"ebs modify-volume\" argument with the flag of \"encryption=<CMS ARN>\" to specify the certificate.","correct":false},{"id":"180e9aecdb74f204b1df00ffe6fa8b56","text":"Stop the instances and create AMIs from the instances.  Copy the AMIs to the same region and select the \"Encrypt target EBS snapshots\".  Redeploy the instances using the AMI copies you made with encrypted root volumes.","correct":true}]},{"id":"9882b1ed-c0de-4205-8a00-54731bef3109","domain":"awscsapro-domain2","question":"You work for an organic produce importer and the company is trying to find ways to better engage with its supply chain.  One idea is to create a public ledger that all members of the supply chain could update and query as products changed hands along the journey to the customer.  Then, your company could create an app that would allow consumers to view the chain from producer to end retailer and have confidence in the product sourcing.  Which AWS service or services could most directly help realize this vision?","explanation":"Amazon Quantum Ledger Database (QLDB) is a fully-managed ledger database that provides a transparent, immutable and verifiable transaction log.  While other products could be used to create such a supply chain logging solution, QLDB is the closest to a ready-made solution.","links":[{"url":"https://aws.amazon.com/qldb/","title":"Amazon QLDB"}],"answers":[{"id":"35b417ac310d92018a8db5515d4fae60","text":"Amazon DynamoDB and Lambda","correct":false},{"id":"a8312fbd65c49606c59b53a8a062ecff","text":"Amazon Managed Blockchain","correct":false},{"id":"680da36fb45c26a1d8e7996c6f5014cd","text":"Amazon CloudTrail and API Gateway","correct":false},{"id":"ca649f7447f846ed8add8c396187b83a","text":"Amazon P2PShare and API Gateway","correct":false},{"id":"a9d83c7f8f0b0f2a8f67b7097ee73e3a","text":"Amazon QLDB","correct":true}]},{"id":"489eea6e-bdaa-42a9-a80c-bfd209129fda","domain":"awscsapro-domain5","question":"You are working in a site reliability engineering team. There are dozens of EC2 instances in production that your team needs to maintain. When issues happen and online troubleshooting is required, the team needs to connect to a bastion host in order to login into an EC2 instance. You want to use the AWS Session Manager in Systems Manager to bypass the bastion host when accessing the instances. Which benefits can Session Manager bring to the team?","explanation":"Session Manager is a service in AWS Systems Manager that lets you access and manage your Amazon EC2 instances. Session Manager offers several benefits to the organization. With Session Manager, no SSH ports need to be open and no SSH keys are required. CloudTrail can also capture information about the Session Manager activities. However, the System Manager agent must be installed for Session Manager. For the Session Manager connections, you can choose to encrypt them with a CMK in KMS instead of an SSH key. And you should associate the IAM policies with IAM entities as an IAM policy cannot be attached in Session Manager.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html","title":"AWS Systems Manager Session Manager"}],"answers":[{"id":"3166b29d289b61b2a127864f52105929","text":"Session Manager integrates with IAM and you can attach an IAM policy in Session Manager to control who can initiate sessions to instances.","correct":false},{"id":"e097db56681740bcf0785fb6555a2a2d","text":"The Session Manager API calls made in the AWS account can be tracked by AWS CloudTrail. You can have a record of the connections made to the instances.","correct":true},{"id":"8660dde753cd22508fc49e487f99b4dd","text":"Session Manager provides highly secure connections via encryptions in transit. You can choose to create an SSH key pair and use the private key to encrypt the connections.","correct":false},{"id":"77650e8a40264f564a70afe948a6b602","text":"You do not need to open inbound SSH ports or PowerShell ports for the remote connections and no SSH keys are required for the connections with Session Manager.","correct":true},{"id":"1a123e85219205112eef2029de78d167","text":"AWS Session Manager handles the connections to EC2 instances and provides an interactive on-click shell. No agents need to be installed in the EC2 instances.","correct":false}]},{"id":"6dc7fe81-03aa-45d6-b8e1-6dc3b70914e0","domain":"awscsapro-domain1","question":"A company owns multiple AWS accounts managed in an AWS Organization. You need to generate daily cost and usage reports that include the activities of all the member accounts. The reports should track the AWS usage for each resource type and provide estimated charges. The report files also need to be delivered to an Amazon S3 bucket for storage. How would you create the required reports?","explanation":"The consolidated billing feature in AWS Organization does not generate billing reports automatically. You need to configure the AWS Cost and Usage Reports in the master account and use an S3 bucket to store the reports. The generated reports include activities for all the member accounts and it is not required to create a report in each member's account. The option of CloudWatch Event rule and Lambda function may work however it is not a straightforward solution.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-reports-costusage.html","title":"AWS Cost and Usage Report"}],"answers":[{"id":"967b7874a080c033470777ce955a4550","text":"In the master account of the AWS Organization, generate the AWS Cost and Usage Reports and save the reports in an S3 bucket. Modify the bucket policy to allow the billing reports service to put objects.","correct":true},{"id":"b66bf1016802f0edb247437b5fda31cb","text":"Login in each AWS account using the root IAM user, configure the daily Cost and Usage Report and set up a central S3 bucket to save the reports from all AWS accounts. Store the reports in different folders in the S3 bucket.","correct":false},{"id":"149237c5674e21794204a4a0ca00bee2","text":"Create a CloudWatch Event rule that runs every day. Register a Lambda function target which calls the PutReportDefinition API to get cost reports of all AWS accounts and store them in an S3 bucket.","correct":false},{"id":"033f176fcb8f66b1ee9fb950c8741cda","text":"Enable the consolidated billing feature in the AWS Organization which automatically generates a daily billing report. Predefine an S3 bucket to store the reports. Make sure the S3 bucket has a bucket policy to allow the AWS Organization service to write files.","correct":false}]},{"id":"777fd0a9-391f-4072-b147-a64a2016f5a1","domain":"awscsapro-domain2","question":"You are working with a company to design a DR strategy for the data layer of their news website.  The site serves customers globally so regional diversity is required.  The RTO is defined as 4 hours and RPO have been defined as 5 minutes. Which of the following provide the most cost-effective DR strategy for this client?","explanation":"While Multi-AZ RDS may be a best practice, the question only stipulates regional resilience.  So, we are looking for options that create regional diversity and fall within our RPO and RTO.  Those options would be cross-region bucket replication and cross-region RDS replicas.  The RPO given means that we must not loose anything more than 5 minutes of data, so any sort of backup that is less frequent than every 5 minutes is eliminated.","links":[{"url":"https://aws.amazon.com/blogs/aws/cross-region-snapshot-copy-for-amazon-rds/","title":"Cross-Region Snapshot Copy for Amazon RDS | AWS News Blog"},{"url":"https://aws.amazon.com/blogs/aws/new-whitepaper-use-aws-for-disaster-recovery/","title":"New Whitepaper: Using AWS for Disaster Recovery | AWS News Blog"}],"answers":[{"id":"0890c4152307ffd34ebeb6ea7c500814","text":"Write a script to export the RDS database to S3 every hour then use cross-regional replication to stage the exports in a backup region.","correct":false},{"id":"cfe51da2c91348146ba1bb989b4c2225","text":"Configure RDS to perform daily backups then copy those to another region.","correct":false},{"id":"aa26654aaaf6239fe7acd7dc4e952d5a","text":"Write a script to create a manual RDS snapshot and transfer it to another region.  Use AWS Batch to run the script every three hours.","correct":false},{"id":"7c7f5169f7bfea8c0aa5d79d8f1f1565","text":"Configure RDS Read Replicas to use cross-region replication from the primary to a backup region.","correct":true},{"id":"035a09840d025b52ad7c808976c94da2","text":"setup cross-region replication for S3 buckets.","correct":true},{"id":"b3810d00767bfbb6b85fe44c1c3d2dd1","text":"Configure RDS to use multi-AZ and automatically fail over in the event of a problem.","correct":false}]},{"id":"07f91ae7-094b-48a9-8924-a4d142cbbcb6","domain":"awscsapro-domain5","question":"On your last Security Penetration Test Audit, the auditors noticed that you were not effectively protecting against SQL injection attacks.  Even though you don't have any resources that are vulnerable to that type of attack, your Chief Information Security Officer insists you do something.  Your organization consists of approximately 30 AWS accounts.  Which steps will allow you to most efficiently protect against SQL injection attacks?","explanation":"Firewall Manager is a very effective way of managing WAF rules across many WAF instances and accounts.  It does require that the accounts be linked as an AWS Organization.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"3927b32bfb85cc040d2b7dcb21015fc5","text":"Use AWS WAF to create an ACL that denies requests that include SQL code.  Assign the ACL to Firewall Manager instances in each account using AWS OpsWorks.","correct":false},{"id":"e072f443d93c569cce94eb4946a912af","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service and use Consolidated Billing. Subscribe to AWS Shield Advanced to automatically enable SQL injection protection across all sub-accounts.","correct":false},{"id":"ee9c067e3dea2fa4cd2e3968abaf86de","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations.  Use CloudFormation to implement request restrictions for SQL code on the CloudFront distributions across all accounts.  Setup a CloudWatch event to notify administrators if requests with SQL code are seen.","correct":false},{"id":"9a5fa8e9a1a9be9149138c6307abde19","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service.  Use Firewall Manager to create an ACL rule to deny requests that contain SQL code.  Apply the ACL to WAF instances across all organizational accounts.","correct":true},{"id":"2238e9ae7489214f767fa479d013cd23","text":"Create a custom NACL filter using Lambda@Edge to check requests for SQL code.  Use OpsWorks to apply the NACL across all public subnets across the organization. ","correct":false}]},{"id":"49f16801-2cc1-48c8-a517-f9192f516318","domain":"awscsapro-domain3","question":"A tire manufacturing company needs to migrate a .NET simulation application to the AWS cloud. The application runs on a single Windows Application Server in their datacentre. It reads large quantities of data from local disks that are attached to the on-premises Application Server. The output from the application is small in size and posted in a queue for downstream processing. On the upstream side, the data acting as the input for the .NET simulation app is generated in the tire-testing Lab during the daytime by processes running on a Linux Lab Server. This data is then copied from the Linux Lab Server to the Windows Application Servers by a nightly process that also runs on the Linux Lab Server. This nightly process mounts the Application Server disks using a Samba client, this is made possible by the Application Server also acting as a Windows File Share Server. When the nightly process runs, it overwrites the input data from last night because of disk space constraint on the Application Server. This is undesirable as the data is permanently lost on a daily basis.\nThe migration is being undertaken because the .NET simulation application needs more CPU and RAM. The company does not want to spend on expensive hardware any more. However, the nightly process is not migrating, nor is the Linux Lab Server. The code of the simulation applications, as well as the nightly process, may change a little as a result of the migration, but leadership wants to keep these changes to a minimum. They also want to stop losing the daily test data and keep it somewhere for possible analytical processing later on.\nAs the AWS architect hired to shepherd this migration and many more possible migrations in the future, which of the following architectures would you choose as the best one, considering the minimization of code changes as the topmost goal, followed by cost-effectiveness as the second but important priority? The data has no security requirement.","explanation":"The two parts of this question are - (a) Do I use EFS or EBS for storing the data from the Lab Servers? (b) Do I copy data from each night to S3 using a NAT Gateway (thereby using the public internet) or a VPC Endpoint (thereby using the private network to copy)?\nThe answer to the first question is EBS because Windows EC2 instances cannot mount EFS, as EFS only supports Linux EC2 instances.\nThe answer to the second question is VPC Endpoint because NAT Gateways are very costly - they are charged 24-7 for just running, in addition to having data transfer rates. S3 VPC Endpoints are a cost-effective mechanism to copy data to S3. Note that the S3 put-object cost will be the same for both cases. The question tries to distract the candidate by stating that there is no security requirement, trying to confuse the candidate into selecting NAT Gateway in case they perceive the only distinction between NAT Gateway and S3 VPC Endpoint to be the usage of public network versus private.\nThis is an example of highly verbose question describing a complex scenario. There will definitely be quite a few such questions in the AWS SA-P exam that are challenging in terms of time management. You may use vertical scanning of the answer choices to spot the differences first. That way, you can focus on determining which of the variations is correct because you would know what is different between them.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html","title":"EFS Support for Windows EC2 Instances"},{"url":"https://aws.amazon.com/vpc/pricing/","title":"Search for NAT Gateway Pricing here"}],"answers":[{"id":"fabdb7e05e5bb1c78dd6e166134202ca","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"220d73c7cceb53661a402d59038118ee","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"3ae5ae41663ec801882e10e7fa394613","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":true},{"id":"c7311a2612bfc63a966317d468a5b4dd","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false}]},{"id":"5e92e555-6e42-4128-ae19-302b81d9fe84","domain":"awscsapro-domain2","question":"A university wants to create a regional website on AWS for end-users to download past research papers from. Most of the site will be static, as it will display metadata about the whitepapers like author, topic, date, excerpts and price. It will not show any link to download the full whitepaper. There will, however, be a payment link for each whitepaper. If users click on the payment link, the Javascript browser-side code will connect directly to a 3rd-party payment processing service which will return success on completion of payment. On success, the Javascript code will make a REST call to your website back-end to fetch a pre-signed S3 URL for that whitepaper and show a new page with the download link.\nThe pre-signed download URL must be a https URL, as you must protect the whitepaper authors from their valuable IP from being pirated using man-in-the-middle attacks while being downloaded. The website itself may be served over HTTP. None of the whitepapers must be publicly accessible without payment, though the website will not deal with any authentication or user profiles.\nIdentify the most suitable solution that will strike a good balance between quickness and convenience, cost and security for building this website.","explanation":"Admittedly verbose and lengthy, this type of question will test the time-management ability along with technical knowledge. It is clearly stated in the question that the website itself does not need to be served over HTTPS, only the pre-signed download URL (which will be used to download the full whitepaper after successful payment) needs to be HTTPS. Therefore, the choice that eliminates S3 static website citing the reason that S3 static websites do not support HTTPS is incorrect. While it is actually correct that S3 static websites do not support HTTPS, the reason for not choosing S3 static websites as a solution cannot be this fact, as the requirement clearly states that the website may be served over HTTP. Only the part that needs HTTPS can be developed outside of S3 static website, as it must be, as S3 static websites are, well, static, and would not be of much help running server-side code to generate pre-signed URLs. Also, the same choice uses EC2 instances and ELB - an approach that is not as quick or convenient as using a S3 static website.\nThe choice that uses Cloudfront and Origin Access Identity (OAI) is incorrect as we cannot use OAI if the origin is an S3 static website endpoint. OAI can only be used if the origin is an S3 bucket (and not a website). Also, using Cloudfront will unnecessarily increase the cost, as the requirement states that the website is regional.\nBoth the remaining choices are actually working solutions. Both will work. However, the best answer is the one that uses S3 static website, as it is the least amount of effort. Using API Gateway with Lambda proxy integration to serve the entire website is more work compared to using an S3 static website. Even when using S3 static website, API Gateway and Lambda needs to be used to generate the pre-signed URLs after the end-user pays for the whitepapers, but that is a small part compared to the full website.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/","title":"Various ways to chain Cloudfront and S3 for hosting websites"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html","title":"Restricting Access to Amazon S3 Content by Using an Origin Access Identity"}],"answers":[{"id":"5bfca7bbb3e2e03a395345f5959db09b","text":"Use S3 static website hosting to host the website, including the metadata, whitepapers and Javascript files, where the client-side Javascript code will read the metadata stored as JSON and parse it to render in a tabular format. Deploy a Cloudfront web distribution to enable https between end-users and Cloudfront, using the static website endpoint as the origin. Use Origin Access Identity to restrict access to the S3 content only to the Cloudfront web distribution. If an end-user makes a payment, have the client-side Javascript call an API Gateway over https to retrieve the pre-signed URL. Deploy a Lambda function as the API Gateway backend. Generate the pre-signed URL in the Lambda function using the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the Cloudfront endpoint","correct":false},{"id":"01d44a44c543778d9db261fe16c999bd","text":"Serve the entire website using API Gateway with proxy Lambda integration. When the base website is requested, the Lambda function will read an S3 bucket that hosts the metadata and whitepapers. It will parse the JSON metadata and generate HTML for listing the whitepaper metadata in a tabular format. If an end-user makes a payment, have the client-side Javascript call a different method on the API Gateway to retrieve the pre-signed URL. Generate the pre-signed URL in a second Lambda function using the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the API Gateway URL","correct":false},{"id":"06ec95a0c14f828594dd852570049d26","text":"Use S3 static website hosting to host the website, including the metadata, whitepapers and Javascript files, where the client-side Javascript code will read the metadata stored as JSON and parse it to render in a tabular format. If an end-user makes a payment, have the client-side Javascript call an API Gateway over https to retrieve the pre-signed URL. Deploy a Lambda function as the API Gateway backend. Generate the pre-signed URL in the Lambda function using the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the S3 Static website endpoint","correct":true},{"id":"1ffc7a9d39eeaa99af079745d4ed6ae0","text":"S3 static website cannot be used at all as they do not support https. Use S3 to host the metadata and whitepapers. Use a couple of EC2 instances in different Availability Zones to run the website, with an ELB distributing traffic to them. The website components deployed on the EC2 instances will handle both serving the website (by reading the S3 bucket using an EC2 Role and parsing the JSON metadata to generate HTML) and the subsequent request for the full whitepaper. While responding to the request for pre-signed URL, it will use the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the ELB DNS Name","correct":false}]},{"id":"c9f44641-660b-4c42-9380-9e7f6b0a9ba4","domain":"awscsapro-domain4","question":"As the solution architect, you are assisting your customer design and develop a mobile application using API Gateway, Lambda and DynamoDB. S3 buckets are being used to serve static content. The API created using API Gateway is protected by WAF. The development team has just staged all components to the QA environment. They are using a load testing tool to generate short bursts of a high number of concurrent requests sent to the API Gateway method. During the load testing, some requests are failing with a response of 504 Endpoint Request Timed-out Exception.\nWhat is one possible reason for this error response from API Gateway endpoint?","explanation":"The SA-P exam sometimes focuses on knowledge of response codes from API Gateway and what each distinct HTTP response code could mean.\nThe key to answering this question correctly is being able to distinguish between 4XX and 5XX HTTP error response codes. Though AWS has not been entirely consistent in their error code assignment philosophy, 4XX usually happens any time throttling kicks in because the request in that case never makes to an instance of Lambda function. 5XX happens when a Lambda function is actually instantiated, but some error (like time out) happened inside the Lambda function. One sneaky way to remember this is the fact that 5XX errors are called server errors in HTTP-land, so to generate a 5XX a server process must exist (and must have failed). Of course, in this context, the HTTP server process is a Lambda function - so in scenarios where throttling prevented a Lambda function from getting spawned, the response code cannot be 5XX. This is not consistently followed by AWS API Gateway error design, though, as we can see that AUTHORIZER_CONFIGURATION_ERROR and AUTHORIZER_FAILURE are both 500, though no Lambda function is actually spawned in either case. However, the candidate must remember that throttling always results in 4XX codes. An Endpoint Request Timed-out Exception (504) suggests that the requests in question actually made its way past the API Gateway into a Lambda function instance.\nFor the scenario where request rate exceeds API Gateway limits, the request would be blocked by API Gateway itself. The response would be 429. The exact knowledge of the code 429, however, is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe scenario where 1000 Lambda functions are already running is a similar example of throttling - the 1001st Lambda function will not even be spawned. The response, again, will be 429. However, the exact knowledge of the code 429 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe WAF scenario is yet another example of the request not even crossing the protections placed at the gateway level. If WAF is activated on API Gateway, it will block requests when the rate exceeds the HTTP flood rate-based rule (provided all such requests come from a single client IP address). However, the response, again, will be in the 4XX area (specifically, 403 Forbidden) - however, the exact knowledge of the code 403 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThis leaves Lambda time-out as the only correct answer. The mention of 30 seconds or more is a diversion tactic, in case candidate believes that the relevant Lambda time-out is 5 minutes. A given Lambda function instance may have a time-out limit of 5 minutes, but when it is invoked from API Gateway, the timeout imposed by API Gateway is 29 seconds. If a Lambda function runs for longer than 29 seconds, API Gateway will stop waiting for it and return 504 Endpoint Request Timed-out Exception.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html","title":"Amazon API Gateway Limits and Important Notes"},{"url":"https://aws.amazon.com/blogs/compute/amazon-api-gateway-adds-support-for-aws-waf/","title":"Amazon API Gateway adds support for AWS WAF"}],"answers":[{"id":"370770015087b4ff70656089fc9e3316","text":"The test is triggering too many Lambda functions concurrently. AWS imposes a soft limit of 1000 concurrent Lambda functions per region","correct":false},{"id":"98daaa701198f7e1c541fe8051799129","text":"The Lambda function is sometimes taking 30 seconds or more to finish executing","correct":true},{"id":"8090c6b0fe9036ec84fce24b16a7dc10","text":"The load testing tool has exceeded the soft limit for request rate allowed by API Gateway","correct":false},{"id":"82f6a6eb1a4aeef5dd34f21fcd2069ef","text":"The number of requests generated by the load testing framework has exceeded the threshold for the HTTP flood rate-based rule set in the WAF settings for the stage in question","correct":false}]},{"id":"e8bba7f5-4c0d-42dd-ad7a-74f042ce3dd9","domain":"awscsapro-domain3","question":"Due to a dispute with their co-location hosting company, your client is forced to move some applications as soon as possible to AWS.  The main application uses IBM DB2 for the data store layer and a Java process on AIX which interacts via JMS with IBM MQ hosted on an AS400.  What is the best course of action to reduce risk and allow for fast migration?","explanation":"For a fast migration with minimal risk, we would be looking for a lift-and-shift approach and not spend any time on re-architecting or re-platforming that we don't absolutely have to do.  Amazon MQ is JMS compatible and would provide a shorter path to the cloud than SQS.  DMS does not support DB2 as a target.","links":[{"url":"https://aws.amazon.com/amazon-mq/features/","title":"Amazon MQ Features – Amazon Web Services (AWS)"},{"url":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html","title":"Targets for Data Migration - AWS Database Migration Service"}],"answers":[{"id":"6d8610d6575127564b286722b73ce4be","text":"Install DB2 on an EC2 instance and use DMS to migrate the data.  Encapsulate the Java program in a Docker container and deploy it on ECS.  Spin up an instance of Amazon MQ.","correct":false},{"id":"21cbd80d1d890d3668ba4f841d7901df","text":"Use a physical-to-virtual tool to convert the AIX DB2 server into a virtual machine.  Use AWS CLI to import the VM into AWS and launch the VM.  Deploy the Java program as a Lambda function.  Launch a version of IBM MQ from the AWS Marketplace.","correct":false},{"id":"3700f4c1ab4e0778c4d0ae131d9c277d","text":"Deploy the Java processes as Lambda functions.  Install DB2 on an EC2 instance and migrate the data by doing an export and import.","correct":false},{"id":"4dba67b80e52ce08ef39ca56bf0ddd57","text":"Install DB2 on an EC2 instance and migrate the data by doing an export and import.  Spin up an instance of Amazon MQ in place of IBM MQ.  Install the Java process on a Linux-based EC2 system.","correct":true},{"id":"fbc4f7d74e4d62a2bdce9ca3f55f9fd2","text":"Use DMS and SCT to migrate DB2 to Aurora.  Update the Java application to use SQS and install it on a LInux-based EC2 system.  ","correct":false}]},{"id":"2c204ae8-6bba-49a1-b8f6-1aa4330c3d8c","domain":"awscsapro-domain5","question":"You are helping an IT organization meet some security audit requirements imposed on them by a prospective customer.  The customer wants to ensure their vendors uphold the same security practices as they do before they can become authorized vendors.  The organization's assets consist of around 50 EC2 instances all within a single private VPC.  The VPC is only accessible via an OpenVPN connection to an OpenVPN server hosted on an EC2 instance in the VPC.  The customer's audit requirements disallow any direct exposure to the public internet.  Additionally, prospective vendors must demonstrate that they have a proactive method in place to ensure OS-level vulnerability are remediated as soon as possible.  Which of the following AWS services will fulfill this requirement?","explanation":"AWS Macie is a service that attempts to detect confidential data rather than OS vulnerabilities.  Since there is no public internet access for the VPC, services like GuardDuty and Shield have limited usefulness. They help protect against external threats versus any OS-level needs.  AWS Artifact is simply a document repository and has no monitoring functions.  Only AWS Inspector will proactively monitor instances using a database of known vulnerabilities and suggest patches.","links":[{"url":"https://aws.amazon.com/inspector/faqs/","title":"FAQs - Amazon Inspector - Amazon Web Services (AWS)"},{"url":"https://aws.amazon.com/macie/","title":"Amazon Macie | Discover, classify, and protect sensitive data | Amazon Web  Services (AWS)"}],"answers":[{"id":"178912f5fbd90ab710621756a2ba18ff","text":"Employ AWS Macie to periodically assess my instances for vulnerabilities and proactively correct gaps.","correct":false},{"id":"81133f0650fa1ca2fbe1b920a6c67cc9","text":"Employ Amazon Inspector to periodically assess applications for vulnerabilities or deviations from best practices.","correct":true},{"id":"7ed3972608faa6c3dfc6fda7f151889c","text":"Enable AWS GuardDuty to monitor and remediate threats to my instances.","correct":false},{"id":"6a353f53758a3fd632209b5286a01086","text":"Enable AWS Shield to protect my instances from unauthorized access.","correct":false},{"id":"45d5e166ed185c1f7516650c714423dd","text":"Enable AWS Artifact to periodically scan my instances and prepare a report for the auditors.","correct":false}]},{"id":"bbcb9a8c-f84d-4424-b199-9047a4625e15","domain":"awscsapro-domain2","question":"Your company's DevOps manager has asked you to implement a CI/CD methodology and tool chain for a new financial analysis application that will run on AWS. Code will be written by multiple teams, each team owning a separate AWS account. Each team will also be responsible for a Docker image for their piece of the application. Each team's Docker image will need to include code from other teams. Which approach will provide the most operationally efficient solution?","explanation":"AWS CodePipeline, AWS CodeCommit, and AWS CodeBuild all allow cross-account access once the appropriate resource-level permissions have been granted. Orchestrating deployments from a single DevOps account will provide the most operationally efficient solution, resulting in less need for coordination of services and configurations across development team accounts.","links":[{"url":"https://aws.amazon.com/products/developer-tools/","title":"Developer Tools on AWS"},{"url":"https://aws.amazon.com/blogs/devops/how-to-use-cross-account-ecr-images-in-aws-codebuild-for-your-build-environment/","title":"How to Use Cross-Account ECR Images in AWS CodeBuild for Your Build Environment"}],"answers":[{"id":"b9ae951b67f2fed4a145bd7f591c8631","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the DevOps account to AWS CodeCommit in the team accounts to get the latest code. Perform cross-account access from AWS CodeBuild in the DevOps account to AWS CodeBuild in the team accounts to get the Docker images. Perform deployments from AWS CodeDeploy in the DevOps account","correct":true},{"id":"c32df9c97fbc40e07515d4ca41de63e2","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in the team accounts","correct":false},{"id":"83443cdcabe51198b91ed96d84eed4a6","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform all deployments from AWS CodeDeploy in the DevOps account","correct":false},{"id":"4a4bd59860afb498d97f0d01cff52b7a","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in a single DevOps account","correct":false}]},{"id":"f19a95ac-c0b9-4d00-a84a-67f71b7e2a76","domain":"awscsapro-domain2","question":"You are advising a client on some recommendations to increase performance of their web farm.  You notice that traffic seems to usually spike on the days after public holidays and unfortunately the responsiveness of the web server as collected by a third-party analytics company reflects a customer experience that is slower than targets.  Of these choices, which is the best way to improve performance with minimal cost?","explanation":"Of these options, only one meets the question requirements of performance at minimal cost.  Simply scheduling a scale event during a known period of traffic is a perfectly valid way to address the requirement and does not incur unnecessary cost. CloudTrail records API access and is not suitable for network alarms.  Route 53 would not be able to \"consolidate\" dynamic and static web resources.","links":[{"url":"https://docs.aws.amazon.com/auto scaling/ec2/userguide/schedule_time.html","title":"#N/A"}],"answers":[{"id":"247053f8b211aeace0894f849838ef6f","text":"Configure a scheduled scaling policy to increase server capacity on days after public holidays.  ","correct":true},{"id":"3c6b1f2e20a3204df3886f680991b76d","text":"Use CloudTrail and SNS to trigger a Lambda function to scale the web farm when network traffic spikes over a configured threshold.  Create an additional Internet Gateway and split the traffic equally between the two gateways using an additional route table.  ","correct":false},{"id":"24c302b442af96b7c1eedb04e2c4069b","text":"Configure a dynamic scaling policy based on network traffic or CPU utilization.  Migrate static assets from EBS volumes to S3.  Configure two Cloudfront distributions--one for static content and one for dynamic content.  Use Route 53 to consolidate both Cloudfront distributions under one alias.","correct":false},{"id":"8a2f77fe64a24871e80eae971ce2c877","text":"Create replicas of the existing web farm in multiple regions.  Migrate static assets to S3 and use cross-region replication to synchronize across regions.  Create CloudFront distributions in each region.  Use Route 53 to direct traffic to the closest CloudFront alias based on a geolocation routing policy.","correct":false}]},{"id":"f3fef147-7b9d-45b6-8b2b-d943c90e8920","domain":"awscsapro-domain5","question":"You are assisting a company in the migration of their container-based web landscape over to Amazon.  They have a total of 21 containers which comprise their DEV, QA and Production environments.  All environment are identical in design and size.  Each environment consists of 3 web servers, 3 app servers and 1 datastore server.  Given the landscape, which of the provided options would be best for them to minimize maintenance?","explanation":"Deploying containers via ECS is a good option but we would want to use the EC2 hosted path.  Fargate is generally used for transient workloads and our datastore would be something we'd want to persist.  We might be able to deploy the data store with RDS, but the question does not make it clear if the data store is an RDS-supported database.  It could be a NoSQL data store or some other database unsupported by RDS.  Similarly, a MEAN stack under Elastic Beanstalk might not be compatible with our landscape either.","links":[{"url":"https://aws.amazon.com/ecs/resources/","title":"Resources for Amazon ECS - run containers in production"}],"answers":[{"id":"f05469f4c7578263f4271e7514c338ef","text":"Deploy the web and app servers in each environment using ECS.  Provision an RDS instance for each environment.  Use AWS Systems Manager to provide a common management console.","correct":false},{"id":"c1354e6d48fedccbf7b4e9c18854d980","text":"Redeploy the web landscape on a MEAN stack under Elastic Beanstalk, making use of auto-scaling groups to right-size the respective environments.  ","correct":false},{"id":"619957021a43a829fbb6228467323ca1","text":"Deploy the web, app and database servers using ECS on EC2.  Purchase 1-year reserved instance contracts for the required EC2 instances.","correct":true},{"id":"e0ea997f77cb156d35ec716cf772c49c","text":"Deploy the web, app and database containers using ECS.  Make use of Fargate for the underlying ECS infrastructure.","correct":false}]},{"id":"482e75c9-071e-4a10-83f4-575f9c15b885","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They have just accidentally deleted the private key portion of their EC2 key pair.  Now, they are unable to SSH into their Amazon Linux servers.  Unfortunately the keys were not backed up and are considered gone for good.  What can this customer do to regain access to their instances?","explanation":"The two methods that AWS recommends if you lose a private key for an EC2 key pair are using Systems Manager Automation or using a secondary instance to edit the authorized_keys file.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2reset.html","title":"Reset Passwords and SSH Keys on Amazon EC2 Instances - AWS Systems Manager"}],"answers":[{"id":"be45655cb1d64dff71a97aa729bc4e4a","text":"Open the TELNET port (port 23) on the Security Group for the server.  Use a TELNET client to attach to the instances using the root account and password.  Modify the authorized_key file with the new public key.","correct":false},{"id":"79457a1b908d4a36cfeba625be909d40","text":"Use AWS Systems Manager Automation with the AWSSupport-ResetAccess document to create a new SSH key for your current instance.","correct":true},{"id":"492c38d2fe2c3b96608bb8436592fe26","text":"Use the AWS CLI with the EC2 ModifyInstance action to enable SSH password-only access for the ec2-user account.  Attach using a password rather than an SSH key.  Modify the authorized_key file for the new public key.","correct":false},{"id":"bec3d01a56c851f61a1a09c852635db7","text":"Generate and upload a new key pair.  Stop the instances and select the new key pair from the dropdown on the Instance Settings sub-menu in the Console.","correct":false},{"id":"f17aa014620843abd81fa849982566b0","text":"Create a new key pair in KMS then assign the new public key to the required EC2 instance.","correct":false},{"id":"509a77ae9827c5fcb60ccecc62fc9853","text":"Stop the instances, detach its root volume and attach it as a data volume to another instances.  Modify the authorized_keys file, move the volume back to the original instance and restart the instances.","correct":true}]},{"id":"bdd6d6a9-48a5-45f8-bb74-873f266d85df","domain":"awscsapro-domain2","question":"A hospital would like to reduce the number of readmissions for high risk patients by implementing an interactive voice response system to provide reminders about follow up visit requirements after patients are discharged. The hospital has the capability to automatically send HL7 messages that include the patient's phone number and follow up visit information from its medical records application via Apache Camel. They've chosen to deploy the solution on AWS. They already have a VPN connection to AWS, and all aspects of the application need to be HIPAA eligible. Which architecture will provide the most resilient and cost effective solution for the automated call system?","explanation":"S3 provides a low cost repository for the HL7 messages received. Having Lambda write the object keys to SQS, and having another Lambda function retrieve and parse the messages gives the architecture asynchronous workflow. Amazon Connect provides the capability to define call flows and perform IVR functions. Each of these services is HIPAA eligible. DynamoDB is also a good option for storing message information, but will be more expensive than S3. Amazon Pinpoint can place outbound calls, but is not able to perform interactive voice response functions. Amazon Comprehend Medical doesn't create call flow sequences.","links":[{"url":"https://aws.amazon.com/compliance/hipaa-eligible-services-reference/","title":"HIPAA Eligible Services Reference"},{"url":"https://aws.amazon.com/connect/","title":"Amazon Connect"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/reduce-hospital-readmissions-ra.pdf?did=wp_card&trk=wp_card","title":"Reducing Hospital Readmissions"}],"answers":[{"id":"c9bee4c5e4463b67e2726d3bed2ceed1","text":"Configure Apache Camel to write the HL7 messages to Amazon Kineses Data Firehose, which stores the patient information in Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"a8c26f088ade23063b2bf5f202a74cce","text":"Have Apache Camel write the HL7 messages to Amazon Kineses Data Streams. Configure a Lambda function as a consumer of the stream to parse the HL7 message and write the information to Amazon DynamoDB. Trigger another Lambda function to pull the patient data from DynamoDB and send it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"8121c4e6e285161311cacf7b8031d5af","text":"Configure Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to write each HL7 message object key to Amazon Simple Queue Service FIFO. Have another Lambda function read messages in sequence from the SQS queue and use the object key to retrieve and parse the HL7 messages. Use that same Lambda function to write patient information to Amazon Connect to place the call using an established call flow.","correct":true},{"id":"9585e2e5994ba5cdc2db33c22d9230cf","text":"Set up Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Connect to place the call to the patient.","correct":false}]},{"id":"2eee6f1c-96d7-4d2b-821f-4ce8acaf3de3","domain":"awscsapro-domain5","question":"You've deployed a mobile app for a dance competition television show's viewers to vote on performances. The app's backend leverages Amazon API Gateway, AWS Lambda, and Amazon RDS Oracle, with voting activity going from devices directly to API Gateway. In the middle of the broadcast, you begin receiving errors in CloudWatch indicating that the database connection pool has been exhausted. You also see log entries in CloudWatch with a 429 status code. After the show concludes, ratings for the app indicate a very poor user experience, with multiple retries needed to cast a vote. What would be the best way to increase the scalability of the app going forward?","explanation":"Placing Kineses between API Gateway and Lambda decouples the architecture, making use of an intermediary service to buffer incoming requests. The 429 status code indicates a Lambda concurrency throttling error, which you can resolve by controlling the Kinesis batch size per batch delivery. Database sharding will increase scalability, but will still have an upper limit of capacity. Increasing available Lambda memory will have no effect. Inserting a Lambda traffic manager doesn't address the database scalability issues, nor does increasing the regional Lambda concurrency limit. Modifying RDS DB Parameter Group values will require a database restart to take effect, which won't be feasible during live voting activity.","links":[{"url":"https://aws.amazon.com/blogs/architecture/how-to-design-your-serverless-apps-for-massive-scale/","title":"How to Design Your Serverless Apps for Massive Scale"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/scaling.html","title":"AWS Lambda Function Scaling"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","title":"Using AWS Lambda with Amazon Kinesis"}],"answers":[{"id":"6b6e6fb8b54db42df4d343376ccd8c60","text":"Insert Amazon Kinesis between API Gateway and Lambda, and configure Kinesis as an event source for Lambda. Set the number of records to be read from a Kinesis shard to an optimal value based on volume projections","correct":true},{"id":"2f66b519925956d6a91d0026056904d9","text":"Have API Gateway route requests to a new Lambda function that manages traffic and retries for the voting logic Lambda function. Request that the regional function concurrency limit be increased based on volume projections","correct":false},{"id":"a4051078cf0289e689e79fe70eab1f14","text":"Create a separate Lambda function to increase the maximum DB connection value in the RDS DB Parameter Group when a CloudWatch Metrics DB connection threshold is exceeded. Invoke Lambda functions with an 'event' invocation type to retry failed events automatically","correct":false},{"id":"8f962935878f87ebc9523dc54f505886","text":"Scale the database horizontally by creating additional instances and use sharding to distribute the data across them. Provide the Lambda function with a mapping of the sharding scheme in DynamoDB. Increase the amount of memory available to the Lambda function during execution","correct":false}]},{"id":"b06ef2a9-b122-4b47-b5be-b6d604e78405","domain":"awscsapro-domain2","question":"You are working with a customer to implement some better security policies.  They have a group of remote employees working on a confidential project that uses some proprietary Windows software and stores data in S3.  The Chief Information Security Officer is concerned about the threat of the desktop software or confidential data being smuggled out to a competitor.  What architecture would you recommend to best address this concern? ","explanation":"Using a locked down virtual desktop concept would be the best way to manage this.  AWS WorkSpaces provides this complete with client software to log into the desktops.  These Workspaces can be walled off from the Internet.  Using policies, you could allow access from only those in the Workspaces VPC.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html","title":"What Is Amazon WorkSpaces? - Amazon WorkSpaces"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html","title":"Endpoints for Amazon S3 - Amazon Virtual Private Cloud"}],"answers":[{"id":"f8c297565cec662fd215e6c551daca36","text":"Create a bucket policy using the sourceIP condition to only allow access from a specific VPC CIDR.  Apply a NACL which only permits inbound port 22 and outbound ephemeral ports.  Deploy Amazon Workspaces in the VPC and disable internet access.  Supply the users with instructions on downloading and login into the Workspaces instances.","correct":false},{"id":"36e6b9fb8ba6dd4b59c9f032ba6f78f7","text":"Provision Windows 2016 instances in a private subnet.  Create a specific security group for the Windows machines permitting only SSH inbound.  Create a NACL which allows traffic to S3 services and explicitly deny all other network traffic to and from the subnet.  Assign an S3 bucket policy that only allows access for members of the Windows machine security group.","correct":false},{"id":"21d0f58369770fb84d6bff8bfcf9c265","text":"Use Service Catalog to deploy and manage the proprietary Windows software to the remote employees.  Create an OpenVPN server instances within a VPC.  Create an VPC Interface Endpoint to S3 and use a security group to only permit traffic from the OpenVPN server security group.  Supply the remote employees with instructions to install and login using OpenVPN client software.","correct":false},{"id":"b5b5a93dca37986129c444b7654d600f","text":"Provision Amazon Workspaces in a secured private VPC.  Do not enable Internet access for the Workspaces.  Create a VPC Gateway Endpoint to S3 and implement an endpoint policy that explicitly allows access to the required bucket.  Assign an S3 bucket policy that denies access unless the sourceVpce matches the VPC endpoint.  Supply the users with instructions on downloading and login into the Workspaces instances.","correct":true}]},{"id":"3ca7c5e0-432a-4d40-afee-ab996819b429","domain":"awscsapro-domain3","question":"You are consulting with a client to guide them on migration of an in-house data center to AWS.  The client has stipulated in the contract that the migration cannot require any more than 1 hour downtime at a time and that there is always a fallback path.  Additionally, they want an overall increase in business continuity capabilities when the migration is done.  Their landscape is as follows:  (1) Several databases with about 1TB of data combined which are heavily used 24x7 and considered mission critical; (2) About 40TB of historic files which are read sometimes but almost never updated; (3) About 150 web servers on VMware in various states of customization of which there is a current project underway to standardize them.  The client's team has suggested some next steps but because they aren't yet familiar with AWS, they are not using equivalent AWS terms.  Translating their suggestions, which of the following activities would you choose to meet the requirements, reducing costs and management where possible?","explanation":"The database migration suggestion aligns well with DMS as it can keep the databases in sync until cutover.  SAN replication sounds a lot like Storage Gateway which is a reasonable way to migrate data to AWS.  However, simply using K8s does not convert your VMs into containers or make them serverless.  We can't restore tapes to AWS.  Creating the same VM landscape on AWS just adds an additional layer of complexity that's not needed.","links":[{"url":"https://aws.amazon.com/dms/faqs/","title":"AWS Database Migration Service FAQs - Amazon Web Services"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"}],"answers":[{"id":"d2dde578790a34d9e740015474ea23e4","text":"Migrate the majority of the 150 web servers to a serverless concept by moving the VMs to a Kubernetes cluster.","correct":false},{"id":"801ce55cfc2a125e7d17c729ca3e2e93","text":"Create new high powered stand-alone database instances in AWS and migrate data from on-prem database.  Use log shipping to keep the databases in sync.  Once we better understand AWS, we'll rebuild the servers and repartition the tables. ","correct":true},{"id":"75d528d2ec243c60d1478ae605c89f40","text":"Build a matching VMware environment on AWS and use third-party tools to backup and restore the VMs there.","correct":false},{"id":"3a02ebcd33fe18255e4ce43e8babb730","text":"Over several months, at end of business on Friday, backup all the servers and data to tape and restore to new instances in AWS to prove out AWS capabilities and reliability.","correct":false},{"id":"31ea0eccdcea45ea4fce3b9459de52d4","text":"Use some block-level SAN replication tool to gradually migrate the on-prem historic files to AWS.","correct":true}]},{"id":"63a6def8-9b52-4d89-8248-6079ca1393e2","domain":"awscsapro-domain3","question":"You are helping a client prepare a business case for cloud migration.  One of the required parts of the business case is an estimation of AWS costs per month.  The client has about 200 VMs in their landscape under VMware vCenter.  Due to security concerns, they will not allow any external agents to be installed on their VMs for discovery.  How might you most efficiently gather information about their VMs to build a cost estimate with the least amount of effort? ","explanation":"The Application Discover Service uses agent-based or agentless collection methods.  Agentless collection is only available for those customers using VMware.  The AWS Application Discovery Agentless Connector is delivered as an Open Virtual Appliance (OVA) package that can be deployed to a VMware host. Once configured with credentials to connect to vCenter, the Discovery Connector collects VM inventory, configuration, and performance history such as CPU, memory, and disk usage and uploads it to Application Discovery Service data store.  This data can then be used to estimate monthly costs.","links":[{"url":"https://aws.amazon.com/application-discovery/faqs/?nc=sn&loc=6","title":"AWS Application Discovery Service FAQs"}],"answers":[{"id":"30cbd0a822a4905f8a795dcb7cc3d31e","text":"Use a custom script to iteratively log into each VM and pull network, hardware and performance details of the VM.  Write the data out to S3 in CSV format.  Use that data to select corresponding EC2 instance sizes and calculate estimated monthly cost.","correct":false},{"id":"09d153439d976dabcdd13a4a2f8a4a5f","text":"Use Application Discovery Service to gather details on the network connections, hardware and performance of the VMs.  Export this data as CSV and use it to approximate monthly AWS costs by aligning current VMs with similar EC2 instances types.","correct":true},{"id":"b279286ed54b2394c29d2fbd0061b4c4","text":"Provision an S3 bucket for data collection.  Use SCT to scan the existing VMware landscape for VM hardware, network connection and performance parameters.  Retrieve the SCT CSV data from the data collection bucket and use it to align EC2 instance types with existing VM parameters.  Use this cross-reference to calculate estimated monthly costs for AWS.","correct":false},{"id":"0a144eb3993e48693aab4c9744b6acb2","text":"Use AWS OpsWorks to remotely pull hardware, network connection and performance of the VMs.  Export the collected data from OpsWorks in Excel format.  Use the collected data to align current VMs with similar EC2 instance types and calculate an estimated monthly cost.","correct":false}]},{"id":"baf2349f-71ba-4583-bfe6-31fb5a555bbd","domain":"awscsapro-domain5","question":"The information security group at your company has implemented an automated approach to checking Amazon S3 object integrity for compliance reasons. The solution consists of scripts that launch an AWS Step Functions state machine to invoke AWS Lambda functions. These Lambda functions will retrieve an S3 object, compute its checksum, and validate the computed checksum against the entity tag checksum returned with the S3 object. However, an unexpected number of S3 objects are failing the integrity check. You discover the issue is with objects that where uploaded with S3 multipart upload. What would you recommend that the security group do to resolve this issue?","explanation":"For S3 objects, the entity tag (or ETag) contains an MD5 hash of the object in most cases. But if an object is created by either the Multipart Upload or Part Copy operation, the ETag is not an MD5 digest of the object. The ETag value returned by S3 for objects uploaded using the multipart upload API is computed differently than for objects uploaded with PUT object, and does not represent the MD5 of the object data. The checksum for an object created via multipart upload can be stored in a custom metadata parameter for later integrity checks. The Content-MD5 metadata parameter can not be modified by a user after the object has been created. The complete-multipart-upload API does not have an md5-rehash parameter. The list-multipart-uploads API will only return information about the multipart upload while the upload is running.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html","title":"Multipart Upload Overview"},{"url":"https://aws.amazon.com/solutions/serverless-fixity-for-digital-preservation-compliance/?did=sl_card&trk=sl_card","title":"Serverless Fixity for Digital Preservation Compliance"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/s3-multipart-upload-cli/","title":"How do I use the AWS CLI to perform a multipart upload of a file to Amazon S3?"}],"answers":[{"id":"fe2f446e0e84bb45be4130b327b8b47b","text":"When performing S3 multipart uploads, calculate the checksum of the source file and store it in a custom metadata parameter. Have the Lambda function that compares checksums use the custom metadata parameter if it's present instead of the entity tag checksum. Reload all objects that were written with multipart upload that need to be included in the integrity check.","correct":true},{"id":"df361f3568504f0db56ea3faddc6928a","text":"For all S3 objects created with multipart upload, retrieve the object, compute it's checksum, and store the value in the Content-MD5 metadata parameter. Have the Lambda function that validates checksums use the Content-MD5 metadata parameter if it's present instead of the entity tag checksum.","correct":false},{"id":"15ca1d17e48531d75eda4298140756e6","text":"In the Lambda function that retrieves the objects, if the object was created with a multipart upload, call the list-multipart-uploads API and retrieve each part of the multipart upload along with the entire object. In the Lambda function that computes checksums, compute the checksum of each part along with the checksum of the entire object. In the Lambda function that validates checksums, compare the sum of the checksum parts to the checksum for the entire object.","correct":false},{"id":"2a27b97da81e5d7f237719319a3d85fc","text":"When performing S3 multipart uploads, after all upload-parts API calls have been made, call the complete-multipart-upload API and include the md5-rehash parameter to reset the entity tag checksum to the sum of the parts. Reload all objects that were written with S3 multipart upload that need to be included in the integrity check.","correct":false}]},{"id":"431e43bc-ccbc-480f-9915-210bc7773d2b","domain":"awscsapro-domain5","question":"You are in the process of migrating a large quantity of small log files to S3 for long-term storage.  To accelerate the process and just because you can, you have created quite sophisticated multi-threaded distributed process deployed across 100 VMs which can load hundreds of thousands of files at one time.  For some reason, the process seems to be throttled somewhere along the chain.  You try many things to try to uncover the source of the throttling but nothing works.  Reluctantly, you decide to turn off the KMS encryption setting for your S3 bucket and the throttling goes away.  You turn AMS-KMS back on and the throttling is back. Given the troubleshooting steps, what is the most likely cause of the throttling and how can you correct it?","explanation":"Through a process of elimination, it seems you have identified the variable that is causing the throttling.  KMS, like other AWS services, does have rate limiters which can be increased via Support Case.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/limits.html","title":"Limits - AWS Key Management Service"}],"answers":[{"id":"fe629daf7473efc279d7d8ee6f5a5806","text":"You are maxing out your SYNC requests to S3.  You need to request a limit increase via a Support Case.","correct":false},{"id":"05aeb0bc36d7b53aa30bf9e22b6cd120","text":"You are maxing out your PUT requests to S3.  You need to change over to multi-part upload as a workaround.","correct":false},{"id":"01148eae3319190a0b228c6d02c9572c","text":"You are maxing out your network connection.  You must split the traffic over multiple interfaces.","correct":false},{"id":"1dd4f25e52404e18ddec0b8711a82a13","text":"You are hitting the KMS encrypt request account limit.  You must request a limit increase via a Support Case.","correct":true},{"id":"60ff77ab365c15bb11771e94e3dc271d","text":"You have exceeded the number of API calls for your account.  You must create a new account.","correct":false}]},{"id":"a5d03cc5-f156-4400-bb92-99f41b8da075","domain":"awscsapro-domain2","question":"You have configured a VPC Gateway Endpoint to S3 from your VPC named VPC1 with a CIDR block of 10.0.0.0/16.  You have lots of buckets but following least privilege, you want to only allow the instances in VPC1 access to the only two buckets they need.  What is the most efficient way of doing this?","explanation":"You cannot use an IAM policy or bucket policy to allow access from a VPC IPv4 CIDR range (the private IPv4 address range). VPC CIDR blocks can be overlapping or identical, which may lead to unexpected results. Therefore, you cannot use the aws:SourceIp condition in your IAM policies for requests to Amazon S3 through a VPC Gateway Endpoint.  The most efficient way is to use an endpoint policy to explicitly allow access to the two buckets which has the effect of denying access to the other buckets.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html#vpc-endpoints-policies-s3","title":"Endpoints for Amazon S3 - Amazon Virtual Private Cloud"}],"answers":[{"id":"d614ddb85f137404469aec6d7b8e2422","text":"Create a endpoint policy that explicitly allows access to the two required buckets.","correct":true},{"id":"656a72bc234934fce6cec26dd222fe22","text":"Create a bucket policy for the two required buckets using stringMatch to deny access for any system not coming from VPC1.","correct":false},{"id":"0dbe1d58efa8e52b477a87d218c8dee9","text":"Create a bucket policy for the two required buckets using aws:SourceIp to allow access for any system within the VPCs private CIDR block.","correct":false},{"id":"adcebcb93efce3a968f80ff194367109","text":"Create an endpoint policy that uses a Condition key of aws:SourceVpc equals VPC1 to allow access to the required buckets.","correct":false},{"id":"f972dfc0e001a9013cc0ca818705f14a","text":"Create an endpoint policy that uses a Condition key of sourceIP to only allow instances within the 10.0.0.0/16 block.","correct":false}]},{"id":"06504582-ce03-4252-b1dc-29654ff427bb","domain":"awscsapro-domain5","question":"You have just set up a Service Catalog portfolio and collection of products for your users.  Unfortunately, the users are having difficulty launching one of the products and are getting \"access denied\" messages.  What could be the cause of this?","explanation":"For Service Catalog products to be successfully launched, either a launch constraint must be assigned and have sufficient permission to deploy the product or the user must have the same required permissions.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-launch.html","title":"AWS Service Catalog Launch Constraints - AWS Service Catalog"}],"answers":[{"id":"d11a80651ecb668bdbe507d4e7398b6a","text":"The launch constraint does not have permissions to CloudFormation.","correct":true},{"id":"7e0083aafd999688d628f67e003d79be","text":"The product does not have a launch constraint assigned.","correct":true},{"id":"a987914548e48bb64be70c11a97ec644","text":"A Service Catalog Policy has not yet been applied to the account.","correct":false},{"id":"43d54acb2dbc9f2dc8a0d793553b965e","text":"The user launching the product does not have required permissions to launch the product.","correct":true},{"id":"8bc518c42ab2dfa122390f1a497349a2","text":"The template constraint assigned to the product does not have the proper permissions.","correct":false},{"id":"779fec840f0f81e772ba3137d7ac28ad","text":"The notification constraint did not have access to the S3 location for the product's CloudFront template.","correct":false}]},{"id":"5af539b7-b132-4a3a-bc80-406c620e7325","domain":"awscsapro-domain1","question":"A food service business has begun an initiative to migrate all applications and data to the AWS cloud. Governance needs to be established before any migrations can occur. Business units such as sales, marketing, and product management have fluctuating infrastructure capacity and security requirements, while other business units like finance, operations, and human resources have more static demand. Security policies and compliance needs vary by project group within each business units. Each business unit is responsible for it's own cost center, and the finance group would like cost reporting to be as streamlined as possible. Which AWS account structure will best satisfy the company's governance needs?","explanation":"Leveraging AWS Organizations to manage an account structure with a core Organizational Unit and Organizational Units for each business unit provides flexibility for future organizational changes. Creating an account for each project group facilitates security policy differences within business units, and limits the exposure of a single security event. Managing differing security requirements by project group in a single account will require more governance maintenance. Creating billing, shared services, and log archive accounts in multiple Organizational Units will result in duplication of services, and can be done at the core level.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-laying-the-foundation/introduction.html","title":"Laying the Foundation: Setting Up Your Environment for Cost Optimization"},{"url":"https://aws.amazon.com/solutions/aws-landing-zone/?did=sl_card&trk=sl_card","title":"AWS Landing Zone"}],"answers":[{"id":"a8f7d8fbb7c6c3a1a14c91577dff42e1","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Place business units with similar security requirements in shared Organizational Units. Create accounts for each business unit in the shared Organizational Units. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"03705913700b8d76205d4203c58dc5e1","text":"Use AWS Organizations with a single Organizational Unit to consolidate costs. Create a billing account, a shared services account, and a log archive account in the Organizational Unit. Create individual accounts for each business unit. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs","correct":false},{"id":"bd400ff0d22480599228a0442d2bb8d4","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Create an Organizational Unit for each business unit that contains accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":true},{"id":"a66d8391267460b5800c5c3d07921767","text":"Use AWS Organizations to create Organizational Units for each business unit. Create a billing account, a shared services account, and a log archive account in each Organizational Unit. Create accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false}]},{"id":"34351bd0-7925-4246-bb61-c64bbf4d5baf","domain":"awscsapro-domain4","question":"An application in your company that requires extremely high disk IO is running on m3.2xlarge EC2 instances with Provisioned IOPS SSD EBS Volumes. The EC2 instances have been EBS-optimized to provide up to 8000 IOPS. During a period of heavy usage, the EBS volume on an instance failed, and the volume was completely non-functional. The AWS Operations Team restored the volume from the latest snapshot as quickly as possible, re-attached it to the affected instance and put the instance back into production. However, the performance of the restored volume was found to be extremely poor right after it went live, during which period the latency of I/O operations was significantly high. Thousands of incoming requests timed out during this phase of poor performance.\nYou are the AWS Architect. The CTO wants to know why this happened and how the poor performance from a freshly restored EBS Volume can be prevented in the future. Which answer best reflects the reason and mitigation strategy?","explanation":"Data gap cannot be the reason for high disk I/O latency. Whether the data being requested is on the disk or not cannot be responsible for the extended period of high disk I/O latency, as all operating systems index the contents in some way. They do not scan the whole disk to conclude that something is missing. Hence, the choice that suggests data gap as the reason is eliminated.\nEBS Optimization works straight away after a freshly restored volume is attached to an EBS optimized instance. Hence, the choice that suggests that EBS Optimization takes some time to kick in is eliminated.\nThere is nothing called set-up-cache command. The option that suggests that there is an inbuilt caching mechanism that needs to be activated is completely fictional, and is eliminated.\nThe only correct option is the one that correctly states that every new block read from a freshly restored EBS Volume must first be downloaded from S3. This is because EBS Snapshots are saved in S3. Remember that EBS Snapshots are incremental in nature. Every time a new snapshot is taken, only the data that changed is written to that particular snapshot. Internally, it maintains the pointers to older data that was written to S3 as part of previous snapshots. These blocks of data continue to reside on S3 even after an EBS Volume is restored, and is read the first time they are accessed. Linux utilities like dd or fio can be used after restoring an EBS Volume to read the whole volume first to get rid of this latency problem when the instance is put back in production.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-initialize.html","title":"Initializing Amazon EBS Volumes"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html","title":"Amazon EBS–Optimized Instances"}],"answers":[{"id":"51e87b2dbdfcb476b026779380119b06","text":"A freshly restored EBS Volume cannot utilize EBS Optimization Instances straight away, as the network traffic and EBS traffic traverse the same 10-gigabit network interface. Only after the entire volume is scanned by an asynchronous process, EBS Optimization kicks in. This increases the I/O latency until the volume is ready to utilize EBS Optimization. To fix this, update the restoration process to wait and run random I/O tests on a freshly restored EBS Volume. Put the instance back to production only after the desired I/O levels are reached.","correct":false},{"id":"1a47200401a925a2ad1df3d3286e26dc","text":"When a data block is accessed for the first time on a freshly restored EBS Volume, EBS has to download the block from S3 first. This increases the I/O latency until all blocks are accessed at least once. To fix this, update the restoration process to run tools to read the entire volume before putting the instance back to production.","correct":true},{"id":"d3a1c3b2669127dabe2eaf2c490fcd30","text":"A freshly restored EBS Volume needs pre-warming to activate the inbuilt caching mechanism. To fix this, update the restoration process to run the set-up-cache command on the freshly restored EBS Volume first before the instance is put back in production. Also, include random I/O tests to ensure that desired I/O levels are reached before putting the instance back to production.","correct":false},{"id":"ae32d192c3829287819a74ded72e0da7","text":"The latest snapshot did not have the most current data. It only had the data from the last time a snapshot was taken. The requests timed out because of this data gap. To mitigate this, increase the frequency of taking EBS snapshots.","correct":false}]},{"id":"447b50dd-cf00-4688-8181-2d87c302c538","domain":"awscsapro-domain2","question":"An application that collects time-series data uses DynamoDB as its data store and has amassed quite a collection of data--1TB in all.  Over time, you have noticed a regular query has slowed down to the point where it is causing issues.  You have verified that the query is optimized to use the partition key so you need to look elsewhere for performance improvements.  Which of the following when done together could you do to improve performance without increasing AWS costs?","explanation":"As a DynamoDB table grows, it will spit into more partitions.  If the RCU and WCU remain constant, they are divided equally across the partitions.  When the allocation of that partition is used up, you risk throttling.  AWS will permit burst RCU and WCU at times but it is not assured.  You could increase the RCU and WCU but this would increase cost. Therefore, we can archive off as much data as possible but also need to shrink the partitions down to something more reasonable.  We can do this by backing up and recreating/restoring the table.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html","title":"Best Practices for DynamoDB - Amazon DynamoDB"}],"answers":[{"id":"e132b6523705a41bc5ed88930a62a21c","text":"Increase the provisioned read and write capacity units.","correct":false},{"id":"28004d3db4c1cf8f98d735e3893f14d0","text":"Export the data then import it into a newly created table.","correct":true},{"id":"21bb1c71dec739e9fc76b69fb51ee515","text":"Create a secondary global index on the most common fields to increase performance. ","correct":false},{"id":"59b19c32945103a6f4da6ef17acd7f75","text":"Change the query to use a secondary local index instead of the partition key.","correct":false},{"id":"be0c8b120e1730a1fc73ec1b0ae38d50","text":"Archive off as much old data as possible to reduce the size of the table.","correct":true}]},{"id":"79f2f5be-b591-44e6-957b-eb0383640d7d","domain":"awscsapro-domain5","question":"You have a standard SQS queue to receive messages from the frontend application. The backend application is JAVA based and the AWS SDK is used to get the messages from the queue for processing. The SQS queue is not busy most of the time. According to the backend application logs, there is a high number of empty ReceiveMessageResponse instances returned. You want to adjust the settings to minimize the number of empty responses and reduce the cost. How would you implement this? ","explanation":"Amazon SQS long polling is preferable to short polling in most of the cases. Long polling requests let the consumers receive messages as soon as they arrive in the queue. It can help to reduce the number of empty responses. In order to enable long polling, the attribute ReceiveMessageWaitTimeSeconds should be more than 0. Short polling is incorrect. Visibility timeout and delivery delay do not address the problem of empty responses.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html","title":"Amazon SQS short and long polling"}],"answers":[{"id":"220352e5b3779c1f2030cfd4b391b19e","text":"Modify AWS SDK to get the messages in the SQS queue by short polling. The ReceiveMessage call from the consumer sets the WaitTimeSeconds attribute to 0. As a result, the empty responses are eliminated.","correct":false},{"id":"a5cdcd2968c3566cbb7fc7bcd5fef01a","text":"Add a delivery delay in the SQS queue such as 1 minute. The delay helps to postpone the delivery of new messages to the queue for some time. When the JAVA application polls the messages from the queue, there will be a lower chance to get an empty response.","correct":false},{"id":"c3cbf51c591cb7fa17bd023ab814f95c","text":"Consume the messages in the SQS queue using long polling. Set the queue attribute ReceiveMessageWaitTimeSeconds to be more than 0. Amazon SQS will wait until there is an available message in a queue before sending a response.","correct":true},{"id":"203fa6faf2e6bf53939b43300ec6dac2","text":"Increase the default visibility timeout of the queue to reduce the possibilities that the messages become visible to consumers again. The application can also use the ChangeMessageVisibility API to specify a suitable timeout value.","correct":false}]},{"id":"b401741c-5b37-4b47-8e61-7802fbc9d7d6","domain":"awscsapro-domain1","question":"You are helping a client consolidate several separate accounts into a single account.  This consolidation will result in approximately 50 new VPCs in their one account.  They want to continue to use Route 53 for DNS but only want it accessible privately. How can you accomplish this most efficiently?","explanation":"Private Hosted Zones provide DNS services to VPCs but cannot be access from the internet.  They can be associated with VPCs either by the console, CLI or programmatically via SDK.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html","title":"Associating More VPCs with a Private Hosted Zone - Amazon Route 53"}],"answers":[{"id":"cbffb64b6b43e6fe45496b6e77ce17b8","text":"Create a Private Hosted Zone within Route 53 for each respective VPC.  Configure replication between the private hosted zones to keep records in sync.","correct":false},{"id":"3cc28b12f45b3dee8f7f16a0f93d00ce","text":"Install BIND on an EC2 instance in a single VPC.  Create VPC peering connections between the DNS VPC and any new VPCs.  Configure a DHCP Option Set to assign a DNS and link that to each VPC.","correct":false},{"id":"f74daa2300ac594c111ca9fce198f19c","text":"Create a central DNS server using EC2 and BIND.  Configure Route 53 to reference this DNS server as a resolver.  Update DNS records at the registrar to point to the central DNS.","correct":false},{"id":"82d5ef7e7176200aa4350ef90dd4c354","text":"Create a Public Hosted Zone within Route 53 and associate it to each VPC.  Configure a NACL on each VPC to deny inbound DNS queries (UDP port 53).","correct":false},{"id":"315372936e7ffba65896da15d0f45c2d","text":"Create a Private Hosted Zone within Route 53.  As the new VPCs are created, associate them with the Private Hosted Zone.","correct":true}]},{"id":"7c5f884f-c0f9-4028-a725-50819d704324","domain":"awscsapro-domain5","question":"You deploy an application load balancer and an Auto Scaling group (ASG) in production for a new project. When instances in the ASG have a high CPU utilization, a new instance is launched. However, the new instance fails the health check from the ASG and has been terminated after some time. You check the logs in the instance and find that the startup script does not finish yet before the instance is terminated. How would you resolve the problem?","explanation":"Amazon EC2 Auto Scaling waits until the health check grace period ends before checking the health status of the instance. The grace period timer should be increased to give the instance more time to finish the startup script. Increasing the healthy threshold makes the instance more difficult to become healthy. Decreasing the timeout value also does not help as the instance may become unhealthy very quickly. Modifying the health check type from ELB to EC2 is unsuitable as the ASG cannot get the instance status from the application level. Even if the instance shows as healthy in ASG, the application may not be ready yet.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Check Grace Period"}],"answers":[{"id":"e322fd6a55a044826665dfce4ae7020b","text":"Decrease the timeout value in the ELB health check from 5 seconds to 1 second so that when the ELB performs the health check on the backend instances, the instances are able to respond in time before a timeout occurs.","correct":false},{"id":"b5ca84726bf6fcfd6522ab188c8224ea","text":"Increase the default healthy threshold in the health check of elastic load balancer from 5 to 10 so that the instance will become healthy more quickly once the startup script finishes in the new instance.","correct":false},{"id":"1d8964ced10da94d49d1a1efe02bbbca","text":"Modify the health check type from ELB to EC2 in the Auto Scaling group. Configure ASG to check the EC2 instance status. As long as the instance does not have a system level issue, it will not fail the health check in the ASG even when the startup script is still running.","correct":false},{"id":"e7ad9ea949ae87c6a6001a94f5a9bf48","text":"Increase the health check grace period in the Auto Scaling group configurations. When a new instance boots up, it is given more time to execute the startup scripts and run applications before the health check from ASG.","correct":true}]},{"id":"3b286cd3-ec67-4bc4-8f37-e19e6f629198","domain":"awscsapro-domain2","question":"You are the solution architect for a research paper monetization company that makes large PDF Research papers available for download from an S3 bucket. The S3 bucket is configured as a static website. A Route53 CNAME record points the custom website domain to the website endpoint of the S3-hosted static website. As demand for downloads has increased throughout the world, the architecture board has decided to use a Cloudfront web distribution that fetches content from the website endpoint of the static website hosted on S3. The Route 53 CNAME record will be modified to point at the Cloudfront distribution URL.\nFor security, it is required that all request from client browsers use HTTPS. Additionally, the system must block anyone from accessing the S3-hosted static website directly other than the Cloudfront distribution. Which approach meets the above requirements?","explanation":"The key to answering this question correctly is to note the fact that the origin is a website and not just a plain S3 bucket - note the usage of the phrase website endpoint in the question. While setting up such an origin, one cannot just pick the S3 bucket as the origin, or use OAI. Hence, the two choices that rely on picking the S3 bucket as the origin and using OAI to restrict access are incorrect.\nIn the given scenario, the Cloudfront web distribution is being configured to use the website endpoint of the static website as the origin. A big difference between these two scenarios is - if you use an S3 bucket as the origin, Cloudfront uses the REST API interface of S3 to communicate with the origin. If you use the website endpoint as the origin, Cloudfront uses the website URL as the origin. These endpoints have different behaviours - see the link titled Key Differences Between the Amazon Website and the REST API Endpoint. S3 REST API is more versatile, allowing the client to pass richer information like AWS Identity, thereby allowing the exchange of information that makes OAI possible. That is the reason why OAI cannot be used when Cloudfront is using the website endpoint where only GET and HEAD requests are allowed on objects.\nTherefore, in this scenario, OAI cannot be used. Instead, we have to use a custom header that only Cloudfront can inject into the Origin-bound HTTP request. The bucket policy of the S3 bucket hosting the static website can then check for the existence of said header. The assumption here is that if any browser ever directly uses the website URL of the S3-hosted static website (which is of the format examplestaticwebsitebucket.s3-website-us-east-1.amazonaws.com), their request will not contain this header, and hence will be rejected by the bucket policy.\nAlso, S3-hosted static websites do not support HTTPS. Therefore, Origin Protocol Policy, in this case, cannot be set to HTTPS Only. We can only set Viewer Protocol Policy. Only the browser to Cloudfront half will be HTTPS. The Cloudfront to Origin half cannot be HTTPS in this case","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteEndpoints.html#WebsiteRestEndpointDiff","title":"Key Differences Between the Amazon Website and the REST API Endpoint"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html","title":"Values That You Specify When You Create or Update a Distribution"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/","title":"How do I use CloudFront to serve a static website hosted on Amazon S3?"}],"answers":[{"id":"99836ac4dd080c1897cc5b3fc4d05bf7","text":"While setting up the Cloudfront Web Distribution, select the S3 bucket as the origin. Select Restrict Bucket Access to Yes, and create a new Origin Access Identity (OAI) that will prevent anyone else other than the Cloudfront web distribution to access the S3 bucket. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS. Additionally, set the value of Origin Protocol Policy to HTTPS Only.","correct":false},{"id":"bf689f9ae9dcc4c34420f34e012811a4","text":"While setting up the Cloudfront Web Distribution, use the website endpoint of the S3-hosted static website as the Origin Domain Name. Also, set up Origin Custom Header. Then specify a header like Referrer, with its value set to some secret value. Set the bucket policy of the S3 bucket to allow s3 GetObject on the condition that the HTTP request includes the custom Referrer header. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS.","correct":true},{"id":"3a34f270648088ee645d11ba0aa6420d","text":"While setting up the Cloudfront Web Distribution, select the S3 bucket as the origin. Select Restrict Bucket Access to Yes, and create a new Origin Access Identity (OAI) that will prevent anyone else other than the Cloudfront web distribution to access the S3 bucket. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS.","correct":false},{"id":"2e8883471b2e107d4883062beed92ea6","text":"While setting up the Cloudfront Web Distribution, use the website endpoint of the S3-hosted static website as the Origin Domain Name. Also, set up Origin Custom Header. Then specify a header like Referrer, with its value set to some secret value. Set the bucket policy of the S3 bucket to allow s3 GetObject on the condition that the HTTP request includes the custom Referrer header. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS. Additionally, set the value of Origin Protocol Policy to HTTPS Only.","correct":false}]},{"id":"f7d7767b-9159-4e53-8e37-ff9bf41ace17","domain":"awscsapro-domain5","question":"You are working with a client to help them design a future AWS architecture for their web environment.  They are open with regard to the specific services and tools used but it needs to consist of a presentation layer and a data store layer.  In a brainstorming session, these options were conceived.  As the consulting architect, which of these would you consider feasible?","explanation":"The only two options which contain feasible options are the Beanstalk and S3/Dynamo methods.  One would not create a new K8s deployment for for a new web update.  CodeBuild and AWS Config are not the correct tools for how they are being suggested.","links":[{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild – Fully Managed Build Service"}],"answers":[{"id":"01a7f4c09f65e41884b0a72843a8d55b","text":"Deploy an auto scaling group of EC2 instances behind an Application Load Balancer.  Provision a Mulit-AZ RDS instance to act as the data store, configuring a caching layer to offload queries from the database.  Use a User Script in the AMI definition to download the latest web assets from S3 upon boot-up.  When changes are required, use AWS Config to automatically fetch a new version of web content from S3 when a new version is created.","correct":false},{"id":"f5c7f9be39386b15747c0fe57d5040ba","text":"Deploy Kubernetes on an auto-scaled group of EC2 instances.  Define pods to represent the multiple tiers of the landscape.  Use ElastiCache for Memcached to offload queries from a Multi-AZ RDS instance.  To deploy changes to the landscape, create a new EKS deployment containing all the updated service containers and deploy them to replace all the previous existing tiers.  Ensure the DevOps team understands the rollback procedures.","correct":false},{"id":"ca4cc92538f3b73d221c9b5d4378e1f8","text":"Setup a traditional three tier architecture with a CloudFormation template per tier and one master template to link in the others.  Configure a CodeBuild stack and set this stack to perform automated Blue Green deployments whenever any code change is made.","correct":false},{"id":"d6a73928290b05c25d87e26ece9e94a6","text":"Create a monolithic architecture using Elastic Beanstalk configured in the console.  Create an RDS instance outside the Beanstalk environment and configure it for multi-AZ availability.  When a new landscape change is required, use a command line script to implement the change.","correct":true},{"id":"c27a151a7715575fb1ebf0225c6aee09","text":"Use the AngularJS framework to create a single-page application.  Use the API Gateway to provide public access to DynamoDB to serve as the data layer.  Store the web page on S3 and deploy it using CloudFront.  When changes are required, upload the new web page to S3.  Use S3 Events to trigger a Lambda function which expires the cache on CloudFront.","correct":true}]},{"id":"083b20e3-95ff-4b8a-b655-aedf1de67c6c","domain":"awscsapro-domain4","question":"The security monitor team informs you that two EC2 instances are not compliant reported by an AWS Config rule and the team receives SNS notifications. They require you to fix the issues as soon as possible for security concerns. You check that the Config rule uses a custom Lambda function to inspect if EBS volumes are encrypted using a key with imported key material. However, at the moment the EBS volumes in the EC2 instances are not encrypted at all. You know that the EC2 instances are owned by developers but you do not know the details about how the instances are created. What is the best way for you to address the issue?","explanation":"The key must have imported key material according to the AWS Config rule. It should be a new key created in KMS. Existing KMS cannot import a new key material and AWS Managed Key such as aws/ebs cannot be modified either. CloudHSM is more expensive than KMS and is not required in this scenario. Besides, when the new encrypted EBS volume is attached, it should be attached to the same device name such as /dev/xvda1.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html","title":"How to import key material in AWS Key Management Service (AWS KMS)?"}],"answers":[{"id":"94975e581f509af38c350ecdd5b951f5","text":"Import a new key material to an existing Customer Managed Key (CMK) in KMS. Create an AMI from the EC2 instance. Then launch a new EC2 instance from the AMI. Encrypt the EBS volume in the new instance. Terminate the old instance after the new one is in service.","correct":false},{"id":"5a6b8e38c5d149a42e259d93323f12aa","text":"Modify the AWS Managed Key (AWS/EBS) in KMS to include an imported key material. Create a snapshot of the EBS volume. Then create a new volume from the snapshot with the volume encrypted. Detach the original volume and attach the new encrypted EBS to another device name of the instance.","correct":false},{"id":"03c530623f9019c80c05daa34ad8fab1","text":"Create a new EBS key from CloudHSM with imported key material. Create a new EBS volume encrypted with the new key. Attach the volume to the EC2 instance. Use Linux dd command to copy data from non-encrypted volume to encrypted volume. Unmount the old volume after the sync is complete.","correct":false},{"id":"62f46fb2afeb695bf73a050f1662cd44","text":"Create a Customer Managed Key (CMK) in KMS with imported key material. Create a snapshot of the EBS volume. Copy the snapshot and encrypt the new one with the new CMK. Then create a volume from the snapshot. Detach the original volume and attach the new encrypted EBS to the same device name of the instance.","correct":true}]},{"id":"dd8b46c7-d1d5-4326-a092-927b9333fd2a","domain":"awscsapro-domain5","question":"You are helping a company transition their website assets over to AWS.  The project is nearing completion with one major portion left.  They want to be able to direct traffic to specific regional EC2 web servers based on which country the end user is located.  At present, the domain name they use is registered with a third-party registrar.  What can they do?","explanation":"You can use Route 53 if the domain is registered under a third-party registrar.  When using Geolocation routing policies in Route 53, you always want to specify a default option in case the country cannot be identified.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy - Amazon Route 53"}],"answers":[{"id":"381d1621ab4c93aca8cc780f05e98c50","text":"Initiate a domain transfer request with the current registrar.  Once the request goes through, create a public hosted zone in Route 53.  Create SRV records for each regional EC2 instance using a Geolocation routing policy.  Create an alias record for the top-level domain and link that to the SRV records.","correct":false},{"id":"294ce2854fbe727bd4f4917543d45bec","text":"Create a private hosted zone for the domain in Route 53.  Update the DNS record entries in the registrars database to use AWS DNS Servers.  Once the DNS changes are fully propagated across the internet and the TTL has expired, convert the private hosted zone to a public hosted zone.  Create A-type records for all the regional EC2 instances and configure a Geo-proximity policy for each record, ensuring the bias across all records sums to 100.","correct":false},{"id":"724dab26998c86ab7ddc7faa063285b8","text":"You cannot use Route 53 routing policies unless AWS is the registrar of record for your domain.  A workaround could be to configure your own top-level DNS server using BIND.  Ensure the NS and SOA records point to this instances.  Create A-type records pointing to the IP addresses of the regional EC2 web servers.  Dynamically redirect requests using customized BIND rules and a third-party IP geolocation database.","correct":false},{"id":"81efc0b140e618c378c9e9bc59dd4ca8","text":"Create a public hosted zone for the domain in Route 53.  Update the DNS entries in the registrars database to use AWS DNS Servers as defined in the NS record on Route 53.  Create A-type records for all EC2 instances. Configure CNAME records for the main FQDN that point to regional A records using a Geolocation routing policy.  Create another CNAME record as a default route.","correct":true}]},{"id":"24937858-d37e-4f9b-b195-d87c42b3f1ca","domain":"awscsapro-domain2","question":"You are designing a workflow that will handle very confidential healthcare information.  You are designing a loosely coupled system comprised of different services.  One service handles a decryption activity using a CMK stored in AWS KMS.  To meet very strict audit requirements, you must demonstrate that you are following the Principle of Least Privilege dynamically--meaning that processes should only have the minimal amount of access and only precisely when they need it.  Given this requirement and AWS limitations, what method is the most efficient to secure the Decryption service?","explanation":"Grants in KMS are useful for dynamically and programmatically allowing a process the ability to use the key then revoking after the need is over.  This is more efficient than manipulating IAM roles or policies.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/grants.html","title":"Using Grants - AWS Key Management Service"}],"answers":[{"id":"45b8475be8fa894d6c7145b5536d21fe","text":"Use a grant constraint to deny access to the key except for the service account that is running the workflow processes.  Enable CloudTrail alerts if any other role attempts to access the CMK.","correct":false},{"id":"3cc7443088dcd1d7935eaf578a49d078","text":"Create an IAM key policy that explicitly denies access to the Decryption operation of the CMK.  Assign that policy to a role that is then assigned to the process executing the Decryption service.  Use a Lambda function to programmatically remove and add the IAM policy to the role as needed by the decryption process.","correct":false},{"id":"b5f054e3c6369a2c4a9256e55359f9ff","text":"The current AWS platform services are not well suited for implementing Principle of Least Privilege in a dynamic manner.  Consider a different design that makes use of a more monolithic architecture rather than services.","correct":false},{"id":"2202679b0d915deca81caf1d431477a4","text":"Create a IAM key policy that explicitly allows access to the CMK and assign that to a role.  Assign the role to the process that is executing the Decryption service.  At the end of the day, programmatically revoke that role until the start of the next day.","correct":false},{"id":"6c5638772104a82518657e9bfbc0970d","text":"In the step right before the Decryption step, programmatically apply a grant to the CMK that allows the service access to the CMK key.  In the step immediately after the decryption, explicitly revoke the grant.","correct":true}]},{"id":"c9d5dfbb-8dda-4a84-bf04-49aa7f88d4db","domain":"awscsapro-domain2","question":"A healthcare provider has a recent history of failing to match patient clinical records from other providers with their own records. This has resulted in missed diagnoses and delayed treatments. To address the issue, they've begun an initiative to store patient records, both those generated in-house and those coming from other providers, in CSV format on an Amazon S3 data lake. They'll use other AWS services to aggregate and deduplicate patient information. Which HIPAA compliant solution will provide them with the highest level of accuracy to improve their level of patient care?","explanation":"An AWS Glue crawler can scan the raw patient CSV data and store the schemas in the Glue data catalog. A Glue ML Transform can be written to find patient information matches in tables defined in the Glue data catalog. If you don't want to write an ML Transform to find the matches, AWS provides a custom one called FindMatches. After training the ML Transform, you can use it as part of a scheduled Glue ETL job, which can write results back to the S3 data lake. Kinesis Data Analytics isn't capable of querying data in S3. It only provides SQL manipulation of the incoming stream. An EMR Hive job will be able to perform some basic record matching, but machine learning will provide greater matching accuracy over time. While SageMaker could provide a viable solution, the Caffe framework is most commonly used for computer vision use cases, not record matching.","links":[{"url":"https://aws.amazon.com/glue/","title":"AWS Glue"},{"url":"https://aws.amazon.com/about-aws/whats-new/2019/08/aws-glue-provides-findmatches-ml-transform-to-deduplicate/","title":"AWS Glue now provides FindMatches ML transform to deduplicate and find matching records in your dataset"},{"url":"https://aws.amazon.com/blogs/big-data/matching-patient-records-with-the-aws-lake-formation-findmatches-transform/","title":"Matching patient records with the AWS Lake Formation FindMatches transform"}],"answers":[{"id":"4745b6f6f2c711702b03c99faaab04b1","text":"Implement an AWS Glue crawler to determine patient record formats. Create and train a Glue ML Transform to match patient records. Execute a Glue ETL job using the ML Transform, and store results back in the data lake. Configure the Glue scheduler to run the crawler and the ETL jobs periodically","correct":true},{"id":"50bb504039515eef7c1372c744aa4d79","text":"As new patient records are ingested to S3, trigger an AWS Lambda function to start an AWS Glue crawler to update the Glue data catalog. Have the Lambda function then run an Amazon EMR Hive job to perform patient record matching and write the results back to the data lake","correct":false},{"id":"7a5ba1c8cc4da315c99a4458891c790d","text":"Prior to depositing patient records in the S3 data lake, ingest them with Amazon Kinesis Data Streams and configure Amazon Kinesis Data Analytics as the consumer of the stream. Have Kinesis Data Analytics correlate records with those already ingested to S3 and write them to the corresponding patient folder","correct":false},{"id":"734eda6752df4074b7d1de2dc63055d6","text":"Create and train an Amazon SageMaker Caffe model to match patient records. Schedule an AWS Lambda function to periodically run the SageMaker model and deposit the results back into S3","correct":false}]},{"id":"115e1b30-23e4-4f3f-9c13-a0086f6af223","domain":"awscsapro-domain2","question":"You are working with a pharmaceutical company on designing a workflow for processing data.  Once a day, a large 2TB dataset is dropped off at a pre-defined file share where the file is processed by a Python script containing some proprietary data aggregation routines.  On average, it takes 20-30 minutes to complete the processing.  At the end, a notification has to be sent to the submitter of the dataset letting them know processing is complete.  Which of the following architectures will work in this scenario?","explanation":"While it may not be the most cost-effective, the EFS option is the only one that can work.  A processing time of 20-30 minutes rules out Lambda (at present with a processing limit of 15 minutes).  If we create an EBS volume with a full OS on it and mount as root for a new instance with the data set included, we still would not be able to dismount the root volume without shutting down the instance.  This would not let us issue an SES SDK call.  The database is also far too large for SQS.","links":[{"url":"https://aws.amazon.com/efs/features/","title":"Amazon Elastic File System (EFS) | Cloud File Storage | Product Features"}],"answers":[{"id":"3e1542fcef251cf86c8bdcc89d83aaa7","text":"Stand up memory optimized instances and provision an EFS volume. Pre-load the data on the EFS volume.  Use a User Data script to sync the data from the EFS share to the local instance store.  Use an SDK call to SNS to notify when the processing is complete, sync the processed data back to the EFS volume and shutdown the instance. ","correct":true},{"id":"dee7711bae75cac018761467694d89e3","text":"Use SQS to take in the data set.  Use a Step Function to Launch Lambda functions in a fan-out architecture for data processing and then send an SNS message to notify when the processing is complete.  Store the processed data on S3.","correct":false},{"id":"1831d929918e3c425b20e476c1716ccc","text":"Load inbound dataset on an EBS volume.  Stand up an EBS-optimized instance and mount the data volume as the root volume.  Once the data processing is complete, unmount the EBS volume and issue an SDK call to SES to notify of completion.  Configure SES to trigger an instance shutdown after the notification is sent.","correct":false},{"id":"5f8da9106eba72e74c1a0d6415a235af","text":"Create an S3 bucket to store the incoming dataset.  Once the dataset has been fully received, use S3 Events to launch a Lambda function with the Python script to process the data.  When finished, use an SDK call to SNS to notify when the processing is complete.  Store the processed data back on S3.","correct":false}]},{"id":"33803c8a-b588-4dca-8067-e500383254f3","domain":"awscsapro-domain4","question":"You work for a retail services company that has 8 S3 buckets in us-east-1 region. Some of the buckets have a lot of objects in them. There are Lambda functions and EC2-hosted custom application code where the names of these buckets are hardcoded. Your manager is worried about disaster recovery. As part of her business continuity plan, she has requested you to set up Cross-Region Replication of these S3 buckets to us-west-1, ensuring that the replicated objects are using a less expensive Storage Class because they would not be accessed unless disaster strikes. You are worried that in the event of failover due to the entire us-east-1 region being unavailable, the application code, once deployed in us-west-1, must continue to work while trying to access the S3 buckets in the new region. She has also requested you to start taking periodic snapshots of EBS Volumes and make these snapshots available in the us-west-1 region so that EC2 instances can be launched in us-west-1 using these snapshots if needed. How would you ensure that (a) the launching of EC2 instances works in us-west-1 and (b) your application code works with the us-west-1 S3 buckets?","explanation":"This question presents two problems - (1) how to ensure that EBS snapshots are created periodically and are also made available in a different region for launching required EC2 instances in case of failure of the primary region (2) how to deal with application code where S3 bucket names are hardcoded and whether this hardcoding will impact disaster recovery while trying to run in a different region. Both of these problems are real-life issues AWS customers face when designing and planning their disaster recovery solutions.\n(1)Remember that Data Lifecycle Manager can only schedule snapshot creation in the same Region. If we want to copy that snapshot into a different region, we must write our own scripts or Lambda functions for doing that. Hence, the choices that state that DLM can be used to directly create the snapshot into different regions are eliminated. Additionally, only root volume snapshots can be used to create an AMI. Non-root EBS Volume snapshots cannot be used to generate an AMI. Hence, the choices that specify using non-root volume snapshots are eliminated.\n(2)Remember that S3 bucket names are globally unique. Hence, one cannot create a second S3 bucket in the DR Region with the same name as the bucket in the primary region. Hence, the options that hint the creation of S3 buckets by the same name are eliminated. This results in a problem if S3 names are hardcoded in the application - that application will simply not run in a new region, it will fail. Hence, it is best to avoid hardcoding, and fetch the S3 bucket name from a key-value storage service like AWS Systems Manager Parameter Store at runtime. Creating this Parameter Store in each region and storing the correct bucket names in them can help in designing this non-hardcoded solution. Additionally, enabling Cross-Region Replication does not copy pre-existing content. Hence, the choices that suggest that pre-existing content will be automatically copied are eliminated.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-launch-snapshot.html","title":"Launching an Instance from a Backup"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/ec2/copy-snapshot.html","title":"Copy-snapshot documentation"}],"answers":[{"id":"824dc187ab89444593b50521f60b8ff3","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":true},{"id":"fc1a2efe00ef610249a55dadb0dd64fe","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false},{"id":"fae496411f2d461e0926abfdf8ad8b64","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":false},{"id":"02e8aed40d51e4bf9256f1ed436aa069","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false}]},{"id":"02a9611c-591c-4280-bb83-6c65c7c4921f","domain":"awscsapro-domain5","question":"A sporting goods retailer runs WordPress on Amazon EC2 Linux instances to host their customer-facing website. An ELB Application Load Balancer sits in front of the EC2 instances in Auto Scaling Groups in two different Availability Zones of a single AWS region. The load balancer serves as an origin for Amazon CloudFront. Amazon Aurora provides the database for WordPress with the master instance in one of the Availability Zones and a read replica in the other. Many custom and downloaded WordPress plugins have been installed. Much of the DevOps teams' time is spent manually updating plugins across the EC2 instances in the two Availability Zones. The website suffers from poor performance between the Thanksgiving and Christmas holidays due to a high occurrence of product catalog lookups. What should be done to increase ongoing operational efficiency and performance during high-volume periods?","explanation":"ElastiCache Memcached will provide in-memory access speeds for the catalog read transactions. A WordPress plugin is required to leverage caching. WordPress can access an EFS Mount Target for file sharing across all instances. Aurora offers a MySQL option, and WordPress requires MySQL, so the solution would have been set up that way already. CodeDeploy could update plugins on all instances, and will work well for the custom in-house code, but triggering the updates of downloaded plugins will need to be orchestrated. Aurora Auto Scaling will distribute catalog reads across multiple replicas for increased performance, but not to the extent of in-memory caching. Elastic File System is a managed service providing operational advantages over NFS file shares. ElastiCache Redis will provide the in-memory read performance desired, but changing the wp-config.php file won't provide access to it, as a plugin is needed for that. WordPress does work with S3, but a shared file system is easier to implement.","links":[{"url":"https://aws.amazon.com/getting-started/projects/build-wordpress-website/","title":"Build a WordPress Website"},{"url":"https://github.com/aws-samples/aws-refarch-wordpress?did=wp_card&trk=wp_card","title":"Hosting WordPress on AWS"}],"answers":[{"id":"f060160b20c4408b2442010d3ea4d387","text":"Use Amazon ElastiCache Redis as a caching layer between the EC2 instances and the database. Change wp-config.php to point to the Redis caching layer, and have Redis point to Aurora. Move the WordPress files to S3 and have WordPress access them there.","correct":false},{"id":"aeb27370afc3b672eb0a1afcb28e9176","text":"Implement Aurora Auto Scaling to increase the number of replicas automatically as demand increases. Create an NFS file share to hold the WordPress files. Access the file share from the EC2 instances in both Availability Zones.","correct":false},{"id":"bc643d3342a5a675e65e5baed00e88b9","text":"Deploy Amazon ElastiCache Memcached as a caching layer between the EC2 instances and the database. Install a WordPress plugin to read from Memcached. Implement Amazon Elastic File System to store the WordPress files and create mount targets in each EC2 subnet.","correct":true},{"id":"17b12e57cd85610e888cda82b5a8a145","text":"Migrate the WordPress database to RDS MySQL since MySQL is WordPress's native database and WordPress is performance optimized for MySQL. Implement AWS CodeDeploy to update WordPress plugins on all EC2 instances.","correct":false}]},{"id":"230f422f-7118-4096-8dce-59c642fb55c8","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a new Direct Connect connection.  The connection is up and you can ping the AWS peer IP address, but the BGP peering session cannot be established.  What should be your next logical troubleshooting steps?","explanation":"Because the connection is up and we can ping the AWS peer, the problem must be at a higher level on the OSI model than the Physical or Data layers.  BGP uses TCP port 179 to communicate routes so we should check that no NACL or SG is blocking it.  Additionally, we should make sure the ASNs are properly configured in the proper ranges.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/Troubleshooting.html","title":"Troubleshooting AWS Direct Connect - AWS Direct Connect"}],"answers":[{"id":"3d2a55832b90f19a2137e8715525d717","text":"Make sure no firewalls or ACLs are blocking TCP port 179 or any high-numbered ephemeral ports.","correct":true},{"id":"8fc27418eee2ce07b64bc672007d2c1b","text":"Contact the co-location provider and request a written report for the Tx/Rx optical signal across the cross connect.","correct":false},{"id":"45d4c1753395277878b9a17343628c52","text":"Ensure that the VLAN is configured properly between your on-prem router the provider. ","correct":false},{"id":"81977d7a1eb5714746851077b93f44d6","text":"Power cycle all the equipment to clear ARP table cache.","correct":false},{"id":"16e5aea88df69cc18f99e3f066ec99c1","text":"Ensure that the local ASNs and AWS-side ASNs are properly configured.","correct":true},{"id":"edd3f9408cecbbf9182678ccc51d7981","text":"Ask your network provider to provide you with a cross connect completion notice and compare the ports with those listed on your LOA-CFA","correct":false}]},{"id":"c29d0343-9d60-4882-b1f3-2897ef7e889a","domain":"awscsapro-domain2","question":"Your team starts using Docker to manage a web application. The Docker image is pushed to AWS ECR and the application is hosted in Amazon Elastic Container Service (Amazon ECS) containers. After the application is deployed, you find that there are occasions where the application is attacked by SQL injection or cross-site scripting. You would like to set up certain rules to protect the web application from the common application-layer exploits so that specific traffic patterns that you define are filtered out. How would you implement this?","explanation":"AWS WAF is a web application firewall that can protect an Amazon CloudFront distribution, an Amazon API Gateway API, or an Application Load Balancer. It cannot associate with an ECS cluster. However you can configure the Amazon ECS to use an Application Load Balancer and then associate the WAF ACL with the ELB. CloudFront distribution can not use the ECS cluster as the origin. AWS Shield Advanced is only for DDoS attacks and is not suitable.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html","title":"Use AWS WAF to protect applications that are hosted in Amazon Elastic Container"}],"answers":[{"id":"51f6c122390f4f18e845faf465045044","text":"Set up access control lists (ACLs), rules, and conditions in AWS WAF to define acceptable or unacceptable traffic. Configure Amazon ECS to use an Application Load Balancer to distribute the traffic. Use WAF to protect the application behind the Application Load balancer.","correct":true},{"id":"7db2f0b98cf72e4dcc0d93100a6230cd","text":"Enable AWS Shield Advanced to protect the application from common attacks such as SQL injection, DDoS and cross-site scripting. In AWS Firewall Manager, associate AWS Shield with the ECS cluster name so that the ECS cluster is monitored by AWS Shield and the traffic is filtered based on the created rules.","correct":false},{"id":"9ee0fc90381d7cb22768fda1080073af","text":"Configure an ACL in AWS WAF and the ACL contains rules to block common attack patterns, such as SQL injection or cross-site scripting. Setup a CloudFront distribution to use the ECS cluster as the origin. Associate the WAF ACL with the CloudFront distribution.","correct":false},{"id":"d804d87fdada57fe73d5fb20b76721f4","text":"Configure rules to protect the application from common attacks including SQL injection and cross-site scripting in AWS Firewall Manager. Use an Application Load Balancer to distribute traffic to the ECS cluster. In AWS Firewall Manager, apply the rules in the Application Load Balancer.","correct":false}]}]}}}}
