{"data":{"createNewExamAttempt":{"attempt":{"id":"e4f0c557-aef4-49ba-b004-ce3aba76a730"},"exam":{"id":"2aeb8c88-d2b9-4171-ae79-403238cd4b4e","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"dc5b1869-9feb-4074-a582-f42e8f358272","domain":"awscsapro-domain4","question":"You have decided to make some changes to your landscape. Your landscape consists of four EC2 instances within a VPC interacting mostly with S3 buckets.  You decide to move your EC2 instances into a spread placement group.  You then create a VPC Gateway Endpoint for S3.  These changes have which of these impacts?","explanation":"Costs will decrease because you are using the VPC Gateway Endpoint to reach S3 rather than Internet egress.  Security will be improved because the traffic is not routed out through the Internet.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html","title":"Endpoints for Amazon S3 - Amazon Virtual Private Cloud"}],"answers":[{"id":"ec2facee172127252b37883f153ee675","text":"Costs will increase.","correct":false},{"id":"1ea65cd7f8d031e9a625162d59461522","text":"Security profile will stay the same.","correct":false},{"id":"7a549fb0ca57680e691ab656b55d0277","text":"Costs will stay the same.","correct":false},{"id":"1de433430d814a079012ae9f2bb7b404","text":"Costs will decrease.","correct":true},{"id":"cdbabf3a8907b7fc48761f3c8c22864e","text":"Security profile will be improved.","correct":true},{"id":"a98a8d72a35888659eaf4659936a6809","text":"Security profile will be reduced.","correct":false}]},{"id":"35fc536d-d968-472d-84d5-a7ae5d343564","domain":"awscsapro-domain1","question":"You work for a genetics company that has extremely large datasets stored in S3. You need to minimize storage costs, while maintaining mandated restore times that depend on the age of the data. Data 30-59 days old must be available immediately, and data ≥ 60 days old must be available within 12 hours. Which of the following options below should you consider?","explanation":"You should use S3 - IA for the data that needs to be accessed immediately, and you should use Glacier for the data that must be recovered within 12 hours. S3 - RRS and 1Zone-IA would not be suitable solution for irreplaceable data or data that required immediate access (reduced Durability or Availability), and CloudFront is a CDN service, not a storage solution.  The use of absolute words like 'Must' is an important clue as it will eliminate options where the case may not be possible such as with OneZone-IA.","links":[{"url":"https://aws.amazon.com/s3/faqs/#sia","title":"S3 - Infrequent Access"},{"url":"https://aws.amazon.com/s3/faqs/#glacier","title":"About Glacier"}],"answers":[{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":true},{"id":"e9a5105fa288ef2b71c037e42d665d91","text":"S3 - OneZone-IA","correct":false},{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false},{"id":"31e831ec49678aed7f467f791d1f8704","text":"S3 - RRS","correct":false},{"id":"4340570ba672bfa48cd45e3f026c01d1","text":"S3 - IA","correct":true}]},{"id":"722221be-beb9-4a2b-8ae1-dff52b80125c","domain":"awscsapro-domain3","question":"You work for a technology company with two leased data centres (one on the east coast and one on the west coast) and one owned on-premises data centre. Management has decided to move the two leased data centres to the AWS cloud - one to us-east-1 and the other to us-west-1. The on-premises data centre will still continue running workloads which are not ready to move to the cloud.\nThis on-premises data centre must be always connected to the VPC-s in us-east-1 and us-west-1 for (a) the continuous replication of several databases and (b) the need to access some data residing on the on-premises data centre from applications running in both the AWS regions. The peak bandwidth required for these connections is (a) 500 Mbps between us-east-1 and on-premises, and (b) 8 Gbps between us-west-1 and on-premises. The applications would still be able to function at lower bandwidth, but the experience will be poor, which is not desirable. Both these connections must be Highly Available with 99.999% uptime. The connectivity solution must be cost-effective as well.\nAs the AWS Architect, what connectivity solution would you propose, so that all Bandwidth, HA and cost-effectiveness requirements are met?","explanation":"We can eliminate the VPC Peering solution immediately, as VPC Peering is for connecting two VPC-s on AWS. VPC Peering cannot be used to connect an AWS VPC with an on-premises network.\nOut of the remaining choices, the one that proposes connecting to us-east-1 using VPN and us-west-1 using Direct Connect comes very close to fulfilling all requirements. It suffers from two problems, however. One - the peak bandwidth requirement for us-east-1 is 500 Mbps. A VPN connection cannot be expected to provide 500 Mbps most of the time, as the true bandwidth someone can get from a VPN connection depends on a lot of factors including internet traffic it is sharing the route with. Secondly, if we are paying for a Direct Connect connection for the other region anyway, why not just use that one for this region too? Now, there is something called Direct Connect Gateways that makes it possible to share multiple AWS Regions using the same Direct Connect connection. The knowledge of Direct Connect Gateways is important for the AWS SA-P exam. Hence, this question tests this knowledge. The correct answer is the only one that uses Direct Connect Gateway.\nThe other choice that uses two separate Direct COnnect connections (one for each region) is not cost-effective, especially because since 2017, Direct Connect Gateways make it possible to connect to multiple AWS Regions using the same Direct Connect connection.\nRegarding HA, it is always a good practice to set up a VPN connection as a back-up for Direct Connect. The only requirement to do this is that the back-up VPN connection must also use the same Virtual Private Gateway on the AWS VPC side, otherwise traffic cannot fail over easily.\nNote about Direct Connect Gateways - they not only allow a customer to connect to two AWS Regions using a single Direct Connect connection, they also let the connected Regions communicate with each other! (This is why the VPC CIDR-s in us-east-1 and us-west-1 in the correct answer have to be non-overlapping.) There may be questions testing this aspect as well. Before Direct Connect Gateways existed, VPC Peering would be the only way for Inter-Region VPC Access. There is also another solution now - Transit Gateway, but this was announced late 2018. Usually, topics do not start appearing on the exam unless they have been more than 6 months in GA. Expect Transit Gateways to start appearing in questions now as well!","links":[{"url":"https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/","title":"AWS Direct Connect Gateway for Inter-Region VPC Access"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/","title":"Configuring VPN as back up of Direct Connect"},{"url":"https://aws.amazon.com/directconnect/sla/","title":"Direct Connect SLA puts uptime target at 99.9%. Therefore, if we need more than that, we should set up VPN as back up"}],"answers":[{"id":"f0671c30ae58d58c0d64162640e69527","text":"Use two Direct Connect connections - an 1 Gbps one between the on-premises data centre and us-east-1, and a 10 Gbps one between the on-premises data centre and us-west-1. For each Direct Connect connection, set up a back-up VPN connection that must use the same Virtual Private Gateway as the Direct Connect circuit","correct":false},{"id":"608dc6c035921ef09c640ebb70de9ddd","text":"Use VPC Peering to connect the on-premises network with both the us-east-1 and us-west-1 VPC-s independently. Bandwidth provided by VPC Peering is virtually unlimited, limited only by the instance sizes used. Also, VPC peering connections are fault-tolerant and scalable, so no back-up connectivity is needed","correct":false},{"id":"85b6b49a01a28a90d71ef3c20ca9da8d","text":"Connect the on-premises data centre and us-east-1 using redundant site-to-site VPN connections as its bandwidth requirements do not require a costly Direct Connect connection. The redundant VPN connections must use different customer gateways and will provide an HA solution for that region. Connect the on-premises data centre with us-west-1 using a 10 Gbps Direct Connect circuit. Set up a back-up VPN connection for this region such that it uses the same Virtual Private Gateway as the Direct Connect circuit","correct":false},{"id":"3ea4a9645ac22c0b2c6c427764a7ae01","text":"Set up a Direct Connect Gateway. Associate the Virtual Private Gateways from both the us-east-1 and us-west-1 VPC-s with this Direct Connect Gateway. Then set up a single 10 Gbps Direct Connect connection between the on-premises data centre and the Direct Connect Gateway, using a Private Virtual Interface. Ensure that the VPC CIDR-s in the two AWS Regions are non-overlapping. To increase HA, set up separate back-up VPN connections between the on-premises data centre and each of the two AWS Regions","correct":true}]},{"id":"8aa313fe-cd0f-4899-a2f4-e8f2fd64c245","domain":"awscsapro-domain4","question":"Your business depends on AWS S3 to host three kinds of files - images, documents and compressed installation packages. These files are accessed and downloaded by end-users from all US regions and west EU, though the compressed installation packages are downloaded rarely as users tend to access the service from their browsers instead of installing anything on their machines. Each installation package bundles several images and documents, and also includes binaries that are downloaded from a 3rd party service while creating the package files.\nThe images and documents range from a few KBs to a few hundred KBs in size and they are mostly static in nature. However, the compressed installation package files are generated every few hours because of changes done by the 3rd party service to their binaries, and some of them are as large as a few hundred GB-s. The installation package files can be regenerated from the images and documents fairly quickly if required. It is important to be able to retrieve older versions of the images and documents.\nWhich of the following storage solutions is the most cost-effective approach to design the storage for these files?","explanation":"The areas tested by this question are:\\n1. Versioning cannot be enabled at the object level. It is a bucket-level feature. This rules out the choice where we have a single bucket and selectively turn on versioning on for some objects only.\\n2. If you enable Versioning for a bucket containing large objects that are frequently created/uploaded, it will result in higher storage cost as all the previous versions will result in storage volume growing quickly because of frequent writes. In the given scenario, the compressed installation package files are large and also frequently generated (every few hours). There is no requirement to version them, as they can be quickly generated on-demand. Hence, putting them in a bucket that has Versioning enabled is not a good cost-effective solution. This rules out two choices - one where we have a single versioned bucket, the other where we enable versioning for both buckets.\\n3. Note that all options except one correctly identify the storage class requirements - the compressed installation package files should be stored as One-Zone IA because durability is not a prime requirement for these files (simply because they can be regenerated on-demand easily). They are rarely downloaded, hence IA is the correct class. Combined with low durability, One Zone IA is the most cost-effective solution. Only one option uses the incorrect storage tier for these files - note that IA is more expensive than One-Zone IA, and the question is about cost-effectiveness.\nHence, the only correct answer is the one that addresses both Versioning and Storage Class requirements correctly.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html","title":"Documentation on Object Versioning"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-howtoset","title":"Setting the Storage Class of an Object"}],"answers":[{"id":"90f6b37d063a0158682b034d578bf29b","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for both the buckets. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":false},{"id":"9e88398b12bdf0dd2fdc12a19f4962a1","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for Bucket A only. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":true},{"id":"2e4e12f367286b12f14ae74b1fd4e350","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the image and document objects only, but not for the compressed installation package files. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to Infrequent Access while uploading compressed installation package files","correct":false},{"id":"fd1d330daf4d05b4fa4c888a0584130f","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the bucket. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to One-Zone Infrequent Access while uploading compressed installation package files","correct":false}]},{"id":"482e75c9-071e-4a10-83f4-575f9c15b885","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They have just accidentally deleted the private key portion of their EC2 key pair.  Now, they are unable to SSH into their Amazon Linux servers.  Unfortunately the keys were not backed up and are considered gone for good.  What can this customer do to regain access to their instances?","explanation":"The two methods that AWS recommends if you lose a private key for an EC2 key pair are using Systems Manager Automation or using a secondary instance to edit the authorized_keys file.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2reset.html","title":"Reset Passwords and SSH Keys on Amazon EC2 Instances - AWS Systems Manager"}],"answers":[{"id":"79457a1b908d4a36cfeba625be909d40","text":"Use AWS Systems Manager Automation with the AWSSupport-ResetAccess document to create a new SSH key for your current instance.","correct":true},{"id":"492c38d2fe2c3b96608bb8436592fe26","text":"Use the AWS CLI with the EC2 ModifyInstance action to enable SSH password-only access for the ec2-user account.  Attach using a password rather than an SSH key.  Modify the authorized_key file for the new public key.","correct":false},{"id":"509a77ae9827c5fcb60ccecc62fc9853","text":"Stop the instances, detach its root volume and attach it as a data volume to another instances.  Modify the authorized_keys file, move the volume back to the original instance and restart the instances.","correct":true},{"id":"bec3d01a56c851f61a1a09c852635db7","text":"Generate and upload a new key pair.  Stop the instances and select the new key pair from the dropdown on the Instance Settings sub-menu in the Console.","correct":false},{"id":"be45655cb1d64dff71a97aa729bc4e4a","text":"Open the TELNET port (port 23) on the Security Group for the server.  Use a TELNET client to attach to the instances using the root account and password.  Modify the authorized_key file with the new public key.","correct":false},{"id":"f17aa014620843abd81fa849982566b0","text":"Create a new key pair in KMS then assign the new public key to the required EC2 instance.","correct":false}]},{"id":"663fbd6a-87bd-4fa6-a0ea-428ba2de5b51","domain":"awscsapro-domain5","question":"You manage a relatively complex landscape across multiple AZs.  You notice that the incoming requests vary mostly depending on the time of day but also there is a more unpredictable component resulting in smaller spikes and valleys for your resources.  Fortunately, you manage this landscape via OpsWorks Stacks.  What options, if any, are available to you as part of the OpsWorks featureset.","explanation":"OpsWorks Stacks offers three types of scaling: 24/7 for instances that remain on all the time; time-based for instances that can be scheduled for a certain time of day and on certain days of the week; and load-based scaling which will add instances based on metrics.  All this can be configured from within the OpsWorks Stack console.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/best-practices-autoscale.html","title":"Best Practices: Optimizing the Number of Application Servers - AWS OpsWorks"}],"answers":[{"id":"b7ff5b06f51facca179494cb2bb00e55","text":"You can enabled CloudFormation Anticipated Scaling that uses past CloudWatch metrics and machine learning to automatically design a scaling policy optimized for the incoming request patterns.","correct":false},{"id":"75ab4de4ea42c1971b0ee09ae04ca591","text":"You would define a baseline level of resources and configure them for 24/7 instances.  Then you could define a time-based instances to cover certain times of day.  Finally, you could cover the volatile spikes with a load-based instances.  All this can be done within OpsWorks Stacks.","correct":true},{"id":"3622d494dceb973760a46dea038d1dc2","text":"If you need the ability to dynamically scale, you will need to use OpsWorks for Chef Automate.  OpsWorks Stacks does not support scaling.","correct":false},{"id":"216c997091da6e24174ad1b83d0be8b9","text":"You would define a baseline level of resources within the OpsWorks Stack Console to cover the average load.  But for the periodic load, that requires a scheduled auto-scaling policy.  Similarly, for the volatile spikes, you must use a stepped auto-scaling policy defined in an auto scaling group. ","correct":false}]},{"id":"07f91ae7-094b-48a9-8924-a4d142cbbcb6","domain":"awscsapro-domain5","question":"On your last Security Penetration Test Audit, the auditors noticed that you were not effectively protecting against SQL injection attacks.  Even though you don't have any resources that are vulnerable to that type of attack, your Chief Information Security Officer insists you do something.  Your organization consists of approximately 30 AWS accounts.  Which steps will allow you to most efficiently protect against SQL injection attacks?","explanation":"Firewall Manager is a very effective way of managing WAF rules across many WAF instances and accounts.  It does require that the accounts be linked as an AWS Organization.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"ee9c067e3dea2fa4cd2e3968abaf86de","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations.  Use CloudFormation to implement request restrictions for SQL code on the CloudFront distributions across all accounts.  Setup a CloudWatch event to notify administrators if requests with SQL code are seen.","correct":false},{"id":"e072f443d93c569cce94eb4946a912af","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service and use Consolidated Billing. Subscribe to AWS Shield Advanced to automatically enable SQL injection protection across all sub-accounts.","correct":false},{"id":"3927b32bfb85cc040d2b7dcb21015fc5","text":"Use AWS WAF to create an ACL that denies requests that include SQL code.  Assign the ACL to Firewall Manager instances in each account using AWS OpsWorks.","correct":false},{"id":"9a5fa8e9a1a9be9149138c6307abde19","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service.  Use Firewall Manager to create an ACL rule to deny requests that contain SQL code.  Apply the ACL to WAF instances across all organizational accounts.","correct":true},{"id":"2238e9ae7489214f767fa479d013cd23","text":"Create a custom NACL filter using Lambda@Edge to check requests for SQL code.  Use OpsWorks to apply the NACL across all public subnets across the organization. ","correct":false}]},{"id":"02a9611c-591c-4280-bb83-6c65c7c4921f","domain":"awscsapro-domain5","question":"A sporting goods retailer runs WordPress on Amazon EC2 Linux instances to host their customer-facing website. An ELB Application Load Balancer sits in front of the EC2 instances in Auto Scaling Groups in two different Availability Zones of a single AWS region. The load balancer serves as an origin for Amazon CloudFront. Amazon Aurora provides the database for WordPress with the master instance in one of the Availability Zones and a read replica in the other. Many custom and downloaded WordPress plugins have been installed. Much of the DevOps teams' time is spent manually updating plugins across the EC2 instances in the two Availability Zones. The website suffers from poor performance between the Thanksgiving and Christmas holidays due to a high occurrence of product catalog lookups. What should be done to increase ongoing operational efficiency and performance during high-volume periods?","explanation":"ElastiCache Memcached will provide in-memory access speeds for the catalog read transactions. A WordPress plugin is required to leverage caching. WordPress can access an EFS Mount Target for file sharing across all instances. Aurora offers a MySQL option, and WordPress requires MySQL, so the solution would have been set up that way already. CodeDeploy could update plugins on all instances, and will work well for the custom in-house code, but triggering the updates of downloaded plugins will need to be orchestrated. Aurora Auto Scaling will distribute catalog reads across multiple replicas for increased performance, but not to the extent of in-memory caching. Elastic File System is a managed service providing operational advantages over NFS file shares. ElastiCache Redis will provide the in-memory read performance desired, but changing the wp-config.php file won't provide access to it, as a plugin is needed for that. WordPress does work with S3, but a shared file system is easier to implement.","links":[{"url":"https://aws.amazon.com/getting-started/projects/build-wordpress-website/","title":"Build a WordPress Website"},{"url":"https://github.com/aws-samples/aws-refarch-wordpress?did=wp_card&trk=wp_card","title":"Hosting WordPress on AWS"}],"answers":[{"id":"f060160b20c4408b2442010d3ea4d387","text":"Use Amazon ElastiCache Redis as a caching layer between the EC2 instances and the database. Change wp-config.php to point to the Redis caching layer, and have Redis point to Aurora. Move the WordPress files to S3 and have WordPress access them there.","correct":false},{"id":"17b12e57cd85610e888cda82b5a8a145","text":"Migrate the WordPress database to RDS MySQL since MySQL is WordPress's native database and WordPress is performance optimized for MySQL. Implement AWS CodeDeploy to update WordPress plugins on all EC2 instances.","correct":false},{"id":"bc643d3342a5a675e65e5baed00e88b9","text":"Deploy Amazon ElastiCache Memcached as a caching layer between the EC2 instances and the database. Install a WordPress plugin to read from Memcached. Implement Amazon Elastic File System to store the WordPress files and create mount targets in each EC2 subnet.","correct":true},{"id":"aeb27370afc3b672eb0a1afcb28e9176","text":"Implement Aurora Auto Scaling to increase the number of replicas automatically as demand increases. Create an NFS file share to hold the WordPress files. Access the file share from the EC2 instances in both Availability Zones.","correct":false}]},{"id":"1520156f-0918-4ab4-a759-ce33a931c744","domain":"awscsapro-domain5","question":"Your company has an online shopping web application. It has adopted a microservices architecture approach and a standard SQS queue is used to receive the orders placed by the customers. A Lambda function sends orders to the queue and another Lambda function fetches messages from the queue and processes them. On some occasions the message in the queue cannot be handled properly. For example, when an order has a deleted production ID, the message cannot be consumed successfully and is returned to the queue. The problematic messages in the queue keep growing and the ability to process normal messages is affected. You need a mechanism to handle the message failure and isolate error messages for further analysis. Which method would you choose?","explanation":"It is not a good idea to adjust the retention period or simply delete the messages that fail to be processed as the question asks for a mechanism to isolate the messages for further troubleshooting. A redrive policy should be used to auto-forward error message to a dead letter queue. Then you can analyze the contents of messages to diagnose the producer’s or consumer’s issues. One thing to note is that a standard queue can only have another standard queue as the dead letter queue. Therefore a FIFO dead letter queue is incorrect as this scenario uses a standard SQS queue and requires a standard dead letter queue.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","title":"Amazon SQS dead-letter queues"}],"answers":[{"id":"f7b3898cfcb4851b120c9b14d044ab90","text":"Decrease the message retention period of the queue to 1 day. When the messages are not processed properly and put back in the queue, they can be quickly deleted when the retention period expires.","correct":false},{"id":"3c8453a6c57faf61e761f771bab6f1af","text":"Create a standard queue as the dead letter queue and configure a redrive policy to put error messages to the dead letter queue. Analyze the contents of messages in the dead letter queue to diagnose the issues.","correct":true},{"id":"64fd54fe78b534f0eac9222a6e32747d","text":"Create a FIFO (First-In-First-Out) queue as the dead letter queue and use a redrive policy to forward problematic messages to this new queue. Create a Lambda function to read the message contents in the FIFO queue for further analysis.","correct":false},{"id":"a7eb53a7df09334677590165f666c58f","text":"Modify the error handling logic of the Lambda function to delete the messages whenever the processing is unsuccessful with an error or exception. The error messages do not return to the queue and the normal message handling is not blocked.","correct":false}]},{"id":"d58acd29-561d-4017-a55d-bcff8eb40f90","domain":"awscsapro-domain4","question":"Last month's AWS service cost is much higher than the previous months. You check the billing information and find that the used hours of Elastic Load Balancer (ELB) increases dramatically. You manager asks you to plan and control the ELB usage. When the ELB service has been used for over 5000 hours in a month, the team should get an email notification immediately and further actions will be taken accordingly. Which of the following options is the easiest one for you to choose?","explanation":"AWS Budgets include the types of cost budget, usage budget, reservation budget and savings plan budget. The usage budget enables you to plan the usage of ELB service and receive budget alerts when the actual usage becomes more than a threshold (5000 hours in this scenario). The cost budget type is incorrect as it evaluates the cost instead of usage and you cannot receive budget alerts from either Cost Explorer or AWS Config.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing your costs with Budgets"}],"answers":[{"id":"15fe48e7010a90e06774d9aa9668704a","text":"Create a usage budget in AWS Budgets. Check the ELB running hours for every month and set the budgeted amount to be 5000 hours. Configure an email alert or an Amazon Simple Notification Service (SNS) notification when the actual usage is more than the threshold.","correct":true},{"id":"39f1073c8a5f51031cb743e7cf45dce2","text":"Calculate the estimated ELB cost when the total ELB usage is 5000 hours in a month. Configure a cost budget in AWS Budgets for the EC2 ELB service and set the number as the threshold. When the cost is greater than the user-defined threshold, send an email alert to the team.","correct":false},{"id":"0f88bd3ee9fd3724922aa89baa9f656d","text":"Launch the Cost Explorer in AWS billing dashboard, filter the EC2 ELB service and configure a CloudWatch alert to track its actual monthly usage. When the monthly ELB usage grows more than 5000 hours, raise the CloudWatch alert and notify an SNS topic.","correct":false},{"id":"baf09736ecf556311eb6e77579877ccd","text":"In AWS Config, monitor the usage of all ELB resources within the AWS account. Create a custom Config rule via a Lambda function that calculates the ELB usage and sends an alert message to an SNS topic when the usage is over 5000 hours.","correct":false}]},{"id":"abc17e7b-6b75-45e3-a5c0-ea0f55d4df97","domain":"awscsapro-domain2","question":"Your client is a software company starting their initial architecture steps for their new multi-tenant CRM application.  They are concerned about responsiveness for companies with employees scattered around the globe.  Which of the following ideas should you suggest to help with the overall latency of the application?","explanation":"CloudFront can cache both static and dynamic content.  By setting a high TTL, we allow CloudFront to serve content longer before having to refresh from the origin.  Additionally, Lambda@Edge can intercept the request and direct the requester to a region based on the geographic origin of the request.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/11/lambda-at-edge-now-supports-content-based-dynamic-origin-selection-network-calls-from-viewer-events-and-advanced-response-generation/","title":"Lambda@Edge Now Supports Content-Based Dynamic Origin Selection, Network  Calls from Viewer Events, and Advanced Response Generation"}],"answers":[{"id":"0fc3951d630f285646b22cdd30f43eee","text":"Architect the system to use as many static objects as possible with high TTL.  Use CloudFront to retrieve both static and dynamic objects.  POST and PUT new data through CloudFront.","correct":true},{"id":"35483961564002569ee69763e24961fa","text":"Install the application in several regions around the globe.  As new customers and users are on-boarded, pre-cache their user data in CloudFront for that region.  Use AWS Batch to routinely expire the cache to ensure the latest updates are visible.","correct":false},{"id":"afa9743126cd2b0644654f2439a2aa0a","text":"Install the application on several regions around the globe.  Use RDS cross-region read replication for PostgreSQL to ensure a strongly consistent data store.","correct":false},{"id":"35638855dc45f62b3801906fd9a6d87c","text":"Install key parts of the application in multiple AWS regions chosen to balance latency for geographically diverse users.  Use Lambda@Edge to dynamically select the appropriate region based on the users location.","correct":true},{"id":"702a6122527d0830134717e0e7323bd0","text":"Store the data in a DynamoDB Global Table.  Use an auto scaling ElastiCache cluster with Memcached as a caching layer.  Distribute static elements of the application via CloudFront.  Use Route 53 Weighted routing to dynamically route users to the nearest region.","correct":false}]},{"id":"115e1b30-23e4-4f3f-9c13-a0086f6af223","domain":"awscsapro-domain2","question":"You are working with a pharmaceutical company on designing a workflow for processing data.  Once a day, a large 2TB dataset is dropped off at a pre-defined file share where the file is processed by a Python script containing some proprietary data aggregation routines.  On average, it takes 20-30 minutes to complete the processing.  At the end, a notification has to be sent to the submitter of the dataset letting them know processing is complete.  Which of the following architectures will work in this scenario?","explanation":"While it may not be the most cost-effective, the EFS option is the only one that can work.  A processing time of 20-30 minutes rules out Lambda (at present with a processing limit of 15 minutes).  If we create an EBS volume with a full OS on it and mount as root for a new instance with the data set included, we still would not be able to dismount the root volume without shutting down the instance.  This would not let us issue an SES SDK call.  The database is also far too large for SQS.","links":[{"url":"https://aws.amazon.com/efs/features/","title":"Amazon Elastic File System (EFS) | Cloud File Storage | Product Features"}],"answers":[{"id":"1831d929918e3c425b20e476c1716ccc","text":"Load inbound dataset on an EBS volume.  Stand up an EBS-optimized instance and mount the data volume as the root volume.  Once the data processing is complete, unmount the EBS volume and issue an SDK call to SES to notify of completion.  Configure SES to trigger an instance shutdown after the notification is sent.","correct":false},{"id":"3e1542fcef251cf86c8bdcc89d83aaa7","text":"Stand up memory optimized instances and provision an EFS volume. Pre-load the data on the EFS volume.  Use a User Data script to sync the data from the EFS share to the local instance store.  Use an SDK call to SNS to notify when the processing is complete, sync the processed data back to the EFS volume and shutdown the instance. ","correct":true},{"id":"5f8da9106eba72e74c1a0d6415a235af","text":"Create an S3 bucket to store the incoming dataset.  Once the dataset has been fully received, use S3 Events to launch a Lambda function with the Python script to process the data.  When finished, use an SDK call to SNS to notify when the processing is complete.  Store the processed data back on S3.","correct":false},{"id":"dee7711bae75cac018761467694d89e3","text":"Use SQS to take in the data set.  Use a Step Function to Launch Lambda functions in a fan-out architecture for data processing and then send an SNS message to notify when the processing is complete.  Store the processed data on S3.","correct":false}]},{"id":"2d9a7fba-ee40-4128-8f36-b32ef55ce662","domain":"awscsapro-domain3","question":"You have just been informed that your company's data center has been struck by a meteor and it is a total loss.  Your company's applications were not capable of being deployed with high availability so everything is currently offline.  You do have a recent VM images and DB backup stored off-site.  Your CTO has made a crisis decision to migrate to AWS as soon as possible since it would take months to rebuild the data center.  Which of the following options will get your company's applications up and running again in the fastest way possible?","explanation":"The Server Migration Service uses the Server Migration Service Connector which is an appliance VM that needs to be loaded locally in vCenter.  We don't have a VMware system...only a backup of an image so this won't work.  The best thing we can do is import the VM and restore the database.","links":[{"url":"https://docs.aws.amazon.com/server-migration-service/latest/userguide/prereqs.html","title":"Server Migration Service (SMS) Requirements - AWS Server Migration Service"},{"url":"https://aws.amazon.com/ec2/vm-import/","title":"VM Import/Export"}],"answers":[{"id":"af2dfb68e123596c40e5014f3e8dc491","text":"Copy the VMs into AWS and create new AMIs from them.  Create a clustered auto scaling group across multiple AZs for your application servers.  Provision a multi-AZ RDS instance to eliminate the single-point-of-failure problem.  Restore the data from the backups using the database admin tools.","correct":false},{"id":"364a256b0572da0bd43823d35b22e01d","text":"Call your data communications provider and order a Direct Connect link to your main office.  Order a Snowball Edge to serve as a mobile data center.  Restore the VM image to the Snowball Edge device as an EC2 instance.  Restore the backup to an RDS instance on the Edge device.  When the Direct Connect link is installed, use that to smoothly migrate to AWS.","correct":false},{"id":"b8015467f854fe29575bd8fd26c819a5","text":"Use VM Import to upload the VM image to S3 and create the AMI of key servers.  Manually start them in a single AZ.  Stand-up a single AZ RDS instance and use the backup files to restore the database data.","correct":true},{"id":"c3692f609e87c0dc0416f0ad05897b3f","text":"Explain to company stakeholders that it is not possible to migrate from the backups directly to AWS.  Recommend that we first find a co-location site, procure similar hardware as before the disaster and restore everything there.  Then, we can carefully migrate to AWS.","correct":false},{"id":"e50f3fc14c0feac5ff24b30bf605d687","text":"Use Server Migration Service to import the VM into EC2.  Use DMS to restore the backup to an RDS instance on AWS.","correct":false}]},{"id":"3b08a75a-01b7-4083-bbd1-af1acd7e5314","domain":"awscsapro-domain2","question":"A clothing retailer has decided to run all of their online applications on AWS. These applications are written in Java and currently run on Tomcat application servers hosted on VMware ESXi Linux virtual machines on-premises. Because many of the applications require extremely high availability, they've deployed Oracle RAC as their database layer. Some business logic resides in stored procedures in the database. Due to the timing of other business initiatives, the migration needs to take place in a span of four months. Which architecture will provide the most reliable and operationally efficient solution?","explanation":"Elastic Beanstalk provides the most operationally efficient solution for the application server layer. With Elastic Beanstalk, you can quickly deploy and manage applications without worrying about the infrastructure that runs those applications. VMware Cloud on AWS delivers a robust environment for running Oracle RAC environments. Oracle RAC doesn't run natively on EC2. Due to the time constraints for the project, a migration to Aurora Multi-Master is probably not feasible in four months, especially when the migration of stored procedure code is involved. RDS Oracle Multi-AZ provides active/passive failover, whereas Oracle RAC is active/active, providing no-downtime failovers. Oracle Recovery Manager can run on EC2 and access the database via the VMware Cloud ENIs to perform backups to S3 over a VPC Endpoint.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_Java.html","title":"Creating and Deploying Java Applications on AWS Elastic Beanstalk"},{"url":"https://aws.amazon.com/vmware/","title":"VMware Cloud on AWS"},{"url":"https://d1.awsstatic.com/VMwareCloudonAWS/aws_reference_architecture_oracle_rac_on_vmware_cloud.pdf?did=wp_card&trk=wp_card","title":"Oracle RAC on VMware Cloud on AWS"}],"answers":[{"id":"3cac9f7ec8cba84a36f70ab25c667485","text":"Use a certified Tomcat AMI to deploy the application servers on VMware Cloud on AWS EC2 instances with Auto Scaling across multiple Availability Zones. Configure Oracle on Amazon RDS with Multi-AZ. Connect the Tomcat servers to the database instances with VMware Cloud ENI route table entries. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":false},{"id":"69173b8054edfd0b9be84428b694bc53","text":"Implement AWS Elastic Beanstalk to run the Tomcat servers in multiple Availability Zones. Run Oracle RAC on VMware Cloud on AWS in multiple Availability Zones. Connect the Tomcat servers to the database instances with VMware Cloud ENI route table entries. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":true},{"id":"2a2d87b2ff0208d44d780f2cf354f3f2","text":"Use a certified Tomcat AMI to deploy the application servers on EC2 instances with Auto Scaling across multiple Availability Zones. Install Oracle RAC on EC2 across multiple Availability Zones. Connect the Tomcat servers to the database via JDBC. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":false},{"id":"b555cc3afefbe63e6b58594cd86b4fa5","text":"Deploy AWS Elastic Beanstalk to run the Tomcat servers in multiple Availability Zones. Migrate the database to Amazon Aurora Multi-Master. Connect the Tomcat servers to the database via JDBC. Leverage Aurora's Multi-AZ and automated backup capabilities to achieve high availability.","correct":false}]},{"id":"dac73d1f-8c64-48b1-90be-3432e789933d","domain":"awscsapro-domain2","question":"Your company is bringing to market a new Windows-based application for Computer Aided Manufacturing.  As part of the promotion campaign, you want to allow users an opportunity to try the software without having to purchase it.  The software is quite complex and requires specialized drivers so it's not conducive to allowing the public to download and install in their own systems.  Rather you want to control the installation and configuration.  Therefore, you want something such as a VDI concept.  You'll also need to have a landing page as well as a custom subdomain (demo.company.com) and limit users to 1 hour of use at a time to contain costs.  Which of the following would you recommend to minimize cost and complexity?","explanation":"AppStream is a way to deploy an application on a virtual desktop and allow anyone with a browser to use the application.  This is the most efficient and simplest option given the other choices.","links":[{"url":"https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html","title":"What Is Amazon AppStream 2.0? - Amazon AppStream 2.0"}],"answers":[{"id":"129d13b81f52a8ab01ce9e8ea7009289","text":"Create a landing page in HTML and deploy to an S3 bucket configured as a Static Web Host.  Use Route 53 to create a DNS record for the \"demo\" subdomain as alias record for the S3 bucket.  Deploy your application using Amazon AppStream.  Set Maximum Session Duration for 1 hour.","correct":true},{"id":"9b85d56999fa276cfa4a01df8195700c","text":"Create a landing page in HTML and deploy to an S3 bucket configured as a Static Web Host.  Embed in the HTML a Javascript-based RDP client that is downloaded with the webpage.  Create a CloudFront distribution with the S3 bucket as the origin.  Use an S3 Event to launch a Lambda function which starts up an EC2 instance with your golden AMI.  Once the instance is up and running, use a web socket call from the Lambda function to initiate the RDP client and log the user in.  After 1 hour, have the Lambda function issue a shutdown command to the EC2 instance.","correct":false},{"id":"a1c67181ea5765383a7b477e821391b4","text":"Configure an EC2 auto scaling fleet of spot instances with your golden AMI.  Create security groups to allow inbound RDP for the auto scaling group.  Deploy Apache Guacamole on an EC2 instance and place your landing page in its web server directory.  Use Guacamole to provide an RDP session into one of the EC2 instances directly in the users browser.  Use AWS Batch to reboot the EC2 instances after 1 hour of runtime.  ","correct":false},{"id":"bdc9c29dcecb6611cd71c4660fc235ca","text":"Deploy your application as an app in the Workspaces Application Manager.  Spin up several Workspaces and configure them to automatically install your application via WAM. Create the landing page such that it redirects to the web client for Workspaces and deploy the landing page via S3 configured as a web host.  Use Route 53 to create a DNS record for the demo subdomain as an alias record for the S3 bucket.  Configure the Workspaces for a 1 hour timeout.  ","correct":false}]},{"id":"91006a08-8658-479c-9f96-4f9cd9c770c6","domain":"awscsapro-domain2","question":"Your customer is a commercial real-estate company who owns parking lots in all major cities in the world. They are building a website on AWS that will be globally accessed by international travellers. The website will provide near-real-time information on available parking lot spaces for all cities. You need a back-end comprising a multi-region multi-master data storage solution, with writes happening in all regions. Writes must be replicated between regions in near real-time so that different regional website compute instances can query the database instance in its own region and retrieve the data pertaining to all other regions. In addition, all users should be able to access the website using the same domain name, but they need to be routed to the Elastic Load Balancer that responds most quickly to their request (which should be the one closest to them most of the time). The website must also serve locally cached static content and have protection against malicious attacks. Which AWS-based architecture should you choose?","explanation":"The only AWS managed database service that offers multi-region multi-master is DynamoDB Global Tables. AWS Aurora does offer multi-master clusters, but all nodes of an Aurora multi-master must be in the same region. Aurora also offers cross-region replication, but the nodes, in that case, are Read Replicas, they are not Masters (in the database world, a master is an instance that accepts write requests). Similarly, AWS RDS does not currently have a multi-master multi-region solution to leverage. Thus, the only choice that can satisfy the multi-region multi-master nature of the requirement is DynamoDB Global Tables with DynamoDB Streams enabled for Replication. Additionally, the question tests knowledge in other areas. However, just focusing on the above fact about the database tier is enough to eliminate all incorrect answers.\nRoute 53 latency-based multi-region routing is the correct Routing solution here. Multi-value answering records results in randomly picking one, which is not the requirement here (in this scenario, the region must be picked based on latency, not randomly). Similarly, geolocation-based routing honours country boundaries, not latency. If we use geolocation-based routing, an end-user in country A will always go to the AWS Region in country A, but that may not be the one with least latency, especially if the user is close to an international boundary, and the AWS data centre happens to be on the other end of his country.\nLambda@Edge can be used with Cloudfront to re-write origins for requests. This is an important usage of Lambda@Edge. Similarly, WAF can be activated with a Cloudfront web distribution for protection against malicious traffic.","links":[{"url":"https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/","title":"How to use Amazon DynamoDB global tables to power multiregion architectures"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html","title":"Search for - all DB instances in a multi-master cluster must be in the same AWS Region"},{"url":"https://aws.amazon.com/blogs/aws/latency-based-multi-region-routing-now-available-for-aws/","title":"Multi-Region Latency Based Routing now Available for AWS"}],"answers":[{"id":"32667dba126e63f9c8c6adb288419974","text":"For the database, use DynamoDB Global Tables, with instances in each AWS Region configured as master, and replication enabled using DynamoDB Streams. For routing, use Route 53 multi-region latency based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":true},{"id":"c74e4146c5eb59b1a892f52ac51bfbd7","text":"For the database, use Amazon Aurora Cross-Region Replication, with instances in each AWS Region configured as master. For routing, use Route 53 geolocation-based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":false},{"id":"9faf11728364108db71381c81e091523","text":"For the database, use Amazon RDS with Cross Region Replication, with instances in each AWS Region configured as master. For routing, use Route 53 multi-region multi-value answering based routing. Use Cloudfront to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF with Cloudfront geo restriction feature.","correct":false},{"id":"25c4e1f274a66d3e2b5d67c75702ef4b","text":"In AWS, all cross-region replication solutions treat secondary instances as read-only, so multiple masters are not possible in a managed multi-region database solution. Use Cassandra on EC2 instances in different regions that are peered to each other and configure Cassandra in multi-master mode. For routing, use Route 53 multi-region latency based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":false}]},{"id":"489eea6e-bdaa-42a9-a80c-bfd209129fda","domain":"awscsapro-domain5","question":"You are working in a site reliability engineering team. There are dozens of EC2 instances in production that your team needs to maintain. When issues happen and online troubleshooting is required, the team needs to connect to a bastion host in order to login into an EC2 instance. You want to use the AWS Session Manager in Systems Manager to bypass the bastion host when accessing the instances. Which benefits can Session Manager bring to the team?","explanation":"Session Manager is a service in AWS Systems Manager that lets you access and manage your Amazon EC2 instances. Session Manager offers several benefits to the organization. With Session Manager, no SSH ports need to be open and no SSH keys are required. CloudTrail can also capture information about the Session Manager activities. However, the System Manager agent must be installed for Session Manager. For the Session Manager connections, you can choose to encrypt them with a CMK in KMS instead of an SSH key. And you should associate the IAM policies with IAM entities as an IAM policy cannot be attached in Session Manager.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html","title":"AWS Systems Manager Session Manager"}],"answers":[{"id":"8660dde753cd22508fc49e487f99b4dd","text":"Session Manager provides highly secure connections via encryptions in transit. You can choose to create an SSH key pair and use the private key to encrypt the connections.","correct":false},{"id":"1a123e85219205112eef2029de78d167","text":"AWS Session Manager handles the connections to EC2 instances and provides an interactive on-click shell. No agents need to be installed in the EC2 instances.","correct":false},{"id":"e097db56681740bcf0785fb6555a2a2d","text":"The Session Manager API calls made in the AWS account can be tracked by AWS CloudTrail. You can have a record of the connections made to the instances.","correct":true},{"id":"3166b29d289b61b2a127864f52105929","text":"Session Manager integrates with IAM and you can attach an IAM policy in Session Manager to control who can initiate sessions to instances.","correct":false},{"id":"77650e8a40264f564a70afe948a6b602","text":"You do not need to open inbound SSH ports or PowerShell ports for the remote connections and no SSH keys are required for the connections with Session Manager.","correct":true}]},{"id":"d8bfc54e-024a-4fbb-9daa-9a218a10b738","domain":"awscsapro-domain5","question":"Your company has an Inventory Control database running on Amazon Aurora deployed as a single Writer role. Over the years more departments have started querying the database and you have scaled up when necessary.  Now the Aurora instance cannot be scaled vertically any longer, but demand is still growing.  The traffic is 90% Read based.  Choose an option from below which would meet the needs of the company in the future.","explanation":"This question is about scaling, and if you have scaled up to the maximum level (db.r4.16xlarge) the next step is to consider scaling out.  In this case the application is Read heavy, which lends itself perfectly to adding extra Read replicas and using Read-Write splitting to help future growth.  Changing the max_connections value or using Query plan optimisation may make performance more efficient, but they are not long term solutions. Adding Multi-AZ simply adds High Availability.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html","title":"Managing Performance and Scaling for Aurora DB Clusters"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-replicas-adding.html","title":"Adding Aurora Replicas to a DB Cluster"}],"answers":[{"id":"d1fe9035612273c2d0c6b9f4229e1d3d","text":"Convert Aurora to a Multi-AZ Deployment in three or more zones","correct":false},{"id":"ce644fa65dde55c85ae051c5081ba860","text":"Use Query plan management to allow the optimizer to choose the most efficient plan for each job and make transactions quicker","correct":false},{"id":"15cd367246a8991991de371856908334","text":"Create multiple additional Readers within the Aurora cluster and alter the application to make use of Read-Write splitting","correct":true},{"id":"c6229282cfe5ed1024d22e629709b94d","text":"Increase the maximum number of connections into the database by changing the max_connections parameter","correct":false}]},{"id":"254eeafd-1183-4117-936e-f6f7ffa9d88a","domain":"awscsapro-domain2","question":"A client is having a challenge with performance of a custom data collection application.  The application collects data from machines on their factory floor at up to 1000 records per second.  It uses a Python script to collect data from the machines and write records to a DynamoDB table.  Unfortunately, under times of peak data generation, which only last 1-2 minutes at a time, the Python application has timeouts when trying to write to DynamoDB.  They don't do any analytics on the data but only have to keep it for potential warranty issues.  They are willing to re-architect the whole solution if it will mean a more reliable process.  Which of the following options would you recommend to give them the most scalable and cost-efficient solution?","explanation":"The application is likely running into throttling when writing to DynamoDB.  Kinesis Firehose makes for a good option in this case to accommodate streaming records.  Since we do not have to perform any analytics, we can simply store it on S3 using Firehose.","links":[{"url":"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html","title":"What Is Amazon Kinesis Data Firehose? - Amazon Kinesis Data Firehose"}],"answers":[{"id":"61f03e55a88efffb8060d5d3e07f247d","text":"Turn on DynamoDB Auto Scaling and configure appropriate upper and lower limits.","correct":false},{"id":"7e2676bbdbc658bd6a68274495e2376f","text":"Change the application design to use Kinesis to take in the data.  Use Kinesis Firehose to spool the data files out to S3.  Use S3 Lifecycle to transition the files to Glacier after a few days.","correct":true},{"id":"6ac1d0de8c41824a325523f050a3784e","text":"Change the application design to write the data records to EMR.  Use a Pig script to transfer the data from EMR to DynamoDB periodically.","correct":false},{"id":"b30cd1d2aae071493c960798a422ade4","text":"Change the application design to use SWF to take in the data.  Use Amazon Elasticache in front of the DynamoDB database as a buffer to throttle the writes.","correct":false},{"id":"0bd59f25e091570f7b171c3dd76b0206","text":"Change the application design to use SQS and a custom process on an EC2 spot fleet to throttle inbound messages into DynamoDB.","correct":false},{"id":"80999a665ea6597ad65e91f9d4a84b3f","text":"Change the application design to use Kinesis Streams to take in the data.  Provision at least 5 shards to ensure enough peak capacity.  Configure the Kinesis Streams to load the data into DynamoDB.  Increase the RCU and WCU for the DynamoDB table to match peak needs.","correct":false}]},{"id":"63a6def8-9b52-4d89-8248-6079ca1393e2","domain":"awscsapro-domain3","question":"You are helping a client prepare a business case for cloud migration.  One of the required parts of the business case is an estimation of AWS costs per month.  The client has about 200 VMs in their landscape under VMware vCenter.  Due to security concerns, they will not allow any external agents to be installed on their VMs for discovery.  How might you most efficiently gather information about their VMs to build a cost estimate with the least amount of effort? ","explanation":"The Application Discover Service uses agent-based or agentless collection methods.  Agentless collection is only available for those customers using VMware.  The AWS Application Discovery Agentless Connector is delivered as an Open Virtual Appliance (OVA) package that can be deployed to a VMware host. Once configured with credentials to connect to vCenter, the Discovery Connector collects VM inventory, configuration, and performance history such as CPU, memory, and disk usage and uploads it to Application Discovery Service data store.  This data can then be used to estimate monthly costs.","links":[{"url":"https://aws.amazon.com/application-discovery/faqs/?nc=sn&loc=6","title":"AWS Application Discovery Service FAQs"}],"answers":[{"id":"30cbd0a822a4905f8a795dcb7cc3d31e","text":"Use a custom script to iteratively log into each VM and pull network, hardware and performance details of the VM.  Write the data out to S3 in CSV format.  Use that data to select corresponding EC2 instance sizes and calculate estimated monthly cost.","correct":false},{"id":"09d153439d976dabcdd13a4a2f8a4a5f","text":"Use Application Discovery Service to gather details on the network connections, hardware and performance of the VMs.  Export this data as CSV and use it to approximate monthly AWS costs by aligning current VMs with similar EC2 instances types.","correct":true},{"id":"b279286ed54b2394c29d2fbd0061b4c4","text":"Provision an S3 bucket for data collection.  Use SCT to scan the existing VMware landscape for VM hardware, network connection and performance parameters.  Retrieve the SCT CSV data from the data collection bucket and use it to align EC2 instance types with existing VM parameters.  Use this cross-reference to calculate estimated monthly costs for AWS.","correct":false},{"id":"0a144eb3993e48693aab4c9744b6acb2","text":"Use AWS OpsWorks to remotely pull hardware, network connection and performance of the VMs.  Export the collected data from OpsWorks in Excel format.  Use the collected data to align current VMs with similar EC2 instance types and calculate an estimated monthly cost.","correct":false}]},{"id":"0906c4cf-83a1-4cec-b2ab-c010dcdee73f","domain":"awscsapro-domain1","question":"The alternative energy company you work for has four different business units, each of which would like to run workloads on AWS. Each business unit has it's own AWS account, and a shared services AWS account has been created. An established process for tracking software license usage exists for on-premises applications, but the finance department has concerns that the self-serve nature of the cloud may result in license overages for applications deployed on AWS. You've been tasked with setting up a governance model whereby users are only given access to a standard list of products. Which architecture will provide an effective way to implement the governance requirements and manage software license usage on AWS?","explanation":"AWS Service Catalog allows organizations to create and manage catalogs of approved products for use on AWS. Products are defined as CloudFormation Templates. Software license information can be associated with Service Catalog products through tags. AWS Step Functions can orchestrate the process of incrementing usage counts and notifying of over-usage situations when products are launched by users. AWS License Manager is a robust solution for managing software licenses, but it needs to be coupled with Service Catalog to meet the requirement for limiting access to a standard set of products. A Lambda trigger is not currently available for Service Catalog product deployments. Elastic Container Registry provides tagging at the repository level, not at the individual container image level.","links":[{"url":"https://aws.amazon.com/servicecatalog/","title":"AWS Service Catalog"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://aws.amazon.com/blogs/mt/tracking-software-licenses-with-aws-service-catalog-and-aws-step-functions/","title":"Tracking software licenses with AWS Service Catalog and AWS Step Functions"}],"answers":[{"id":"e2441352bdc2d4db956149f0a10b6738","text":"Create Docker images for each of the standardized applications that will be deployed and register them with Amazon Elastic Container Registry (ECR). Populate ECR tags with software license metadata. Create an Amazon DynamoDB table to store software license usage counts. Whenever a container is launched in Amazon Elastic Container Service, trigger an AWS Lambda function to increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"62788b95bb6b249e6511d14023a20364","text":"Implement AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Create an Amazon DynamoDB table to store software license usage counts. Trigger an AWS Lambda function to run each time a Service Catalog product is launched. Have the Lambda function increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"41d11d1142468e8b0d2a29674d1eaa2c","text":"Deploy AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Populate Service Catalog product tags with software license information. Create an Amazon DynamoDB table to store software license usage counts. Have Amazon CloudWatch detect when a user deploys a Service Catalog product. Launch an AWS Step Functions process to increment license counts in the DynamoDB table, and send notifications when overage thresholds are met","correct":true},{"id":"33db838110cfb8b002590cbff630b825","text":"Create Amazon Machine Images for all of the instance configurations that will be deployed. Implement AWS License Manager license configurations and attach them to the AMIs. Create AWS CloudFormation StackSets for the AMIs in the shared AWS account and make them available to users in each business unit","correct":false}]},{"id":"7c5f884f-c0f9-4028-a725-50819d704324","domain":"awscsapro-domain5","question":"You deploy an application load balancer and an Auto Scaling group (ASG) in production for a new project. When instances in the ASG have a high CPU utilization, a new instance is launched. However, the new instance fails the health check from the ASG and has been terminated after some time. You check the logs in the instance and find that the startup script does not finish yet before the instance is terminated. How would you resolve the problem?","explanation":"Amazon EC2 Auto Scaling waits until the health check grace period ends before checking the health status of the instance. The grace period timer should be increased to give the instance more time to finish the startup script. Increasing the healthy threshold makes the instance more difficult to become healthy. Decreasing the timeout value also does not help as the instance may become unhealthy very quickly. Modifying the health check type from ELB to EC2 is unsuitable as the ASG cannot get the instance status from the application level. Even if the instance shows as healthy in ASG, the application may not be ready yet.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Check Grace Period"}],"answers":[{"id":"b5ca84726bf6fcfd6522ab188c8224ea","text":"Increase the default healthy threshold in the health check of elastic load balancer from 5 to 10 so that the instance will become healthy more quickly once the startup script finishes in the new instance.","correct":false},{"id":"1d8964ced10da94d49d1a1efe02bbbca","text":"Modify the health check type from ELB to EC2 in the Auto Scaling group. Configure ASG to check the EC2 instance status. As long as the instance does not have a system level issue, it will not fail the health check in the ASG even when the startup script is still running.","correct":false},{"id":"e7ad9ea949ae87c6a6001a94f5a9bf48","text":"Increase the health check grace period in the Auto Scaling group configurations. When a new instance boots up, it is given more time to execute the startup scripts and run applications before the health check from ASG.","correct":true},{"id":"e322fd6a55a044826665dfce4ae7020b","text":"Decrease the timeout value in the ELB health check from 5 seconds to 1 second so that when the ELB performs the health check on the backend instances, the instances are able to respond in time before a timeout occurs.","correct":false}]},{"id":"edb30172-3f76-4423-a6bb-78a3d2fdeb42","domain":"awscsapro-domain2","question":"Your team is managing hundreds of Linux and Windows EC2 instances in different environments such as development, QA, staging and production. You need a tool to help you automate the process of patching instances so that the operating systems have latest patches and meet the compliance policies. You want to manage the patching in different groups depending on the environment. For example, patches should be deployed and tested in the QA environment first before the production environment. How would you achieve this requirement through an AWS service?","explanation":"AWS Systems Manager Patch Manager is the most appropriate tool to manage the patching for large groups of EC2 or on-premises instances. For different environments, users can configure patch groups using the \"Patch Group\" tag and then establish a patch baseline for each patch group. It is better to manage instances with the \"Patch Group\" tag rather than other customized tags. AWS SSM Session Manager and AWS SSM Run Command are not suitable to deploy patches across a large number of instances. The AWS-RunRemoteScript command is also incorrect as it is used to execute scripts stored in a remote location.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-patchgroups.html","title":"Patch Manager Patch Groups"}],"answers":[{"id":"32ac8d27221ec8e699eb4e872d3f6ed0","text":"Centrally manage the instances in AWS SSM Managed Instances and divide them into different categories. Perform the patching activities from AWS SSM Session Manager in a maintenance window.","correct":false},{"id":"b99f4271e073e1e030f3c26c383c5959","text":"In AWS Systems Manager Patch Manager, create different patch groups using the tag key \"Patch Group\" and configure a patch baseline for each patch group. Schedule the patching in a maintenance window by selecting a patch group.","correct":true},{"id":"6c4763d45c6cb24622b0db9533f95e0c","text":"Create environmental tags in EC2 instances such as a tag key named \"env\". In AWS SSM Patch Manager, configure patching activities by selecting the instances using the tag. Patch on the QA environment first and perform the necessary testing.","correct":false},{"id":"08b9feba2a12ba67141a0ffe02938798","text":"Add patch group tags in the EC2 instances. Perform the patching using the command AWS-RunRemoteScript in AWS SSM Run Command. Patch on the QA environment first by selecting the QA patch group tag.","correct":false}]},{"id":"bdd6d6a9-48a5-45f8-bb74-873f266d85df","domain":"awscsapro-domain2","question":"A hospital would like to reduce the number of readmissions for high risk patients by implementing an interactive voice response system to provide reminders about follow up visit requirements after patients are discharged. The hospital has the capability to automatically send HL7 messages that include the patient's phone number and follow up visit information from its medical records application via Apache Camel. They've chosen to deploy the solution on AWS. They already have a VPN connection to AWS, and all aspects of the application need to be HIPAA eligible. Which architecture will provide the most resilient and cost effective solution for the automated call system?","explanation":"S3 provides a low cost repository for the HL7 messages received. Having Lambda write the object keys to SQS, and having another Lambda function retrieve and parse the messages gives the architecture asynchronous workflow. Amazon Connect provides the capability to define call flows and perform IVR functions. Each of these services is HIPAA eligible. DynamoDB is also a good option for storing message information, but will be more expensive than S3. Amazon Pinpoint can place outbound calls, but is not able to perform interactive voice response functions. Amazon Comprehend Medical doesn't create call flow sequences.","links":[{"url":"https://aws.amazon.com/compliance/hipaa-eligible-services-reference/","title":"HIPAA Eligible Services Reference"},{"url":"https://aws.amazon.com/connect/","title":"Amazon Connect"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/reduce-hospital-readmissions-ra.pdf?did=wp_card&trk=wp_card","title":"Reducing Hospital Readmissions"}],"answers":[{"id":"c9bee4c5e4463b67e2726d3bed2ceed1","text":"Configure Apache Camel to write the HL7 messages to Amazon Kineses Data Firehose, which stores the patient information in Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"a8c26f088ade23063b2bf5f202a74cce","text":"Have Apache Camel write the HL7 messages to Amazon Kineses Data Streams. Configure a Lambda function as a consumer of the stream to parse the HL7 message and write the information to Amazon DynamoDB. Trigger another Lambda function to pull the patient data from DynamoDB and send it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"9585e2e5994ba5cdc2db33c22d9230cf","text":"Set up Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Connect to place the call to the patient.","correct":false},{"id":"8121c4e6e285161311cacf7b8031d5af","text":"Configure Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to write each HL7 message object key to Amazon Simple Queue Service FIFO. Have another Lambda function read messages in sequence from the SQS queue and use the object key to retrieve and parse the HL7 messages. Use that same Lambda function to write patient information to Amazon Connect to place the call using an established call flow.","correct":true}]},{"id":"79f2f5be-b591-44e6-957b-eb0383640d7d","domain":"awscsapro-domain5","question":"You have a standard SQS queue to receive messages from the frontend application. The backend application is JAVA based and the AWS SDK is used to get the messages from the queue for processing. The SQS queue is not busy most of the time. According to the backend application logs, there is a high number of empty ReceiveMessageResponse instances returned. You want to adjust the settings to minimize the number of empty responses and reduce the cost. How would you implement this? ","explanation":"Amazon SQS long polling is preferable to short polling in most of the cases. Long polling requests let the consumers receive messages as soon as they arrive in the queue. It can help to reduce the number of empty responses. In order to enable long polling, the attribute ReceiveMessageWaitTimeSeconds should be more than 0. Short polling is incorrect. Visibility timeout and delivery delay do not address the problem of empty responses.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html","title":"Amazon SQS short and long polling"}],"answers":[{"id":"c3cbf51c591cb7fa17bd023ab814f95c","text":"Consume the messages in the SQS queue using long polling. Set the queue attribute ReceiveMessageWaitTimeSeconds to be more than 0. Amazon SQS will wait until there is an available message in a queue before sending a response.","correct":true},{"id":"a5cdcd2968c3566cbb7fc7bcd5fef01a","text":"Add a delivery delay in the SQS queue such as 1 minute. The delay helps to postpone the delivery of new messages to the queue for some time. When the JAVA application polls the messages from the queue, there will be a lower chance to get an empty response.","correct":false},{"id":"220352e5b3779c1f2030cfd4b391b19e","text":"Modify AWS SDK to get the messages in the SQS queue by short polling. The ReceiveMessage call from the consumer sets the WaitTimeSeconds attribute to 0. As a result, the empty responses are eliminated.","correct":false},{"id":"203fa6faf2e6bf53939b43300ec6dac2","text":"Increase the default visibility timeout of the queue to reduce the possibilities that the messages become visible to consumers again. The application can also use the ChangeMessageVisibility API to specify a suitable timeout value.","correct":false}]},{"id":"e3a59454-94fa-4b98-8d8a-80882a7d0e30","domain":"awscsapro-domain5","question":"You are setting up a new EC2 instance for an ERP upgrade project.  You have taken a snapshot and built an AMI from your production landscape and will be creating a duplicate of that system for testing purposes in a different VPC and AZ.  Because you will only be testing an upgrade process on this new landscape and it will not have the user volume of your production landscape, you select an EC2 instance that is smaller than the size of your production instance.  You create some EBS volumes from your snapshots but when you go to mount those on the EC2 instances, you notice they are not available.  What is the most likely cause?","explanation":"In order to mount an EBS volume on an EC2 instance, both must be in the same AZ.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html","title":"Amazon EBS Volumes - Amazon Elastic Compute Cloud"}],"answers":[{"id":"5a9ffc3875f9e3f50c9fc2684a6006b2","text":"The original volume is encrypted and you failed to check the encryption flag when creating the new volume.","correct":false},{"id":"134c64ea3d25d70a667400e778a13c1a","text":"You created them in a different availability zone than your testing EC2 instance.","correct":true},{"id":"c2f8c791750ad6ce4c8c41ac45b246a0","text":"You have reached your account limit for EBS volumes.  You will need to create a support ticket to request an increase to the limit.","correct":false},{"id":"a9e147ffe118e514fd5069020e86cca6","text":"The instance that you selected for your testing landscape is too small.  It must be equal to or larger than the source of the AMI.","correct":false},{"id":"7a11f6b6797c8dd005c9ce25c77a37fe","text":"An SCP applied to the account you are in has restricted you from attaching EBS volumes to instances outside the original VPC","correct":false}]},{"id":"c3ac5de9-a343-4cde-af1b-6c9f89824d2f","domain":"awscsapro-domain5","question":"An external auditor is reviewing your process documentation for a Payment Card Industry (PCI) audit.  The scope of this audit will extend to your immediate vendors where you store, transmit or process cardholder data.  Because you do store cardholder data in the AWS Cloud, the auditor would like to review AWS's PCI DSS Attestation of Compliance and Responsibility.  How would you go about getting this document? ","explanation":"AWS Artifact provides on-demand downloads of AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use. You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls.","links":[{"url":"https://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html?icmpid=docs_artifact_console","title":"What Is AWS Artifact? - AWS Artifact"}],"answers":[{"id":"d9208942349d1c6f7dbaba3661069bc1","text":"AWS WorkDocs","correct":false},{"id":"d7cb47dd1f374d3ed079b14cc6f2cd75","text":"Submit a Support Case requesting the document","correct":false},{"id":"1d16d307ee989a80e421198a01993a9c","text":"AWS IAM Console","correct":false},{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false},{"id":"63df0d05cd43af35c95cf04d92aaf685","text":"AWS Legal Services website","correct":false},{"id":"09e838e873f25f954fef911d50b3d1ab","text":"AWS Pinpoint","correct":false},{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":true}]},{"id":"f3fef147-7b9d-45b6-8b2b-d943c90e8920","domain":"awscsapro-domain5","question":"You are assisting a company in the migration of their container-based web landscape over to Amazon.  They have a total of 21 containers which comprise their DEV, QA and Production environments.  All environment are identical in design and size.  Each environment consists of 3 web servers, 3 app servers and 1 datastore server.  Given the landscape, which of the provided options would be best for them to minimize maintenance?","explanation":"Deploying containers via ECS is a good option but we would want to use the EC2 hosted path.  Fargate is generally used for transient workloads and our datastore would be something we'd want to persist.  We might be able to deploy the data store with RDS, but the question does not make it clear if the data store is an RDS-supported database.  It could be a NoSQL data store or some other database unsupported by RDS.  Similarly, a MEAN stack under Elastic Beanstalk might not be compatible with our landscape either.","links":[{"url":"https://aws.amazon.com/ecs/resources/","title":"Resources for Amazon ECS - run containers in production"}],"answers":[{"id":"c1354e6d48fedccbf7b4e9c18854d980","text":"Redeploy the web landscape on a MEAN stack under Elastic Beanstalk, making use of auto-scaling groups to right-size the respective environments.  ","correct":false},{"id":"619957021a43a829fbb6228467323ca1","text":"Deploy the web, app and database servers using ECS on EC2.  Purchase 1-year reserved instance contracts for the required EC2 instances.","correct":true},{"id":"f05469f4c7578263f4271e7514c338ef","text":"Deploy the web and app servers in each environment using ECS.  Provision an RDS instance for each environment.  Use AWS Systems Manager to provide a common management console.","correct":false},{"id":"e0ea997f77cb156d35ec716cf772c49c","text":"Deploy the web, app and database containers using ECS.  Make use of Fargate for the underlying ECS infrastructure.","correct":false}]},{"id":"2afb0db9-a43f-4e97-8272-5ce423ded162","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They notice on their RDS console that one of their mission-critical production databases has an \"Available\" listed under the Maintenance column.  They are extremely concerned that any sort of updates to the database will negatively impact their DB-intensive mission-critical application.  They at least want to review the update before it gets applied, but they are not sure when they will get around to that.  What do you suggest they do?","explanation":"For RDS, certain OS updates are marked as Required. If you defer a required update, you receive a notice from Amazon RDS indicating when the update will be performed. Other updates are marked as Available, and these you can defer indefinitely.  You can also apply the maintenance items immediately or schedule the maintenance for your next maintenance window.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html","title":"Maintaining a DB Instance - Amazon Relational Database Service"}],"answers":[{"id":"35c3b3d1cb8d9866e1abc96dec8bfa3f","text":"Apply the maintenance items immediately.  AWS validates each update with each customer's RDS instances using a shadow image so there is little risk here.","correct":false},{"id":"93df9eb71cb48a0c821fe555e35f5b62","text":"Defer the updates indefinitely until they are comfortable.","correct":true},{"id":"6aeada3a9046319bc858239b15031f66","text":"Backup the database immediate because the updates could come at any time.  If possible, create a Read Replica to act as a standby in case problems are introduced with the update.","correct":false},{"id":"66ed278812c2a52913954afa52952b97","text":"The maintenance will be automatically performed during the next maintenance window.  They have no choice in the matter.","correct":false},{"id":"b9f81cfac73d5c4d2871a7f969ecb9f3","text":"Disable the Maintenance Window so the updates will not be applied.","correct":false}]},{"id":"4ebf4191-8562-4f67-945d-ea5aae2f9f26","domain":"awscsapro-domain3","question":"You are consulting for a client who is trying to define a comprehensive cloud migration roadmap.  They have a legacy custom ERP system written in RPG running on an AS400 system.  RPG programmers are becoming rare so support is an issue.  They run Lotus Notes email which has not been upgraded in years and thus out of support.  They do have a web application that serves as their CRM created several years ago by a consulting group.  It is a Java and JSP-based application running on Tomcat with MySQL as the data layer hosted on a Red Hat Linux server. The company is in a real growth cycle and realizes their current platforms cannot sustain them.  So, they are about to launch a project to implement SAP as a replacement for their legacy ERP system over the next year.  What migration strategy would you recommend for their landscape that would allow them to modernize as soon as possible?","explanation":"In this case, retiring Lotus Notes is the better move because it would just prolong the inevitable by simply migrating to EC2.  The CRM system is fairly new and can be re-platformed on Elastic Beanstalk.  Due to the impending ERP upgrade, it makes no sense to do anything with the legacy ERP.  It would take lots of work to port over an RPG application to run on AWS--if it's even possible.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud | AWS Cloud Enterprise  Strategy Blog"}],"answers":[{"id":"20ef21af4ab4ecda70e90e90861b154c","text":"Retire the Lotus Notes email and implement AWS Workmail.  Replatform the CRM application Tomcat portion to Elastic Beanstalk and the data store to MySQL RDS.  Invest time in training Operations staff CloudFormation and spend time architecting the landscape for the new SAP platform.  Do nothing to the legacy ERP platform until the SAP implementation is complete.  ","correct":true},{"id":"7579c185856bdbbcffbe49a649866488","text":"Rehost Lotus Notes mail on EC2 instances.  Refactor the CRM application to make use of Lambda and DynamoDB.  Use a third-party RPG to Java conversion tool to create Java versions of the legacy ERP to make it more supportable. Invest time in training developers continuous integration and continuous deployment concepts.  Because SAP implementations always take longer than estimated, rehost the legacy ERP system on EC2 instances so the AS400 can be retired.","correct":false},{"id":"480496ebe4100958e2f466291752ae2d","text":"Retire the CRM application and migrate the MySQL data over to Aurora.  Use QuickSight to provide access to the application for users.  Pay back support agreements to bring Lotus Notes back into support so it can be upgraded.  Migrate Notes email to EC2 instances.  Invest time in training Operations staff CloudFormation.  Create the complete SAP landscape as scriptable elements.  Do nothing to the legacy ERP platform until the SAP implementation is complete.","correct":false},{"id":"c7be9ce93a4a7a74b44e281cb697dcc4","text":"Begin a product search for a new CRM system that is cloud-ready.  Once identified, migrate the existing CRM into the new CRM system.  Migrate Lotus Notes to Workmail using the AWS Migration Hub.  Invest in training the IT staff about AWS through a Certified AWS Training Partner.  Provision and run the various SAP environments from scratch using EKS.  Do nothing to the legacy ERP until the SAP implementation is complete.","correct":false}]},{"id":"1727d24a-6bf1-4fc6-b99b-24c0bc4552e9","domain":"awscsapro-domain2","question":"NextGen Appliances Corporation is developing a new smart refrigerator that sends telemetry data to a backend application running on AWS. The refrigerator will also be able to receive commands from a mobile dashboard app that will be provided with the product. Features of the mobile dashboard app include data query, analytics, notifications, and settings commands. The mobile dashboard app will be developed using AWS Amplify and will be deployed to an Amazon S3 bucket configured for web hosting. The software on the refrigerator will be developed with the AWS IoT Device SDK, and will communicate with AWS IoT Core. How should the company architect the backend application to provide management capabilities for the refrigerator from the mobile dashboard app?","explanation":"The smart refrigerator solution has an AWS IoT Core rule set up to route messages to a Lambda function which stores event data in DynamoDB and sends desired notifications to an SNS topic for viewing in the mobile dashboard app. The mobile dashboard app can be developed in AWS Amplify, and deployed to an S3 bucket configured for web hosting. CloudFront can be used to provide public access to the bucket. The app can send RESTful API requests to the backend through API Gateway to a Lambda function that performs the DynamoDB queries and sends commands to the refrigerator through IoT Core. Analytics requests can be routed to QuickSight Mobile, which uses IOT Analytics as its data source. The mobile app won't be able to send settings commands to IoT Core directly. API Gateway and Lambda need to be involved for that. QuickSight can't use Kinesis Data Analytics as a data source. The data will need to be written to S3 for QuickSight queries. IoT Analytics will provide analytics capabilities more suited to this use case than general solutions like Kinesis Data Analytics and Redshift. Amazon Pinpoint is used for sending personalized marketing messages, not general notifications. ","links":[{"url":"https://aws.amazon.com/iot-analytics/","title":"AWS IoT Analytics"},{"url":"https://aws.amazon.com/amplify/","title":"AWS Amplify"},{"url":"https://docs.aws.amazon.com/quicksight/latest/user/using-quicksight-mobile.html","title":"Using the Amazon QuickSight Mobile App"},{"url":"https://aws.amazon.com/solutions/smart-product-solution/?did=sl_card&trk=sl_card","title":"Smart Product Solution"}],"answers":[{"id":"f0ebf0f93a7448dab82c61c11dcc2c4a","text":"Deliver IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and publishes notifications to an Amazon Simple Notification Service topic. Also write IoT Core messages to AWS IoT Analytics. Have the mobile dashboard app send settings commands to AWS IoT Core. Configure the AWS Mobile Hub NoSQL Database option and register the DynamoDB event table for app data queries. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses IoT Analytics as its source.","correct":false},{"id":"5840ee0fec04ee6fca146272a5e75892","text":"Write IoT Core messages to Amazon Kinesis Data Streams with Amazon Kinesis Data Analytics as one consumer and Amazon EC2 instances with Auto Scaling as a second consumer. Have the EC2 instances write product event data to Amazon DynamoDB and publish notifications to an Amazon Simple Notification Service topic. Configure the mobile dashboard app to call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses Kinesis Data Analytics as its source.","correct":false},{"id":"af8854f867f77abde2a38d7b3d1d84b7","text":"Route IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and sends notifications to Amazon Pinpoint. Also write IoT Core messages to an Amazon S3 bucket to be loaded into Amazon Redshift by AWS Glue. Have the mobile dashboard app call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses Redshift as its source.","correct":false},{"id":"57c04a93b4f460af88471726dcca35b2","text":"Send IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and publishes notifications to an Amazon Simple Notification Service topic. Also write IoT Core messages to AWS IoT Analytics. Have the mobile dashboard app call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses IoT Analytics as its source.","correct":true}]},{"id":"93badb2c-68ab-4715-b29e-1209af0c7b27","domain":"awscsapro-domain2","question":"You are a developer for a Aerospace company.  As part of an outreach and education program, the company has financed the construction of a free public service that provides weather forecasts for the sun.  Anyone can make a call to this REST service and receive up-to-date information on forecasted sun flare or sun spots that might have an electromagnetic impact here on Earth.  You are in the final stages of developing this new serverless application based on DynamoDB, Lambda and API Gateway.  During performance testing, you notice inconsistent response times for the service.  You had expected the API to be relatively consistent since its just retrieving data from DynamoDB and returning it as JSON via the API Gateway.  What might account for this variation in response time?","explanation":"Inconsistent response times can have a few different causes.  The exact nature of the testing is not explained but we can anticipate a few causes.  If you have enabled API Gateway caching, the gateway can return a result from its cache without having to go back to a supplying service or database.  This can result in various response rates depending on if an item is in the cache or not.  (The question did not specify we had slow response...just inconsistent response which could be a response faster than we expected.)  When a Lambda function is run for the first time or after an update, AWS must provision the Lambda environment and pull in any external dependencies.  This can result in a slower response time at first but faster later.  Also, if we do not have sufficient RCU for our DynamoDB table, we could run into throttling of the reads which could appear as inconsistent response times.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html","title":"AWS Lambda Execution Context - AWS Lambda"},{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"Enable API Caching to Enhance Responsiveness - Amazon API Gateway"}],"answers":[{"id":"158838df2d14636ed1429fffbcc6825e","text":"Your DynamoDB RCUs are underprovisioned.","correct":true},{"id":"f13e59f5ef22ace9340e24f434eb09cb","text":"You are experiencing a cold start.","correct":true},{"id":"43fb818a88f47b6b3b136abdd40a15a6","text":"There are not enough open inbound ports in your VPC.","correct":false},{"id":"4c3254ba1ddb0fb7a3ce3824707a7105","text":"You have enabled caching on the API Gateway.","correct":true},{"id":"b03fe2e16e6904dc4b5fa34c91c1f855","text":"You are using HTTP rather than HTTPS.","correct":false},{"id":"3bd10d778b04bfa32a7338a04e832d38","text":"The CloudFront distribution used by API Gateway is not deployed fully yet.","correct":false},{"id":"4922e0fbbf140cd0dbb6458bbadbf268","text":"The data is being updated on DynamoDB at the exact same time you are trying to read it.","correct":false}]},{"id":"5f6d53a1-1b9c-46fe-9a8e-a5706e72914d","domain":"awscsapro-domain4","question":"Which of the following is an example of buffer-based approach to controlling costs?","explanation":"The buffer-based approach to controlling costs is discussed in the Cost Optimization Pillar of the AWS Well-Architected Framework.  A buffer is a mechanism to ensure that applications can communicate with each other when they are running at different rates over time.  By decoupling the throughput rate of a process, you can better govern and smooth demand--creating a less volatile and reactionary landscape.  As a result, costs can be reduced by optimizing for the steady state. ","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"5bc62ebd024ca5594793ca76f08cd960","text":"A public-facing API is created using API Gateway and Lambda.  As a serverless architecture, it scales seamlessly in step with demand.","correct":false},{"id":"878d4fde3965b2e5f84c543b2cca1dfc","text":"A production ERP landscape is scaled up during the month-end financial close period to provide some padding for the additional processing and reports so they do not impact the normal business processes.","correct":false},{"id":"f867e23e60a3917c1ebe5e2c4ced818c","text":"An auto-scaling fleet is created to dynamically adjust available compute resources based network connection events as reported by CloudWatch.","correct":false},{"id":"26437f21964d56c3e373f55d997101ed","text":"A mobile image upload and processing service makes use of SQS to smooth an erratic demand curve.","correct":true}]},{"id":"b401741c-5b37-4b47-8e61-7802fbc9d7d6","domain":"awscsapro-domain1","question":"You are helping a client consolidate several separate accounts into a single account.  This consolidation will result in approximately 50 new VPCs in their one account.  They want to continue to use Route 53 for DNS but only want it accessible privately. How can you accomplish this most efficiently?","explanation":"Private Hosted Zones provide DNS services to VPCs but cannot be access from the internet.  They can be associated with VPCs either by the console, CLI or programmatically via SDK.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html","title":"Associating More VPCs with a Private Hosted Zone - Amazon Route 53"}],"answers":[{"id":"cbffb64b6b43e6fe45496b6e77ce17b8","text":"Create a Private Hosted Zone within Route 53 for each respective VPC.  Configure replication between the private hosted zones to keep records in sync.","correct":false},{"id":"82d5ef7e7176200aa4350ef90dd4c354","text":"Create a Public Hosted Zone within Route 53 and associate it to each VPC.  Configure a NACL on each VPC to deny inbound DNS queries (UDP port 53).","correct":false},{"id":"f74daa2300ac594c111ca9fce198f19c","text":"Create a central DNS server using EC2 and BIND.  Configure Route 53 to reference this DNS server as a resolver.  Update DNS records at the registrar to point to the central DNS.","correct":false},{"id":"3cc28b12f45b3dee8f7f16a0f93d00ce","text":"Install BIND on an EC2 instance in a single VPC.  Create VPC peering connections between the DNS VPC and any new VPCs.  Configure a DHCP Option Set to assign a DNS and link that to each VPC.","correct":false},{"id":"315372936e7ffba65896da15d0f45c2d","text":"Create a Private Hosted Zone within Route 53.  As the new VPCs are created, associate them with the Private Hosted Zone.","correct":true}]},{"id":"65288d1e-af34-43b4-9be7-0c1696c649fc","domain":"awscsapro-domain2","question":"What backup and restore options are available to you when using RDS for Oracle?","explanation":"Amazon RDS for Oracle can use the standard backup methods for RDS which is Snapshot and Point In Time Recovery.  You can also use Data Pump to export logical data to binary files, which you can later import into the database as well as the standard 'exp' and 'imp' utilities.  RMAN is not supported in RDS as a backup mechanism, although you can run certain RMAN commands against the database using the rdsadmin.rdsadmin_rman_util package.  Replication Backups is not a valid function within RDS for Oracle.","links":[{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"Amazon RDS for Oracle FAQs"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Oracle.html","title":"Oracle on Amazon RDS"},{"url":"https://aws.amazon.com/rds/oracle/features/","title":"Amazon RDS for Oracle features"}],"answers":[{"id":"5cd4a59bc9ce2cd88904b1edee24be49","text":"Oracle Recovery Manager (RMAN)","correct":false},{"id":"bf7eb3e57b2ec9b7b52b41ebe6c155ce","text":"RDS Snapshot and Point In Time Recovery","correct":true},{"id":"d93f3c575a1b1947fc462dc84cd425bc","text":"Replication Backups","correct":false},{"id":"d7b065bec7ec0dbd1d1985543b28aec5","text":"Oracle Data Pump Export and Import","correct":true},{"id":"4cff81b1f97c64130a64b1ea818c7f24","text":"Oracle Export/Import Utilities","correct":true}]},{"id":"3f7fa126-1155-4aa3-802d-e9eeb75f5e5a","domain":"awscsapro-domain3","question":"You work for a Clothing Retailer and have just been informed the company is planning a huge promotional sale in the coming weeks.  You are very concerned about the performance of your eCommerce site because you have reached capacity in your data center.  Just normal day-to-day traffic pushes your web servers to their limit.  Even your on-prem load balancer is maxed out, mostly because that's where you terminate SSL and use sticky sessions.  You have evaluated various options including buying new hardware but there just isn't enough time.  Your company is a current AWS customer with a nice large Direct Connect pipe between your data center and AWS.  You already use Route 53 to manage your public domains.  You currently use VMware to run your on-prem web servers and sadly, the decision was made long ago to move the eCommerce site over to AWS last.  Your eCommerce site can scale easily by just adding VMs, but you just don't have the capacity.  Given this scenario, what is the best choice that would leverage as much of your current infrastructure as possible but also allow the landscape to scale in a cost-effective manner?","explanation":"A Target Group for an ALB can contain instances or IP addresses.  In this case, we can define the private IP addresses of our on-prem web servers along side the private IP addresses of any EC2 instances we spin up.  The caveat is that we can only use private IP addresses when defining a target group in this way.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancing-via-ip-address-to-aws-on-premises-resources/","title":"New – Application Load Balancing via IP Address to AWS & On-Premises  Resources | AWS News Blog"}],"answers":[{"id":"6d3db4c52e96931f925f17fe8e9fd50f","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using public IP addresses of your on-prem web servers and additional EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"77592781918fa63474b5efbd5cc9555f","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an NLB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the NLB as an alias record.","correct":false},{"id":"4df24111c113846bfe0505ad0c84d9a3","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define two target groups:  one containing the public IP addresses of your on-prem load balancer and one including an auto scaling group of additional EC2 instances created from the imported AMI.  Assign both target groups to the ALB using the same listener port.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"3e47f65e4524f53faba23e6995b592f5","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":true}]},{"id":"9cab3dd3-61ad-44c6-b27a-2999e7aedf61","domain":"awscsapro-domain1","question":"For large organizationally complex AWS landscapes, it is considered a best practice to combine a tagging strategy with lifecycle tracking of various projects to identify orphaned resources that are no longer generating value for the organization and should be decommissioned.  With which AWS Well-Architected Framework Pillar is this best practice most aligned?","explanation":"Tagging has many uses but one strong use-case is in being able to tie resources that incur costs with cost centers or projects to create a direct line of sight to actual AWS expenses.  If this visibility does not exist, costs tend to increase because \"someone else is paying.\"  A Best Practice of the Cost Optimization Pillar is to maintain expenditure awareness.","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"820b8f74ad843d1574106ec4cadfc07e","text":"Reliability","correct":false},{"id":"d0bf6ab153eaa104c878925472ff129a","text":"Cost Optimization","correct":true},{"id":"2fae32629d4ef4fc6341f1751b405e45","text":"Security","correct":false},{"id":"2fa2e9f495df0cdcfa992171784f89b7","text":"Operational Excellence","correct":false},{"id":"bc368b371822f4e7df28c5768582dde8","text":"Performance Efficiency","correct":false}]},{"id":"2c688b4f-f267-472d-a68f-db7c9070bfae","domain":"awscsapro-domain5","question":"An application has a UI automation test suite based on Selenium and the testing scripts are stored in a GitHub repository. The UI tests need a username and password to login to the application for the testing. You check the test scripts and find that the credentials are saved in the GitHub repository using plain text. This may bring in some potential security issues. You suggest saving the username and password in a secure, highly available and trackable place. Which of the following methods is the easiest one?","explanation":"AWS Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. The credentials can be stored as ciphertext. The service is highly scalable, available, and durable. It also integrates with CloudTrail so the usage is easy to track. DynamoDB, DocumentDB and S3 are not designed to store parameters. These services need more configurations and are not as simple as AWS Parameter Store.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html","title":"AWS Systems Manager Parameter Store"}],"answers":[{"id":"fed8d54a1ce26b883744ac61a13e5ac9","text":"Configure an Amazon DocumentDB cluster with the d5.r5.large instance class. Create a schema for username and password. Enable Auto Scaling for the cluster and set up the default number of instances to be 3.","correct":false},{"id":"d540b786961136abf5dfd186e84fcca0","text":"Save the username and password in AWS Parameter Store. Select the SecureString type to encrypt the data with the default key in Key Management Service (KMS). Modify the scripts to fetch the parameter values through AWS SDK.","correct":true},{"id":"26e06a6fc6a107b0dd5aac356ad3908d","text":"Edit a JSON file to store the username and password and upload the file to an S3 bucket. Encrypt the S3 bucket with SSE-S3. Modify the S3 bucket policy to only allow the testing machines to get the file.","correct":false},{"id":"33442a701fc2a058eea89266ea439ee4","text":"Create a table in DynamoDB that has a primary key for username and a sort key for password. Select the encryption type as \"KMS - AWS managed CMK\". Enable the on-demand read/write capacity mode.","correct":false}]},{"id":"78111d9b-922f-435f-8e91-4ae84e990761","domain":"awscsapro-domain3","question":"A hotel chain has decided to migrate their business analytics functions to AWS to achieve higher agility when future analytics needs change, and to lower their costs. The primary data sources for their current on-premises solution are CSV downloads from Adobe Analytics and transactional records from an Oracle database. They've entered into a multi-year agreement with Tableau to be their visualization platform. For the time being, they will not be migrating their transactional systems to AWS. Which architecture will provide them with the most flexible analytics capability at the lowest cost?","explanation":"AWS Database Migration Service can be configured with an on-premises Oracle database as a source and S3 as a target. It can provide continuous replication between the two. AWS Glue can aggregate the data from S3 according to desired reporting dimensions and store the summaries in Redshift. Keeping the transactional detail in S3 and only keeping the aggregate information in Redshift will save on costs. The same is true for keeping transactional detail in S3 instead of RDS Oracle. AWS Glue is a great solution for transforming the Adobe Analytics CSV files to Parquet format in S3. Parquet's columnar organization will provide excellent performance for Redshift Spectrum queries that join between Redshift tables and S3. Tableau's Redshift connector supports Redshift Spectrum queries. For this use case, using Amazon QuickSight would not make sense since the company has already committed payments to Tableau via their multi-year agreement.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/glue/","title":"AWS Glue"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"dc33d336682223190f2d8cb22449cf81","text":"Implement AWS Database Migration Service to continuously replicate Oracle transactional data to an Amazon RDS Oracle instance. Use AWS Glue to write the Adobe Analytics data to the RDS Oracle instance. Install Tableau on Amazon EC2 and write queries against the RDS Oracle database.","correct":false},{"id":"1bc635be85a059a6135751bf21fd3550","text":"Use Oracle Data Guard to continuously replicate Oracle transactional data to an Oracle instance on Amazon EC2. Configure AWS Glue to aggregate the transactional data from the Oracle instance for each dimension into Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Redshift. Use Amazon QuickSight to query the data for visualization.","correct":false},{"id":"86c10f6cca438461e60f5c04886f57c9","text":"Configure AWS Database Migration Service to continuously replicate Oracle transactional data to Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Redshift. Use Amazon QuickSight to query the data for visualization.","correct":false},{"id":"e1f7ec66baca2c4e27cf072a8ca91424","text":"Employ AWS Database Migration Service to continuously replicate Oracle transactional data to Amazon S3. Configure AWS Glue to aggregate the transactional data from S3 for each dimension into Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Amazon S3 in Parquet format. Install Tableau on Amazon EC2 and write queries to Amazon Redshift Spectrum.","correct":true}]},{"id":"f43ec458-0ff5-4633-a57b-6bf82f60bd14","domain":"awscsapro-domain5","question":"You have a target group in an elastic load balancer (ELB) and its target type is \"instance\". You attach an Auto Scaling group (ASG) in the target group. All the instances pass the health check and have a healthy state in the target group. Due to a new requirement, the ELB target group needs to forward the incoming traffic to an IP address that belongs to an on-premise server. The ASG is no longer needed. There is already a VPN connection between the on-premise server and AWS VPC. How would you configure the target in the ELB target group?","explanation":"The target type of existing target groups cannot be changed from \"instance\" to \"IP\". Because of this, users have to create a new target group and set the target type to be \"IP\". After that, the on-premise IP address can be registered as a target. A domain name cannot be registered as a target in the target group. You also do not need to create a new elastic load balancer since you only need a new target group to register the IP address.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#target-type","title":"Target type in ELB target group"}],"answers":[{"id":"99f63c1cf5bcfbcb188328714abb8ed3","text":"Register a record set in AWS Route 53 to forward a domain name to the on-premise IP address. Modify the target group to register the domain name as its target. Remove the previous Auto Scaling group from the target group.","correct":false},{"id":"e0b619a421d68626ffb82b1a6e1d22d5","text":"Remove the Auto Scaling group from the target group and modify the target type to be \"IP\". Attach the IP address to the target and set up the IP address and port in the health check configurations.","correct":false},{"id":"49646fca04901301d145a2a814a7e481","text":"In the elastic load balancer, create a new target group with an \"IP\" target type. Register the on-premise IP address as its target. Monitor if the target becomes healthy after some time. Remove the old target group.","correct":true},{"id":"265de6fdcab5323b156774dcb949d309","text":"Create a new network load balancer with a new listener and target group. Configure the target type to be \"IP\" in the target group and attach the on-premise IP address to it. Set up the health check using the HTTP protocol.","correct":false}]},{"id":"1141835f-8419-4642-8777-81c354976178","domain":"awscsapro-domain2","question":"You are implementing a new eCommerce system for your organization.  It requires Red Hat Linux and uses either multicast or external cache (Redis or Memcached) to share sessions.  You need to implement SSL but do not want to manage individual certificates on each EC2 instance.  Additionally, you want to be sure all parts of the landscape are setup for high availability.  Which of the following architectures best fits the situation at the least cost?","explanation":"Because multicast is not supported in VPCs, we have to use a cache.  Redis supports more high availability configurations than Memcached.  Exclusive use of a spot fleet could leave us with no running instances, so we avoid that option.","links":[{"url":"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/SelectEngine.html","title":"Comparing Memcached and Redis - Amazon ElastiCache for Redis"}],"answers":[{"id":"f792a5026bd6de7e4734492c311889f2","text":"Use ElastiCache for Memcached as a session cache.  Use a Network Load Balancer attached to an auto scaling group of RHEL instances.  Use Certificate Manager to assign a certificate to the load balancer and terminate SSL there.","correct":false},{"id":"55247c98d5a84134fd3e58f8b41b977f","text":"Use ElastiCache for Redis as a session cache.  Use an Application Load Balancer attached to an auto scaling group of RHEL instances.  Use Certificate Manager to assign a certificate to the ALB and terminate SSL there.","correct":true},{"id":"2cfd86b2c2bf79a4de540a7db3b9949b","text":"Use ElastiCache for Memcache as a session cache.  Use an Application Load Balancer attached to an auto scaling group of RHEL instances.  Use Certificate Manager to assign a certificate to the load balancer and terminate SSL there.","correct":false},{"id":"f046d563a942b8afa2bb70fa9daf2b13","text":"Use an Application Load Balancer attached to a spot fleet of RHEL.  Use ElastiCache for Redis as a session cache.  Use Certificate Manager to assign a certificate to the ALB and terminate SSL there.","correct":false},{"id":"38b528a48d4729c06200bead282115fb","text":"Use an Application Load Balancer attached to an auto scaling group of RHEL instances.  Enable multicast support in the VPC containing the web servers.  Use Certificate Manager to assign a certificate to the ALB and terminate SSL there.","correct":false}]},{"id":"58fa6440-b521-4dff-b6be-b1f8818c718d","domain":"awscsapro-domain2","question":"You currently manage a website that consists of two web servers behind an Application Load Balancer.  You currently use Route 53 as a DNS service.  Going with the current trend of websites doing away with the need to enter \"www\" in front of the domain, you want to allow your users to simply enter your domain name. What is required to allow this?","explanation":"Route 53 allows you to create a record for a zone apex.  In this case, we have created an alias record for the ALB.","links":[{"url":"https://docs.aws.amazon.com/govcloud-us/latest/ug-west/setting-up-route53-zoneapex-elb.html","title":"Setting Up Amazon Route 53 Zone Apex Support with an AWS GovCloud (US-West)  Elastic Load Balancing Load Balancer - AWS GovCloud (US-West) User Guide"}],"answers":[{"id":"935c6d54a7555f482da11c1ded5dfc9c","text":"Create a CNAME record for the root domain and configure it to resolve to www subdomain name.","correct":false},{"id":"5c48fe59299d582ae6074d269711de20","text":"Route 53 does not currently support zone apex records.  You must use a third-party DNS provider.","correct":false},{"id":"f70aff68786523bc13fd44cb89feb502","text":"Create an A record for your top-level domain name using the public IP of your ALB.","correct":false},{"id":"0dc522eb50b1c1320900ac02fcfb4dd6","text":"Create an S3 bucket as a static web host.  Create simple HTML file that redirects to the www subdomain.  Use CloudFront custom origins as a front-end for your top-level domain name.","correct":false},{"id":"9e87834a987d5b3556ce3e9f35ac6a82","text":"Create an A record for your top-level domain name as an alias for the ALB.","correct":true}]},{"id":"6b6689f4-b150-482a-aa96-eab1674cb232","domain":"awscsapro-domain5","question":"Quality Auto Parts, Inc. has installed IoT sensors across all of their manufacturing lines. The devices send data to both AWS IoT Core and Amazon Kinesis Data Streams. Kinesis Data Streams triggers a Lambda function to format the data, and then forwards it to AWS IoT Analytics to perform monitoring and time-series analyses, and to take actions based on business processes. After an equipment failure on one of the manufacturing lines causes tens of thousands of dollars in revenue losses, it's determined that alarms for a specific piece of equipment where received seventy-five seconds after the issue originated, and that automated corrective action within a few seconds of the problem could have avoided the financial losses altogether. What changes should be made to the architecture to improve the latency of device alerts?","explanation":"AWS IoT Analytics is useful for understanding long-term device performance, performing business reporting, and identifying predictive fleet maintenance needs, but common latencies run from seconds to minutes. If you need to analyze IoT data in real-time for device monitoring, use Kinesis Data Analytics, which provides latencies in the millisecond to seconds range. A Lambda function can be used as the destination for Kinesis Data Analytics to perform corrective actions. IoT Core rules can write messages to a Kinesis stream, but not directly to Kinesis Data Analytics. Having a Lambda function perform anomaly detection will work, but will require more logic to be written for query setup and execution than using a specialized service like Kinesis Data Analytics. With Amazon CloudWatch Alarms, an alarm will watch a single metric over a period time, but will not provide the capabilities of SQL to detect complex anomaly conditions.","links":[{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/aws-reference-architecture-time-series-processing.pdf?did=wp_card&trk=wp_card","title":"Processing IoT Time Series Data on AWS"},{"url":"https://aws.amazon.com/iot-analytics/faq/","title":"AWS IoT Analytics FAQs"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/05/introducing-real-time-iot-device-monitoring-with-kinesis-data-analytics/","title":"Introducing Real-Time IoT Device Monitoring with Kinesis Data Analytics"}],"answers":[{"id":"7e2eb8f3a96a390aab88e66000821c26","text":"Create an AWS IoT Core rule to write the message to Amazon Kinesis Data Analytics to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":false},{"id":"fe8ff20697982ca413f81ea14472e603","text":"Add Amazon Kinesis Data Analytics as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":true},{"id":"71ade679994c0c1e6e55b3853194e4c5","text":"Add another AWS Lambda function as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Have the Lambda function write the anomalies to Amazon DynamoDB and perform device corrective action when needed.","correct":false},{"id":"522af3cd7d520d3e94e97c02d19c0672","text":"Create an AWS IoT Core rule to write the message to Amazon CloudWatch Alarms to detect anomalies in the data. Invoke another AWS Lambda function from CloudWatch Alarms to perform device corrective action when needed.","correct":false}]},{"id":"f3efb368-9a43-4e1d-bf0a-1cf55bb918a8","domain":"awscsapro-domain4","question":"An AWS customer makes use of a wide variety of AWS services across multiple AWS regions. As the usage cost keeps increasing, the customer wants to get the detailed billing and cost information of his AWS account. In the meantime, the customer needs to quickly analyze the cost and usage data, visualize it, and share it through data dashboards so that he can get a better understanding of the billing and resource utilization. Which of the following methods would you choose?","explanation":"Users can get the billing and usage information from the AWS Cost and Usage Reports. The reports downloaded in an S3 bucket can be further processed and analyzed by QuickSight. The reports cannot be created in AWS Cost Explorer or billing dashboard. The question also asks for data visualization and dashboards. QuickSight, AWS's Business Intelligence service, is the most appropriate service for this requirement. Athena, Redshift or Elastic MapReduce are not suitable.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-upload-aws-cost-usage-reports-to-redshift-and-quicksight/","title":"Upload AWS Cost & Usage Reports to QuickSight"}],"answers":[{"id":"36fb0d3b243e97c8ed14e32e6a2bc329","text":"Generate the billing reports from the AWS management console and upload the files to an S3 bucket. Create an Amazon Elastic MapReduce (EMR) cluster by specifying data inputs and outputs. Process the data in the cluster and generate reports for the admin IAM user.","correct":false},{"id":"78f31d3919e8b532d502611700598d75","text":"Create the hourly AWS Cost and Usage Reports and enable the data integration with Amazon QuickSight. Analyze and visualize the data in QuickSight by creating interactive dashboards and generating custom reports.","correct":true},{"id":"d3ef8ffbe6a0ff9378c5e248f2f7e165","text":"Download the AWS Cost and Usage Reports from the AWS billing dashboard in an S3 bucket which is owned by the administrators. Integrate the reports with an Amazon Redshift cluster. Analyze and view the data in the Redshift data warehouse.","correct":false},{"id":"fc09f5c144565de3b8472bde413e896a","text":"Create a data pipeline that downloads the monthly cost and usage reports from AWS Cost Explorer and uploads the reports to an S3 bucket. Set up a specific billing table with Amazon Athena and analyze the billing and usage data using SQL query commands.","correct":false}]},{"id":"375e7161-43df-4d2f-adab-75cc6166a453","domain":"awscsapro-domain5","question":"You build a CloudFormation stack for a new project. The CloudFormation template includes an AWS::EC2::Volume resource that specifies an Amazon Elastic Block Store (Amazon EBS) volume. The EBS volume is mounted in an EC2 instance and contains some important customer data and logs. However, when the CloudFormation stack is deleted, the EBS volume is deleted as well and the data is lost. You want to create a snapshot of the volume when the resource is deleted by CloudFormation. What is the easiest method for you to take?","explanation":"The easiest method is using the DeletePolicy attribute in the CloudFormation template. The \"Snapshot\" value ensures that a snapshot is created before the CloudFormation stack is deleted. The \"Retain\" value is incorrect as it keeps the volume rather than creates a snapshot. The EBS lifecycle manager can create daily snapshot however it is not required in the question. When the CloudWatch Event rule is triggered, the EBS volume may already be deleted and no snapshot can be taken. Besides, the CloudFormation deletion cannot be suspended by a Lambda function.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html","title":"DeletionPolicy Attribute"}],"answers":[{"id":"061af6db30e4b65a1d544d4e64546085","text":"Modify the CloudFormation template by adding a DeletePolicy variable for the AWS::EC2::Volume resource. Specify the value of \"Retain\" to automatically create a snapshot of the EBS volume before the stack is deleted.","correct":false},{"id":"692ea271f5ada1d555a5889be933267e","text":"Modify the CloudFormation template to create an EBS snapshot strategy in EBS lifecycle manager which creates a daily snapshot as backup and also another snapshot when the EBS volume’s CloudFormation stack is being deleted.","correct":false},{"id":"555e7ef93e63402dd6a074edd5b8a16d","text":"Create a CloudWatch Event rule that checks the CloudFormation delete-stack event. Trigger a Lambda function that pauses the CloudFormation stack deletion, creates the EBS snapshot of the volume and resumes the stack deletion after the snapshot is created successfully.","correct":false},{"id":"0719057e0f4aca49eb47cd94c333e599","text":"Add a DeletePolicy attribute in the CloudFormation template and specify \"Snapshot\" to have AWS CloudFormation create a snapshot for the EBS volume before deleting the resource.","correct":true}]},{"id":"9882b1ed-c0de-4205-8a00-54731bef3109","domain":"awscsapro-domain2","question":"You work for an organic produce importer and the company is trying to find ways to better engage with its supply chain.  One idea is to create a public ledger that all members of the supply chain could update and query as products changed hands along the journey to the customer.  Then, your company could create an app that would allow consumers to view the chain from producer to end retailer and have confidence in the product sourcing.  Which AWS service or services could most directly help realize this vision?","explanation":"Amazon Quantum Ledger Database (QLDB) is a fully-managed ledger database that provides a transparent, immutable and verifiable transaction log.  While other products could be used to create such a supply chain logging solution, QLDB is the closest to a ready-made solution.","links":[{"url":"https://aws.amazon.com/qldb/","title":"Amazon QLDB"}],"answers":[{"id":"35b417ac310d92018a8db5515d4fae60","text":"Amazon DynamoDB and Lambda","correct":false},{"id":"ca649f7447f846ed8add8c396187b83a","text":"Amazon P2PShare and API Gateway","correct":false},{"id":"680da36fb45c26a1d8e7996c6f5014cd","text":"Amazon CloudTrail and API Gateway","correct":false},{"id":"a9d83c7f8f0b0f2a8f67b7097ee73e3a","text":"Amazon QLDB","correct":true},{"id":"a8312fbd65c49606c59b53a8a062ecff","text":"Amazon Managed Blockchain","correct":false}]},{"id":"06504582-ce03-4252-b1dc-29654ff427bb","domain":"awscsapro-domain5","question":"You have just set up a Service Catalog portfolio and collection of products for your users.  Unfortunately, the users are having difficulty launching one of the products and are getting \"access denied\" messages.  What could be the cause of this?","explanation":"For Service Catalog products to be successfully launched, either a launch constraint must be assigned and have sufficient permission to deploy the product or the user must have the same required permissions.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-launch.html","title":"AWS Service Catalog Launch Constraints - AWS Service Catalog"}],"answers":[{"id":"8bc518c42ab2dfa122390f1a497349a2","text":"The template constraint assigned to the product does not have the proper permissions.","correct":false},{"id":"779fec840f0f81e772ba3137d7ac28ad","text":"The notification constraint did not have access to the S3 location for the product's CloudFront template.","correct":false},{"id":"7e0083aafd999688d628f67e003d79be","text":"The product does not have a launch constraint assigned.","correct":true},{"id":"a987914548e48bb64be70c11a97ec644","text":"A Service Catalog Policy has not yet been applied to the account.","correct":false},{"id":"d11a80651ecb668bdbe507d4e7398b6a","text":"The launch constraint does not have permissions to CloudFormation.","correct":true},{"id":"43d54acb2dbc9f2dc8a0d793553b965e","text":"The user launching the product does not have required permissions to launch the product.","correct":true}]},{"id":"08a68d51-48ba-43b7-b0c3-c24e04bb33a8","domain":"awscsapro-domain3","question":"You have just completed the move of a Microsoft SQL Server database over to a Windows Server EC2 instance.  Rather than logging in periodically to check for patches, you want something more proactive.  Which of the following would be the most appropriate for this?","explanation":"The default predefined patch baseline for Windows servers in Patch Manager is AWS-DefaultPatchBaseline.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html","title":"Default and Custom Patch Baselines - AWS Systems Manager"}],"answers":[{"id":"5ee836bac8680ff00d457a8d7f90fad6","text":"Make use of Patch Manager and the AWS-WindowsDefaultPatchBaseline pre-defined baseline","correct":false},{"id":"4b5715f588231d7445bb512181ea13a2","text":"Make use of Server Manager and the AWS-LinuxWindowsDefaultPatchBaseline pre-defined baseline","correct":false},{"id":"78beead90b3e8deb4bf1ee7e3544a309","text":"Make use of AWS Batch to apply patches as they appear on the RSS feed from Microsoft","correct":false},{"id":"42c46b2fcd3034cf79b83f0d5dc37f7d","text":"Make use of Patch Manager and the AWS-DefaultPatchBaseline pre-defined baseline","correct":true},{"id":"b40084bb5d5139b353505e5f942afc34","text":"Make use of Patch Manager to apply patches as you have defined in the Patch Groups","correct":false}]},{"id":"5af539b7-b132-4a3a-bc80-406c620e7325","domain":"awscsapro-domain1","question":"A food service business has begun an initiative to migrate all applications and data to the AWS cloud. Governance needs to be established before any migrations can occur. Business units such as sales, marketing, and product management have fluctuating infrastructure capacity and security requirements, while other business units like finance, operations, and human resources have more static demand. Security policies and compliance needs vary by project group within each business units. Each business unit is responsible for it's own cost center, and the finance group would like cost reporting to be as streamlined as possible. Which AWS account structure will best satisfy the company's governance needs?","explanation":"Leveraging AWS Organizations to manage an account structure with a core Organizational Unit and Organizational Units for each business unit provides flexibility for future organizational changes. Creating an account for each project group facilitates security policy differences within business units, and limits the exposure of a single security event. Managing differing security requirements by project group in a single account will require more governance maintenance. Creating billing, shared services, and log archive accounts in multiple Organizational Units will result in duplication of services, and can be done at the core level.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-laying-the-foundation/introduction.html","title":"Laying the Foundation: Setting Up Your Environment for Cost Optimization"},{"url":"https://aws.amazon.com/solutions/aws-landing-zone/?did=sl_card&trk=sl_card","title":"AWS Landing Zone"}],"answers":[{"id":"a66d8391267460b5800c5c3d07921767","text":"Use AWS Organizations to create Organizational Units for each business unit. Create a billing account, a shared services account, and a log archive account in each Organizational Unit. Create accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"a8f7d8fbb7c6c3a1a14c91577dff42e1","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Place business units with similar security requirements in shared Organizational Units. Create accounts for each business unit in the shared Organizational Units. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"03705913700b8d76205d4203c58dc5e1","text":"Use AWS Organizations with a single Organizational Unit to consolidate costs. Create a billing account, a shared services account, and a log archive account in the Organizational Unit. Create individual accounts for each business unit. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs","correct":false},{"id":"bd400ff0d22480599228a0442d2bb8d4","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Create an Organizational Unit for each business unit that contains accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":true}]},{"id":"bbcb9a8c-f84d-4424-b199-9047a4625e15","domain":"awscsapro-domain2","question":"Your company's DevOps manager has asked you to implement a CI/CD methodology and tool chain for a new financial analysis application that will run on AWS. Code will be written by multiple teams, each team owning a separate AWS account. Each team will also be responsible for a Docker image for their piece of the application. Each team's Docker image will need to include code from other teams. Which approach will provide the most operationally efficient solution?","explanation":"AWS CodePipeline, AWS CodeCommit, and AWS CodeBuild all allow cross-account access once the appropriate resource-level permissions have been granted. Orchestrating deployments from a single DevOps account will provide the most operationally efficient solution, resulting in less need for coordination of services and configurations across development team accounts.","links":[{"url":"https://aws.amazon.com/products/developer-tools/","title":"Developer Tools on AWS"},{"url":"https://aws.amazon.com/blogs/devops/how-to-use-cross-account-ecr-images-in-aws-codebuild-for-your-build-environment/","title":"How to Use Cross-Account ECR Images in AWS CodeBuild for Your Build Environment"}],"answers":[{"id":"b9ae951b67f2fed4a145bd7f591c8631","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the DevOps account to AWS CodeCommit in the team accounts to get the latest code. Perform cross-account access from AWS CodeBuild in the DevOps account to AWS CodeBuild in the team accounts to get the Docker images. Perform deployments from AWS CodeDeploy in the DevOps account","correct":true},{"id":"c32df9c97fbc40e07515d4ca41de63e2","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in the team accounts","correct":false},{"id":"4a4bd59860afb498d97f0d01cff52b7a","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in a single DevOps account","correct":false},{"id":"83443cdcabe51198b91ed96d84eed4a6","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform all deployments from AWS CodeDeploy in the DevOps account","correct":false}]},{"id":"0a4b2449-9275-4c2f-af02-0f8c51614f3a","domain":"awscsapro-domain2","question":"You are part of a business continuity team at a consumer products manufacturer.  In scope for the current project is the company web server which serves up static content like product manuals and specification sheets which customers can download.  This landscape consists only of a single NGINX web server and 5TB of local attached storage for the static content.  In the case of a failover, RTO has been defined as 15 minutes with RPO as 24 hours as the content is only updated a few times a year.  Staff reductions and budget constraints for the year mean that you need to carefully evaluate and choose the most cost-effective and most automated solution in the case of a failover.  Which of the following would be the most appropriate given the situation?","explanation":"In this case, the most cost-effective and most automated way to ensure the reliability statistics would be to migrate the static content to S3.  This option has built-in robustness and will cost less than any other option presented.","links":[{"url":"http://d36cz9buwru1tt.cloudfront.net/AWS_Disaster_Recovery.pdf","title":"Using Amazon Web Services for Disaster Recovery"}],"answers":[{"id":"925fb4f48c2c90011e7e1f92d3412dcd","text":"Migrate the static content to an EFS share.  Mount the EFS share via NFS from on-prem to serve up the web content.  Configure another EC2 instances with NGINX to also mount the same share.  Upon fail-over, manually redirect the Route 53 record for the web server to the IP address of the EC2 instance.","correct":false},{"id":"69fd92fc5be4948bfc0128d02ed2f392","text":"Install the CloudWatch agent on the web server and configure an alarm based on a health check.  Create an EC2 replica installation of the web server and stop the instances.  Create a Lambda function that is triggered by the health check alarm which starts the dormant EC2 instance and updates a DNS entry in Route 53 pointing to the new server.","correct":false},{"id":"2d14eb477a2f2c9dc3605ea5740297cd","text":"Create a small pilot-light EC2 instance and configure with NGINX. Configure a CRON job to run every 24 hours that syncs the data from the on-prem web server to the pilot-light EC2 EBS volumes.  Configure an Application Load Balancer to direct traffic to the on-prem web server until a health check fails.  Then, the ALB will redirect traffic to the pilot light EC2 instances. ","correct":false},{"id":"46b84866da301243767946743c6024a1","text":"Download and configure the AWS Storage Gateway, creating a volume which can be replicated to AWS S3.  Attach that volume to the web server via iSCSI and migrate the content to that Storage Gateway volume.  Locate an AMI from the AWS Marketplace for NGINX.  If a failover is required, manually launch the AMI and run an RSYNC between the on-prem server and the EC2 server to migrate the content.","correct":false},{"id":"bceaccaea071754d3724eaf31f0f6189","text":"Migrate the website content to an S3 bucket configured for static web hosting.  Create a Route 53 alias record for the web server domain.  End-of-life the on-prem web server.","correct":true}]},{"id":"0e57f592-af77-4ff7-b32a-752702278ab5","domain":"awscsapro-domain2","question":"Several years ago, the company you are consulting for started an SOA concept to enable more modularity.  At that time, they chose to deploy each microservice as separate LAMP stacks launched in Elastic Beanstalk instances due to the ease of deployment and scalability.  They are now in the process of migrating all the services over to Docker containers.  Which of the following options would make the most efficient use of AWS resources?","explanation":"ECS might be a possible choice, but pods are a concept with Kubernetes and not ECS.  Elastic Beanstalk deployments run in their own self-contained environment and don't share resources.  Of the choices presented, the only feasible choice is K8s on EC2.","links":[{"url":"https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html","title":"Pod Networking - Amazon EKS"},{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html","title":"What Is AWS Elastic Beanstalk? - AWS Elastic Beanstalk"}],"answers":[{"id":"f9f924e7f824f3e085294f8e97e7b362","text":"Create an auto-scaled group of EC2 instances and run Kubernetes across them to orchestrate the containers.","correct":true},{"id":"25ffdcfeb30ada0ea8a578689b8d0a49","text":"Create an ECS cluster and configure each container to exist in its own pod.  Use the Calico add-on to manage access to each service.","correct":false},{"id":"a473fc9c398c6c24b4c0514eb9fb546a","text":"Configure Elastic Beanstalk to more efficiently run the existing landscapes across a single auto-scaled group rather than separate instances.","correct":false},{"id":"661aa72d0ab709e1931e20010a6e3ad7","text":"Use the Elastic Container Repository and AWS Spectrum to fully automate the deployment and scaling of the Docker containers.","correct":false},{"id":"d93d65de15f35d86079a7a18bda05be2","text":"Package the containers as JAR files and deploy them with Lambda to take advantage of the pay-per-use model","correct":false}]},{"id":"af6501cb-5a69-4af7-a5e7-cf03d6ce09c1","domain":"awscsapro-domain1","question":"As an AWS Solutions Architect, you are responsible for the configuration of your company's Organization in AWS. In the Organization, the Root is connected with two Organizational Units (OUs) called Monitor_OU and Project_OU. Monitor_OU has AWS accounts to manage and monitor AWS services. Project_OU has another two OUs as its children named Team1_OU and Team2_OU. Both Team1_OU and Team2_OU have invited several AWS accounts as their members. To simplify management, all the AWS accounts under Team1_OU and Team2_OU were added with a common administrative IAM role which is supposed to be used by EC2 instances in their accounts. For security concerns, this role should not be deleted or modified by users in these AWS accounts. How would you implement this?","explanation":"The requirement of this scenario is that the IAM role should not be modified by IAM principals in Team1_OU and Team2_OU. The best place to implement this is in SCP as it provides central management. Since Team1_OU and Team2_OU can inherit the SCP policy from Project_OU, only Project_OU needs to attach the SCP that denies the action. In the meantime, the Root should have a default full access policy. It is improper to use IAM policies or Permissions Boundary to achieve this requirement as they may be easily changed at a later stage by other IAM users, and it is also complicated to implement if there is a large number of IAM principals.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"How SCP works in AWS Organization?"}],"answers":[{"id":"e5acb8782282566185828b0ca39813fc","text":"Configure a new SCP policy to prevent IAM principals from deleting or modifying the IAM role. Attach the SCP policy to Project_OU, Team1_OU and Team2_OU. Configure a default full access SCP policy to Monitor_OU.","correct":false},{"id":"f868081ae3429a7ed8318636537ec834","text":"Make sure that Root node has a SCP policy that allows all actions. Create a SCP policy that restricts IAM principals from changing this particular IAM role. Attach the SCP policy to Project_OU.","correct":true},{"id":"46f9090f957b494e2e0075ceef9bac8b","text":"For the AWS accounts in Team1_OU and Team2_OU, configure a Permissions Boundary in IAM principals to prevent them from making modifications to this particular IAM role. As a result, all IAM principals will not have IAM policies to potentially change the role.","correct":false},{"id":"77abf7e62575051a27444694461acccd","text":"The nodes of Root, Project_OU and Monitor_OU in the Organization should allow all actions. For AWS accounts in Team1_OU and Team2_OU, attach an IAM policy that denies modifying the IAM role to IAM users, groups and roles.","correct":false}]},{"id":"19591d08-60c8-494e-9c39-d69c6c3390f0","domain":"awscsapro-domain4","question":"Your company's AWS migration was not planned out very well across the enterprise.  As as result, different business units created their own accounts and managed their own resources.  Recently, an internal audit of costs show that there may be some room for improvement with regard to how reserved instances are being used throughout the enterprise.  What is the most efficient way to ensure that the reserved instance spend is being best used?","explanation":"The discounts for Reserved Instances can be shared across accounts that are linked with Consolidated Billing but Reserved Instance Sharing must be enabled.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-ri-consolidated-billing/","title":"EC2 Reserved Instance Consolidated Billing"}],"answers":[{"id":"f58169ec8f86bed7c84e4510ed491661","text":"Load CloudTrail instance usage data into Redshift for analytics.  Use QuickSight to create some utilization reports on existing reserved instances.  Relocate on-demand instances into regions where reserved instances are underutilized.","correct":false},{"id":"af8967525080f0a545bb6fbafc8a8f3c","text":"Setup Consolidated Billing for a single account and link all the other various accounts in the organization.  Ensure that Reserved Instance Sharing is turned on.  The discounts for specific reserved instances will automatically be applied to the consolidated invoice.","correct":true},{"id":"34d8e4aeeb67918e8bd27daba0923994","text":"Use Cost Explorer reports to analyze coverage of reserved instances.  Where there are coverage gaps, purchase more reserved instance capacity for that account. Where there is excess, place those on the Reserved Instance Marketplace.","correct":false},{"id":"617beec9b1a6fd3ca1cec7764ddfd9fb","text":"Use AWS Organizations to organize accounts under one organizational unit.  Use AWS Budgets to analyze utilization of reserved instances.  Reassign those RIs to other accounts that are currently using On-Demand instances.","correct":false}]},{"id":"b06ef2a9-b122-4b47-b5be-b6d604e78405","domain":"awscsapro-domain2","question":"You are working with a customer to implement some better security policies.  They have a group of remote employees working on a confidential project that uses some proprietary Windows software and stores data in S3.  The Chief Information Security Officer is concerned about the threat of the desktop software or confidential data being smuggled out to a competitor.  What architecture would you recommend to best address this concern? ","explanation":"Using a locked down virtual desktop concept would be the best way to manage this.  AWS WorkSpaces provides this complete with client software to log into the desktops.  These Workspaces can be walled off from the Internet.  Using policies, you could allow access from only those in the Workspaces VPC.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html","title":"What Is Amazon WorkSpaces? - Amazon WorkSpaces"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html","title":"Endpoints for Amazon S3 - Amazon Virtual Private Cloud"}],"answers":[{"id":"21d0f58369770fb84d6bff8bfcf9c265","text":"Use Service Catalog to deploy and manage the proprietary Windows software to the remote employees.  Create an OpenVPN server instances within a VPC.  Create an VPC Interface Endpoint to S3 and use a security group to only permit traffic from the OpenVPN server security group.  Supply the remote employees with instructions to install and login using OpenVPN client software.","correct":false},{"id":"36e6b9fb8ba6dd4b59c9f032ba6f78f7","text":"Provision Windows 2016 instances in a private subnet.  Create a specific security group for the Windows machines permitting only SSH inbound.  Create a NACL which allows traffic to S3 services and explicitly deny all other network traffic to and from the subnet.  Assign an S3 bucket policy that only allows access for members of the Windows machine security group.","correct":false},{"id":"f8c297565cec662fd215e6c551daca36","text":"Create a bucket policy using the sourceIP condition to only allow access from a specific VPC CIDR.  Apply a NACL which only permits inbound port 22 and outbound ephemeral ports.  Deploy Amazon Workspaces in the VPC and disable internet access.  Supply the users with instructions on downloading and login into the Workspaces instances.","correct":false},{"id":"b5b5a93dca37986129c444b7654d600f","text":"Provision Amazon Workspaces in a secured private VPC.  Do not enable Internet access for the Workspaces.  Create a VPC Gateway Endpoint to S3 and implement an endpoint policy that explicitly allows access to the required bucket.  Assign an S3 bucket policy that denies access unless the sourceVpce matches the VPC endpoint.  Supply the users with instructions on downloading and login into the Workspaces instances.","correct":true}]},{"id":"1239c235-107c-4f5e-8bac-9dc824c00680","domain":"awscsapro-domain5","question":"You are helping a client with some process automation.  They have managed to get their website landscape and deployment process encapsulated in a large CloudFormation template.  They have recently contracted with a third-party service to provide some automated UI testing.  To initiate the test scripts, they need to make a call out to an external REST API.  They would like to integrate this into their existing CloudFormation template but not quite sure of the best way to do that.  Help them decide which of the following ideas is feasible and incurs the least extra cost.","explanation":"To integrate external services into a CloudFormation template, we can use a custom resource.  Lambda makes a very good choice for this scenario because it can handle some logic if needed and make a call out to an external API.  Using an EC2 instances to make this call is excessive and we likely would not have the ability to configure the third-party API to poll an SQS queue.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html","title":"AWS Lambda-backed Custom Resources - AWS CloudFormation"}],"answers":[{"id":"43569df3ec4b7db0265dea4051c04644","text":"Add a small EC2 instance definition to the CloudFormation template.  Define a User Script for that instance which will install a custom application from S3 to call out to the external REST API endpoint using the POST method to trigger the testing process.  Add a CleanUp parameter to the EC2 instance definition that will shut down the instance once the activity has completed.","correct":false},{"id":"6b3b26e17f2323a91f04f792f0c2d20c","text":"Create a Lambda function which issues a call out to the external REST API using the POST method.  Define a custom resources in the CloudFormation template and associate the Lambda function and execution role with the custom resource.  Include DependsOn to ensure that the function is only called after the other instances are ready.","correct":true},{"id":"f761e84ee0cd0f689465458a41b69fae","text":"Include an SQS queue definition in the CloudFormation template.  Define a User Script on the deployed EC2 instance which will insert a message into the SQS queue only once it has fully booted.  Configure the external REST API to use long polling to check the queue for new messages in order to initiate the testing process.","correct":false},{"id":"97a123d11bbf3be0e3e1788e2f0874ac","text":"Add an API Gateway deployment to the CloudFormation template.  Add the DependsOn parameter to the API Gateway resource to ensure that the call to the external API only happens after all the other resources have been created.  Create a POST method and define it as a proxy for the external REST API endpoint.  Using SWF, call the API Gateway endpoint to trigger the testing process.","correct":false}]},{"id":"0ee4566a-508e-472d-9789-3318e3284aca","domain":"awscsapro-domain5","question":"You are an AWS Solutions Architect and you maintain a CloudFormation stack that includes the resources of a network load balancer and an Auto Scaling group. The ASG has one running instance. A developer uses the instance for feature development and testing. However, after he adds some configurations and restarts an application process, the instance is terminated by the Auto Scaling group and a new instance is created. The new configurations are lost in the new instance. You need to modify the resource settings to make sure that the instance is not terminated by the ASG when application processes are restarted. Which of the following methods would best achieve this?","explanation":"The instance fails the health check in the ELB target group and is then terminated by ASG whenever the application processes are restarted. The prevent the ASG from terminating the EC2 instance you need to modify the health check type from ELB to EC2. As a result, even if the instance fails the health check in the ELB target group, it will not be terminated by the Auto Scaling group. You do not need to create an AMI or a new launch configuration to address the issue. And the custom health check script that runs every minute cannot prevent the instance from being terminated.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health checks for Auto Scaling instances"}],"answers":[{"id":"745390da2b4433786bf4cdf17df3d2d3","text":"Edit a health check shell script that performs some sanity checks in the EC2 instance. If the sanity checks pass, the shell script uses AWS CLI “aws autoscaling set-instance-health” to set its status to be healthy. Run the script every minute.","correct":false},{"id":"c66bf36d77a2aa4cf5ef75a4ac494df8","text":"Store all custom configuration scripts in an S3 bucket and create a new launch configuration. In its user data section, download the scripts from the S3 bucket and execute them. Whenever a new instance is launched, the configurations can be installed automatically. ","correct":false},{"id":"8fad414c494de200ee3990b334d22b13","text":"Update the CloudFormation script and modify the health check type from ELB to EC2.","correct":true},{"id":"1584209c598c68a956227b3770a97fb2","text":"Create an AMI and configure a new launch configuration with the AMI. Then modify the Auto Scaling group to use the new launch configuration and launch a new instance.","correct":false}]},{"id":"12556935-2b08-4f73-b7f6-6b82bd7fa1a0","domain":"awscsapro-domain1","question":"Your company has recently acquired another business unit and is in the process of integrating it into the corporate structure.  Like your company, the acquisition's IT assets are fully hosted on AWS.  They have a mix of EC2 instances, RDS instances and Lambda-based applications but these will become end-of-life as the new business unit transitions to your company's standard applications over the next year.  Fortunately, the CIDR blocks of the respective VPCs do not overlap.  If the goal is to integrate the new network into your current hub-and-spoke network architecture to provide full access to each other's resource, what can you do that will require the least amount of disruption and management?","explanation":"VPC Peering provides a way to connect VPCs so they can communicate with each other.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/peering/invalid-peering-configurations.html","title":"Unsupported VPC Peering Configurations - Amazon Virtual Private Cloud"}],"answers":[{"id":"d8d7beaa505f84c757a93d64f70a0ae7","text":"Initiate a VPC peering request from each of your spoke VPCs to each new VPC.  Configure route tables in each spoke VPC and new VPC to route traffic to the respective VPC.","correct":false},{"id":"0d3332dc947d3085b33697562a70c9f5","text":"Initiate a VPC peering request between your hub VPC and all VPCs from the new business.  Setup routes in the hub to direct traffic to and from the new VPCs.","correct":true},{"id":"b4cd9cd07c83868a626224de0d92c70a","text":"Initiate a Transitive Peering request from each new VPC to your hub VPC.  Configure routes in the hub to direct traffic to and from the new VPCs.","correct":false},{"id":"8e438cf633d07a0d70f2d21978e083bb","text":"Configure a VPC Gateway Endpoint for EC2, RDS and Lambda services.  Configure route tables in your existing VPCs to use the endpoints to communicate with the new VPCs.","correct":false},{"id":"6088d94d774b3a3e9be7fc71331181d1","text":"setup a VPN connection via Internet Gateway between each new and existing VPC to create a mesh network.  Update route tables to direct traffic to the appropriate VPC.","correct":false}]},{"id":"e720cd54-de67-42de-ba10-593dee0582e6","domain":"awscsapro-domain3","question":"You are the Enterprise Architect in a Risk Quantification firm. The firm has a website which end-users can use to apply for loans and also track the status of their loan application if they log in. When a loan application comes in, several downstream systems need to independently process the application. Right now, the website server-side code invokes these systems one after the other, synchronously, in a tight loop. If one of these downstream systems times out or throws an exception, the entire loan application processing errors out. Even if none of these downstream systems fail, the time it takes to process a loan application is very high due to the serial nature of these systems being invoked. Your CTO wants only the loan-processing application moved to the AWS cloud and re-architected at the same time.\nThe downstream systems are all hosted on-premises and will continue to remain on-premises. They expose REST endpoints that accept POST HTTPS requests, use self-signed certificates and respond synchronously only when they are done processing an application. After re-architecture, all downstream systems must independently start processing an incoming loan application simultaneously.\nYour CTO wants to know how the loan-processing website application can be architected in the AWS Cloud, and what supporting changes will be needed in the downstream systems on-premises. He wants to minimize code changes to the downstream on-premises systems. Choose the best option","explanation":"This is an example of a verbose question with verbose answer choices. You can expect a few such questions in the exam, testing your time management skills. Try to vertically scan the answers to see which parts differ between them. Sometimes, though the answers seem big, a large part of each is identical. You can ignore those parts, as there is nothing to choose between the.\nAmong the four choices, two use SQS and two use SNS to feed the incoming loan applications to the downstream systems. You cannot automatically eliminate either SQS or SNS, as a working solution can be designed with either.\nLet us see how we can achieve this using SNS first. The basic requirement here is fan-out - a single loan application must be processed by several downstream systems, so there are multiple consumers. Hence, SNS is a natural fit. SNS supports multiple subscribers for a topic. SNS also supports HTTP/HTTPS subscribers. SNS makes POST REST API call to as many HTTP/HTTPS subscribers exist on the topic, so it fits the bill. However, there is a small problem - the requirement states that the downstream systems must be changed as little as possible. If we follow this design, we must change the HTTP Listening part of the downstream systems significantly. Because SNS is directly calling them now, SNS will use its own headers and body format. In fact, SNS POST-s two kinds of messages - one is Subscription Confirmation and one is Notification. A special HTTP header (x-amz-sns-message-type) has the right type in its value. The server side now must parse this header out and look for only the Notification type of message. The body itself will then be JSON formatted with the payload. While the server is probably used to process just the core payload (loan application data) as the HTTP body, the same will now be hidden inside a JSON field called Message inside the request body. Additionally, the downstream systems will have to deal with SNS retries, thus the loan application part must be made idempotent (if the same loan application lands twice, it will ignore the duplicates). Thus, though it is technically possible to design the solution using SNS, it will result in a lot of changes in the downstream systems. Hence, though the SNS option will work, it is not the correct answer because of this reason.\nNow, let us see how we can design this using SQS. While SQS does not support fan-out (multiple consumers for the same message), the proposed solution uses a Lambda function to achieve fan-out. The Lambda function will pick up the message, and then call the downstream systems one by one. The key to making this work is, of course, to modify the downstream systems from synchronous monolithic beasts to asynchronous servers so that they can instantly respond to the Lambda function and then continue to process the application. We will then have to provide a callback for when it is done. The solution uses an API Gateway for that purpose. Overall, the solution is elegant, and changes to the downstream systems are less than what SNS requires. Hence, SQS is the correct answer.\nNote that one version of the SNS design proposes to retain the synchronous nature of the downstream systems. That will not work as SNS will not wait more than 15 seconds for a response. The response will then be lost and the main website app will never know the results from the downstream systems.\nAlso, note that though SNS requires the HTTPS subscriber to present a trusted CA-signed certificate, there is no such requirement for Lambda because Lambda is basically your code, you can decide to trust anyone.","links":[{"url":"https://docs.aws.amazon.com/sns/latest/dg/sns-http-https-endpoint-as-subscriber.html","title":"Using Amazon SNS for System-to-System Messaging with an HTTP/S Endpoint as a Subscriber"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","title":"Using AWS Lambda with Amazon SQS"}],"answers":[{"id":"48d42d3290142cbfc8207e042690b35f","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting so that SNS does not retry, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (c) Make them idempotent for the same loan application as SNS may retry in case of lost messages or timeouts (d) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as SNS will not be able to POST to a server with self-signed certificate","correct":false},{"id":"3fb725a8e71fa96168f18e50a146b4f0","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed and (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems","correct":true},{"id":"3d032e65493c0733ebe65683fb66a562","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems and (c) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as your Lambda function will not be able to POST to a server with self-signed certificate","correct":false},{"id":"eec2740374df8038093d636a17252168","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Override the default delivery policy on the subscriber endpoint to remove retries so that downstream systems do not have to worry about synchronous responses taking time or idempotency of retries. Make the following changes in the downstream systems - (a) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (b) Procure server certificates from a trusted Certificate Authority (CA) instead of using the self-signed certificate as SNS will not be able to POST to a server with a self-signed certificate","correct":false}]},{"id":"c9f44641-660b-4c42-9380-9e7f6b0a9ba4","domain":"awscsapro-domain4","question":"As the solution architect, you are assisting your customer design and develop a mobile application using API Gateway, Lambda and DynamoDB. S3 buckets are being used to serve static content. The API created using API Gateway is protected by WAF. The development team has just staged all components to the QA environment. They are using a load testing tool to generate short bursts of a high number of concurrent requests sent to the API Gateway method. During the load testing, some requests are failing with a response of 504 Endpoint Request Timed-out Exception.\nWhat is one possible reason for this error response from API Gateway endpoint?","explanation":"The SA-P exam sometimes focuses on knowledge of response codes from API Gateway and what each distinct HTTP response code could mean.\nThe key to answering this question correctly is being able to distinguish between 4XX and 5XX HTTP error response codes. Though AWS has not been entirely consistent in their error code assignment philosophy, 4XX usually happens any time throttling kicks in because the request in that case never makes to an instance of Lambda function. 5XX happens when a Lambda function is actually instantiated, but some error (like time out) happened inside the Lambda function. One sneaky way to remember this is the fact that 5XX errors are called server errors in HTTP-land, so to generate a 5XX a server process must exist (and must have failed). Of course, in this context, the HTTP server process is a Lambda function - so in scenarios where throttling prevented a Lambda function from getting spawned, the response code cannot be 5XX. This is not consistently followed by AWS API Gateway error design, though, as we can see that AUTHORIZER_CONFIGURATION_ERROR and AUTHORIZER_FAILURE are both 500, though no Lambda function is actually spawned in either case. However, the candidate must remember that throttling always results in 4XX codes. An Endpoint Request Timed-out Exception (504) suggests that the requests in question actually made its way past the API Gateway into a Lambda function instance.\nFor the scenario where request rate exceeds API Gateway limits, the request would be blocked by API Gateway itself. The response would be 429. The exact knowledge of the code 429, however, is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe scenario where 1000 Lambda functions are already running is a similar example of throttling - the 1001st Lambda function will not even be spawned. The response, again, will be 429. However, the exact knowledge of the code 429 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe WAF scenario is yet another example of the request not even crossing the protections placed at the gateway level. If WAF is activated on API Gateway, it will block requests when the rate exceeds the HTTP flood rate-based rule (provided all such requests come from a single client IP address). However, the response, again, will be in the 4XX area (specifically, 403 Forbidden) - however, the exact knowledge of the code 403 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThis leaves Lambda time-out as the only correct answer. The mention of 30 seconds or more is a diversion tactic, in case candidate believes that the relevant Lambda time-out is 5 minutes. A given Lambda function instance may have a time-out limit of 5 minutes, but when it is invoked from API Gateway, the timeout imposed by API Gateway is 29 seconds. If a Lambda function runs for longer than 29 seconds, API Gateway will stop waiting for it and return 504 Endpoint Request Timed-out Exception.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html","title":"Amazon API Gateway Limits and Important Notes"},{"url":"https://aws.amazon.com/blogs/compute/amazon-api-gateway-adds-support-for-aws-waf/","title":"Amazon API Gateway adds support for AWS WAF"}],"answers":[{"id":"98daaa701198f7e1c541fe8051799129","text":"The Lambda function is sometimes taking 30 seconds or more to finish executing","correct":true},{"id":"82f6a6eb1a4aeef5dd34f21fcd2069ef","text":"The number of requests generated by the load testing framework has exceeded the threshold for the HTTP flood rate-based rule set in the WAF settings for the stage in question","correct":false},{"id":"8090c6b0fe9036ec84fce24b16a7dc10","text":"The load testing tool has exceeded the soft limit for request rate allowed by API Gateway","correct":false},{"id":"370770015087b4ff70656089fc9e3316","text":"The test is triggering too many Lambda functions concurrently. AWS imposes a soft limit of 1000 concurrent Lambda functions per region","correct":false}]},{"id":"599dee9a-6ae7-4c85-a7c6-49edc6ae7d6b","domain":"awscsapro-domain5","question":"A development team is comprised of 20 different developers working remotely around the globe all in different timezones.  They are currently practicing Continuous Delivery and desperately want to mature to true Continuous Deployment.  Given a very large codebase and distributed nature of the team, enforcing consistent coding standards has become the top priority.  Which of the following would be the most effective to address this problem and get them closer to Continuous Deployment?","explanation":"Including an automated style check prior to the build can move them closer to a fully automated Continuous Deployment process.  A style check only before UI testing is too far in the SDLC.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"1f40591b9d9dbe7a2371e5e82ec05997","text":"Introduce a peer review step into their deployment pipeline during the daily stand-up, requiring sign off for each commit.","correct":false},{"id":"f4cd7f15eb32d8ddd77234b38d0b35b8","text":"Incorporate a code style check right before user interface testing to ensure standards are being followed.","correct":false},{"id":"ec61b60c7eeb3bf9ca9c4149c09c5f3d","text":"Issue a department directive that standards must be followed and require the developers to sign the document.","correct":false},{"id":"e60e97c4fb6cbf6c1dcf3e806624762f","text":"Require all developers to use the Pair Programming feature of Cloud9.  The commits must be signed by both developers before merging.","correct":false},{"id":"db4ecdbd1c7c8fda5d3e0792a15411ab","text":"Include code style check in the build stage of the deployment pipeline using a linting tool.  ","correct":true},{"id":"628453003287afe2200912bb38d0456b","text":"After integrating and load testing, run a code compliance check against the binary created during the build.","correct":false}]},{"id":"2c034786-9b7e-4933-aad2-d0c4b1d89ca8","domain":"awscsapro-domain2","question":"A beach apparel company has begun an initiative to improve their sales analytics capabilities using AWS services. They'll need to be able to visualize summary sales data by product line, territory, and sales channel for each day, month, and year, and they'll need to be able to drill-down with ad-hoc queries on individual sales records. There are multiple data sources that provide transactional information in different formats. The company has chosen Amazon QuickSight as their visualization tool for the summary information. Visualizations and drill-down queries will require three years of rolling sales history, which estimates to seven petabytes of data. Which architecture will provide the best performance and cost efficiency?","explanation":"Using S3 to store the detailed sales transaction data and using Lambda to standardize data formats is the most cost effective option. Storing the summary data in Redshift provides a high performance option for reads from QuickSight, and keeping the detailed transaction data out of Redshift allows for smaller node sizes and lower cost. Amazon Redshift Spectrum can be used for drill-down queries that join tables from both Redshift and S3. For answer number two, Redshift will be a better option than Aurora for OLAP query performance due to it's columnar organization. Answer number four provides no simple way to perform ad-hoc drill down queries.","links":[{"url":"https://aws.amazon.com/redshift/","title":"Amazon Redshift"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"a08e38f138d23c0f1759ab2d1801f67e","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Firehose and load them into Amazon Redshift. Run AWS Glue jobs to format the transaction data in a standard way and perform aggregate functions to write the data into summary tables in Redshift","correct":false},{"id":"4740b70569f15040edf0916e47386757","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Streams and write them to Amazon Elastic Block Store on EC2 instances in Auto Scaling Groups. Perform data format standardization and summary aggregation on EC2, and write the summary results to Amazon Redshift tables","correct":false},{"id":"ebf4a6c248510d05a046c8f0ea4298b7","text":"Use Amazon Kinesis Data Analytics to format the data source transactions in a standard way and load it into Amazon Aurora. Invoke Lambda functions to aggregate the data and write it into summary tables in Aurora","correct":false},{"id":"b3deac265c2195cb988c345d096800fd","text":"Ingest individual sales transactions from each data source into Amazon S3 with Amazon Kineses Data Firehose. Trigger an AWS Lambda function to format the transaction data in a standard way and redeposit the results in S3. Run AWS Glue jobs to aggregate the summary data into Amazon Redshift","correct":true}]},{"id":"b303f8e0-2c68-44aa-93bb-45b987b17d95","domain":"awscsapro-domain3","question":"You are helping a client build some internal training documentation to serve as architectural guidelines for their in-house Solutions Architects.  You suggest creating something inspired by the AWS Well-Architected Framework.  The client agrees and wants you to come up with some examples of each pillar.  Which of the following are examples of the Reliability pillar?","explanation":"The Reliability pillar includes five design principles:  Test recovery procedures, Automatically recovering from failure, Scaling horizontally to increase aggregate system availability, Manage change in automation and Stop guessing capacity.  By being able to closely monitor resource utilization, we can increase the Reliability and efficiency to right-size capacity.","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"3eb456756ecb767ab18179d87ec49a6b","text":"We can drive improvement through lessons learned from all operational events and failures. Share what is learned across teams and through the entire organization.","correct":false},{"id":"c257f1eec7a5a7f1eae9338f4de45cb0","text":"On AWS, we'll be able to monitor demand and system utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over or under-provisioning.  We can stop guessing on capacity needs.","correct":true},{"id":"c57c8249b3885f2078d8dac40d695dec","text":"We can design workloads to allow components to be updated regularly, making changes in small increments that can be reversed if they fail.","correct":false},{"id":"e82a19c63c4c9fedccc997eece7eccdc","text":"With virtual and automatable resources, we can quickly carry out comparative testing using different types of instances, storage, or configurations.  This will allow us to experiment more often.","correct":false}]},{"id":"4eb19466-6d1a-4ccd-987c-3f8f0cc71479","domain":"awscsapro-domain1","question":"A client wants help setting up a way to manage access to the AWS Console and various services on AWS for their employees.  They are starting out small but expect to provide AWS-hosted services to their 20,000 employees within the year.  They currently have Active Directory on-premises, use VMware to host their VMs.  They want something that will allow for minimal administrative overhead and something that could scale out to work for their 20,000 employees when they have more services on AWS.  Due to audit requirements, they need to ensure that the solution can centrally log sign-in activity.  Which option is best for them?","explanation":"For userbases more than 5,000 and if they want to establish a trust relationship with on-prem directories, AWS recommends using AWS Directory Service for Microsoft Active Directory.  This is also compatible with AWS Single Sign-On which provides a simple way to provide SSO for your users across AWS Organizations.  Additionally, you can monitor and audit sign-in activity centrally using CloudTrail. ","links":[{"url":"https://aws.amazon.com/single-sign-on/faqs/","title":"AWS Single Sign-On FAQs – Amazon Web Services (AWS)"},{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ad_connector_best_practices.html","title":"Best Practices for AD Connector - AWS Directory Service"}],"answers":[{"id":"a3c1cc18aa02ff66c06476aa7b8e78ed","text":"Download and install the AWS ActiveDirectory Sync appliance and install it in vCenter.  Configure the Sync appliance to connect to the local AD and replicate to an instance of Simple AD on AWS.  In IAM, create corresponding roles and policies for the permissions you want to allow on AWS.  Assign these roles to the synchronized Simple AD users in IAM.","correct":false},{"id":"32479d3559d6ead0328c6419249c7859","text":"Connect the multiple accounts together using AWS Organizations.  Deploy AD Connector on AWS and configure their on-prem AD.  Create corresponding roles and groups in IAM and map those to their local AD groups.  Use STS to allow users to authenticate into AWS.","correct":false},{"id":"b1b70c7c05b814cf9685b3a64175cd80","text":"Connect the multiple accounts with AWS Organizations.  Deploy AWS Directory Service for Microsoft Active Directory on AWS and configure a trust with your on-premises AD.  Configure AWS Single Sign-On with the users and groups who are permitted to log into AWS.  Give the users the URL to the AWS SSO sign-in web page.","correct":true},{"id":"878c950f9d14c2d81185f1950edda98c","text":"Configure Cognito with web federation against the on-prem Active Directory.  In IAM, create corresponding users corresponding to the Cognito accounts you want to allow on AWS.  Assign these roles to the user pools within Cognito.  Distribute the Cognito SSO client to your users.","correct":false},{"id":"1eb0420847890f99ddd38092ac88c5f0","text":"Create a OAuth Identity Provider in IAM and create roles and policies with the appropriate level of permissions.  In AD, create groups which correspond to the roles you have created in IAM and populate the AD groups with the desired users.  Download and install the OAuth Identity Connector for AD.  Configure the connector for the OAuth Identity Provider on AWS.  ","correct":false}]},{"id":"f7e5105f-7ae9-4c04-b1e7-2d6165db236c","domain":"awscsapro-domain2","question":"You work for a specialty retail organization. They are building out their AWS VPC for running a few applications. They store sensitive customer information in two different encrypted S3 buckets. The applications running in the VPC access, store and process sensitive customer information by reading from and writing to both the S3 buckets. The company is also using a hybrid approach and has several workloads running on-premises. The on-premises datacenter is connected to their AWS VPC using Direct Connect.\nYou have proposed that an S3 VPC Endpoint be created to access the two S3 buckets from the VPC so that sensitive customer data is not exposed to the internet.\nSelect two correct statements from the following that relate to designing this solution using VPC Endpoint.","explanation":"S3 VPC Endpoint is a common topic tested in the SA-P Exam, as it enables S3 access over a private network, which is a common security requirement in many organizations. It is also a cost-effective way to establish outbound connection to S3, as the alternative is to use NAT Gateways, which are charged by the hour even if there is no traffic using them.\nOn vertical scanning of the answer choices, it should be obvious that one of the two closely worded choices is correct, and one of the other two choices is correct as well. That is because if there are 2 or 3 or 4 closely worded choices, only one (or in some rare cases, two) is correct - this is a common pattern in the SA-P test.\nFor the closely worded pair - the bucket policy of an S3 bucket will always specify who can or cannot access the bucket. It will not dictate how a VPC Endpoint behaves. Hence, the choice that suggests that a bucket policy can control a VPC Endpoint is incorrect.\nBetween the other two choices, remember that a VPC Endpoint can connect to any number of S3 buckets by default. One Endpoint for each bucket is simply not scalable, and should stand out as incorrect.\nThe remaining choice is correct because the S3 VPC Endpoint is of type Gateway Endpoint as opposed to Interface Endpoint, and a subnet needs Routes in the Routing Table for sources in the subnet to be able to connect to it. Read the links provided to understand the differences","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies-vpc-endpoint.html","title":"Example bucket policies including ones that discuss use of sourceVpce attribute"},{"url":"https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/","title":"How to set up VPC Endpoint for S3 access including Route Table for subnets accessing S3"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html","title":"VPC Endpoint documentation discussing differences between gateway endpoints and interface endpoints"}],"answers":[{"id":"96bd9d9981d98c1d98550618acedc673","text":"Bucket policies on the two S3 buckets can specify the id of each VPC Endpoint using AWS attribute sourceVpce to further restrict which S3 buckets can be accessed by the VPC Endpoint","correct":false},{"id":"19bc72413543fc85eb58b5edf43fb2db","text":"Each VPC Endpoint is a Gateway Endpoint that also requires correct routes in the Route Table associated with each subnet that wants to access the endpoint","correct":true},{"id":"28b96a455027a61e12e57f73ae7956fc","text":"You need two VPC Endpoints, one for each S3 bucket, as a single VPC Endpoint can only access a single S3 bucket","correct":false},{"id":"55b07e707272bf499484e3f28a8fac88","text":"Bucket policies on the two S3 buckets can specify the id of each VPC Endpoint using AWS attribute sourceVpce to further restrict which VPC Endpoints can access each bucket","correct":true}]},{"id":"3ca7c5e0-432a-4d40-afee-ab996819b429","domain":"awscsapro-domain3","question":"You are consulting with a client to guide them on migration of an in-house data center to AWS.  The client has stipulated in the contract that the migration cannot require any more than 1 hour downtime at a time and that there is always a fallback path.  Additionally, they want an overall increase in business continuity capabilities when the migration is done.  Their landscape is as follows:  (1) Several databases with about 1TB of data combined which are heavily used 24x7 and considered mission critical; (2) About 40TB of historic files which are read sometimes but almost never updated; (3) About 150 web servers on VMware in various states of customization of which there is a current project underway to standardize them.  The client's team has suggested some next steps but because they aren't yet familiar with AWS, they are not using equivalent AWS terms.  Translating their suggestions, which of the following activities would you choose to meet the requirements, reducing costs and management where possible?","explanation":"The database migration suggestion aligns well with DMS as it can keep the databases in sync until cutover.  SAN replication sounds a lot like Storage Gateway which is a reasonable way to migrate data to AWS.  However, simply using K8s does not convert your VMs into containers or make them serverless.  We can't restore tapes to AWS.  Creating the same VM landscape on AWS just adds an additional layer of complexity that's not needed.","links":[{"url":"https://aws.amazon.com/dms/faqs/","title":"AWS Database Migration Service FAQs - Amazon Web Services"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"}],"answers":[{"id":"801ce55cfc2a125e7d17c729ca3e2e93","text":"Create new high powered stand-alone database instances in AWS and migrate data from on-prem database.  Use log shipping to keep the databases in sync.  Once we better understand AWS, we'll rebuild the servers and repartition the tables. ","correct":true},{"id":"3a02ebcd33fe18255e4ce43e8babb730","text":"Over several months, at end of business on Friday, backup all the servers and data to tape and restore to new instances in AWS to prove out AWS capabilities and reliability.","correct":false},{"id":"d2dde578790a34d9e740015474ea23e4","text":"Migrate the majority of the 150 web servers to a serverless concept by moving the VMs to a Kubernetes cluster.","correct":false},{"id":"75d528d2ec243c60d1478ae605c89f40","text":"Build a matching VMware environment on AWS and use third-party tools to backup and restore the VMs there.","correct":false},{"id":"31ea0eccdcea45ea4fce3b9459de52d4","text":"Use some block-level SAN replication tool to gradually migrate the on-prem historic files to AWS.","correct":true}]},{"id":"74aec97e-c092-4588-8da4-43dca3ddd0eb","domain":"awscsapro-domain5","question":"You are trying to help a customer figure out a puzzling issue they recently experienced during a Disaster Recovery Drill.  They wanted to test the failover capability of their Multi-AZ RDS instance.  They initiated a reboot with failover for the instance and expected only a short outage while the standby replica was promoted and the DNS path was updated.  Unfortunately after the failover, they could not reach the database from their on-prem network despite the database being in an \"Available\" state.  Only when they initiated a second reboot with failover were they again able to access the database.  What is the most likely cause for this?","explanation":"The routes for all subnets in an RDS subnet group for a Multi-AZ deployment should be the same to ensure all master and stand-by units can be reached in the event of a failover.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/avoid-route-table-issues-rds/","title":"Avoid route table issues RDS Multi-AZ fail over"}],"answers":[{"id":"1dbb40d7bdb88fbe54fead30b1bf5f12","text":"There was a lag in the state update on the AWS console showing \"Available\".  If they would have waited longer, it likely would have changed to \"Degraded\".  A failover can take 30 minutes or more because AWS automatically creates a snapshot before promoting the standby.","correct":false},{"id":"1105b24fe7a3c1eb0a86d0fa9a3cdc62","text":"This was most likely an AWS error.  They should submit a support ticket with the RDS instance identifier and the approximate time of the failover test via the Support Center.","correct":false},{"id":"260ae29233a6047971894eff9d05fa55","text":"They initiated a failover with an IAM account that did not have sufficient rights to perform the reboot.  This resulted in an incomplete failover that was only corrected by executing the failover again to reset the DNS entries.","correct":false},{"id":"f637d9886a1ea7b52d608046e233e504","text":"The subnets in the subnet group did not have the same routing rules.  The standby subnet did not have a valid route back to the on-prem network so the database could not be reached despite being available.","correct":true},{"id":"46b9b5188d94533190c30de816355165","text":"They used the AWS Console to issue the reboot.  You can only force a failover of RDS by using the AWS CLI and adding the --force-failover parameter to the \"aws rds reboot-db-instance\" command.","correct":false}]},{"id":"c1333471-d052-4710-bcdb-facadc095d70","domain":"awscsapro-domain5","question":"You are setting up a corporate newswire service for a global news company.  The service consists of a REST API deployed on EC2 instances where customers can retrieve the latest news articles in real-time that happen to contain their company name.  This allows companies to monitor all news sources for stories where they are mentioned.  Because of the worldwide reach of the new site, you want to position servers around the globe.  You want to publish one subdomain name globally (api.domain.com) and have the requesters directed to the nearest region based on latency.  In each region, you want to be able to accommodate blue-green deployments without downtime as well.  What steps do you take?","explanation":"We want to use weighted routing records for local instances so we have the ability to adjust weights and shift traffic during blue-green deployments.  Latency-based routing would take care of funneling requests to the site with the lowest latency.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html","title":"How Health Checks Work in Complex Amazon Route 53 Configurations - Amazon  Route 53"}],"answers":[{"id":"6e56101de6ed2ca97550d5025ddf559a","text":"Using Route 53, we first create the top-level api.domain.com with a geolocation policy.  We then create latency-based routing records for the instances in each region (us-east-2.api.domain.com).  Next, we configure the countries closest to each region in the geolocation policy to direct them to the regional records.","correct":false},{"id":"b850f1c18972d022213271c5d673e07f","text":"First setup weighted routing records for the local instances in the region in Route 53.  Assign equal weights with all sharing the same regional subdomain name (us-east-2.api.domain.com).  Next, create latency alias records by creating multiple entries for api.domain.com--each pointing to the regional subdomains.","correct":true},{"id":"41e30b8e30cbd737ced85953d7e3e939","text":"Use CloudFormation to create a distribution of the website.  Create an alias record for the subdomain (api.domain.com) in Route 53 and assign it to the CloudFront distribution.  To ensure no lag in news retrieval, set the maximum TTL on the CloudFront distribution to 0.","correct":false},{"id":"a60aaaf971cbd546c1ec57e08ea38274","text":"We would first create geo-spatial records for the local resources in each region (us-east-2.api.domain.com) and assign equal weights.  Next, we create latency-based routing records for the top level subdomain (api.domain.com) and direct those to the regional records as an alias.  We must also disable Health Check on the latency record to ensure the localized Health Check is used.","correct":false}]},{"id":"5d35c6d3-3eaf-49d0-b64e-611d74d40af0","domain":"awscsapro-domain2","question":"You need to design a new CloudFormation template for several security groups. The security groups are required in different environments such as QA, Dev and Production. The allowed CIDR range in the security group ingress rule depends on environments. For example, the allowed inbound address range is 10.0.0.0/16 for non-production and 10.1.0.0/16 for production. You prefer to maintain a single template for all the environments. What is the best method for you to choose?","explanation":"CloudFormation has an optional Conditions section that contains statements to determine whether or not entities should be created or configured. Then you can use the \"Fn::If\" function to check the condition and return different values. You do not need to use Jenkins to pre-process the template and there are no \"Fn::Case\" and \"Fn::Switch\" intrinsic functions.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html","title":"CloudFormation conditions"}],"answers":[{"id":"d273c3216496e7d41cb97094f83d3e33","text":"Create an environment variable and pass it to the CloudFormation template. In the security group resources, use Fn::Case and Fn::Switch to check the variable value and return different CIDR IP range.","correct":false},{"id":"d2661ed9bcd0f8b132885660604a665a","text":"Create a Jenkins pipeline with an environment variable. Depending on the variable value, modify the template script to use the correct CIDR IP range. Deploy the CloudFormation stack with the updated template.","correct":false},{"id":"7afa8c2f0fa2ef8966917c74044516dd","text":"CloudFormation does not provide functions for conditions. Use Terraform instead. Create a variable file to manage different CIDR IP ranges. Pass in the environment variable value and use \"Terraform apply\" to deploy all the security group resources at one time.","correct":false},{"id":"c4a393b1cce09df81069eb93f2d38054","text":"Use the environment type as an input parameter and create a condition based on the parameter. In the AWS::EC2::SecurityGroupIngress resource, use Fn::If to check the condition and return related source CIDR range.","correct":true}]},{"id":"b77449a6-88a8-4ac9-ae75-664623acccd3","domain":"awscsapro-domain4","question":"Which of the following activities will have the most cost impact (increase or decrease) on your AWS bill?","explanation":"Provisioning an EIP to a running instance or using Placement Groups or App Mesh all do not cost anything.  OpsWorks Stacks on EC2 does not cost anything but using it for on-prem systems does cost a small amount.  The only thing on this list that would increase your AWS bill is adding a Route 53 hosted zone.","links":[{"url":"https://aws.amazon.com/route53/pricing/","title":"AWS | Amazon Route 53 | Pricing"}],"answers":[{"id":"a1597dd35c1fb3560a6831b69b72bd26","text":"Add a new Route 53 hosted zone.","correct":true},{"id":"f6e98aff4272a541442980d4782b0524","text":"Start using AWS App Mesh to improve the stability of your existing service landscape.","correct":false},{"id":"f80a8cefb8a1936309b0092f00c241b2","text":"Begin using AWS OpsWorks Stacks on EC2 to manage your landscape","correct":false},{"id":"fb2023a84d6c95591c6f756c641eae0d","text":"Deploy existing reserved instances into a Placement Group.","correct":false},{"id":"568c5ec8d576ba0532c9e440440a3be9","text":"Provision an Elastic IP and associate it to a running instance.","correct":false}]},{"id":"91e4ebb5-18ac-45d6-867e-4c2eddefc075","domain":"awscsapro-domain4","question":"Your company has come under some hard times resulting in downsizing and cuts in operating budgets.  You have been asked to create a process that will increase expense awareness and your enhance your team's ability to contain costs.  Given the reduction in staff, any sort of manual analysis would not be popular so you need to leverage the AWS platform itself for automation.  What is the best design for this objective?","explanation":"AWS Budgets is specifically designed for creating awareness and transparency in your AWS spending rate and trends.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing Your Costs with Budgets - AWS Billing and Cost Management"}],"answers":[{"id":"aadb36f869250c141ae2eeec088d3bb3","text":"Implement a Cost Allocation Tagging strategy.  Create a periodic aggregation process in AWS Glue to read costs based on the tags.  Configure Glue to send the report to SNS where it can be either emailed or texted to the team depending on their own preference.","correct":false},{"id":"8e9e7154ed9b1f92015c5fde5ca7a2a3","text":"Use Kinesis Streams to Ingest CloudTrail log streams.  Create a Lambda function to parse the log stream and insert a record into DynamoDB whenever a pay-per-use activity happens.  Use DynamoDB streams to alert the team when a threshold has been exceeded.","correct":false},{"id":"ce7a1fa4194d00b76a1491e66b2a10ec","text":"Use AWS Budgets to create a budget.  Choose to be notified when monthly costs are forecasted to exceed your updated monthly target.","correct":true},{"id":"be1b52e799fe44ef89f78b167401c067","text":"Export AWS bill data into Redshift.  Use Quicksight to create reports per cost center.  Provide access for users to QuickSight to monitor their usage.","correct":false},{"id":"0e5c5be553400912c2517757d9aba6b2","text":"Provide access to Cost Explorer for your team for transparency.  Allow them to periodically review the accrued costs for the month and take the appropriate action.","correct":false}]},{"id":"3440b6ff-6fe9-495d-a765-f69f6b82a628","domain":"awscsapro-domain3","question":"You work for a health record management company which operates a view-only portal for end-users to check their health records online. Users can also raise disputes if anything is incorrect. This 2-tier website that supports only HTTPS will be moved to the AWS Cloud. There are 5 web servers on-premises and an F5 Load Balancer that controls traffic to these web servers. SSL is terminated at the web servers. An Oracle Real Application Cluster (RAC) serves as the database tier. Due to the sensitive nature of personal health information, the web site uses mutual authentication - the server requests browsers for a valid client certificate before establishing a trusted session.\nSelect two alternate working architectures for this application to be migrated to AWS so that code changes are minimized. Choose two responses each of which can act as an independent and functional solution, and also result in minimum application code changes.","explanation":"The areas tested by this question are as follows.\nFirst, if an ELB is not terminating SSL, its listener protocol cannot be HTTPS - it has to be TCP. Additionally, AWS ELB does not support terminating client-side certificates. Therefore, if a website requires client SSL certificates, and if it also uses AWS ELB, the ELB must let the target EC2 instances to terminate SSL and validate the client certificate. This requires the protocol to be TCP/443. Both these facts (one, SSL is not terminated at the Load Balancer level, and two, mutual authentication is used) are stated explicitly in the question so that the candidate identifies at least one of them and is thus able to conclude that HTTPS is not the correct protocol choice for ELB. Hence, amongst the two choices that use ELB, the one that says TCP is the correct one.\nSecondly, RDS does not support Oracle RAC. Hence, the database tier must use EC2 instances. Thus, for the second alternate solution that uses Route 53 multi-value answer records instead of ELB, we should select the option that deploys EC2 instances for the database tier.","links":[{"url":"https://forums.aws.amazon.com/thread.jspa?threadID=109180","title":"Discussion Forums - HTTPS Client certificate validation while using client ELB"},{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"RDS FAQ-s, search for phrase - Is Oracle RAC supported on Amazon RDS"}],"answers":[{"id":"11a4ad1f948d7b2fd3631cb763fa8a00","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring TCP/443 as listener","correct":true},{"id":"307f49d40547367c203a0fcfa1a46be8","text":"Migrate the database to RDS Oracle and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":false},{"id":"e7a1d88af1880a82062437a4c1005adf","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":true},{"id":"7250b61db9c9e69abf4f9e7bd2bfb268","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring HTTPS/443 as listener","correct":false}]},{"id":"a7c939f1-277e-469f-a209-9b290e8136c9","domain":"awscsapro-domain5","question":"Your company has contracted with a third-party Security Consulting company to perform some risk assessments on existing AWS resources.  As part of a routine list of activities, they inform you that they will be launching a simulated attack on one of your EC2 instances.  After the Security Group performed all their activities, they issue their report.  In their report, they claim that they were successful at taking the EC2 instance offline because it stopped responding soon after the simulated attack began.  However, you're quite certain that machine did not go offline and have the logs prove it.  What might explain the Security company's experience?","explanation":"AWS Shield and other counter-measure technologies work to protect all AWS customers from DDoS attacks.  Unless AWS was aware of the test time and expected duration, its likely the traffic was blocked as suspicious.  AWS Firewall Manager is used to manage WAF ACLs and not dynamically blacklist IPs.  Similarly, VPC Flow Logs cannot automatically implement NACL changes as described here. Despite being a permitted service, traffic suspected of being malicious will still be blocked","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/penetration-testing/","title":"Submit a Penetration Testing Request"}],"answers":[{"id":"7d4826f179b8dc854c9cfb6e43678373","text":"The VPC Flow Logs record the spike in suspicious traffic and implement an update to the inbound NACL to block the remote IP address.","correct":false},{"id":"f3f963b71e307f8c28109631df115418","text":"The EC2 instance is using an ENI and the Security Company temporarily exceeded the throughput limit resulting in a throttling of their connection.","correct":false},{"id":"82a05cb45adab6d248655e827de16c6f","text":"AWS Firewall Manager is dynamically adding a blacklist entry for the Security Company's testing machine because it sees the traffic as a threat.","correct":false},{"id":"e3facacbe52b6423f9cf2e700d8e0b81","text":"The Security Company's traffic was seen as a threat and blocked dynamically by AWS.  AWS must grant permission before any penetration testing is done.","correct":true}]},{"id":"374fde7d-232a-4cfe-b5d6-7755d564c6ca","domain":"awscsapro-domain1","question":"You are consulting for a company that has decided to partially migrate some resources to AWS from their two data centers (DC1 and DC2).  Their first order of business is to design a robust, redundant and cost-effective network connection between their data centers and AWS.  They already have redundant links between DC1 and DC2.  Which of the following architectures provides the highest availability at the least cost?","explanation":"A common and cost effective way to provide a redundant link to AWS with Direct Connect is a VPN connection.  In the event that the Direct Connect path fails at DC1, your on-prem router can redirect traffic over the VPN at DC2 via the DC1-DC2 link.  Having dual Direct Connect links is definitely redundant but more expensive than a VPN.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/","title":"Multiple Data Center HA Network Connectivity – AWS Answers"}],"answers":[{"id":"425adb8435a7c170eef3698fa729f5ea","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure a default route in both DC1 and DC2 to route traffic to the local Direct Connect link.","correct":false},{"id":"77c9fb7459e1b19f6f655d63556016e8","text":"Configure a Direct Connect connection from DC1 to a Virtual Private Gateway on AWS.  Setup a VPN connection from DC2 to a Virtual Private Gateway on AWS.  Configure a dynamic route across DC1 and DC2 for both paths with a route priority favoring the Direct Connect path to AWS.","correct":true},{"id":"bcac2f1d84c5f367ede3342f0adda492","text":"Ensure that DC1 and DC2 have separate ISPs.  Setup VPN connections from DC1 and DC2 to a Virtual Private Gateway on AWS.  Create static routes at each DC to use the local VPN to AWS.  Use CloudTrail to monitor traffic on the Virtual Private Gateway and trigger a script to update the static route if one of the VPN connections goes down.","correct":false},{"id":"abd8b38f393b6a0d0b42f68dca5ec24d","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure BGP to dynamically route traffic across the nearest Direct Connect link.","correct":false}]},{"id":"dc82c397-347d-4f69-bb06-03822238c7a0","domain":"awscsapro-domain1","question":"You are consulting for a large multi-national company that is designing their AWS account structure.  The company policy says that they must maintain a centralized logging repository but localized security management.  For economic efficiency, they also require all sub-account charges to roll up under one invoice.  Which of the following solutions most efficiently addresses these requirements?","explanation":"Service Control Policies are an effective way to broadly restrict access to certain features of sub-accounts.  Use of a single separate logging account is an effective way to create a secure logging repository.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"871870fa49beedfb95106595e4a1c9f4","text":"Configure billing for each account to load into a consolidated RedShift instance.  Create a centralized security account and establish trust relationships between each sub-account.  Configure admin roles within IAM of each sub-account for local administrators.  Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  ","correct":false},{"id":"74a6c6df518100b16da3f16e870b5d5c","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Create localized IAM policies to restrict modification of CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create a centralized security account and establish trust relationships between each sub-account.","correct":false},{"id":"0c9b5a803a99a3d2ef53869b6857c0e0","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use ACLs to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.  Establish trust relationships between the Consolidated Billing account and all sub-accounts.","correct":false},{"id":"cbec34b5388f7f183659e82c20fb3abf","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use an SCP to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.","correct":true}]},{"id":"f37f4967-9ef0-4cec-b63f-15b52dc44ca2","domain":"awscsapro-domain2","question":"You are an AWS solutions architect in a company. A team is building up a new application using AWS resources including application load balancers. In order to capture detailed information about the requests to the load balancers, all application load balancers need to activate the access logs and save the log files in an S3 bucket. The access logs should be encrypted when they are stored in the S3 bucket for data protection. How would you enable the encryption for access logs?","explanation":"The encryption at rest should be configured in the S3 bucket rather than the ELB access logs. For access logs of application load balancers, only the server-side encryption with Amazon S3-managed encryption keys (SSE-S3) is supported. Users cannot store the access logs in an S3 bucket where encryption with SSE-KMS is configured. For ELB access logs, you cannot perform the client-side encryption before the files are transferred in the S3 bucket. As a summary, the server-side encryption with SSE-S3 is the correct method.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html","title":"Access Logs for Your Application Load Balancer"}],"answers":[{"id":"efeff66ea347b9c1586198badebdd3f1","text":"Generate a symmetric encryption key locally, upload the key to S3 and enable client-side encryption with Amazon S3 by modifying the default encryption in the bucket properties. Make sure the encryption key is stored safely.","correct":false},{"id":"01a3559ab89c1a90f64a2d125b0ca2d4","text":"Create a customer managed key (CMK) in AWS Key Management Service (KMS). Add the \"delivery.logs.amazonaws.com\" service as the key user. Enable server-side encryption with AWS KMS-Managed Keys (SSE-KMS) by selecting the CMK in the S3 bucket.","correct":false},{"id":"6050b8b3764542173c87d57af013f345","text":"Create a Lambda function using Python boto3 to enable access logging for the application load balancers and activate the encryption feature. Configure an S3 bucket policy to allow the \"delivery.logs.amazonaws.com\" service to put objects.","correct":false},{"id":"7105831f473423801649e7f1302abac6","text":"In the AWS S3 Management Console, enable server-side encryption with Amazon S3-Managed Keys (SSE-S3) by modifying the default encryption to be AES-256.","correct":true}]},{"id":"7b8f1346-92b4-4a87-86d2-c8e65935589b","domain":"awscsapro-domain2","question":"A non-profit organization is putting on a comedy night fundraiser. Tickets are sold online, and attendees are asked to upload a facial photo to check-in at the venue the night of the event. The organization would like to provide the most streamlined entry process possible. A non-profit representative will take a photo at the door to match the photo submitted by the attendee at the time of ticket purchase. Which architecture will provide the highest level of automation to meet their needs?","explanation":"Amazon Rekognition can extract facial features into a feature vector and store that vector in a face collection via the IndexFaces API. Storing the face_id of the vector in DynamoDB, along with the attendee's name, provides the capability to validate the ticket sale at the door. At the event, the picture that's taken can be sent to Rekognition's SearchFaceByImage API to look for a match in the face collection. When a match is found, the face_id can be used to retrieve the attendee's name from DynamoDB to validate the event check-in. AWS DeepLens doesn't provide a SearchFaceByImage API to look for matching images in S3. Having Lambda call the Rekognition CompareFaces API will work, but will require much more coding to compare all the images one-by-one rather than using the automated face collection functionality. Rekognition can not be configured as a target for a Kinesis stream.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition"},{"url":"https://aws.amazon.com/solutions/auto-check-in-app/?did=sl_card&trk=sl_card#","title":"Auto Check-In App"}],"answers":[{"id":"8f60e8fc429b98a1dc7a37e6c71ca61b","text":"Upload the attendee's facial photo to Amazon S3 when the ticket is purchased. Trigger a Lambda function, which calls Amazon Rekognition to create a feature vector and store it in a face collection. Store the attendee's name and vector id in Amazon DynamoDB. At the event, have a Python-based UI use a camera to take the attendee's picture and send it to Amazon API Gateway, which triggers another Lambda function. Have this Lambda function call Rekognition to search the face collection and return an id if a match is found. Retrieve the attendee's name from the DynamoDB.","correct":true},{"id":"2caca948e6ce72046a513b4b48b36d19","text":"Send the attendee's facial photo to Amazon S3 when the ticket is purchased. Trigger a Lambda function to store the attendee's name and photo S3 object key in Amazon DynamoDB. At the event, have an AWS DeepLens take the attendee's picture and look to match it with an image in S3 via the SearchFaceByImage API. If a match is found, trigger an AWS Lambda function use the S3 object key to retrieve the attendee's name from the DynamoDB.","correct":false},{"id":"9369a3a40d7028e2ff2c87f02c0339a9","text":"Write the attendee's facial photo to an Amazon Kinesis stream when the ticket is purchased. Configure Amazon Rekognition as the target of the stream to create a facial feature vector and store it in Amazon S3. At the event, have a Python-based UI use a camera to take the attendee's picture and send it to another Amazon Kinesis stream with Rekognition as the target. Have Rekognition compare the attendee's image with those in S3 for a match.","correct":false},{"id":"eea359cb33a2ae4f87ccbe358a59201d","text":"Store the attendee's facial photo in Amazon S3 when the ticket is purchased. Trigger a Lambda function to write the attendee's name and photo S3 object key to Amazon DynamoDB. At the event, have a Python-based UI use a camera to take the attendee's picture and send it to Amazon API Gateway, which triggers another Lambda function. Have this Lambda function store the image in S3, and call the Rekognition CompareFaces API to search S3 images for a match. Retrieve the matched attendee's name from the DynamoDB.","correct":false}]}]}}}}
