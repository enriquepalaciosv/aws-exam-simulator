{"data":{"createNewExamAttempt":{"attempt":{"id":"631fc97a-99e7-4efc-9f8b-56920a13bcae"},"exam":{"id":"a3210a29-3b4a-4c11-a093-3e2cb9bd7b7a","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"c1333471-d052-4710-bcdb-facadc095d70","domain":"awscsapro-domain5","question":"You are setting up a corporate newswire service for a global news company.  The service consists of a REST API deployed on EC2 instances where customers can retrieve the latest news articles in real-time that happen to contain their company name.  This allows companies to monitor all news sources for stories where they are mentioned.  Because of the worldwide reach of the new site, you want to position servers around the globe.  You want to publish one subdomain name globally (api.domain.com) and have the requesters directed to the nearest region based on latency.  In each region, you want to be able to accommodate blue-green deployments without downtime as well.  What steps do you take?","explanation":"We want to use weighted routing records for local instances so we have the ability to adjust weights and shift traffic during blue-green deployments.  Latency-based routing would take care of funneling requests to the site with the lowest latency.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html","title":"How Health Checks Work in Complex Amazon Route 53 Configurations - Amazon  Route 53"}],"answers":[{"id":"41e30b8e30cbd737ced85953d7e3e939","text":"Use CloudFormation to create a distribution of the website.  Create an alias record for the subdomain (api.domain.com) in Route 53 and assign it to the CloudFront distribution.  To ensure no lag in news retrieval, set the maximum TTL on the CloudFront distribution to 0.","correct":false},{"id":"a60aaaf971cbd546c1ec57e08ea38274","text":"We would first create geo-spatial records for the local resources in each region (us-east-2.api.domain.com) and assign equal weights.  Next, we create latency-based routing records for the top level subdomain (api.domain.com) and direct those to the regional records as an alias.  We must also disable Health Check on the latency record to ensure the localized Health Check is used.","correct":false},{"id":"6e56101de6ed2ca97550d5025ddf559a","text":"Using Route 53, we first create the top-level api.domain.com with a geolocation policy.  We then create latency-based routing records for the instances in each region (us-east-2.api.domain.com).  Next, we configure the countries closest to each region in the geolocation policy to direct them to the regional records.","correct":false},{"id":"b850f1c18972d022213271c5d673e07f","text":"First setup weighted routing records for the local instances in the region in Route 53.  Assign equal weights with all sharing the same regional subdomain name (us-east-2.api.domain.com).  Next, create latency alias records by creating multiple entries for api.domain.com--each pointing to the regional subdomains.","correct":true}]},{"id":"19591d08-60c8-494e-9c39-d69c6c3390f0","domain":"awscsapro-domain4","question":"Your company's AWS migration was not planned out very well across the enterprise.  As as result, different business units created their own accounts and managed their own resources.  Recently, an internal audit of costs show that there may be some room for improvement with regard to how reserved instances are being used throughout the enterprise.  What is the most efficient way to ensure that the reserved instance spend is being best used?","explanation":"The discounts for Reserved Instances can be shared across accounts that are linked with Consolidated Billing but Reserved Instance Sharing must be enabled.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-ri-consolidated-billing/","title":"EC2 Reserved Instance Consolidated Billing"}],"answers":[{"id":"34d8e4aeeb67918e8bd27daba0923994","text":"Use Cost Explorer reports to analyze coverage of reserved instances.  Where there are coverage gaps, purchase more reserved instance capacity for that account. Where there is excess, place those on the Reserved Instance Marketplace.","correct":false},{"id":"af8967525080f0a545bb6fbafc8a8f3c","text":"Setup Consolidated Billing for a single account and link all the other various accounts in the organization.  Ensure that Reserved Instance Sharing is turned on.  The discounts for specific reserved instances will automatically be applied to the consolidated invoice.","correct":true},{"id":"617beec9b1a6fd3ca1cec7764ddfd9fb","text":"Use AWS Organizations to organize accounts under one organizational unit.  Use AWS Budgets to analyze utilization of reserved instances.  Reassign those RIs to other accounts that are currently using On-Demand instances.","correct":false},{"id":"f58169ec8f86bed7c84e4510ed491661","text":"Load CloudTrail instance usage data into Redshift for analytics.  Use QuickSight to create some utilization reports on existing reserved instances.  Relocate on-demand instances into regions where reserved instances are underutilized.","correct":false}]},{"id":"722221be-beb9-4a2b-8ae1-dff52b80125c","domain":"awscsapro-domain3","question":"You work for a technology company with two leased data centres (one on the east coast and one on the west coast) and one owned on-premises data centre. Management has decided to move the two leased data centres to the AWS cloud - one to us-east-1 and the other to us-west-1. The on-premises data centre will still continue running workloads which are not ready to move to the cloud.\nThis on-premises data centre must be always connected to the VPC-s in us-east-1 and us-west-1 for (a) the continuous replication of several databases and (b) the need to access some data residing on the on-premises data centre from applications running in both the AWS regions. The peak bandwidth required for these connections is (a) 500 Mbps between us-east-1 and on-premises, and (b) 8 Gbps between us-west-1 and on-premises. The applications would still be able to function at lower bandwidth, but the experience will be poor, which is not desirable. Both these connections must be Highly Available with 99.999% uptime. The connectivity solution must be cost-effective as well.\nAs the AWS Architect, what connectivity solution would you propose, so that all Bandwidth, HA and cost-effectiveness requirements are met?","explanation":"We can eliminate the VPC Peering solution immediately, as VPC Peering is for connecting two VPC-s on AWS. VPC Peering cannot be used to connect an AWS VPC with an on-premises network.\nOut of the remaining choices, the one that proposes connecting to us-east-1 using VPN and us-west-1 using Direct Connect comes very close to fulfilling all requirements. It suffers from two problems, however. One - the peak bandwidth requirement for us-east-1 is 500 Mbps. A VPN connection cannot be expected to provide 500 Mbps most of the time, as the true bandwidth someone can get from a VPN connection depends on a lot of factors including internet traffic it is sharing the route with. Secondly, if we are paying for a Direct Connect connection for the other region anyway, why not just use that one for this region too? Now, there is something called Direct Connect Gateways that makes it possible to share multiple AWS Regions using the same Direct Connect connection. The knowledge of Direct Connect Gateways is important for the AWS SA-P exam. Hence, this question tests this knowledge. The correct answer is the only one that uses Direct Connect Gateway.\nThe other choice that uses two separate Direct COnnect connections (one for each region) is not cost-effective, especially because since 2017, Direct Connect Gateways make it possible to connect to multiple AWS Regions using the same Direct Connect connection.\nRegarding HA, it is always a good practice to set up a VPN connection as a back-up for Direct Connect. The only requirement to do this is that the back-up VPN connection must also use the same Virtual Private Gateway on the AWS VPC side, otherwise traffic cannot fail over easily.\nNote about Direct Connect Gateways - they not only allow a customer to connect to two AWS Regions using a single Direct Connect connection, they also let the connected Regions communicate with each other! (This is why the VPC CIDR-s in us-east-1 and us-west-1 in the correct answer have to be non-overlapping.) There may be questions testing this aspect as well. Before Direct Connect Gateways existed, VPC Peering would be the only way for Inter-Region VPC Access. There is also another solution now - Transit Gateway, but this was announced late 2018. Usually, topics do not start appearing on the exam unless they have been more than 6 months in GA. Expect Transit Gateways to start appearing in questions now as well!","links":[{"url":"https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/","title":"AWS Direct Connect Gateway for Inter-Region VPC Access"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/","title":"Configuring VPN as back up of Direct Connect"},{"url":"https://aws.amazon.com/directconnect/sla/","title":"Direct Connect SLA puts uptime target at 99.9%. Therefore, if we need more than that, we should set up VPN as back up"}],"answers":[{"id":"3ea4a9645ac22c0b2c6c427764a7ae01","text":"Set up a Direct Connect Gateway. Associate the Virtual Private Gateways from both the us-east-1 and us-west-1 VPC-s with this Direct Connect Gateway. Then set up a single 10 Gbps Direct Connect connection between the on-premises data centre and the Direct Connect Gateway, using a Private Virtual Interface. Ensure that the VPC CIDR-s in the two AWS Regions are non-overlapping. To increase HA, set up separate back-up VPN connections between the on-premises data centre and each of the two AWS Regions","correct":true},{"id":"608dc6c035921ef09c640ebb70de9ddd","text":"Use VPC Peering to connect the on-premises network with both the us-east-1 and us-west-1 VPC-s independently. Bandwidth provided by VPC Peering is virtually unlimited, limited only by the instance sizes used. Also, VPC peering connections are fault-tolerant and scalable, so no back-up connectivity is needed","correct":false},{"id":"f0671c30ae58d58c0d64162640e69527","text":"Use two Direct Connect connections - an 1 Gbps one between the on-premises data centre and us-east-1, and a 10 Gbps one between the on-premises data centre and us-west-1. For each Direct Connect connection, set up a back-up VPN connection that must use the same Virtual Private Gateway as the Direct Connect circuit","correct":false},{"id":"85b6b49a01a28a90d71ef3c20ca9da8d","text":"Connect the on-premises data centre and us-east-1 using redundant site-to-site VPN connections as its bandwidth requirements do not require a costly Direct Connect connection. The redundant VPN connections must use different customer gateways and will provide an HA solution for that region. Connect the on-premises data centre with us-west-1 using a 10 Gbps Direct Connect circuit. Set up a back-up VPN connection for this region such that it uses the same Virtual Private Gateway as the Direct Connect circuit","correct":false}]},{"id":"af6501cb-5a69-4af7-a5e7-cf03d6ce09c1","domain":"awscsapro-domain1","question":"As an AWS Solutions Architect, you are responsible for the configuration of your company's Organization in AWS. In the Organization, the Root is connected with two Organizational Units (OUs) called Monitor_OU and Project_OU. Monitor_OU has AWS accounts to manage and monitor AWS services. Project_OU has another two OUs as its children named Team1_OU and Team2_OU. Both Team1_OU and Team2_OU have invited several AWS accounts as their members. To simplify management, all the AWS accounts under Team1_OU and Team2_OU were added with a common administrative IAM role which is supposed to be used by EC2 instances in their accounts. For security concerns, this role should not be deleted or modified by users in these AWS accounts. How would you implement this?","explanation":"The requirement of this scenario is that the IAM role should not be modified by IAM principals in Team1_OU and Team2_OU. The best place to implement this is in SCP as it provides central management. Since Team1_OU and Team2_OU can inherit the SCP policy from Project_OU, only Project_OU needs to attach the SCP that denies the action. In the meantime, the Root should have a default full access policy. It is improper to use IAM policies or Permissions Boundary to achieve this requirement as they may be easily changed at a later stage by other IAM users, and it is also complicated to implement if there is a large number of IAM principals.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"How SCP works in AWS Organization?"}],"answers":[{"id":"f868081ae3429a7ed8318636537ec834","text":"Make sure that Root node has a SCP policy that allows all actions. Create a SCP policy that restricts IAM principals from changing this particular IAM role. Attach the SCP policy to Project_OU.","correct":true},{"id":"e5acb8782282566185828b0ca39813fc","text":"Configure a new SCP policy to prevent IAM principals from deleting or modifying the IAM role. Attach the SCP policy to Project_OU, Team1_OU and Team2_OU. Configure a default full access SCP policy to Monitor_OU.","correct":false},{"id":"77abf7e62575051a27444694461acccd","text":"The nodes of Root, Project_OU and Monitor_OU in the Organization should allow all actions. For AWS accounts in Team1_OU and Team2_OU, attach an IAM policy that denies modifying the IAM role to IAM users, groups and roles.","correct":false},{"id":"46f9090f957b494e2e0075ceef9bac8b","text":"For the AWS accounts in Team1_OU and Team2_OU, configure a Permissions Boundary in IAM principals to prevent them from making modifications to this particular IAM role. As a result, all IAM principals will not have IAM policies to potentially change the role.","correct":false}]},{"id":"b401741c-5b37-4b47-8e61-7802fbc9d7d6","domain":"awscsapro-domain1","question":"You are helping a client consolidate several separate accounts into a single account.  This consolidation will result in approximately 50 new VPCs in their one account.  They want to continue to use Route 53 for DNS but only want it accessible privately. How can you accomplish this most efficiently?","explanation":"Private Hosted Zones provide DNS services to VPCs but cannot be access from the internet.  They can be associated with VPCs either by the console, CLI or programmatically via SDK.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html","title":"Associating More VPCs with a Private Hosted Zone - Amazon Route 53"}],"answers":[{"id":"3cc28b12f45b3dee8f7f16a0f93d00ce","text":"Install BIND on an EC2 instance in a single VPC.  Create VPC peering connections between the DNS VPC and any new VPCs.  Configure a DHCP Option Set to assign a DNS and link that to each VPC.","correct":false},{"id":"f74daa2300ac594c111ca9fce198f19c","text":"Create a central DNS server using EC2 and BIND.  Configure Route 53 to reference this DNS server as a resolver.  Update DNS records at the registrar to point to the central DNS.","correct":false},{"id":"315372936e7ffba65896da15d0f45c2d","text":"Create a Private Hosted Zone within Route 53.  As the new VPCs are created, associate them with the Private Hosted Zone.","correct":true},{"id":"cbffb64b6b43e6fe45496b6e77ce17b8","text":"Create a Private Hosted Zone within Route 53 for each respective VPC.  Configure replication between the private hosted zones to keep records in sync.","correct":false},{"id":"82d5ef7e7176200aa4350ef90dd4c354","text":"Create a Public Hosted Zone within Route 53 and associate it to each VPC.  Configure a NACL on each VPC to deny inbound DNS queries (UDP port 53).","correct":false}]},{"id":"374fde7d-232a-4cfe-b5d6-7755d564c6ca","domain":"awscsapro-domain1","question":"You are consulting for a company that has decided to partially migrate some resources to AWS from their two data centers (DC1 and DC2).  Their first order of business is to design a robust, redundant and cost-effective network connection between their data centers and AWS.  They already have redundant links between DC1 and DC2.  Which of the following architectures provides the highest availability at the least cost?","explanation":"A common and cost effective way to provide a redundant link to AWS with Direct Connect is a VPN connection.  In the event that the Direct Connect path fails at DC1, your on-prem router can redirect traffic over the VPN at DC2 via the DC1-DC2 link.  Having dual Direct Connect links is definitely redundant but more expensive than a VPN.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/","title":"Multiple Data Center HA Network Connectivity – AWS Answers"}],"answers":[{"id":"abd8b38f393b6a0d0b42f68dca5ec24d","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure BGP to dynamically route traffic across the nearest Direct Connect link.","correct":false},{"id":"bcac2f1d84c5f367ede3342f0adda492","text":"Ensure that DC1 and DC2 have separate ISPs.  Setup VPN connections from DC1 and DC2 to a Virtual Private Gateway on AWS.  Create static routes at each DC to use the local VPN to AWS.  Use CloudTrail to monitor traffic on the Virtual Private Gateway and trigger a script to update the static route if one of the VPN connections goes down.","correct":false},{"id":"77c9fb7459e1b19f6f655d63556016e8","text":"Configure a Direct Connect connection from DC1 to a Virtual Private Gateway on AWS.  Setup a VPN connection from DC2 to a Virtual Private Gateway on AWS.  Configure a dynamic route across DC1 and DC2 for both paths with a route priority favoring the Direct Connect path to AWS.","correct":true},{"id":"425adb8435a7c170eef3698fa729f5ea","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure a default route in both DC1 and DC2 to route traffic to the local Direct Connect link.","correct":false}]},{"id":"49107f33-5b31-4d7e-a2cb-95f3ce8a2d75","domain":"awscsapro-domain1","question":"Your customer has setup AWS Organizations to help manage a collection of AWS Accounts.  They are running into a problem though and need your help.  They have created accounts for each business unit and applied SCPs to those OUs. However, they notice that root accounts in in those sub-accounts can still change root access keys and disable MFA.  How do you instruct your customer?","explanation":"Service Control Policies can control many aspects but they cannot restrict root account actions of changing root access keys or disabling MFA.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"77df34553819fdc2e31fb79762948993","text":"You can establish a trust with the top-level account and use the \"organizations:ServicePrinciple\" condition key to restrict root access at the sub-account level.","correct":false},{"id":"94a2f948d3d2f9c317a6ebb1f5a24ea5","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the specific sub-accounts.","correct":false},{"id":"194cc2d07b5c378b62b1e090f0aea956","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the entire OU in the root account.","correct":false},{"id":"3d722952f024bcec9174a311c17dcc14","text":"You can not use SCPs to restrict root account activities of changing the root password or managing MFA settings.","correct":true}]},{"id":"9fc0785a-d5cb-47e3-bc2f-829b5a36ba26","domain":"awscsapro-domain3","question":"You work for a Genomics company which has decided to migrate its DNA Sequencing application to the AWS Cloud. The application is containerized. Currently, container image A works on genomics data residing on an on-premises file server, validating the data and updating the metadata in a local database. When it is done, engineers manually trigger 100 or more instances of container image B that process this data in parallel by reading the metadata, creating output files. When all these container instances have done their job, engineers manually trigger container image C that validates the results, cleans up and sends notifications.\nThe CTO has decided to use S3 for storing the input and output data files. She has also mandated that the parallel processing phase should run on a fleet of Spot EC2 instances to reduce compute costs. She also wants to automate the workflow, so that engineers do not have to manually trigger the next set of actions. The requirement is to minimize administrative overhead and custom development for the migration.\nAs the AWS Architect, which of the following approaches should you recommend?","explanation":"AWS ECS does not natively provide workflow management. In an ECS service definition file, you cannot specify a sequence of tasks with execution dependencies such that one will be run only after the previous one completes. Hence, the two ECS choices are ruled out.\nDistraction warning - Fargate does not allow you to specify Spot instances as it is serverless in nature (it absolves you from specifying server details). This effectively creates a distraction - when the candidate rules out ECS Fargate due to this reason, they may be relieved to see the ECS EC2 choice and jump to a conclusion because it is relatively easy to remember that EC2 launch type actually lets you select Spot instances. However, this distraction is designed to take focus away from the fact that neither of these two choices is correct. Both of the choices require service definition files to set up execution workflows. Task instances mentioned in an ECS service definition file are executed in parallel - ECS does not control the sequence of tasks.\nAWS SWF does not let you specify Spot instances either. Also, SWF is usually used in cases where human intervention is needed in the workflow.\nThis leaves AWS Batch as the correct answer. AWS Batch is indeed the most suitable AWS service for this scenario as it meets all requirements.","links":[{"url":"https://docs.aws.amazon.com/batch/latest/userguide/create-compute-environment.html","title":"How to create a compute environment for AWS Batch"},{"url":"https://docs.aws.amazon.com/batch/latest/userguide/example_array_job.html","title":"Example AWS Batch Array Job Workflow"},{"url":"https://aws.amazon.com/ec2/spot/containers-for-less/get-started/","title":"How to run ECS clusters in EC2 Spot Instances"}],"answers":[{"id":"e46ada36d33a9e5b23aa37ee94c4c5d6","text":"Use AWS ECS with EC2 Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"ceb4c03a526e8ddb01ada7a40bb60001","text":"Use AWS Batch, setting up an array job with 100 or more copies preceded by pre-requisite and follow-up jobs where the workflow is controlled by dependencies between jobs. Also, use Spot as the Provisioning Model for compute environment","correct":true},{"id":"49755d6c34da495b8c91964f52946d29","text":"Use AWS SWF workers and deciders to manage the workflow. Configure the workers to use EC2 Spot Instances","correct":false},{"id":"757ddde350053553e44844d066c91386","text":"Use AWS ECS with Fargate Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false}]},{"id":"8765bd56-057b-488c-9a0a-f5bd413dd240","domain":"awscsapro-domain5","question":"Due to new corporate policies on data security, you are now required to use encryption at rest for all data.  You have some EC2 Linux instances on AWS that were created without encryption for the root EBS volume.  What can you do that meet the requirement and reduce administrative overhead?","explanation":"AWS does support encrypted root volumes but conversion from unencrypted root to an encrypted root requires a bit of a process. You must first create an AMI then copy that newly created AMI to the same region, specifying that you want to encrypt the EBS volumes during the copy.  You can then create a new instance with an encrypted root volume from the copied AMI.  You can use either a generated key from KMS or your own CMK imported into KMS.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html","title":"AMIs with Encrypted Snapshots - Amazon Elastic Compute Cloud"},{"url":"https://aws.amazon.com/blogs/aws/new-encrypted-ebs-boot-volumes/","title":"New – Encrypted EBS Boot Volumes | AWS News Blog"}],"answers":[{"id":"1e30ccf0b75e9e70fd76c6e041510c75","text":"At present, EC2 does not support encrypted root volumes.  Create new encrypted EBS data volumes and attach the new volumes to the existing instances.  Use RSYNC to migrate all the non-OS data over to the encrypted data volumes.","correct":false},{"id":"9fbbb2e71386cb7f7a7ed77129a1a960","text":"Create an encrypted EFS instance and mount-points in the respective subnets.  Log into the instance and mount an encrypted EFS mount-point.  Copy all the root files over to the EFS mount point.  Edit the FSTAB file to mount the EFS mount point as the root volume instead of the root EBS device and reboot.","correct":false},{"id":"180e9aecdb74f204b1df00ffe6fa8b56","text":"Stop the instances and create AMIs from the instances.  Copy the AMIs to the same region and select the \"Encrypt target EBS snapshots\".  Redeploy the instances using the AMI copies you made with encrypted root volumes.","correct":true},{"id":"50c17b27f0bd0390ae321943f7db5c3d","text":"Create a certificate in CMS for the encryption key.  Stop the instances and temporarily detach the root volumes.  Via the AWS CLI, enable encryption on the root volumes using the \"ebs modify-volume\" argument with the flag of \"encryption=<CMS ARN>\" to specify the certificate.","correct":false},{"id":"4430b7492a6c058c3574ce4e8ea43955","text":"Stop the instances and temporarily detach the EBS volumes.  Attach the root volumes to another EC2 instance and mount them a data volume.  Use a encryption tool like GPG or OpenPGP to recursively encrypt all the files on the mounted root volumes.  Detach and reattach the encrypted EBS volumes to the original instances and restart.  Import the encryption keys in KMS as a CMK.","correct":false}]},{"id":"0ca782d3-466e-4376-ab92-e4eec0374e13","domain":"awscsapro-domain2","question":"You've begun deploying EC2 and VMware Cloud on AWS instances to host various applications which you'd like to make accessible to those who authenticate via an on-premises Active Directory domain. You've configured AWS Managed Microsoft AD in the same region as the EC2 and VMware Cloud on AWS instances with a one-way trust back to the corporate AD domain. You're able to seamlessly join new EC2 Windows instances to the Managed AD domain at launch, but the EC2 Linux and VMware Cloud on AWS instances don't show up in the domain when launched. What additional actions need to take place in order to seamlessly join all the instances to the domain at launch?","explanation":"A bootstrap script that installs a Kerberos client package and performs a Realm Join will successfully join an EC2 Linux instance to an Active Directory domain. Active Directory uses Kerberos as it's authentication protocol between a server and a client. VMC Compute Gateway (CGW) Firewall Rules block traffic to all uplinks by default, so 'allow' rules need to be added. This particular issue would not be due to a role or Security Group configuration problem because the EC2 Windows instances are able to join the domain successfully. The instance's SSH service does need to be configured to allow password authentication, but this is not necessary for the domain join operation.","links":[{"url":"https://d1.awsstatic.com/VMwareCloudonAWS/aws_reference_architecture_hybrid_active_directory_trusted_domains.pdf?did=wp_card&trk=wp_card","title":"Hybrid Active Directory Trusted Domains"},{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/join_linux_instance.html","title":"Manually Join a Linux Instance"},{"url":"https://docs.vmware.com/en/VMware-Cloud-on-AWS/services/com.vmware.vmc-aws.networking-security/GUID-A5114A98-C885-4244-809B-151068D6A7D7.html","title":"Add or Modify Compute Gateway Firewall Rules"}],"answers":[{"id":"5d3e04eaf8f190a5e4314afbf01a181e","text":"Create a bootstrap script to install a Kerberos client package and perform a Realm Join command for the EC2 Linux instances, and add a VMware Cloud NSX Compute Gateway (CGW) Firewall Rule for the VMware Cloud on AWS instances","correct":true},{"id":"89f2d086e3f1e6302636559551197099","text":"Have EC2 Linux instances configure SSH services to allow password authentication, and configure the VMware Cloud NSX Compute Gateway (CGW) to not perform Network Address Translation on the domain server's IP address","correct":false},{"id":"43bbc2a5c48b9ae653075577b0f2555d","text":"Have EC2 Linux instances assume a role with permissions to write to the Managed AD domain, and configure the VMware Cloud NSX Compute Gateway (CGW) to pass Active Directory requests through without VMware Tunneling","correct":false},{"id":"aa2adec42d76f83b2f45af8ab617e0bb","text":"Add inbound rules for the EC2 Linux and the VMware Cloud on AWS instances to allow traffic on port 389 for LDAP","correct":false}]},{"id":"f7e5105f-7ae9-4c04-b1e7-2d6165db236c","domain":"awscsapro-domain2","question":"You work for a specialty retail organization. They are building out their AWS VPC for running a few applications. They store sensitive customer information in two different encrypted S3 buckets. The applications running in the VPC access, store and process sensitive customer information by reading from and writing to both the S3 buckets. The company is also using a hybrid approach and has several workloads running on-premises. The on-premises datacenter is connected to their AWS VPC using Direct Connect.\nYou have proposed that an S3 VPC Endpoint be created to access the two S3 buckets from the VPC so that sensitive customer data is not exposed to the internet.\nSelect two correct statements from the following that relate to designing this solution using VPC Endpoint.","explanation":"S3 VPC Endpoint is a common topic tested in the SA-P Exam, as it enables S3 access over a private network, which is a common security requirement in many organizations. It is also a cost-effective way to establish outbound connection to S3, as the alternative is to use NAT Gateways, which are charged by the hour even if there is no traffic using them.\nOn vertical scanning of the answer choices, it should be obvious that one of the two closely worded choices is correct, and one of the other two choices is correct as well. That is because if there are 2 or 3 or 4 closely worded choices, only one (or in some rare cases, two) is correct - this is a common pattern in the SA-P test.\nFor the closely worded pair - the bucket policy of an S3 bucket will always specify who can or cannot access the bucket. It will not dictate how a VPC Endpoint behaves. Hence, the choice that suggests that a bucket policy can control a VPC Endpoint is incorrect.\nBetween the other two choices, remember that a VPC Endpoint can connect to any number of S3 buckets by default. One Endpoint for each bucket is simply not scalable, and should stand out as incorrect.\nThe remaining choice is correct because the S3 VPC Endpoint is of type Gateway Endpoint as opposed to Interface Endpoint, and a subnet needs Routes in the Routing Table for sources in the subnet to be able to connect to it. Read the links provided to understand the differences","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies-vpc-endpoint.html","title":"Example bucket policies including ones that discuss use of sourceVpce attribute"},{"url":"https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/","title":"How to set up VPC Endpoint for S3 access including Route Table for subnets accessing S3"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html","title":"VPC Endpoint documentation discussing differences between gateway endpoints and interface endpoints"}],"answers":[{"id":"55b07e707272bf499484e3f28a8fac88","text":"Bucket policies on the two S3 buckets can specify the id of each VPC Endpoint using AWS attribute sourceVpce to further restrict which VPC Endpoints can access each bucket","correct":true},{"id":"19bc72413543fc85eb58b5edf43fb2db","text":"Each VPC Endpoint is a Gateway Endpoint that also requires correct routes in the Route Table associated with each subnet that wants to access the endpoint","correct":true},{"id":"96bd9d9981d98c1d98550618acedc673","text":"Bucket policies on the two S3 buckets can specify the id of each VPC Endpoint using AWS attribute sourceVpce to further restrict which S3 buckets can be accessed by the VPC Endpoint","correct":false},{"id":"28b96a455027a61e12e57f73ae7956fc","text":"You need two VPC Endpoints, one for each S3 bucket, as a single VPC Endpoint can only access a single S3 bucket","correct":false}]},{"id":"c9f44641-660b-4c42-9380-9e7f6b0a9ba4","domain":"awscsapro-domain4","question":"As the solution architect, you are assisting your customer design and develop a mobile application using API Gateway, Lambda and DynamoDB. S3 buckets are being used to serve static content. The API created using API Gateway is protected by WAF. The development team has just staged all components to the QA environment. They are using a load testing tool to generate short bursts of a high number of concurrent requests sent to the API Gateway method. During the load testing, some requests are failing with a response of 504 Endpoint Request Timed-out Exception.\nWhat is one possible reason for this error response from API Gateway endpoint?","explanation":"The SA-P exam sometimes focuses on knowledge of response codes from API Gateway and what each distinct HTTP response code could mean.\nThe key to answering this question correctly is being able to distinguish between 4XX and 5XX HTTP error response codes. Though AWS has not been entirely consistent in their error code assignment philosophy, 4XX usually happens any time throttling kicks in because the request in that case never makes to an instance of Lambda function. 5XX happens when a Lambda function is actually instantiated, but some error (like time out) happened inside the Lambda function. One sneaky way to remember this is the fact that 5XX errors are called server errors in HTTP-land, so to generate a 5XX a server process must exist (and must have failed). Of course, in this context, the HTTP server process is a Lambda function - so in scenarios where throttling prevented a Lambda function from getting spawned, the response code cannot be 5XX. This is not consistently followed by AWS API Gateway error design, though, as we can see that AUTHORIZER_CONFIGURATION_ERROR and AUTHORIZER_FAILURE are both 500, though no Lambda function is actually spawned in either case. However, the candidate must remember that throttling always results in 4XX codes. An Endpoint Request Timed-out Exception (504) suggests that the requests in question actually made its way past the API Gateway into a Lambda function instance.\nFor the scenario where request rate exceeds API Gateway limits, the request would be blocked by API Gateway itself. The response would be 429. The exact knowledge of the code 429, however, is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe scenario where 1000 Lambda functions are already running is a similar example of throttling - the 1001st Lambda function will not even be spawned. The response, again, will be 429. However, the exact knowledge of the code 429 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe WAF scenario is yet another example of the request not even crossing the protections placed at the gateway level. If WAF is activated on API Gateway, it will block requests when the rate exceeds the HTTP flood rate-based rule (provided all such requests come from a single client IP address). However, the response, again, will be in the 4XX area (specifically, 403 Forbidden) - however, the exact knowledge of the code 403 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThis leaves Lambda time-out as the only correct answer. The mention of 30 seconds or more is a diversion tactic, in case candidate believes that the relevant Lambda time-out is 5 minutes. A given Lambda function instance may have a time-out limit of 5 minutes, but when it is invoked from API Gateway, the timeout imposed by API Gateway is 29 seconds. If a Lambda function runs for longer than 29 seconds, API Gateway will stop waiting for it and return 504 Endpoint Request Timed-out Exception.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html","title":"Amazon API Gateway Limits and Important Notes"},{"url":"https://aws.amazon.com/blogs/compute/amazon-api-gateway-adds-support-for-aws-waf/","title":"Amazon API Gateway adds support for AWS WAF"}],"answers":[{"id":"82f6a6eb1a4aeef5dd34f21fcd2069ef","text":"The number of requests generated by the load testing framework has exceeded the threshold for the HTTP flood rate-based rule set in the WAF settings for the stage in question","correct":false},{"id":"98daaa701198f7e1c541fe8051799129","text":"The Lambda function is sometimes taking 30 seconds or more to finish executing","correct":true},{"id":"370770015087b4ff70656089fc9e3316","text":"The test is triggering too many Lambda functions concurrently. AWS imposes a soft limit of 1000 concurrent Lambda functions per region","correct":false},{"id":"8090c6b0fe9036ec84fce24b16a7dc10","text":"The load testing tool has exceeded the soft limit for request rate allowed by API Gateway","correct":false}]},{"id":"11c28d09-1ccf-46ac-a56e-3998bff9c4e4","domain":"awscsapro-domain2","question":"You bought a domain name \"example.com\" from GoDaddy which is a domain registrar and the domain name will expire in several months. You plan to start using AWS Route 53 to manage the domain and resolve its DNS queries. The transferred domain in Route 53 should be automatically renewed every year so that the domain name will never expire and you do not need to renew it manually. Which method would you use to transfer the domain name properly?","explanation":"Users can register their new domain names in Route 53 or transfer existing domain names from other registrars such as GoDaddy to Route 53. After the transfer, the domain can automatically renew every year if the Auto Renew feature is enabled. To transfer the domain name, you do not need to wait until the domain name expires. And you cannot register the same domain name in both GoDaddy and Route 53 at the same time.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-transfer-to-route-53.html","title":"Transferring Registration for a Domain to Amazon Route 53"}],"answers":[{"id":"bd0fd22629d8016f6941db326aeb4ba4","text":"Wait until the domain name expires in GoDaddy and then register the domain in AWS Route 53 by clicking the \"Register Domain\" button in AWS management console. Turn on the features of Auto Renew and Transfer Lock for the new domain.","correct":false},{"id":"32c733a4bf01f533d38704f340cb7eb5","text":"Login in the GoDaddy admin account, unlock the domain transfer and request the domain transfer to Route 53. Accept the domain transfer in Route 53 and extend the expiration date to 10 years as transferred domains cannot automatically renew.","correct":false},{"id":"e27ac2c31c859905f6a51f22ab3a34bf","text":"Confirm that the domain is transferable in GoDaddy. In the Route 53 AWS Management Console, click \"Transfer Domain\" to transfer registration for the domain name from GoDaddy to Route 53. Enable the automatic renewal for this domain name.","correct":true},{"id":"d8a7c0a25d22a5534a30c668de1be1d1","text":"Register the same domain name \"example.com\" in Route 53 three months before it expires in GoDaddy. Enable the feature of Transfer Lock in Route 53 to prevent it from being transferred to another registrar. Do not renew the original domain name in GoDaddy.","correct":false}]},{"id":"9ff9e447-acaa-4dae-9bbf-08db786af693","domain":"awscsapro-domain2","question":"You are a AWS Solutions Architect, and your team is working on a new Java based project. After the application is deployed in an EC2 instance in the development AWS account, an encrypted EBS snapshot is created. You need to deploy the application in the production AWS account using the EBS snapshot. For security purposes, a different customer managed key (CMK) in key management service (KMS) is required to encrypt the EBS volume in the new EC2 instance. What is the best method to achieve this requirement?","explanation":"In order to share an encrypted EBS snapshot between AWS accounts, you must also share the customer managed key (CMK) used to encrypt the snapshot. You can then copy the snapshot to a new one encrypted with another encryption key. You cannot directly copy an EBS volume between accounts. An EBS snapshot encrypted by the default AWS key is not allowed to be shared with another account.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html","title":"Sharing an Amazon EBS Snapshot"}],"answers":[{"id":"941319ca17d0d8915ed822083e1dc2d4","text":"Re-encrypt the EBS snapshot with the default EBS encryption key in the development account. Share both the snapshot and the default EBS key to the production account. In the production account, create a volume from the snapshot and attach the volume to a new EC2 instance.","correct":false},{"id":"5751e42f86f4695e7309214d370c4d2b","text":"Share the snapshot and the encryption key to the production AWS account. Copy the snapshot to a new snapshot in the production account and encrypt it using another CMK in KMS. Create an AMI using the snapshot and launch a new EC2 instance.","correct":true},{"id":"536a3b4a4f859146660d3b6dc2036650","text":"Create an AMI from the EBS snapshot and copy the AMI from the development account to the production account. Launch an Auto Scaling group with the AMI and encrypt it with a new customer managed key.","correct":false},{"id":"16c8a2b39dcb6723939653ed8deb9f90","text":"Share the CMK with the production account. In the development account, create a volume from the EBS snapshot. Copy the volume to the production account and encrypt it with another customer managed key. Launch a new EC2 instance and attach the new EBS volume to it.","correct":false}]},{"id":"230f422f-7118-4096-8dce-59c642fb55c8","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a new Direct Connect connection.  The connection is up and you can ping the AWS peer IP address, but the BGP peering session cannot be established.  What should be your next logical troubleshooting steps?","explanation":"Because the connection is up and we can ping the AWS peer, the problem must be at a higher level on the OSI model than the Physical or Data layers.  BGP uses TCP port 179 to communicate routes so we should check that no NACL or SG is blocking it.  Additionally, we should make sure the ASNs are properly configured in the proper ranges.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/Troubleshooting.html","title":"Troubleshooting AWS Direct Connect - AWS Direct Connect"}],"answers":[{"id":"8fc27418eee2ce07b64bc672007d2c1b","text":"Contact the co-location provider and request a written report for the Tx/Rx optical signal across the cross connect.","correct":false},{"id":"16e5aea88df69cc18f99e3f066ec99c1","text":"Ensure that the local ASNs and AWS-side ASNs are properly configured.","correct":true},{"id":"45d4c1753395277878b9a17343628c52","text":"Ensure that the VLAN is configured properly between your on-prem router the provider. ","correct":false},{"id":"3d2a55832b90f19a2137e8715525d717","text":"Make sure no firewalls or ACLs are blocking TCP port 179 or any high-numbered ephemeral ports.","correct":true},{"id":"edd3f9408cecbbf9182678ccc51d7981","text":"Ask your network provider to provide you with a cross connect completion notice and compare the ports with those listed on your LOA-CFA","correct":false},{"id":"81977d7a1eb5714746851077b93f44d6","text":"Power cycle all the equipment to clear ARP table cache.","correct":false}]},{"id":"9624e171-081f-43e5-a74c-b4b792676b63","domain":"awscsapro-domain2","question":"A connected home solutions company is creating a central console to manage smart home environments. Some of the capabilities needed include capturing device telemetry data, making that data available to query from client applications, alerting on undesirable device situations, and providing device analytics. The company has decided to run the console's application backend on AWS. Devices will communicate with the console via MQTT protocol. Which architecture will provide the best scalability and the highest operational efficiency for the smart home console?","explanation":"AWS IoT Core provides all the capability needed to create a smart home console. IoT rules can be configured to send device messages to DynamoDB for client application querying, and to send device messages to IoT Events for alerting and notification. IoT Analytics is fully integrated with IoT Core to receive messages from connected devices as they stream in. These are all managed services which maximize operational efficiency. AWS IoT Greengrass can perform all the required functions, but must run on a local device, which may not be desirable to customers since another piece of hardware would need to be acquired. Amazon Athena cannot query from DynamoDB directly. IoT Events and IoT Analytics are IoT-centric services which will be better solutions than CloudWatch Events and Amazon Redshift.","links":[{"url":"https://aws.amazon.com/iot-core/","title":"AWS IoT Core"},{"url":"https://docs.aws.amazon.com/iotevents/latest/developerguide/what-is-iotevents.html","title":"What is AWS IoT Events?"},{"url":"https://docs.aws.amazon.com/iotanalytics/latest/userguide/welcome.html","title":"What Is AWS IoT Analytics"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/connected_home_telemetry_ra.pdf?did=wp_card&trk=wp_card","title":"Measuring and collecting data from smart home devices"}],"answers":[{"id":"d70e3ad71c36d95078725805fcfaebf8","text":"Implement AWS IoT Greengrass to receive MQTT messages from the devices. Create Lambda functions on IoT Greengrass to send device information to Amazon DynamoDB and Amazon CloudWatch Events. Have client applications use Amazon Athena to query DynamoDB. Publish to Amazon Simple Notification Service topics when device issues are identified by CloudWatch Events. Use an AWS Glue job to aggregate the device information in DynamoDB and write summaries to Amazon Redshift. Create visualizations of the Redshift summaries in Amazon QuickSight.","correct":false},{"id":"1e93269720ca34bcc18e1d67c5ccee18","text":"Implement AWS IoT Core to receive MQTT messages from the devices. Create an IoT rule to store the device messages in Amazon DynamoDB. Make an Amazon API Gateway API available to client applications. Have API Gateway invoke an AWS Lambda function to query DynamoDB. Create a second IoT rule in IoT Core to forward telemetry information to AWS IoT Events. Publish to Amazon Simple Notification Service topics when device issues are identified by IoT Events. Have AWS IoT Analytics receive device messages from IoT Core for visualization with Amazon QuickSight.","correct":true},{"id":"678fa8ab5656d7f8a5fd360c0cbb3586","text":"Implement AWS IoT Core to receive MQTT messages from the devices. Create an IoT rule to store the device messages in Amazon DynamoDB. Have client applications use Amazon Athena to query DynamoDB. Create a second IoT rule in IoT Core to forward telemetry information to AWS IoT Events. Publish to Amazon Simple Notification Service topics when device issues are identified by IoT Events. Have AWS IoT Analytics receive device messages from IoT Core for visualization with Amazon QuickSight.","correct":false},{"id":"518de0ee58995da859fef3f8b0108750","text":"Implement AWS IoT Greengrass to receive MQTT messages from the devices. Create Lambda functions on IoT Greengrass to send device information to Amazon DynamoDB, Amazon CloudWatch Events, and AWS IoT Analytics. Make an Amazon API Gateway API available to client applications. Have API Gateway invoke a Lambda function to query DynamoDB. Publish to Amazon Simple Notification Service topics when device issues are identified by CloudWatch Events. Send IoT Analytics summaries to Amazon QuickSight for visualization.","correct":false}]},{"id":"f3efb368-9a43-4e1d-bf0a-1cf55bb918a8","domain":"awscsapro-domain4","question":"An AWS customer makes use of a wide variety of AWS services across multiple AWS regions. As the usage cost keeps increasing, the customer wants to get the detailed billing and cost information of his AWS account. In the meantime, the customer needs to quickly analyze the cost and usage data, visualize it, and share it through data dashboards so that he can get a better understanding of the billing and resource utilization. Which of the following methods would you choose?","explanation":"Users can get the billing and usage information from the AWS Cost and Usage Reports. The reports downloaded in an S3 bucket can be further processed and analyzed by QuickSight. The reports cannot be created in AWS Cost Explorer or billing dashboard. The question also asks for data visualization and dashboards. QuickSight, AWS's Business Intelligence service, is the most appropriate service for this requirement. Athena, Redshift or Elastic MapReduce are not suitable.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-upload-aws-cost-usage-reports-to-redshift-and-quicksight/","title":"Upload AWS Cost & Usage Reports to QuickSight"}],"answers":[{"id":"36fb0d3b243e97c8ed14e32e6a2bc329","text":"Generate the billing reports from the AWS management console and upload the files to an S3 bucket. Create an Amazon Elastic MapReduce (EMR) cluster by specifying data inputs and outputs. Process the data in the cluster and generate reports for the admin IAM user.","correct":false},{"id":"d3ef8ffbe6a0ff9378c5e248f2f7e165","text":"Download the AWS Cost and Usage Reports from the AWS billing dashboard in an S3 bucket which is owned by the administrators. Integrate the reports with an Amazon Redshift cluster. Analyze and view the data in the Redshift data warehouse.","correct":false},{"id":"fc09f5c144565de3b8472bde413e896a","text":"Create a data pipeline that downloads the monthly cost and usage reports from AWS Cost Explorer and uploads the reports to an S3 bucket. Set up a specific billing table with Amazon Athena and analyze the billing and usage data using SQL query commands.","correct":false},{"id":"78f31d3919e8b532d502611700598d75","text":"Create the hourly AWS Cost and Usage Reports and enable the data integration with Amazon QuickSight. Analyze and visualize the data in QuickSight by creating interactive dashboards and generating custom reports.","correct":true}]},{"id":"d8b88385-e15f-4313-bc53-e9fb82f89cc3","domain":"awscsapro-domain2","question":"You are developing an application to be hosted on EC2. The application uses some environmental configuration data and other necessary parameters when running. For example, the application needs to get the correct username and password in order to communicate with a RDS database. You want to find a free AWS service to store these parameters. To meet security requirements, these stored parameters must be encrypted by the AWS Key Management Service. Which of the following methods is the best?","explanation":"You can manage parameters in Systems Manager Parameter Store. The type of the parameters must be SecureString so that they are encrypted by KMS. Parameter Store has standard tier and advanced tier. In this scenario, standard tier is enough and advanced tier is not a free service. AWS Secrets Manager does not have the concept of standard or secure parameter. It also charges you per secret per month.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html","title":"How AWS Systems Manager Parameter Store uses AWS KMS"}],"answers":[{"id":"3295f034025fe6d21155d228ee3dd0a2","text":"Store secure string parameters in AWS Systems Manager Parameter Store so that the parameters are encrypted by KMS. Use the standard tier as there is no additional charge for it. Use AWS Encryption SDK in the application to get the parameters.","correct":true},{"id":"4f357d94d3f20ee0d0c3c7c892ef0d70","text":"Create standard parameters in AWS Secrets Manager. Use the advanced tier as it uses envelope encryption to encrypt the stored parameters with KMS. You can also configure Secrets Manager to rotate the stored secrets or API keys automatically.","correct":false},{"id":"41e6f55938cd5c32278c6f4b6708641e","text":"Create standard string parameters in AWS Systems Manager Parameter Store as it is a free service. The parameters are automatically encrypted with envelope encryption by the default AWS managed key (aws/ssm) in KMS. Use AWS Encryption SDK in the application to fetch the parameters.","correct":false},{"id":"c08527f2eeacd9f56133d1dfed701f1a","text":"Create secure parameters in AWS Secrets Manager. Secrets Manager protects secrets by integrating with KMS and all stored parameters are automatically encrypted by the AWS managed key \"aws/secretsmanager\". You can also configure Secrets Manager to rotate the secrets.","correct":false}]},{"id":"0bfd631e-c08c-406a-acf5-a07416aab129","domain":"awscsapro-domain5","question":"Your team uses a CloudFormation stack to manage AWS infrastructure resources in production. As the AWS resources are used by a large number of customers, the update to the CloudFormation stack should be very cautious. Your manager asks for additional insight into the changes that CloudFormation is planning to perform when it updates the stack with a new template. The change needs to be reviewed before being applied by a DevOps engineer. What is the best method to achieve this requirement?","explanation":"CloudFormation Change Sets are able to provide the information on how the running resources are affected by a stack update. The outputs can be reviewed before being executed. Users can view the Change Set through AWS Console or CLI. The Retain option in the DeletionPolicy, CloudFormation stack policy or termination protection helps on protecting the stack resources. However, they cannot provide a summary of  changes in a stack update.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"935df51ce2c7f07994c2b8a257489e00","text":"Create a CloudFormation Change Set using AWS Management Console or CLI, review the changes to see if the modifications are as expected and execute the changes to update the stack.","correct":true},{"id":"4657e544cc1daf4315865e230d92dd00","text":"For key AWS resources in the CloudFormation stack, add a Retain option in the DeletionPolicy attribute, which prevents the resources from being accidentally deleted by a stack update. Add a Delete option for the resources that you want to delete along with the stack.","correct":false},{"id":"700aa7cb0f0e8cfb417b67ae5d49e962","text":"Add a CloudFormation stack policy to prevent updates to stack resources. Only after the changes are reviewed and approved, change the stack policy to allow the stack update. Revert the stack policy after the change.","correct":false},{"id":"550c80eaf15eeb89d49aa2a86eb747a6","text":"Enable termination protection in the CloudFormation stack so that the AWS resources cannot be accidentally deleted or modified. Disable the protection only if the changes are approved. Execute the changes in a maintenance window.","correct":false}]},{"id":"338ce579-a236-4282-9670-8da3b0baf2e9","domain":"awscsapro-domain3","question":"You are helping a Retail client migrate some of their assets over to AWS.  Presently, they are in the process of moving their Enterprise Data Warehouse.  They are planning to re-host their very large Oracle data warehouse on EC2 in a high availability configuration across AZs.  They presently have several Scala scripts that process some detailed Point of Sale data that is collected each day.  The scripts perform some aggregation on the data and import the aggregate into their Oracle database.  They want to move this process to AWS as well.  Which option would be the most cost-effective way for them to do this?","explanation":"AWS Glue is a fully managed extract, translate and loading service and is compatible with Scala.  EMR could do this but represents more overhead than necessary.  Lambda is not compatible with Scala and migrating to Redshift does not bring anything in this case if the customer wants to retain their Oracle database.","links":[{"url":"https://aws.amazon.com/glue/faqs/","title":"AWS Glue Features - Amazon Web Services"}],"answers":[{"id":"4f1ff8b853c3ba363bdd2bda53538ab4","text":"Migrate from Oracle to Redshift and use Kinesis Firehose.","correct":false},{"id":"8b01d948d5ad2f4b1c8e817c2d98e7c2","text":"Migrate the processing to AWS Glue.","correct":true},{"id":"04578ae8419780f9dc441d01fe11582d","text":"Create Lambda functions using your Scala scripts.","correct":false},{"id":"1a4de6676c8c078310e08aad71d9dce6","text":"Migrate the processing to AWS EMR.","correct":false},{"id":"a445a1a877009cd7c31858687a818116","text":"Import your Scala scripts into AWS SCT for processing.","correct":false}]},{"id":"f7d7767b-9159-4e53-8e37-ff9bf41ace17","domain":"awscsapro-domain5","question":"You are working with a client to help them design a future AWS architecture for their web environment.  They are open with regard to the specific services and tools used but it needs to consist of a presentation layer and a data store layer.  In a brainstorming session, these options were conceived.  As the consulting architect, which of these would you consider feasible?","explanation":"The only two options which contain feasible options are the Beanstalk and S3/Dynamo methods.  One would not create a new K8s deployment for for a new web update.  CodeBuild and AWS Config are not the correct tools for how they are being suggested.","links":[{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild – Fully Managed Build Service"}],"answers":[{"id":"f5c7f9be39386b15747c0fe57d5040ba","text":"Deploy Kubernetes on an auto-scaled group of EC2 instances.  Define pods to represent the multiple tiers of the landscape.  Use ElastiCache for Memcached to offload queries from a Multi-AZ RDS instance.  To deploy changes to the landscape, create a new EKS deployment containing all the updated service containers and deploy them to replace all the previous existing tiers.  Ensure the DevOps team understands the rollback procedures.","correct":false},{"id":"ca4cc92538f3b73d221c9b5d4378e1f8","text":"Setup a traditional three tier architecture with a CloudFormation template per tier and one master template to link in the others.  Configure a CodeBuild stack and set this stack to perform automated Blue Green deployments whenever any code change is made.","correct":false},{"id":"d6a73928290b05c25d87e26ece9e94a6","text":"Create a monolithic architecture using Elastic Beanstalk configured in the console.  Create an RDS instance outside the Beanstalk environment and configure it for multi-AZ availability.  When a new landscape change is required, use a command line script to implement the change.","correct":true},{"id":"01a7f4c09f65e41884b0a72843a8d55b","text":"Deploy an auto scaling group of EC2 instances behind an Application Load Balancer.  Provision a Mulit-AZ RDS instance to act as the data store, configuring a caching layer to offload queries from the database.  Use a User Script in the AMI definition to download the latest web assets from S3 upon boot-up.  When changes are required, use AWS Config to automatically fetch a new version of web content from S3 when a new version is created.","correct":false},{"id":"c27a151a7715575fb1ebf0225c6aee09","text":"Use the AngularJS framework to create a single-page application.  Use the API Gateway to provide public access to DynamoDB to serve as the data layer.  Store the web page on S3 and deploy it using CloudFront.  When changes are required, upload the new web page to S3.  Use S3 Events to trigger a Lambda function which expires the cache on CloudFront.","correct":true}]},{"id":"74aec97e-c092-4588-8da4-43dca3ddd0eb","domain":"awscsapro-domain5","question":"You are trying to help a customer figure out a puzzling issue they recently experienced during a Disaster Recovery Drill.  They wanted to test the failover capability of their Multi-AZ RDS instance.  They initiated a reboot with failover for the instance and expected only a short outage while the standby replica was promoted and the DNS path was updated.  Unfortunately after the failover, they could not reach the database from their on-prem network despite the database being in an \"Available\" state.  Only when they initiated a second reboot with failover were they again able to access the database.  What is the most likely cause for this?","explanation":"The routes for all subnets in an RDS subnet group for a Multi-AZ deployment should be the same to ensure all master and stand-by units can be reached in the event of a failover.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/avoid-route-table-issues-rds/","title":"Avoid route table issues RDS Multi-AZ fail over"}],"answers":[{"id":"260ae29233a6047971894eff9d05fa55","text":"They initiated a failover with an IAM account that did not have sufficient rights to perform the reboot.  This resulted in an incomplete failover that was only corrected by executing the failover again to reset the DNS entries.","correct":false},{"id":"1dbb40d7bdb88fbe54fead30b1bf5f12","text":"There was a lag in the state update on the AWS console showing \"Available\".  If they would have waited longer, it likely would have changed to \"Degraded\".  A failover can take 30 minutes or more because AWS automatically creates a snapshot before promoting the standby.","correct":false},{"id":"f637d9886a1ea7b52d608046e233e504","text":"The subnets in the subnet group did not have the same routing rules.  The standby subnet did not have a valid route back to the on-prem network so the database could not be reached despite being available.","correct":true},{"id":"46b9b5188d94533190c30de816355165","text":"They used the AWS Console to issue the reboot.  You can only force a failover of RDS by using the AWS CLI and adding the --force-failover parameter to the \"aws rds reboot-db-instance\" command.","correct":false},{"id":"1105b24fe7a3c1eb0a86d0fa9a3cdc62","text":"This was most likely an AWS error.  They should submit a support ticket with the RDS instance identifier and the approximate time of the failover test via the Support Center.","correct":false}]},{"id":"4ebf4191-8562-4f67-945d-ea5aae2f9f26","domain":"awscsapro-domain3","question":"You are consulting for a client who is trying to define a comprehensive cloud migration roadmap.  They have a legacy custom ERP system written in RPG running on an AS400 system.  RPG programmers are becoming rare so support is an issue.  They run Lotus Notes email which has not been upgraded in years and thus out of support.  They do have a web application that serves as their CRM created several years ago by a consulting group.  It is a Java and JSP-based application running on Tomcat with MySQL as the data layer hosted on a Red Hat Linux server. The company is in a real growth cycle and realizes their current platforms cannot sustain them.  So, they are about to launch a project to implement SAP as a replacement for their legacy ERP system over the next year.  What migration strategy would you recommend for their landscape that would allow them to modernize as soon as possible?","explanation":"In this case, retiring Lotus Notes is the better move because it would just prolong the inevitable by simply migrating to EC2.  The CRM system is fairly new and can be re-platformed on Elastic Beanstalk.  Due to the impending ERP upgrade, it makes no sense to do anything with the legacy ERP.  It would take lots of work to port over an RPG application to run on AWS--if it's even possible.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud | AWS Cloud Enterprise  Strategy Blog"}],"answers":[{"id":"c7be9ce93a4a7a74b44e281cb697dcc4","text":"Begin a product search for a new CRM system that is cloud-ready.  Once identified, migrate the existing CRM into the new CRM system.  Migrate Lotus Notes to Workmail using the AWS Migration Hub.  Invest in training the IT staff about AWS through a Certified AWS Training Partner.  Provision and run the various SAP environments from scratch using EKS.  Do nothing to the legacy ERP until the SAP implementation is complete.","correct":false},{"id":"20ef21af4ab4ecda70e90e90861b154c","text":"Retire the Lotus Notes email and implement AWS Workmail.  Replatform the CRM application Tomcat portion to Elastic Beanstalk and the data store to MySQL RDS.  Invest time in training Operations staff CloudFormation and spend time architecting the landscape for the new SAP platform.  Do nothing to the legacy ERP platform until the SAP implementation is complete.  ","correct":true},{"id":"7579c185856bdbbcffbe49a649866488","text":"Rehost Lotus Notes mail on EC2 instances.  Refactor the CRM application to make use of Lambda and DynamoDB.  Use a third-party RPG to Java conversion tool to create Java versions of the legacy ERP to make it more supportable. Invest time in training developers continuous integration and continuous deployment concepts.  Because SAP implementations always take longer than estimated, rehost the legacy ERP system on EC2 instances so the AS400 can be retired.","correct":false},{"id":"480496ebe4100958e2f466291752ae2d","text":"Retire the CRM application and migrate the MySQL data over to Aurora.  Use QuickSight to provide access to the application for users.  Pay back support agreements to bring Lotus Notes back into support so it can be upgraded.  Migrate Notes email to EC2 instances.  Invest time in training Operations staff CloudFormation.  Create the complete SAP landscape as scriptable elements.  Do nothing to the legacy ERP platform until the SAP implementation is complete.","correct":false}]},{"id":"c2d46981-3dac-4e68-81d1-9eedf0cbf264","domain":"awscsapro-domain2","question":"Your company is preparing a special event for its 100th year in business.  As part of that event, the event committee would like to create a kiosk where employees can browse the thousands of photographs captured over the years of the employees and company events.  In a brainstorming session, one event staff member suggests the crazy idea of allowing employees to quickly pull up photographs which they appear in.  What AWS service might be able to make this a reality?","explanation":"AWS Rekognition is a service that can detect and match faces in a photograph.  The kiosk could include a camera that allows event-goers to snap a picture of themselves and then it could scan the photo archive for facial matches.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition – Video and Image - AWS"}],"answers":[{"id":"42e6f83b4b8205e6c2e62ddafdd3bbe3","text":"Amazon Chime","correct":false},{"id":"07c025c347c7483abdd039cd36be4220","text":"AWS Rekognition","correct":true},{"id":"cbe32e918d6a248e3e8ed74e6f0b72f6","text":"AWS Comprehend","correct":false},{"id":"ade161cea9b509c72570ba6ae5238e5f","text":"Kinesis for Video","correct":false},{"id":"438e06c02cba48ce4ffbf026d97488b4","text":"Amazon DeepView","correct":false}]},{"id":"951e49a8-9395-4b49-b623-1fb9e88a9639","domain":"awscsapro-domain5","question":"Your production web farm consists of a minimum of 4 instances.  The application can run on any instance type with at least 16GB of RAM but you have selected m5.xlarge in your current launch configuration.  You have defined a scaling policy such that you scale out when the average CPU across your auto scaling group reaches 70% for 5 minutes.  When this threshold is reached, your launch configuration should add more m5.xlarge instances.  You notice that auto scaling is not working as it should when your existing instances reach the scaling event threshold of 70% for 5 minutes.  Since you are deployed in a heavily used region, you suspect there are capacity issues.  Which of the following would be a reasonable way to solve this issue?","explanation":"If you do not have capacity reserved via a zonal RI or on-demand capacity reservation, it is possible that the AZ is out of available capacity for the type of instance you need.  You can reserve capacity or you can also increase the possible instance types in hopes that some other similarly equipped instance capacity is available.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html","title":"Types of Reserved Instances (Offering Classes) - Amazon Elastic Compute  Cloud"}],"answers":[{"id":"04211a7288d23e07315243e9b492dcb4","text":"Provision some zonal reserved instances of m5.xlarge to ensure you have capacity when you need it. ","correct":true},{"id":"a791a52f0967a913925bc47c7ae1c0a6","text":"Version the launch configuration to include additional instances types that also have at least 16GB of RAM.","correct":false},{"id":"989bc9eb46719d5cf9d9b69d191ee8d8","text":"Lower the CPU threshold to 60% so the scaling event triggers earlier and therefore has a better chance of getting resources.","correct":false},{"id":"04c8568f7761456b829145690b8d0cba","text":"Provision some regional reserved instances of m5.xlarge to ensure you have capacity when you need it. ","correct":false},{"id":"ea27906009e93ed8060db15ee28fa8ae","text":"Reduce the warm-up and cooldown period in the scaling policy to allow more time to provision resources.","correct":false}]},{"id":"083b20e3-95ff-4b8a-b655-aedf1de67c6c","domain":"awscsapro-domain4","question":"The security monitor team informs you that two EC2 instances are not compliant reported by an AWS Config rule and the team receives SNS notifications. They require you to fix the issues as soon as possible for security concerns. You check that the Config rule uses a custom Lambda function to inspect if EBS volumes are encrypted using a key with imported key material. However, at the moment the EBS volumes in the EC2 instances are not encrypted at all. You know that the EC2 instances are owned by developers but you do not know the details about how the instances are created. What is the best way for you to address the issue?","explanation":"The key must have imported key material according to the AWS Config rule. It should be a new key created in KMS. Existing KMS cannot import a new key material and AWS Managed Key such as aws/ebs cannot be modified either. CloudHSM is more expensive than KMS and is not required in this scenario. Besides, when the new encrypted EBS volume is attached, it should be attached to the same device name such as /dev/xvda1.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html","title":"How to import key material in AWS Key Management Service (AWS KMS)?"}],"answers":[{"id":"03c530623f9019c80c05daa34ad8fab1","text":"Create a new EBS key from CloudHSM with imported key material. Create a new EBS volume encrypted with the new key. Attach the volume to the EC2 instance. Use Linux dd command to copy data from non-encrypted volume to encrypted volume. Unmount the old volume after the sync is complete.","correct":false},{"id":"94975e581f509af38c350ecdd5b951f5","text":"Import a new key material to an existing Customer Managed Key (CMK) in KMS. Create an AMI from the EC2 instance. Then launch a new EC2 instance from the AMI. Encrypt the EBS volume in the new instance. Terminate the old instance after the new one is in service.","correct":false},{"id":"5a6b8e38c5d149a42e259d93323f12aa","text":"Modify the AWS Managed Key (AWS/EBS) in KMS to include an imported key material. Create a snapshot of the EBS volume. Then create a new volume from the snapshot with the volume encrypted. Detach the original volume and attach the new encrypted EBS to another device name of the instance.","correct":false},{"id":"62f46fb2afeb695bf73a050f1662cd44","text":"Create a Customer Managed Key (CMK) in KMS with imported key material. Create a snapshot of the EBS volume. Copy the snapshot and encrypt the new one with the new CMK. Then create a volume from the snapshot. Detach the original volume and attach the new encrypted EBS to the same device name of the instance.","correct":true}]},{"id":"ffae5615-188b-4023-aae2-71270158730a","domain":"awscsapro-domain5","question":"Several teams are using an AWS VPC at the same time. The VPC has three subnets (subnet-1a, subnet-1b, subnet-1c) in three availability zones (eu-west-1a, eu-west-1b, eu-west-1c) respectively. As there are more and more AWS resources created, there is a shortage of available IP addresses in subnet-1a and subnet-1b. The subnet subnet-1c in availability zone eu-west-1c still has plenty of IP addresses. You use a CloudFormation template to create an Auto Scaling group (ASG) and an application load balancer for the ASG. You enable three availability zones for the load balancer and the Auto Scaling group also spans all the three subnets. The CloudFormation stack usually fails to launch because there are not enough IP addresses. High availability is not required for your project since it is a proof of concept. Which of the following methods is the easiest one to resolve your problem?","explanation":"The easiest way is to enable only the eu-west-1c availability zone for ELB and launch ASG instances in subnet-1c. Only the IP addresses from eu-west-1c are required to create the resources. For a VPC, the existing CIDR cannot be modified and you also cannot add another CIDR IP range to an existing subnet. For an application load balancer, there is no IP balancing feature and IP addresses from a subnet cannot be reserved by other subnets.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-subnets.html","title":"Availability Zones for Your Application Load Balancer"}],"answers":[{"id":"9cb12513be8578fb017ebb7d1b302f9a","text":"Enable the IP balancing feature in the application load balancer so that the IP addresses are equally distributed among subnets. When elastic load balancer is created, available IP addresses in one subnet can be reserved by other subnets.","correct":false},{"id":"1f29e88227ce0771f9e43093bd53583a","text":"Add more IP addresses by extending the CIDR range for the VPC. Create a new subnet in each availability zone and reserve at least 128 IP addresses in a subnet. Modify the CloudFormation template to use the new subnets for the Auto Scaling group and ELB.","correct":false},{"id":"94887cfd8a06ae986d5721a940c3c52b","text":"Modify your CloudFormation template to enable only the availability zone eu-west-1c for the application load balancer and launch the Auto Scaling group in subnet-1c which belongs to eu-west-1c.","correct":true},{"id":"b8c040b89e07e5150d4a8a3899d41659","text":"Add another IPv4 CIDR to the VPC which should have at least 256 IP addresses. Add another IP CIDR block to subnet-1a and subnet-1b to increase the available IP addresses.","correct":false}]},{"id":"b00cb57f-7191-4f17-aa6d-ac687c418332","domain":"awscsapro-domain5","question":"You have a running EC2 instance and the name of its SSH key pair is \"adminKey\". The SSH private key file was accidentally put into a GitHub public repository by a junior developer and may get leaked. After you find this security issue, you immediately remove the file from the repository and also delete the SSH key pair in AWS EC2 Management Console. Which actions do you still need to do to prevent the running EC2 instance from unexpected SSH access?","explanation":"Although the SSH key pair is deleted in EC2, the public key content is still placed on the instance in an entry within ~/.ssh/authorized_keys. Someone can SSH to the instance if he has a copy of the leaked SSH private key. Users should not configure the instance to support another key pair as the old key pair still works. The correct method is deleting the instance immediately to prevent it from being compromised and launching another instance with a new SSH key pair. There is no need to use the AWS CLI command delete-key-pair as the key is already deleted from AWS EC2.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html","title":"Amazon EC2 Key Pairs"}],"answers":[{"id":"4f9620d4caefa25fafabc98f00c6b192","text":"Use AWS CLI delete-key-pair to completely delete the key pair so that no one can use it to SSH to the instance. Configure CloudWatch logs to monitor the SSH logging events and filter the logs with the SSH key ID to see if the key pair is still used by someone.","correct":false},{"id":"bc01758fc758191425928382b18697ca","text":"Create another SSH key pair via AWS EC2 or a third party tool such as ssh-keygen. Stop the instance and configure the instance with this new key pair in AWS Management Console. Restart the instance to activate the key pair.","correct":false},{"id":"a928b9732684f81d9ce046842965f1f6","text":"No action is required as the SSH key pair \"adminKey\" is already deleted from AWS EC2. Even if someone has the SSH private key, he still cannot use the key to access the instance.","correct":false},{"id":"ee7464becbe88068a5f848419e621bba","text":"Stop and terminate the instance immediately as someone can still SSH to the instance using the key. Launch a new instance with another SSH key pair. SSH to the EC2 instance using the new key.","correct":true}]},{"id":"58fa6440-b521-4dff-b6be-b1f8818c718d","domain":"awscsapro-domain2","question":"You currently manage a website that consists of two web servers behind an Application Load Balancer.  You currently use Route 53 as a DNS service.  Going with the current trend of websites doing away with the need to enter \"www\" in front of the domain, you want to allow your users to simply enter your domain name. What is required to allow this?","explanation":"Route 53 allows you to create a record for a zone apex.  In this case, we have created an alias record for the ALB.","links":[{"url":"https://docs.aws.amazon.com/govcloud-us/latest/ug-west/setting-up-route53-zoneapex-elb.html","title":"Setting Up Amazon Route 53 Zone Apex Support with an AWS GovCloud (US-West)  Elastic Load Balancing Load Balancer - AWS GovCloud (US-West) User Guide"}],"answers":[{"id":"5c48fe59299d582ae6074d269711de20","text":"Route 53 does not currently support zone apex records.  You must use a third-party DNS provider.","correct":false},{"id":"f70aff68786523bc13fd44cb89feb502","text":"Create an A record for your top-level domain name using the public IP of your ALB.","correct":false},{"id":"0dc522eb50b1c1320900ac02fcfb4dd6","text":"Create an S3 bucket as a static web host.  Create simple HTML file that redirects to the www subdomain.  Use CloudFront custom origins as a front-end for your top-level domain name.","correct":false},{"id":"9e87834a987d5b3556ce3e9f35ac6a82","text":"Create an A record for your top-level domain name as an alias for the ALB.","correct":true},{"id":"935c6d54a7555f482da11c1ded5dfc9c","text":"Create a CNAME record for the root domain and configure it to resolve to www subdomain name.","correct":false}]},{"id":"d58acd29-561d-4017-a55d-bcff8eb40f90","domain":"awscsapro-domain4","question":"Last month's AWS service cost is much higher than the previous months. You check the billing information and find that the used hours of Elastic Load Balancer (ELB) increases dramatically. You manager asks you to plan and control the ELB usage. When the ELB service has been used for over 5000 hours in a month, the team should get an email notification immediately and further actions will be taken accordingly. Which of the following options is the easiest one for you to choose?","explanation":"AWS Budgets include the types of cost budget, usage budget, reservation budget and savings plan budget. The usage budget enables you to plan the usage of ELB service and receive budget alerts when the actual usage becomes more than a threshold (5000 hours in this scenario). The cost budget type is incorrect as it evaluates the cost instead of usage and you cannot receive budget alerts from either Cost Explorer or AWS Config.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing your costs with Budgets"}],"answers":[{"id":"15fe48e7010a90e06774d9aa9668704a","text":"Create a usage budget in AWS Budgets. Check the ELB running hours for every month and set the budgeted amount to be 5000 hours. Configure an email alert or an Amazon Simple Notification Service (SNS) notification when the actual usage is more than the threshold.","correct":true},{"id":"baf09736ecf556311eb6e77579877ccd","text":"In AWS Config, monitor the usage of all ELB resources within the AWS account. Create a custom Config rule via a Lambda function that calculates the ELB usage and sends an alert message to an SNS topic when the usage is over 5000 hours.","correct":false},{"id":"39f1073c8a5f51031cb743e7cf45dce2","text":"Calculate the estimated ELB cost when the total ELB usage is 5000 hours in a month. Configure a cost budget in AWS Budgets for the EC2 ELB service and set the number as the threshold. When the cost is greater than the user-defined threshold, send an email alert to the team.","correct":false},{"id":"0f88bd3ee9fd3724922aa89baa9f656d","text":"Launch the Cost Explorer in AWS billing dashboard, filter the EC2 ELB service and configure a CloudWatch alert to track its actual monthly usage. When the monthly ELB usage grows more than 5000 hours, raise the CloudWatch alert and notify an SNS topic.","correct":false}]},{"id":"872cd65b-287b-4fdb-b59f-f07f7bff707f","domain":"awscsapro-domain5","question":"Your client is a small engineering firm which has decided to migrate their engineering CAD files to the cloud.  They currently have an on-prem SAN with 30TB of CAD files and growing at about 1TB a month as they take on new projects.  Their engineering workstations are Windows-based and mount the SAN via SMB shares.  Propose a design solution that will make the best use of AWS services, be easy to manage and reduce costs where possible. ","explanation":"At present, EFS doesn't support Windows-based clients.  Storage Gateway-File Gateway does support SMB mount points.  The other options introduce additional unneeded costs.","links":[{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/limits.html","title":"Amazon EFS Limits - Amazon Elastic File System"}],"answers":[{"id":"340da8cc24330ac5b143b526c880e4f7","text":"Order a Snowball appliance to migrate the bulk of the data.  Setup an EFS share on AWS and configure the CAD workstations to mount via SMB.  ","correct":false},{"id":"9c4821e0d9178e80636a5d4c7d0c6441","text":"Setup Storage Gateway-File Gateway and configure the CAD workstations to mount as iSCSI.  Use a Snowball appliance to sync data daily to S3 buckets at AWS.","correct":false},{"id":"c15496feac5aa7ce58d0c5d4813a5a29","text":"Use AWS CLI to sync the CAD files to S3.  Setup Storage Gateway-File Gateway locally and configure the CAD workstations to mount as SMB.","correct":true},{"id":"7cad520a624f4c3b174014f339f732df","text":"Use AWS CLI to sync the CAD files to S3.  Use EC2 and EBS to create an SMB file server.  Configure the CAD workstations to mount the EC2 instances.  Setup Direct Connect to ensure performance is acceptable.","correct":false},{"id":"f52e177dc6a594ed2c0852c91b6133d3","text":"Order a Snowmobile to migrate the bulk of the data.  Setup S3 buckets on AWS to store the data.  Use AWS WorkDocs to mount the S3 buckets from the engineering workstations.","correct":false}]},{"id":"e720cd54-de67-42de-ba10-593dee0582e6","domain":"awscsapro-domain3","question":"You are the Enterprise Architect in a Risk Quantification firm. The firm has a website which end-users can use to apply for loans and also track the status of their loan application if they log in. When a loan application comes in, several downstream systems need to independently process the application. Right now, the website server-side code invokes these systems one after the other, synchronously, in a tight loop. If one of these downstream systems times out or throws an exception, the entire loan application processing errors out. Even if none of these downstream systems fail, the time it takes to process a loan application is very high due to the serial nature of these systems being invoked. Your CTO wants only the loan-processing application moved to the AWS cloud and re-architected at the same time.\nThe downstream systems are all hosted on-premises and will continue to remain on-premises. They expose REST endpoints that accept POST HTTPS requests, use self-signed certificates and respond synchronously only when they are done processing an application. After re-architecture, all downstream systems must independently start processing an incoming loan application simultaneously.\nYour CTO wants to know how the loan-processing website application can be architected in the AWS Cloud, and what supporting changes will be needed in the downstream systems on-premises. He wants to minimize code changes to the downstream on-premises systems. Choose the best option","explanation":"This is an example of a verbose question with verbose answer choices. You can expect a few such questions in the exam, testing your time management skills. Try to vertically scan the answers to see which parts differ between them. Sometimes, though the answers seem big, a large part of each is identical. You can ignore those parts, as there is nothing to choose between the.\nAmong the four choices, two use SQS and two use SNS to feed the incoming loan applications to the downstream systems. You cannot automatically eliminate either SQS or SNS, as a working solution can be designed with either.\nLet us see how we can achieve this using SNS first. The basic requirement here is fan-out - a single loan application must be processed by several downstream systems, so there are multiple consumers. Hence, SNS is a natural fit. SNS supports multiple subscribers for a topic. SNS also supports HTTP/HTTPS subscribers. SNS makes POST REST API call to as many HTTP/HTTPS subscribers exist on the topic, so it fits the bill. However, there is a small problem - the requirement states that the downstream systems must be changed as little as possible. If we follow this design, we must change the HTTP Listening part of the downstream systems significantly. Because SNS is directly calling them now, SNS will use its own headers and body format. In fact, SNS POST-s two kinds of messages - one is Subscription Confirmation and one is Notification. A special HTTP header (x-amz-sns-message-type) has the right type in its value. The server side now must parse this header out and look for only the Notification type of message. The body itself will then be JSON formatted with the payload. While the server is probably used to process just the core payload (loan application data) as the HTTP body, the same will now be hidden inside a JSON field called Message inside the request body. Additionally, the downstream systems will have to deal with SNS retries, thus the loan application part must be made idempotent (if the same loan application lands twice, it will ignore the duplicates). Thus, though it is technically possible to design the solution using SNS, it will result in a lot of changes in the downstream systems. Hence, though the SNS option will work, it is not the correct answer because of this reason.\nNow, let us see how we can design this using SQS. While SQS does not support fan-out (multiple consumers for the same message), the proposed solution uses a Lambda function to achieve fan-out. The Lambda function will pick up the message, and then call the downstream systems one by one. The key to making this work is, of course, to modify the downstream systems from synchronous monolithic beasts to asynchronous servers so that they can instantly respond to the Lambda function and then continue to process the application. We will then have to provide a callback for when it is done. The solution uses an API Gateway for that purpose. Overall, the solution is elegant, and changes to the downstream systems are less than what SNS requires. Hence, SQS is the correct answer.\nNote that one version of the SNS design proposes to retain the synchronous nature of the downstream systems. That will not work as SNS will not wait more than 15 seconds for a response. The response will then be lost and the main website app will never know the results from the downstream systems.\nAlso, note that though SNS requires the HTTPS subscriber to present a trusted CA-signed certificate, there is no such requirement for Lambda because Lambda is basically your code, you can decide to trust anyone.","links":[{"url":"https://docs.aws.amazon.com/sns/latest/dg/sns-http-https-endpoint-as-subscriber.html","title":"Using Amazon SNS for System-to-System Messaging with an HTTP/S Endpoint as a Subscriber"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","title":"Using AWS Lambda with Amazon SQS"}],"answers":[{"id":"eec2740374df8038093d636a17252168","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Override the default delivery policy on the subscriber endpoint to remove retries so that downstream systems do not have to worry about synchronous responses taking time or idempotency of retries. Make the following changes in the downstream systems - (a) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (b) Procure server certificates from a trusted Certificate Authority (CA) instead of using the self-signed certificate as SNS will not be able to POST to a server with a self-signed certificate","correct":false},{"id":"3d032e65493c0733ebe65683fb66a562","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems and (c) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as your Lambda function will not be able to POST to a server with self-signed certificate","correct":false},{"id":"48d42d3290142cbfc8207e042690b35f","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting so that SNS does not retry, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (c) Make them idempotent for the same loan application as SNS may retry in case of lost messages or timeouts (d) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as SNS will not be able to POST to a server with self-signed certificate","correct":false},{"id":"3fb725a8e71fa96168f18e50a146b4f0","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed and (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems","correct":true}]},{"id":"c01346c8-d230-4b52-b53d-78cdbfbc7794","domain":"awscsapro-domain4","question":"You work for an Insurance Company as an IT Architect. The development team for a microservices-based claim-processing system has created containerized applications to run on ECS. They have spun up a Fargate ECS cluster in their development VPC inside a private subnet. The containerized application uses awslogs driver to send logs to Cloudwatch. The ECS task definition files use private ECR images that are pulled down to ensure that the latest image is running always. The cluster is having connectivity problems as it cannot seem to connect with ECR to pull the latest images and neither can it connect with Cloudwatch to log. The development team has approached you to help troubleshoot the issue. What is a possible reason for this and what is the best way to fix it?","explanation":"ECS Fargate clusters can be deployed in a private subnet. Hence, we can safely eliminate the choice that says that ECS Fargate clusters must be deployed in a public subnet only.\nECS Fargate clusters do not need the user to control the ECS Agent Version on the nodes, as Fargate is serverless by design. Hence, we can safely eliminate the choice that deals with ECS Agent Version.\nThis leaves two options. One proposes using NAT Gateway. The other proposes using ECS Interface VPC Endpoint. Both are working solutions. However, one of them makes a false claim - it states that ECS Fargate clusters connect to ECR or Cloudwatch only over the internet. That is not true, as it can connect either using a public or a private network. Hence, the only fully correct choice is the one that uses ECS Interface VPC Endpoint","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/vpc-endpoints.html","title":"Amazon ECS Interface VPC Endpoints (AWS PrivateLink)"},{"url":"https://aws.amazon.com/blogs/compute/setting-up-aws-privatelink-for-amazon-ecs-and-amazon-ecr/","title":"Setting up AWS PrivateLink for Amazon ECR"}],"answers":[{"id":"d73a8a48aaef0ddbe725f06264c3c6a3","text":"ECS Fargate clusters can connect to ECR (to pull down latest private images) or Cloudwatch (to log) only over the internet. Hence, if it is deployed in a private subnet, it needs a route to a NAT Gateway which must be connected to an Internet Gateway. To fix this issue, deploy a NAT Gateway in a public subnet of the VPC and add appropriate routes to the Routing Table","correct":false},{"id":"43901acf8308ae39f9cf7006afa72751","text":"ECS Fargate clusters can connect to ECR (to pull down latest private images) or Cloudwatch (to log) using either private or public network. Hence, if it is deployed in a private subnet, deploy an ECS Interface VPC Endpoint in the same subnet for connecting to internal services. Add appropriate routes to the Routing Table","correct":true},{"id":"f54cb63446f49ab92455024f17ae67b8","text":"ECS Fargate clusters must be deployed in a public subnet so that it can use the Internet Gateway to communicate with ECR or Cloudwatch. To fix this, redeploy the cluster in a public subnet","correct":false},{"id":"d103c9eb11e007e2a9410cfb2e1bc1ba","text":"The version of the ECS agent may be too old. To fix this, upgrade the ECS Agent version in the cluster nodes to be compatible with connectivity requirements","correct":false}]},{"id":"95f1d7a8-c3d4-4fec-952a-72385aa8b4c8","domain":"awscsapro-domain5","question":"You are consulting for a company that performs specialized customer data analytics.  Their customers can upload raw customer data to a website and receive back demographic statistics.  Their application consists of a REST API created using PHP and Apache.  The application is self-contained and works in real-time to return results as a JSON response to the REST API call.  Because there is customer data involved, company policy states that data must be encrypted in transit and at rest.  Sometimes, there are data quality issues and the PHP application will throw an error.  The company wants to be notified immediately when this occurs so they can proactively reach out to the customer.  Additionally, many of the company's customers use very old mainframe systems that can only access internet resources using IP address rather than a FQDN.  Which architecture will meet these requirements fully?","explanation":"The requirement of a static IP leads us to a Network Load Balancer with an EIP.","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/","title":"Elastic Load Balancing Features"}],"answers":[{"id":"0c8e5c32f081b0484e86b71651ae3642","text":"Provision a Network Load Balancer in front of your EC2 target group and terminate SSL at the load balancer using Certificate Manager.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"e53df806e37b325d7f61be27772875f1","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Enabled SSL termination on the API Gateway using Certificate Manager.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"434ce04b2c3a4d2e679d37df43de2585","text":"Provision an Application Load Balancer with an EIP in front of your EC2 target group and terminate SSL at the ALB.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"a9ed133e35b8332aea2bf603521b891a","text":"Provision an Application Load Balancer in front of your EC2 target group and offload SSL to CloudHSM.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch and configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from CloudHSM.","correct":false},{"id":"569eec0061a1a97be77e3bdab43a1756","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Offload SSL termination using AWS KMS.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"5e4c5230c7e08202a0ea0575d5412d57","text":"Provision a Network Load Balancer with an EIP in front of your EC2 target group.  Install the CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.  Terminate SSL on the EC2 instances.","correct":true}]},{"id":"0906c4cf-83a1-4cec-b2ab-c010dcdee73f","domain":"awscsapro-domain1","question":"The alternative energy company you work for has four different business units, each of which would like to run workloads on AWS. Each business unit has it's own AWS account, and a shared services AWS account has been created. An established process for tracking software license usage exists for on-premises applications, but the finance department has concerns that the self-serve nature of the cloud may result in license overages for applications deployed on AWS. You've been tasked with setting up a governance model whereby users are only given access to a standard list of products. Which architecture will provide an effective way to implement the governance requirements and manage software license usage on AWS?","explanation":"AWS Service Catalog allows organizations to create and manage catalogs of approved products for use on AWS. Products are defined as CloudFormation Templates. Software license information can be associated with Service Catalog products through tags. AWS Step Functions can orchestrate the process of incrementing usage counts and notifying of over-usage situations when products are launched by users. AWS License Manager is a robust solution for managing software licenses, but it needs to be coupled with Service Catalog to meet the requirement for limiting access to a standard set of products. A Lambda trigger is not currently available for Service Catalog product deployments. Elastic Container Registry provides tagging at the repository level, not at the individual container image level.","links":[{"url":"https://aws.amazon.com/servicecatalog/","title":"AWS Service Catalog"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://aws.amazon.com/blogs/mt/tracking-software-licenses-with-aws-service-catalog-and-aws-step-functions/","title":"Tracking software licenses with AWS Service Catalog and AWS Step Functions"}],"answers":[{"id":"e2441352bdc2d4db956149f0a10b6738","text":"Create Docker images for each of the standardized applications that will be deployed and register them with Amazon Elastic Container Registry (ECR). Populate ECR tags with software license metadata. Create an Amazon DynamoDB table to store software license usage counts. Whenever a container is launched in Amazon Elastic Container Service, trigger an AWS Lambda function to increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"33db838110cfb8b002590cbff630b825","text":"Create Amazon Machine Images for all of the instance configurations that will be deployed. Implement AWS License Manager license configurations and attach them to the AMIs. Create AWS CloudFormation StackSets for the AMIs in the shared AWS account and make them available to users in each business unit","correct":false},{"id":"62788b95bb6b249e6511d14023a20364","text":"Implement AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Create an Amazon DynamoDB table to store software license usage counts. Trigger an AWS Lambda function to run each time a Service Catalog product is launched. Have the Lambda function increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"41d11d1142468e8b0d2a29674d1eaa2c","text":"Deploy AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Populate Service Catalog product tags with software license information. Create an Amazon DynamoDB table to store software license usage counts. Have Amazon CloudWatch detect when a user deploys a Service Catalog product. Launch an AWS Step Functions process to increment license counts in the DynamoDB table, and send notifications when overage thresholds are met","correct":true}]},{"id":"34351bd0-7925-4246-bb61-c64bbf4d5baf","domain":"awscsapro-domain4","question":"An application in your company that requires extremely high disk IO is running on m3.2xlarge EC2 instances with Provisioned IOPS SSD EBS Volumes. The EC2 instances have been EBS-optimized to provide up to 8000 IOPS. During a period of heavy usage, the EBS volume on an instance failed, and the volume was completely non-functional. The AWS Operations Team restored the volume from the latest snapshot as quickly as possible, re-attached it to the affected instance and put the instance back into production. However, the performance of the restored volume was found to be extremely poor right after it went live, during which period the latency of I/O operations was significantly high. Thousands of incoming requests timed out during this phase of poor performance.\nYou are the AWS Architect. The CTO wants to know why this happened and how the poor performance from a freshly restored EBS Volume can be prevented in the future. Which answer best reflects the reason and mitigation strategy?","explanation":"Data gap cannot be the reason for high disk I/O latency. Whether the data being requested is on the disk or not cannot be responsible for the extended period of high disk I/O latency, as all operating systems index the contents in some way. They do not scan the whole disk to conclude that something is missing. Hence, the choice that suggests data gap as the reason is eliminated.\nEBS Optimization works straight away after a freshly restored volume is attached to an EBS optimized instance. Hence, the choice that suggests that EBS Optimization takes some time to kick in is eliminated.\nThere is nothing called set-up-cache command. The option that suggests that there is an inbuilt caching mechanism that needs to be activated is completely fictional, and is eliminated.\nThe only correct option is the one that correctly states that every new block read from a freshly restored EBS Volume must first be downloaded from S3. This is because EBS Snapshots are saved in S3. Remember that EBS Snapshots are incremental in nature. Every time a new snapshot is taken, only the data that changed is written to that particular snapshot. Internally, it maintains the pointers to older data that was written to S3 as part of previous snapshots. These blocks of data continue to reside on S3 even after an EBS Volume is restored, and is read the first time they are accessed. Linux utilities like dd or fio can be used after restoring an EBS Volume to read the whole volume first to get rid of this latency problem when the instance is put back in production.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-initialize.html","title":"Initializing Amazon EBS Volumes"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html","title":"Amazon EBS–Optimized Instances"}],"answers":[{"id":"ae32d192c3829287819a74ded72e0da7","text":"The latest snapshot did not have the most current data. It only had the data from the last time a snapshot was taken. The requests timed out because of this data gap. To mitigate this, increase the frequency of taking EBS snapshots.","correct":false},{"id":"d3a1c3b2669127dabe2eaf2c490fcd30","text":"A freshly restored EBS Volume needs pre-warming to activate the inbuilt caching mechanism. To fix this, update the restoration process to run the set-up-cache command on the freshly restored EBS Volume first before the instance is put back in production. Also, include random I/O tests to ensure that desired I/O levels are reached before putting the instance back to production.","correct":false},{"id":"51e87b2dbdfcb476b026779380119b06","text":"A freshly restored EBS Volume cannot utilize EBS Optimization Instances straight away, as the network traffic and EBS traffic traverse the same 10-gigabit network interface. Only after the entire volume is scanned by an asynchronous process, EBS Optimization kicks in. This increases the I/O latency until the volume is ready to utilize EBS Optimization. To fix this, update the restoration process to wait and run random I/O tests on a freshly restored EBS Volume. Put the instance back to production only after the desired I/O levels are reached.","correct":false},{"id":"1a47200401a925a2ad1df3d3286e26dc","text":"When a data block is accessed for the first time on a freshly restored EBS Volume, EBS has to download the block from S3 first. This increases the I/O latency until all blocks are accessed at least once. To fix this, update the restoration process to run tools to read the entire volume before putting the instance back to production.","correct":true}]},{"id":"cb24982e-2c2d-43d4-872f-2dabbdb7e367","domain":"awscsapro-domain2","question":"You are helping an IT Operations group transition into AWS.  They will be created several instances based off the latest Amazon Linux 2 AMI.  They are unsure of the best way to enable secure connections for all members of the group and certainly do not want to share credentials. Which of the following methods would you recommend?","explanation":"Of the provided options, the only one that upholds AWS best practices for providing secure access to EC2 instances is to use AWS Session Manager.  ","links":[{"url":"https://aws.amazon.com/blogs/aws/new-session-manager/","title":"New – AWS Systems Manager Session Manager for Shell Access to EC2 Instances  | AWS News Blog"}],"answers":[{"id":"cce617457cd8701c595d236b7fa4ed7c","text":"Allow each administrator to create their own SSH keypair and assign them all to the SSH Key for the instance upon each launch.","correct":false},{"id":"c594a26a8a9141cdd5615a0761fb2438","text":"Create a bastion host and use it like a jump-box.  Paste each administrators private key into the known_hosts file on the bastion host.","correct":false},{"id":"e36f501af80c8f03e08321d565cc900e","text":"Share the single private SSH key with each administrator in the group.","correct":false},{"id":"93ecb84b9fc90ec0bc4db84968ab5ebf","text":"Allow administrators to update the SSH key of the instance in the AWS console each time they need access to a system.","correct":false},{"id":"b1ab01927f47a067694506df9d5249e3","text":"Configure IAM role access for AWS Systems Manager Session Manager.","correct":true}]},{"id":"c3ac5de9-a343-4cde-af1b-6c9f89824d2f","domain":"awscsapro-domain5","question":"An external auditor is reviewing your process documentation for a Payment Card Industry (PCI) audit.  The scope of this audit will extend to your immediate vendors where you store, transmit or process cardholder data.  Because you do store cardholder data in the AWS Cloud, the auditor would like to review AWS's PCI DSS Attestation of Compliance and Responsibility.  How would you go about getting this document? ","explanation":"AWS Artifact provides on-demand downloads of AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use. You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls.","links":[{"url":"https://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html?icmpid=docs_artifact_console","title":"What Is AWS Artifact? - AWS Artifact"}],"answers":[{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":true},{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false},{"id":"d7cb47dd1f374d3ed079b14cc6f2cd75","text":"Submit a Support Case requesting the document","correct":false},{"id":"63df0d05cd43af35c95cf04d92aaf685","text":"AWS Legal Services website","correct":false},{"id":"1d16d307ee989a80e421198a01993a9c","text":"AWS IAM Console","correct":false},{"id":"d9208942349d1c6f7dbaba3661069bc1","text":"AWS WorkDocs","correct":false},{"id":"09e838e873f25f954fef911d50b3d1ab","text":"AWS Pinpoint","correct":false}]},{"id":"a2fb4f91-4c73-4080-bbf3-6d07a1b2ce03","domain":"awscsapro-domain2","question":"You have been asked to investigate creating a production Oracle server in RDS.  You need to choose the correct options that will allow you to run the latest version of Oracle 12c with High Availability.  You do not currently have any Oracle licenses. Which of the below are valid options?","explanation":"To get to the correct answer, you must first disregard any option with Oracle Data Guard as this is not available in RDS, then remove any answer containing the editions SE or SE1 as they only allow version 11g to be deployed, not 12c.  The remaining two options are correct as they allow High Availability, 12c and either a Bring-You-Own or licence included option, so you can ensure you get the best value.","links":[{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"Amazon RDS for Oracle FAQs"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Oracle.html","title":"Oracle on Amazon RDS"}],"answers":[{"id":"5d1af18d1bfbdb6d990f138d844f75dc","text":"Choose either the SE or SE1 editions, Bring-Your-Own-Licence and Oracle Data Guard","correct":false},{"id":"0cb21536be9ea582178c7e54e365bd33","text":"Choose either the SE or SE1 editions, purchase your own licenses from Oracle and enable Multi-AZ","correct":false},{"id":"1ed8c7e073924060dbe648fbdf8a9c17","text":"Choose the SE2 edition with licence included and enable Multi-AZ","correct":true},{"id":"0d9a63f8cb9a74845f363b450fa5cf11","text":"Purchase your own licenses from Oracle, Choose either the Enterprise or SE2 editions, Bring-Your-Own-Licence and enable Multi-AZ","correct":true}]},{"id":"0abe2292-3f6e-47e1-93d9-6af24d5ea4c2","domain":"awscsapro-domain4","question":"A graphic design company has purchased eighteen m5.xlarge regional Reserved Instances and sixteen c5.xlarge zonal Reserved Instances. They receive their monthly AWS bill and find the invoice amount to be significantly higher than expected. Upon investigation, they discover RI discounted and non-discounted charges for nine m5.xlarge instances, nine m5.2xlarge instances, eight c5.xlarge instances, and eight c5.2xlarge instances. The business will need all of this capacity for at least the next twelve months. As their consultant, what would you advise them to do to maximize and monitor their RI discounts?","explanation":"Regional Reserved Instances allow for application of RI discounts within instance families, so all of the m5 instances are covered. Zonal Reserved Instances only provide discounts for specific instance types and sizes. So purchase of additional RIs would lower costs on the eight c5.2xlarge instances. Unused Reserved Instances are contractual and cannot be cancelled, so looking for another place to use them is the right approach. They could possibly be sold on the Reserved Instance Marketplace. AWS Budgets reservation budgets provide visibility and alerting on RI coverage specifically. Cost budgets and usage budgets may be useful, but they won't target RI coverage specifically.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html","title":"Regional and Zonal Reserved Instances (Scope)"},{"url":"https://aws.amazon.com/blogs/aws-cost-management/launch-instance-family-coverage-budgets/","title":"Launch: Instance Family Coverage Budgets"}],"answers":[{"id":"8e0e716570fad7bcaf6fe8bebe4f9d49","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Cancel the Reserved Instances for the nine unused m5.xlarge instances and eight unused c5.xlarge instances. Create an AWS Budgets cost budget that sends notification whenever costs exceed 80% of usage expectation","correct":false},{"id":"8c1f6db4cd04a86fd43efa1bc97640ea","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances and eight c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":false},{"id":"4b6d77d8423cd16b8e711b482dbcafbf","text":"Purchase an additional nine c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":true},{"id":"1c87a2204ad8da355c8ca0890aa06785","text":"Purchase an additional nine m5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances. Create an AWS Budgets usage budget that sends notification whenever RI coverage drops below 60%","correct":false}]},{"id":"2c204ae8-6bba-49a1-b8f6-1aa4330c3d8c","domain":"awscsapro-domain5","question":"You are helping an IT organization meet some security audit requirements imposed on them by a prospective customer.  The customer wants to ensure their vendors uphold the same security practices as they do before they can become authorized vendors.  The organization's assets consist of around 50 EC2 instances all within a single private VPC.  The VPC is only accessible via an OpenVPN connection to an OpenVPN server hosted on an EC2 instance in the VPC.  The customer's audit requirements disallow any direct exposure to the public internet.  Additionally, prospective vendors must demonstrate that they have a proactive method in place to ensure OS-level vulnerability are remediated as soon as possible.  Which of the following AWS services will fulfill this requirement?","explanation":"AWS Macie is a service that attempts to detect confidential data rather than OS vulnerabilities.  Since there is no public internet access for the VPC, services like GuardDuty and Shield have limited usefulness. They help protect against external threats versus any OS-level needs.  AWS Artifact is simply a document repository and has no monitoring functions.  Only AWS Inspector will proactively monitor instances using a database of known vulnerabilities and suggest patches.","links":[{"url":"https://aws.amazon.com/inspector/faqs/","title":"FAQs - Amazon Inspector - Amazon Web Services (AWS)"},{"url":"https://aws.amazon.com/macie/","title":"Amazon Macie | Discover, classify, and protect sensitive data | Amazon Web  Services (AWS)"}],"answers":[{"id":"7ed3972608faa6c3dfc6fda7f151889c","text":"Enable AWS GuardDuty to monitor and remediate threats to my instances.","correct":false},{"id":"45d5e166ed185c1f7516650c714423dd","text":"Enable AWS Artifact to periodically scan my instances and prepare a report for the auditors.","correct":false},{"id":"6a353f53758a3fd632209b5286a01086","text":"Enable AWS Shield to protect my instances from unauthorized access.","correct":false},{"id":"81133f0650fa1ca2fbe1b920a6c67cc9","text":"Employ Amazon Inspector to periodically assess applications for vulnerabilities or deviations from best practices.","correct":true},{"id":"178912f5fbd90ab710621756a2ba18ff","text":"Employ AWS Macie to periodically assess my instances for vulnerabilities and proactively correct gaps.","correct":false}]},{"id":"401cbed4-e977-4303-9344-586af01a4180","domain":"awscsapro-domain2","question":"You have been contracted by a manufacturing company to create an application that uses DynamoDB to store data collected in a automotive part machining process.  Sometimes this data will be used to replay a process for a given serial number but that's always done within 7 days or so of the manufacture date.  The record consists of a MACHINE_ID (partition key) and a SERIAL_NUMBER (sort key).  Additionally, there is a CREATE_TIMESTAMP attribute that contains the creation timestamp of the record and a DATA attribute that contains a BASE64 encoded stream of machine data.  To keep the DynamoDB table as small as possible, the industrial engineers have agreed that records older than 30 days can be purged on a continual basis.  Given this, what is the best way to implement this with the least impact on provisioned throughput.","explanation":"Using DynamoDB Time to Live feature is a perfect way to purge out old data and not consume any WCU or RCU.  Other methods of deleting records would impact the provisioned capacity units.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","title":"Time To Live - Amazon DynamoDB"}],"answers":[{"id":"081f76fb4967f9d10a3799ae400ad898","text":"Update the table to add a attribute called EXPIRE  Change the application to store EXPIRE as CREATE_TIMESTAMP + 30 days.  Enable Time to Live on the DynamoDB table for the EXPIRE attribute.","correct":true},{"id":"7c29d6c8effb51be4df54033ce45d01f","text":"Enabled Lifecycle Management on the DynamoDB table.  Create a rule that deletes any records where CREATE_TIMESTAMP attribute is greater than 30 days old.","correct":false},{"id":"41f68b5f48ef97cc437ebfe50ae10882","text":"Use AWS Batch to execute a daily custom script which queries the DynamoDB table and deletes those records where CREATE_TIMESTAMP is older than 30 days.  ","correct":false},{"id":"ef62d6ec88d117a0ac0cb7c99cd1abbd","text":"Use DynamoDB Streams to trigger a Lambda function when the record ages past 30 days.  Use the DynamoDB SDK in the Lambda function to delete the record.","correct":false},{"id":"0a945c4865c940ffaddaeade6f6bbdaf","text":"Use Step Functions to track the lifecycle of DynamoDB records.  Once 30 days has elapsed, branch to a Delete step and trigger a Lambda function to remove the record.","correct":false}]},{"id":"e3a59454-94fa-4b98-8d8a-80882a7d0e30","domain":"awscsapro-domain5","question":"You are setting up a new EC2 instance for an ERP upgrade project.  You have taken a snapshot and built an AMI from your production landscape and will be creating a duplicate of that system for testing purposes in a different VPC and AZ.  Because you will only be testing an upgrade process on this new landscape and it will not have the user volume of your production landscape, you select an EC2 instance that is smaller than the size of your production instance.  You create some EBS volumes from your snapshots but when you go to mount those on the EC2 instances, you notice they are not available.  What is the most likely cause?","explanation":"In order to mount an EBS volume on an EC2 instance, both must be in the same AZ.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html","title":"Amazon EBS Volumes - Amazon Elastic Compute Cloud"}],"answers":[{"id":"c2f8c791750ad6ce4c8c41ac45b246a0","text":"You have reached your account limit for EBS volumes.  You will need to create a support ticket to request an increase to the limit.","correct":false},{"id":"7a11f6b6797c8dd005c9ce25c77a37fe","text":"An SCP applied to the account you are in has restricted you from attaching EBS volumes to instances outside the original VPC","correct":false},{"id":"5a9ffc3875f9e3f50c9fc2684a6006b2","text":"The original volume is encrypted and you failed to check the encryption flag when creating the new volume.","correct":false},{"id":"a9e147ffe118e514fd5069020e86cca6","text":"The instance that you selected for your testing landscape is too small.  It must be equal to or larger than the source of the AMI.","correct":false},{"id":"134c64ea3d25d70a667400e778a13c1a","text":"You created them in a different availability zone than your testing EC2 instance.","correct":true}]},{"id":"1aa1b2b9-bb81-44d6-bcee-ec9408859b70","domain":"awscsapro-domain4","question":"A small business owner owns two businesses - a restaurant and a pet grooming facility. Each of her businesses has its own website, respectively www.xyz-restaurant.com and www.pqr-pet-grooming.com, both supporting only HTTP (no HTTPS). One single EC2 instance on AWS serves both the websites. This Ec2 instance has an Elastic IP Address attached to it, with two Route 53 A records pointing at it. The owner wants to add credit card payment processing to both her websites. As a result, she wants to ensure that both sites are served on HTTPS. Also, due to the popularity of her businesses, she needs to run a second EC2 instance which will be an exact copy of the existing one. She plans to create the second instance from the AMI snapshot of the first one, hoping to not needing to make any website configuration or code changes on any of the instances. She wants the traffic for each site to be equally or randomly split between the two servers though all requests from the same user-session must reach the same server.\nAs an AWS architect, what approach would you recommend for functionality as well as cost-effectiveness?","explanation":"This question tests several aspects of Load Balancers, knowledge of SNI and SAN, Sticky Sessions and Route 53.\nApplication Load Balancer supports SNI - hence it can deal with multiple SSL certificates per Listener. Thus, the option that says that a single ALB can handle a single SSL certificate only is incorrect. Also, as the question states that no code or website configuration changes can be done, we cannot terminate SSL at the EC2 instances - as doing so would surely need changes to the website configuration if not both code and website configuration. This eliminates the options that propose the Classic Load Balancer or a completely Load-Balancer-less solution.\nThe Classic Load Balancer option would technically work as long as the website configuration can be updated to terminate SSL at the EC2 instance. In fact, that would be the lowest cost solution from a Load Balancer perspective as CLB costs less than ALB - however, terminating SSL at the EC2 instances usually requires greater compute capacity, so the cost of EC2 instance could go up slightly. Note that the CLB option correctly states that SAN must be used with CLB, as CLB does not support SNI. Also, when SAN is used with CLB, it correctly states that the listener must be configured as TCP instead of HTTPS as the SSL termination must occur at the EC2 instance in that case. However, the only reason this option cannot be selected as correct is the restriction imposed on not changing the AMI at all. We cannot terminate SSL on the EC2 instance without changing the AMI or configuring the website.\nThe Load-Balancer-less solution that achieves routing via Route 53 has two problems - one is terminating SSL at the EC2 instance. The other is the fact that Route 53 does not provide Sticky Sessions.\nHence, the right answer is to use SNI with both SSL certificates bound to the same ALB listener, use Sticky Sessions, and use Route 53 to CNAME the website domains to the ALB","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/","title":"SNI and ALB"},{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-https-load-balancers.html","title":"Classic Load Balancer Listener Configuration for SAN"}],"answers":[{"id":"5344d22fe688e9bd0ec7d0ff5a2ca246","text":"Procure SSL certificates for both the domains. Deploy an Application Load Balancer. As a single ALB can only handle a single SSL certificate, SSL termination must be now done at the EC2 instance. Hence, create a TCP/443 Listener on the ALB. Add both the instances as Targets belonging to the same Target Group to the ALB, assigning the Target Group to the Listener. Turn on Sticky Sessions for this Target Group. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the ALB","correct":false},{"id":"d1e651efb4dab3eecdbcda26841cba22","text":"Procure an SSL certificate for one of the domains and add a Subject Alternative Name (SAN) for the other domain to the same certificate. Deploy a Classic Load Balancer with a single TCP Listener. Deploy the SAN certificate to the Classic Load Balancer. Add both the instances as Targets to the CLB. Turn on Session Affinity on the CLB. Terminate SSL on the EC2 instances. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the CLB","correct":false},{"id":"8b618d3847d47090857df2e1dd20b574","text":"Procure SSL certificates for both the domains. Deploy an Application Load Balancer with a single HTTPS Listener. Bind both certificates to the Listener on the Load Balancer. Add both the instances as Targets belonging to the same Target Group to the ALB, assigning the Target Group to the Listener. Turn on Sticky Sessions for this Target Group. Add two Route53 Alias Records - one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, both pointing at the DNS name of the ALB","correct":true},{"id":"676aa985e464f22918d04ea588f87ae4","text":"Procure SSL certificates for both the domains. Attach a second Elastic IP Address to the second EC2 instance. Add two Multivalue-answer records to Route53, one for www.amazing-restaurant.com and one for www.awesome-pet-grooming.com, each of type A Record, and each with both the Elastic IP addresses - one for each domain name, one domain per hosted zone. This will ensure traffic being equally split between the two instances. Terminate SSL at the EC2 instances. Turn on Session Affinity for both the Route 53 Hosted Zones","correct":false}]},{"id":"1520156f-0918-4ab4-a759-ce33a931c744","domain":"awscsapro-domain5","question":"Your company has an online shopping web application. It has adopted a microservices architecture approach and a standard SQS queue is used to receive the orders placed by the customers. A Lambda function sends orders to the queue and another Lambda function fetches messages from the queue and processes them. On some occasions the message in the queue cannot be handled properly. For example, when an order has a deleted production ID, the message cannot be consumed successfully and is returned to the queue. The problematic messages in the queue keep growing and the ability to process normal messages is affected. You need a mechanism to handle the message failure and isolate error messages for further analysis. Which method would you choose?","explanation":"It is not a good idea to adjust the retention period or simply delete the messages that fail to be processed as the question asks for a mechanism to isolate the messages for further troubleshooting. A redrive policy should be used to auto-forward error message to a dead letter queue. Then you can analyze the contents of messages to diagnose the producer’s or consumer’s issues. One thing to note is that a standard queue can only have another standard queue as the dead letter queue. Therefore a FIFO dead letter queue is incorrect as this scenario uses a standard SQS queue and requires a standard dead letter queue.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","title":"Amazon SQS dead-letter queues"}],"answers":[{"id":"f7b3898cfcb4851b120c9b14d044ab90","text":"Decrease the message retention period of the queue to 1 day. When the messages are not processed properly and put back in the queue, they can be quickly deleted when the retention period expires.","correct":false},{"id":"64fd54fe78b534f0eac9222a6e32747d","text":"Create a FIFO (First-In-First-Out) queue as the dead letter queue and use a redrive policy to forward problematic messages to this new queue. Create a Lambda function to read the message contents in the FIFO queue for further analysis.","correct":false},{"id":"a7eb53a7df09334677590165f666c58f","text":"Modify the error handling logic of the Lambda function to delete the messages whenever the processing is unsuccessful with an error or exception. The error messages do not return to the queue and the normal message handling is not blocked.","correct":false},{"id":"3c8453a6c57faf61e761f771bab6f1af","text":"Create a standard queue as the dead letter queue and configure a redrive policy to put error messages to the dead letter queue. Analyze the contents of messages in the dead letter queue to diagnose the issues.","correct":true}]},{"id":"baf2349f-71ba-4583-bfe6-31fb5a555bbd","domain":"awscsapro-domain5","question":"The information security group at your company has implemented an automated approach to checking Amazon S3 object integrity for compliance reasons. The solution consists of scripts that launch an AWS Step Functions state machine to invoke AWS Lambda functions. These Lambda functions will retrieve an S3 object, compute its checksum, and validate the computed checksum against the entity tag checksum returned with the S3 object. However, an unexpected number of S3 objects are failing the integrity check. You discover the issue is with objects that where uploaded with S3 multipart upload. What would you recommend that the security group do to resolve this issue?","explanation":"For S3 objects, the entity tag (or ETag) contains an MD5 hash of the object in most cases. But if an object is created by either the Multipart Upload or Part Copy operation, the ETag is not an MD5 digest of the object. The ETag value returned by S3 for objects uploaded using the multipart upload API is computed differently than for objects uploaded with PUT object, and does not represent the MD5 of the object data. The checksum for an object created via multipart upload can be stored in a custom metadata parameter for later integrity checks. The Content-MD5 metadata parameter can not be modified by a user after the object has been created. The complete-multipart-upload API does not have an md5-rehash parameter. The list-multipart-uploads API will only return information about the multipart upload while the upload is running.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html","title":"Multipart Upload Overview"},{"url":"https://aws.amazon.com/solutions/serverless-fixity-for-digital-preservation-compliance/?did=sl_card&trk=sl_card","title":"Serverless Fixity for Digital Preservation Compliance"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/s3-multipart-upload-cli/","title":"How do I use the AWS CLI to perform a multipart upload of a file to Amazon S3?"}],"answers":[{"id":"15ca1d17e48531d75eda4298140756e6","text":"In the Lambda function that retrieves the objects, if the object was created with a multipart upload, call the list-multipart-uploads API and retrieve each part of the multipart upload along with the entire object. In the Lambda function that computes checksums, compute the checksum of each part along with the checksum of the entire object. In the Lambda function that validates checksums, compare the sum of the checksum parts to the checksum for the entire object.","correct":false},{"id":"2a27b97da81e5d7f237719319a3d85fc","text":"When performing S3 multipart uploads, after all upload-parts API calls have been made, call the complete-multipart-upload API and include the md5-rehash parameter to reset the entity tag checksum to the sum of the parts. Reload all objects that were written with S3 multipart upload that need to be included in the integrity check.","correct":false},{"id":"fe2f446e0e84bb45be4130b327b8b47b","text":"When performing S3 multipart uploads, calculate the checksum of the source file and store it in a custom metadata parameter. Have the Lambda function that compares checksums use the custom metadata parameter if it's present instead of the entity tag checksum. Reload all objects that were written with multipart upload that need to be included in the integrity check.","correct":true},{"id":"df361f3568504f0db56ea3faddc6928a","text":"For all S3 objects created with multipart upload, retrieve the object, compute it's checksum, and store the value in the Content-MD5 metadata parameter. Have the Lambda function that validates checksums use the Content-MD5 metadata parameter if it's present instead of the entity tag checksum.","correct":false}]},{"id":"ebcf9ff3-82a1-48f8-b2fc-5d2aeb1d018c","domain":"awscsapro-domain3","question":"You are consulting with a company who is at the very early stages of their cloud journey.  As a framework to help work through the process, you introduce them to the Cloud Adoption Framework.  They read over the CAF and come back with a list of activities as next steps.  They are asking you to validate these activities to keep them focused.  Of these activities, which would you recommend delaying until later in the project?","explanation":"External communication usually comes much later in the process once project plans are defined and specific customer impact is better understood.","links":[{"url":"https://aws.amazon.com/professional-services/CAF/","title":"The AWS Cloud Adoption Framework"}],"answers":[{"id":"153eadc71676701cd67fdf00dc6c4723","text":"Work with Marketing business partners to design an external communications strategy to be used during potential outages during the migration.","correct":true},{"id":"937d0c376f475ce2eac7a3356601b8fb","text":"Investigate the need for training for Program and Project Management staff around agile project management.","correct":false},{"id":"525fde29088ab22597d7d8063c7dadf6","text":"Work with internal Finance business partners to design a transparent chargeback model.","correct":false},{"id":"2f063bd85b14ae1bbeec72bf0f6c06f5","text":"Work with the Human Resources business partners to create new job roles, titles and compensation/remuneration scales.","correct":false},{"id":"199907bba5306a10dbadf5a330d5f1f6","text":"Hold a workshop with IT business partners about the creation of an IT Service Catalog concept.","correct":false}]},{"id":"f679d23d-14d4-4021-9749-481bbe11046c","domain":"awscsapro-domain3","question":"A telecommunications company has decided to migrate their entire application portfolio to AWS. They host their customer database and billing application on IBM mainframes. IBM AIX servers running WebSphere provide an API layer into the mainframes. Customer-facing online applications are hosted on Linux systems. The customer service backend application resides on Oracle Solaris and makes use of gigabytes of persistent information. Their ERP and CRM systems also run on Solaris boxes. Telecom switches send call records to Linux-based applications, and their employee productivity suite runs on Windows. They need to complete the project in twelve months to satisfy budgetary constraints. Which migration strategy will provide them with the most resilient, scalable, and operationally efficient cloud environment within the project time frame?","explanation":"Since the company has twelve months to complete the project, they can plan for a highly cloud-centric migration. Refactoring the mainframe billing application to EC2 and the customer database to Aurora will require significant cost and effort, but will result in significant intermediate to long-term business value for most companies. A number of AWS Partner Network (APN) solutions are available to assist with this. The WebSphere layer can be replaced by API Gateway with HTTP, REST, or WebSocket APIs that call modules on the EC2 instances. Refactoring the customer-facing online apps to Lambda serverless and Step Functions will provide high operational efficiency. Performing a replatform of the Solaris customer service application to EC2 with Auto Scaling will achieve elasticity to avoid the excess capacity inefficiencies that were most-likely present in the on-premises environment. Many robust ERP, CRM, and employee productivity SaaS solutions exist and should be leveraged rather than trying to manage these applications with in-house staff. The call record processing Linux system can simply be rehosted to EC2. Repurchasing mainframe capacity from a third party provider only extends the rigidness of making mainframe changes whenever new business requirements arise. A Lambda/Step Functions solution will provide all the functionality needed for the online apps, and will be more economical than Elastic Beanstalk. Refactoring the customer service application to Lambda presents issues with processing the gigabytes of persistent information, so replatforming to EC2 is a better choice.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud"},{"url":"https://aws.amazon.com/blogs/apn/automated-refactoring-of-a-new-york-times-mainframe-to-aws-with-modern-systems/","title":"Automated Refactoring of a New York Times Mainframe to AWS with Modern Systems"},{"url":"https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/","title":"Build a Serverless Web Application"}],"answers":[{"id":"5a404284de876ce2a054dcd916ed80ec","text":"Refactor the mainframe applications onto Amazon EC2 Linux instances, and migrate the customer database to Amazon Aurora. Replace the API layer with Amazon API Gateway. Refactor the customer-facing online applications to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the customer service applications to EC2 Linux with Auto Scaling. Repurchase SaaS solutions for the ERP and CRM systems. Rehost the call record processing applications onto EC2, and repurchase SaaS applications for the employee productivity suite.","correct":true},{"id":"290da60b3bdd11f28e175fa86972386a","text":"Repurchase mainframe capacity from a third party provider and run the customer database and billing application there. Replace the API layer with Amazon API Gateway. Rehost the customer-facing online applications to Amazon Elastic Beanstalk. Refactor the customer service application to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the ERP and CRM applications to EC2 Linux instances with Auto Scaling. Rehost the call record processing applications, and repurchase SaaS applications for the employee productivity suite.","correct":false},{"id":"57d6d888b4cee16e22852910fa09cdb8","text":"Refactor the mainframe applications onto Amazon EC2 Linux instances, and migrate the customer database to Amazon Aurora. Replace the API layer with Amazon API Gateway. Rehost the customer-facing online applications to Amazon Elastic Beanstalk. Refactor the customer service application to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Repurchase SaaS solutions for the ERP and CRM systems. Rehost the call record processing applications and the employee productivity suite onto EC2.","correct":false},{"id":"c875a6eb6d74f098c971af4a638f288f","text":"Repurchase mainframe capacity from a third party provider and run the customer database and billing application there. Replatform the API layer onto EC2 Linux instances. Refactor the customer-facing online applications to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the customer service, ERP, and CRM applications to EC2 Linux instances with Auto Scaling. Rehost the call record processing applications onto EC2, and repurchase SaaS applications for the employee productivity suite.","correct":false}]},{"id":"dd8b46c7-d1d5-4326-a092-927b9333fd2a","domain":"awscsapro-domain5","question":"You are helping a company transition their website assets over to AWS.  The project is nearing completion with one major portion left.  They want to be able to direct traffic to specific regional EC2 web servers based on which country the end user is located.  At present, the domain name they use is registered with a third-party registrar.  What can they do?","explanation":"You can use Route 53 if the domain is registered under a third-party registrar.  When using Geolocation routing policies in Route 53, you always want to specify a default option in case the country cannot be identified.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy - Amazon Route 53"}],"answers":[{"id":"381d1621ab4c93aca8cc780f05e98c50","text":"Initiate a domain transfer request with the current registrar.  Once the request goes through, create a public hosted zone in Route 53.  Create SRV records for each regional EC2 instance using a Geolocation routing policy.  Create an alias record for the top-level domain and link that to the SRV records.","correct":false},{"id":"724dab26998c86ab7ddc7faa063285b8","text":"You cannot use Route 53 routing policies unless AWS is the registrar of record for your domain.  A workaround could be to configure your own top-level DNS server using BIND.  Ensure the NS and SOA records point to this instances.  Create A-type records pointing to the IP addresses of the regional EC2 web servers.  Dynamically redirect requests using customized BIND rules and a third-party IP geolocation database.","correct":false},{"id":"81efc0b140e618c378c9e9bc59dd4ca8","text":"Create a public hosted zone for the domain in Route 53.  Update the DNS entries in the registrars database to use AWS DNS Servers as defined in the NS record on Route 53.  Create A-type records for all EC2 instances. Configure CNAME records for the main FQDN that point to regional A records using a Geolocation routing policy.  Create another CNAME record as a default route.","correct":true},{"id":"294ce2854fbe727bd4f4917543d45bec","text":"Create a private hosted zone for the domain in Route 53.  Update the DNS record entries in the registrars database to use AWS DNS Servers.  Once the DNS changes are fully propagated across the internet and the TTL has expired, convert the private hosted zone to a public hosted zone.  Create A-type records for all the regional EC2 instances and configure a Geo-proximity policy for each record, ensuring the bias across all records sums to 100.","correct":false}]},{"id":"2d9a7fba-ee40-4128-8f36-b32ef55ce662","domain":"awscsapro-domain3","question":"You have just been informed that your company's data center has been struck by a meteor and it is a total loss.  Your company's applications were not capable of being deployed with high availability so everything is currently offline.  You do have a recent VM images and DB backup stored off-site.  Your CTO has made a crisis decision to migrate to AWS as soon as possible since it would take months to rebuild the data center.  Which of the following options will get your company's applications up and running again in the fastest way possible?","explanation":"The Server Migration Service uses the Server Migration Service Connector which is an appliance VM that needs to be loaded locally in vCenter.  We don't have a VMware system...only a backup of an image so this won't work.  The best thing we can do is import the VM and restore the database.","links":[{"url":"https://docs.aws.amazon.com/server-migration-service/latest/userguide/prereqs.html","title":"Server Migration Service (SMS) Requirements - AWS Server Migration Service"},{"url":"https://aws.amazon.com/ec2/vm-import/","title":"VM Import/Export"}],"answers":[{"id":"b8015467f854fe29575bd8fd26c819a5","text":"Use VM Import to upload the VM image to S3 and create the AMI of key servers.  Manually start them in a single AZ.  Stand-up a single AZ RDS instance and use the backup files to restore the database data.","correct":true},{"id":"c3692f609e87c0dc0416f0ad05897b3f","text":"Explain to company stakeholders that it is not possible to migrate from the backups directly to AWS.  Recommend that we first find a co-location site, procure similar hardware as before the disaster and restore everything there.  Then, we can carefully migrate to AWS.","correct":false},{"id":"af2dfb68e123596c40e5014f3e8dc491","text":"Copy the VMs into AWS and create new AMIs from them.  Create a clustered auto scaling group across multiple AZs for your application servers.  Provision a multi-AZ RDS instance to eliminate the single-point-of-failure problem.  Restore the data from the backups using the database admin tools.","correct":false},{"id":"e50f3fc14c0feac5ff24b30bf605d687","text":"Use Server Migration Service to import the VM into EC2.  Use DMS to restore the backup to an RDS instance on AWS.","correct":false},{"id":"364a256b0572da0bd43823d35b22e01d","text":"Call your data communications provider and order a Direct Connect link to your main office.  Order a Snowball Edge to serve as a mobile data center.  Restore the VM image to the Snowball Edge device as an EC2 instance.  Restore the backup to an RDS instance on the Edge device.  When the Direct Connect link is installed, use that to smoothly migrate to AWS.","correct":false}]},{"id":"66d28221-31ce-4cf0-aca3-2b5a69535bb5","domain":"awscsapro-domain3","question":"You are consulting with a small Engineering firm that wants to move to a Bring-Your-Own-Device policy where employees are given some money to buy whatever computer they want (within certain standards).  Because of device management and security concerns, along with this policy is the need to create a virtualized desktop concept.  The only problem is that the specialized engineering applications used by the employees only run on Linux.  Considering current platform limitations, what is the best way to deliver a desktop-as-a-service for this client?","explanation":"AWS Workspaces added support for Linux desktops the middle of 2018.  BYOD scenarios work together well with a DaaS concept to provide security, manageability and cost-effectiveness.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/create-custom-bundle.html","title":"Create a Custom WorkSpaces Bundle - Amazon WorkSpaces"}],"answers":[{"id":"3480305b106307937ed81bba73d294ab","text":"Package the required apps as WAM packages.  When launching new Windows Workspaces, instruct users to allow WAM to auto-install the suite of applications prior to using the Workspace.","correct":false},{"id":"4018dbf7b4646b285f5ceaa7b49a5934","text":"Launch a Linux Workspace in AWS WorkSpaces and customized it with the required software.  Then, create a custom bundle from that image and use that bundle when you launch subsequent Workspaces.","correct":true},{"id":"e8a33593afbd82697d0ab168304265ed","text":"Launch an EC2 Linux instance and install XWindows and Gnome as the GUI.  Configure VNC to allow remote login via GUI and load the required software.  Create an AMI and use that to launch subsequent desktops.","correct":false},{"id":"0cbf457eccae17779144d4e64e92a43e","text":"Launch a Windows Workspace and install VirtualBox along with a minimal Linux image.  Within that Linux image, install the required software.  Create an image of the Windows Workspace and create a custom bundle from that image.  Use that bundle when launching subsequent Workspaces.","correct":false},{"id":"bf3d66705af4677c9ade8605aa6bd89a","text":"Given current limitations, running Linux GUI applications remotely on AWS is not feasible.  They should reconsider their BYOD policy decision.","correct":false}]},{"id":"2c034786-9b7e-4933-aad2-d0c4b1d89ca8","domain":"awscsapro-domain2","question":"A beach apparel company has begun an initiative to improve their sales analytics capabilities using AWS services. They'll need to be able to visualize summary sales data by product line, territory, and sales channel for each day, month, and year, and they'll need to be able to drill-down with ad-hoc queries on individual sales records. There are multiple data sources that provide transactional information in different formats. The company has chosen Amazon QuickSight as their visualization tool for the summary information. Visualizations and drill-down queries will require three years of rolling sales history, which estimates to seven petabytes of data. Which architecture will provide the best performance and cost efficiency?","explanation":"Using S3 to store the detailed sales transaction data and using Lambda to standardize data formats is the most cost effective option. Storing the summary data in Redshift provides a high performance option for reads from QuickSight, and keeping the detailed transaction data out of Redshift allows for smaller node sizes and lower cost. Amazon Redshift Spectrum can be used for drill-down queries that join tables from both Redshift and S3. For answer number two, Redshift will be a better option than Aurora for OLAP query performance due to it's columnar organization. Answer number four provides no simple way to perform ad-hoc drill down queries.","links":[{"url":"https://aws.amazon.com/redshift/","title":"Amazon Redshift"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"ebf4a6c248510d05a046c8f0ea4298b7","text":"Use Amazon Kinesis Data Analytics to format the data source transactions in a standard way and load it into Amazon Aurora. Invoke Lambda functions to aggregate the data and write it into summary tables in Aurora","correct":false},{"id":"b3deac265c2195cb988c345d096800fd","text":"Ingest individual sales transactions from each data source into Amazon S3 with Amazon Kineses Data Firehose. Trigger an AWS Lambda function to format the transaction data in a standard way and redeposit the results in S3. Run AWS Glue jobs to aggregate the summary data into Amazon Redshift","correct":true},{"id":"a08e38f138d23c0f1759ab2d1801f67e","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Firehose and load them into Amazon Redshift. Run AWS Glue jobs to format the transaction data in a standard way and perform aggregate functions to write the data into summary tables in Redshift","correct":false},{"id":"4740b70569f15040edf0916e47386757","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Streams and write them to Amazon Elastic Block Store on EC2 instances in Auto Scaling Groups. Perform data format standardization and summary aggregation on EC2, and write the summary results to Amazon Redshift tables","correct":false}]},{"id":"2b66bd04-756d-4e2f-a628-1e9b76a57066","domain":"awscsapro-domain2","question":"A media company is producing a live streaming video broadcast of a sporting event for a customer. The announcers will be delivering play-by-play analysis in English. The broadcast will be aired over the Internet, and will require real-time subtitles in Spanish. The company has decided to run all aspects of the production on AWS. Which architecture will provide the functionality needed to deliver the broadcast with real-time subtitles?","explanation":"AWS Elemental MediaLive is a broadcast-grade live video processing service that lets you create high-quality video streams for delivery to televisions and internet-connected devices. Storing it's output in S3 can trigger a Lambda function to extract the unsigned PCM audio from the video segments. A second Lambda function can use Amazon Transcribe to convert the audio to text, which can then be run through Amazon Translate to create the Spanish subtitles. The first Lambda function can send the Spanish subtitle files, the manifests, and the video files to AWS Elemental MediaPackage for distribution through CloudFront. AWS Elemental MediaTailor is used to insert targeted advertising into video streams, not enhance video with subtitles. Amazon Comprehend provides text sentiment analysis, not speech to text conversion.","links":[{"url":"https://aws.amazon.com/medialive/","title":"AWS Elemental MediaLive"},{"url":"https://aws.amazon.com/transcribe/","title":"AWS Transcribe"},{"url":"https://aws.amazon.com/translate/","title":"AWS Translate"},{"url":"https://aws.amazon.com/mediapackage/","title":"AWS Elemental MediaPackage"},{"url":"https://aws.amazon.com/solutions/live-streaming-with-automated-multi-language-subtitling/?did=sl_card&trk=sl_card","title":"Live Streaming with Automated Multi-Language Subtitling"}],"answers":[{"id":"1cc4dd5872e2deead228ed9c6651c445","text":"Deliver the live video to AWS Kinesis Data Streams and make Amazon S3 the consumer. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Invoke a second Lambda function, which uses Amazon Comprehend to convert the audio files to text and return the text to the first Lambda function. Have the first Lambda function use Amazon Translate to create the Spanish transcript. Send the subtitle files, manifests, and video files to AWS Elemental MediaPackage. Create an Amazon CloudFront distribution with MediaPackage as its origin.","correct":false},{"id":"2d6eb42c2e56159dbe9d0eb1c06b9681","text":"Transmit the live video to AWS Elemental MediaLive and deliver it's output to Amazon Kinesis Video Streams. Configure S3 and Amazon Comprehend as consumers of the stream. Have Comprehend write the text files to a different S3 bucket than the video files, and trigger a Lambda function on that bucket to have Amazon Translate create the Spanish transcripts. Use AWS Elemental MediaTailor to insert the subtitles into the video segments. Send the video files to AWS Elemental MediaStore. Create an Amazon CloudFront distribution with MediaStore as its origin.","correct":false},{"id":"a8d998390ff1bff69d4a734c8c8747e6","text":"Feed the live video into AWS Elemental MediaLive and deliver it's output to Amazon S3. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Invoke a second Lambda function, which uses Amazon Transcribe to convert the audio files to text and return the text to the first Lambda function. Have the first Lambda function use Amazon Translate to create the Spanish transcript. Send the subtitle files, manifests, and video files to AWS Elemental MediaPackage. Create an Amazon CloudFront distribution with MediaPackage as its origin.","correct":true},{"id":"58eec57699d2f82a27ad049e949eb09e","text":"Send the live video to AWS Elemental MediaLive and store it's output in Amazon S3. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Have the Lambda function call Amazon Transcribe to convert the audio files to text, and then use Amazon Translate to create the Spanish transcript. Use AWS Elemental MediaTailor to insert the subtitles into the video segments. Send the video files to AWS Elemental MediaStore. Create an Amazon CloudFront distribution with MediaStore as its origin.","correct":false}]},{"id":"f43ec458-0ff5-4633-a57b-6bf82f60bd14","domain":"awscsapro-domain5","question":"You have a target group in an elastic load balancer (ELB) and its target type is \"instance\". You attach an Auto Scaling group (ASG) in the target group. All the instances pass the health check and have a healthy state in the target group. Due to a new requirement, the ELB target group needs to forward the incoming traffic to an IP address that belongs to an on-premise server. The ASG is no longer needed. There is already a VPN connection between the on-premise server and AWS VPC. How would you configure the target in the ELB target group?","explanation":"The target type of existing target groups cannot be changed from \"instance\" to \"IP\". Because of this, users have to create a new target group and set the target type to be \"IP\". After that, the on-premise IP address can be registered as a target. A domain name cannot be registered as a target in the target group. You also do not need to create a new elastic load balancer since you only need a new target group to register the IP address.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#target-type","title":"Target type in ELB target group"}],"answers":[{"id":"49646fca04901301d145a2a814a7e481","text":"In the elastic load balancer, create a new target group with an \"IP\" target type. Register the on-premise IP address as its target. Monitor if the target becomes healthy after some time. Remove the old target group.","correct":true},{"id":"e0b619a421d68626ffb82b1a6e1d22d5","text":"Remove the Auto Scaling group from the target group and modify the target type to be \"IP\". Attach the IP address to the target and set up the IP address and port in the health check configurations.","correct":false},{"id":"99f63c1cf5bcfbcb188328714abb8ed3","text":"Register a record set in AWS Route 53 to forward a domain name to the on-premise IP address. Modify the target group to register the domain name as its target. Remove the previous Auto Scaling group from the target group.","correct":false},{"id":"265de6fdcab5323b156774dcb949d309","text":"Create a new network load balancer with a new listener and target group. Configure the target type to be \"IP\" in the target group and attach the on-premise IP address to it. Set up the health check using the HTTP protocol.","correct":false}]},{"id":"42a413d2-b7c0-4f63-ab1c-37b8ec9b724a","domain":"awscsapro-domain2","question":"You have been contracted by a small start-up to help them get ready for their new product release--a web-based application that lets users browse through detailed photographs of the world's most famous paintings.  The company is expecting a huge debut with very heavy traffic so the solution should be robust and scalable with the least amount of hands-on management.  A key feature of their app is that they have created separate web sites specifically optimized for three different form factors: mobile phone, tablet and desktop.  As such, they need the ability to detect the device and direct the requester to the proper version of the site.  Which architecture will do this and meet the requirements? ","explanation":"A common use case for Lambda@Edge is to implement some front-end logic for incoming requests as close to the requester as possible.  In this case, we can use a custom Lambda function to attempt to determine the device type in the HTTP request.  We can then direct them to the CloudFront distribution that is optimized for their form-factor.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/11/lambda-at-edge-now-supports-content-based-dynamic-origin-selection-network-calls-from-viewer-events-and-advanced-response-generation/","title":"Lambda@Edge Now Supports Content-Based Dynamic Origin Selection, Network  Calls from Viewer Events, and Advanced Response Generation"}],"answers":[{"id":"ea4ee05f24a93c3b1188072a0f4777cc","text":"Configure a Network Load Balancer to use SNI to direct the request to different EC2 web servers based on device type.  Configure multiple auto scaling groups to maintain a minimum number of servers for each device type.","correct":false},{"id":"dbb8c65d7e02773f600d4e87fecc45b5","text":"Build a custom Lambda function to dynamically redirect the requester to the proper S3 origin based on device type.  Associate a CloudFront distribution with a Lambda@Edge function.","correct":true},{"id":"d6b70dc60b5f361f7b92d61ac7964558","text":"Configure the Requester Interrogation feature on CloudFront to identify the device used by the requester.  Redirect the requester to the desired S3 origin based on the device type.  ","correct":false},{"id":"2e04372e48d55855d33ef7c797110044","text":"Use Amazon AppSync to detect the type of device issuing the inbound request.  Use a Lambda function to redirect to the proper CloudFront distribution based on device type returned from AppSync.","correct":false},{"id":"f28bf27096a077030c408fe2417a1d0e","text":"Create multiple distributions in CloudFront for each needed origin.  Use Route 53 to dynamically direct the requester to the appropriate alias based on device type.","correct":false}]},{"id":"f19a95ac-c0b9-4d00-a84a-67f71b7e2a76","domain":"awscsapro-domain2","question":"You are advising a client on some recommendations to increase performance of their web farm.  You notice that traffic seems to usually spike on the days after public holidays and unfortunately the responsiveness of the web server as collected by a third-party analytics company reflects a customer experience that is slower than targets.  Of these choices, which is the best way to improve performance with minimal cost?","explanation":"Of these options, only one meets the question requirements of performance at minimal cost.  Simply scheduling a scale event during a known period of traffic is a perfectly valid way to address the requirement and does not incur unnecessary cost. CloudTrail records API access and is not suitable for network alarms.  Route 53 would not be able to \"consolidate\" dynamic and static web resources.","links":[{"url":"https://docs.aws.amazon.com/auto scaling/ec2/userguide/schedule_time.html","title":"#N/A"}],"answers":[{"id":"3c6b1f2e20a3204df3886f680991b76d","text":"Use CloudTrail and SNS to trigger a Lambda function to scale the web farm when network traffic spikes over a configured threshold.  Create an additional Internet Gateway and split the traffic equally between the two gateways using an additional route table.  ","correct":false},{"id":"24c302b442af96b7c1eedb04e2c4069b","text":"Configure a dynamic scaling policy based on network traffic or CPU utilization.  Migrate static assets from EBS volumes to S3.  Configure two Cloudfront distributions--one for static content and one for dynamic content.  Use Route 53 to consolidate both Cloudfront distributions under one alias.","correct":false},{"id":"8a2f77fe64a24871e80eae971ce2c877","text":"Create replicas of the existing web farm in multiple regions.  Migrate static assets to S3 and use cross-region replication to synchronize across regions.  Create CloudFront distributions in each region.  Use Route 53 to direct traffic to the closest CloudFront alias based on a geolocation routing policy.","correct":false},{"id":"247053f8b211aeace0894f849838ef6f","text":"Configure a scheduled scaling policy to increase server capacity on days after public holidays.  ","correct":true}]},{"id":"6d93e859-e1a9-468f-9a05-61a2dbc2be9c","domain":"awscsapro-domain5","question":"You manage a group of EC2 instances that host a critical business application.  You are concerned about the stability of the underlying hardware and want to reduce the risk of a single hardware failure impacting multiple nodes.  Regarding Placement Groups, which of the following would be the best course of action in this case?","explanation":"Spread Placement Groups ensure your instances are each placed on separate underlying hardware so this reduces the risk of a single hardware failure taking down multiple instances.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-spread","title":"Placement Groups - Amazon Elastic Compute Cloud"}],"answers":[{"id":"e346d1667489b5726c0367eff4ea4a34","text":"You would use the AWS CLI to move the existing instances into a spread placement group.","correct":true},{"id":"e6d129a06320300aaf95634a6691b7cb","text":"You would the AWS Console to move the existing instances into a clustered placement group.","correct":false},{"id":"c8b809a2fef3f21146774b82e8f03f12","text":"You would move the instances onto a Dedicated Host.","correct":false},{"id":"b6153af57a0b8805fc28d93a2859fb9e","text":"You cannot move existing instances into a new placement group.  You would create AMIs from the existing instances and redeploy them into a clustered placement group.","correct":false},{"id":"3bde2d7fff7ead39c4035ab0600275f1","text":"You would use the AWS CLI to move the existing instances into a diversified placement group.","correct":false}]},{"id":"777fd0a9-391f-4072-b147-a64a2016f5a1","domain":"awscsapro-domain2","question":"You are working with a company to design a DR strategy for the data layer of their news website.  The site serves customers globally so regional diversity is required.  The RTO is defined as 4 hours and RPO have been defined as 5 minutes. Which of the following provide the most cost-effective DR strategy for this client?","explanation":"While Multi-AZ RDS may be a best practice, the question only stipulates regional resilience.  So, we are looking for options that create regional diversity and fall within our RPO and RTO.  Those options would be cross-region bucket replication and cross-region RDS replicas.  The RPO given means that we must not loose anything more than 5 minutes of data, so any sort of backup that is less frequent than every 5 minutes is eliminated.","links":[{"url":"https://aws.amazon.com/blogs/aws/cross-region-snapshot-copy-for-amazon-rds/","title":"Cross-Region Snapshot Copy for Amazon RDS | AWS News Blog"},{"url":"https://aws.amazon.com/blogs/aws/new-whitepaper-use-aws-for-disaster-recovery/","title":"New Whitepaper: Using AWS for Disaster Recovery | AWS News Blog"}],"answers":[{"id":"035a09840d025b52ad7c808976c94da2","text":"setup cross-region replication for S3 buckets.","correct":true},{"id":"b3810d00767bfbb6b85fe44c1c3d2dd1","text":"Configure RDS to use multi-AZ and automatically fail over in the event of a problem.","correct":false},{"id":"7c7f5169f7bfea8c0aa5d79d8f1f1565","text":"Configure RDS Read Replicas to use cross-region replication from the primary to a backup region.","correct":true},{"id":"0890c4152307ffd34ebeb6ea7c500814","text":"Write a script to export the RDS database to S3 every hour then use cross-regional replication to stage the exports in a backup region.","correct":false},{"id":"cfe51da2c91348146ba1bb989b4c2225","text":"Configure RDS to perform daily backups then copy those to another region.","correct":false},{"id":"aa26654aaaf6239fe7acd7dc4e952d5a","text":"Write a script to create a manual RDS snapshot and transfer it to another region.  Use AWS Batch to run the script every three hours.","correct":false}]},{"id":"cb61ecae-afb0-4437-970e-72ddfe908e6b","domain":"awscsapro-domain1","question":"You are an AWS architect working for a B2B Merger and Acquisitions consulting firm, which has 15 business units spread across several US cities. Each business unit has its own AWS account. For administrative ease and standardization of AWS Usage patterns, corporate headquarters have decided to use AWS Organizations to manage the individual accounts by grouping them into relevant Organization Units (OU-s).\nYou have assisted the Organization Administrator to write and attach Service Control Policies (SCP-s) to the OU-s. SCP-s have been configured as the default Deny list, and they are written to explicitly deny actions wherever required.\nData Scientists in one of the Business Units are complaining that they are unable to spin up or access Sagemaker Clusters for building, training and deploying Machine Learning models. Which of the following can be a possible cause and how can this be fixed?","explanation":"This question tests the conceptual knowledge of Service Control Policies (SCP-s) in AWS Organizations.\nThe choice that requires the SCP to be modified is incorrect because there is no need to grant explicit allows from SCP, especially when it is configured in the default mode (Deny List mode). In this mode, everything is allowed by default. We only need to specify what we want to deny.\nThe choice that requires the IAM Policy to be modified is correct because SCPs do not actually grant any permission. The permission that is missing in this case must be granted via IAM Roles and Policies at the Account level.\nThe choice mentioning Service Linked Roles is incorrect as Trust Policies on Service Linked Roles cannot be modified to let an IAM user assume that role. Service Linked Roles are for AWS Services.\nThe choice that requires re-configuration of SCP as Allow List is incorrect because configuring SCP as Allow List is usually a messy idea. In that case, all permissions will need to be explicitly granted, and it can easily defeat the purpose of streamlining management and reducing administrative overhead by using AWS Organizations. Allow Lists have very specific use cases. In addition, no change in the SCP grants or allows any permission. Permission needs to be granted using IAM Roles and Policies at the Account level.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_denylist","title":"Using SCPs as a Deny List"},{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html#orgs_policies_allowlist","title":"Using SCPs as an Allow List"},{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Documentation for Service Control Policies"}],"answers":[{"id":"f64b0d2d0b81fd2aa0badced0c59abfe","text":"The IAM Policy attached to the IAM Role that the data scientists are assuming in the Business Unit Account does not grant them Sagemaker access. To fix this, add the following to the IAM Policy Statement for that Role - Effect set to Allow, Action set to Everything starting with SageMaker, Resource set to All","correct":true},{"id":"ad6186009ae4678d814923c4929282a5","text":"SCP is configured as a Deny List. To fix this, SCP must be configured as an Allow List instead of a Deny List for the OU. Then, Sagemaker access should be added explicitly","correct":false},{"id":"cd63be86b6c62d032d40d6408a54bda9","text":"The SCP for the OU to which the Business Unit Account belongs does not explicitly allow granting Sagemaker access. To fix this, add the following to the attached policy Statement of the SCP - Effect set to Allow, Action set to Everything starting with SageMaker, Resource set to All","correct":false},{"id":"1b1615b479c5aa7e1e9a3547b926aaef","text":"The Service Linked Role associated with AWS Sagemaker does not allow the data scientists to assume the Role. To fix this, add a Trust Policy to the Sagemaker Service Linked Role that lists the IAM user ids of the data scientists as Principal, with the value of Action is AssumeRole and Effect is set to Allow","correct":false}]},{"id":"5d35c6d3-3eaf-49d0-b64e-611d74d40af0","domain":"awscsapro-domain2","question":"You need to design a new CloudFormation template for several security groups. The security groups are required in different environments such as QA, Dev and Production. The allowed CIDR range in the security group ingress rule depends on environments. For example, the allowed inbound address range is 10.0.0.0/16 for non-production and 10.1.0.0/16 for production. You prefer to maintain a single template for all the environments. What is the best method for you to choose?","explanation":"CloudFormation has an optional Conditions section that contains statements to determine whether or not entities should be created or configured. Then you can use the \"Fn::If\" function to check the condition and return different values. You do not need to use Jenkins to pre-process the template and there are no \"Fn::Case\" and \"Fn::Switch\" intrinsic functions.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html","title":"CloudFormation conditions"}],"answers":[{"id":"d2661ed9bcd0f8b132885660604a665a","text":"Create a Jenkins pipeline with an environment variable. Depending on the variable value, modify the template script to use the correct CIDR IP range. Deploy the CloudFormation stack with the updated template.","correct":false},{"id":"7afa8c2f0fa2ef8966917c74044516dd","text":"CloudFormation does not provide functions for conditions. Use Terraform instead. Create a variable file to manage different CIDR IP ranges. Pass in the environment variable value and use \"Terraform apply\" to deploy all the security group resources at one time.","correct":false},{"id":"c4a393b1cce09df81069eb93f2d38054","text":"Use the environment type as an input parameter and create a condition based on the parameter. In the AWS::EC2::SecurityGroupIngress resource, use Fn::If to check the condition and return related source CIDR range.","correct":true},{"id":"d273c3216496e7d41cb97094f83d3e33","text":"Create an environment variable and pass it to the CloudFormation template. In the security group resources, use Fn::Case and Fn::Switch to check the variable value and return different CIDR IP range.","correct":false}]},{"id":"a21ff5b0-e658-4466-9b30-ad1292dde65d","domain":"awscsapro-domain2","question":"A composite materials company is implementing a new monitoring solution on their manufacturing floor. Wi-Fi enabled IoT devices will be registered with AWS IoT Core to read data from numerous control systems. Dashboards will be created in Amazon QuickSight to present aggregate metrics to users (average, min, max, standard deviation, variance, and percentile). Drill down capabilities will also be needed for deeper analyses of exception scenarios. Which architecture will provide the most reliable and performance efficient solution for the company's monitoring needs?","explanation":"The MQTT protocol is a publish/subscribe protocol that provides clients with independent existence from one another, enhancing the reliability of the solution. HTTP is a document-centric ,request-response protocol, requiring more processing and storage overhead for IoT devices. There is no need to use Kinesis Data Analytics in this case because QuickSight can perform all of the aggregate functions required for this use case. Answer number four won't allow for data drill down because the device messages are not written to any persistent storage service.","links":[{"url":"https://aws.amazon.com/iot-core/","title":"AWS IoT Core"},{"url":"https://aws.amazon.com/blogs/compute/visualizing-sensor-data-in-amazon-quicksight/","title":"Visualizing Sensor Data in Amazon QuickSight"},{"url":"https://aws.amazon.com/quicksight/","title":"Amazon QuickSight"},{"url":"https://aws.amazon.com/athena/","title":"Amazon Athena"}],"answers":[{"id":"35c4c115504645bedabfaccb200fee7f","text":"Install MQTT libraries on the IoT devices. Create an IoT Core rule that forwards the MQTT messages to an Amazon Kineses Data Analytics stream, which writes aggregate data to an Amazon Kinesis Data Streams stream. Have an AWS Lambda function trigger to read the aggregate data and deposit it into Amazon DynamoDB tables","correct":false},{"id":"48b8bb005b2b6181fa6fc7161df04e9d","text":"Install HTTP libraries on the IoT devices. Create an IoT Core rule that forwards the HTTP messages to an AWS Lambda function. Have the Lambda function write the messages to S3, and to an Amazon Kinesis Data Analytics stream to aggregate the data. Have an AWS Lambda function trigger to read the aggregate data and deposit it into Amazon DynamoDb tables","correct":false},{"id":"2287d42764ee3a22b9bdcb617461dbc6","text":"Install MQTT libraries on the IoT devices. Create an IoT Core rule that forwards the MQTT messages to an AWS Lambda function. Have the Lambda function write the messages to an Amazon Kinesis Data Firehose stream, which deposits them into S3","correct":true},{"id":"d3578dd1f9b95b2daf716b5298605a4d","text":"Install HTTP libraries on the IoT devices. Create an IoT Core rule that forwards the HTTP messages to an Amazon Kineses Data Firehose stream, which deposits the data into S3, and writes the data to an Amazon Kinesis Data Analytics stream to aggregate the data. Have an AWS Lambda function trigger to read the aggregate data and deposit it into S3.","correct":false}]},{"id":"3b08a75a-01b7-4083-bbd1-af1acd7e5314","domain":"awscsapro-domain2","question":"A clothing retailer has decided to run all of their online applications on AWS. These applications are written in Java and currently run on Tomcat application servers hosted on VMware ESXi Linux virtual machines on-premises. Because many of the applications require extremely high availability, they've deployed Oracle RAC as their database layer. Some business logic resides in stored procedures in the database. Due to the timing of other business initiatives, the migration needs to take place in a span of four months. Which architecture will provide the most reliable and operationally efficient solution?","explanation":"Elastic Beanstalk provides the most operationally efficient solution for the application server layer. With Elastic Beanstalk, you can quickly deploy and manage applications without worrying about the infrastructure that runs those applications. VMware Cloud on AWS delivers a robust environment for running Oracle RAC environments. Oracle RAC doesn't run natively on EC2. Due to the time constraints for the project, a migration to Aurora Multi-Master is probably not feasible in four months, especially when the migration of stored procedure code is involved. RDS Oracle Multi-AZ provides active/passive failover, whereas Oracle RAC is active/active, providing no-downtime failovers. Oracle Recovery Manager can run on EC2 and access the database via the VMware Cloud ENIs to perform backups to S3 over a VPC Endpoint.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_Java.html","title":"Creating and Deploying Java Applications on AWS Elastic Beanstalk"},{"url":"https://aws.amazon.com/vmware/","title":"VMware Cloud on AWS"},{"url":"https://d1.awsstatic.com/VMwareCloudonAWS/aws_reference_architecture_oracle_rac_on_vmware_cloud.pdf?did=wp_card&trk=wp_card","title":"Oracle RAC on VMware Cloud on AWS"}],"answers":[{"id":"69173b8054edfd0b9be84428b694bc53","text":"Implement AWS Elastic Beanstalk to run the Tomcat servers in multiple Availability Zones. Run Oracle RAC on VMware Cloud on AWS in multiple Availability Zones. Connect the Tomcat servers to the database instances with VMware Cloud ENI route table entries. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":true},{"id":"b555cc3afefbe63e6b58594cd86b4fa5","text":"Deploy AWS Elastic Beanstalk to run the Tomcat servers in multiple Availability Zones. Migrate the database to Amazon Aurora Multi-Master. Connect the Tomcat servers to the database via JDBC. Leverage Aurora's Multi-AZ and automated backup capabilities to achieve high availability.","correct":false},{"id":"3cac9f7ec8cba84a36f70ab25c667485","text":"Use a certified Tomcat AMI to deploy the application servers on VMware Cloud on AWS EC2 instances with Auto Scaling across multiple Availability Zones. Configure Oracle on Amazon RDS with Multi-AZ. Connect the Tomcat servers to the database instances with VMware Cloud ENI route table entries. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":false},{"id":"2a2d87b2ff0208d44d780f2cf354f3f2","text":"Use a certified Tomcat AMI to deploy the application servers on EC2 instances with Auto Scaling across multiple Availability Zones. Install Oracle RAC on EC2 across multiple Availability Zones. Connect the Tomcat servers to the database via JDBC. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":false}]},{"id":"91006a08-8658-479c-9f96-4f9cd9c770c6","domain":"awscsapro-domain2","question":"Your customer is a commercial real-estate company who owns parking lots in all major cities in the world. They are building a website on AWS that will be globally accessed by international travellers. The website will provide near-real-time information on available parking lot spaces for all cities. You need a back-end comprising a multi-region multi-master data storage solution, with writes happening in all regions. Writes must be replicated between regions in near real-time so that different regional website compute instances can query the database instance in its own region and retrieve the data pertaining to all other regions. In addition, all users should be able to access the website using the same domain name, but they need to be routed to the Elastic Load Balancer that responds most quickly to their request (which should be the one closest to them most of the time). The website must also serve locally cached static content and have protection against malicious attacks. Which AWS-based architecture should you choose?","explanation":"The only AWS managed database service that offers multi-region multi-master is DynamoDB Global Tables. AWS Aurora does offer multi-master clusters, but all nodes of an Aurora multi-master must be in the same region. Aurora also offers cross-region replication, but the nodes, in that case, are Read Replicas, they are not Masters (in the database world, a master is an instance that accepts write requests). Similarly, AWS RDS does not currently have a multi-master multi-region solution to leverage. Thus, the only choice that can satisfy the multi-region multi-master nature of the requirement is DynamoDB Global Tables with DynamoDB Streams enabled for Replication. Additionally, the question tests knowledge in other areas. However, just focusing on the above fact about the database tier is enough to eliminate all incorrect answers.\nRoute 53 latency-based multi-region routing is the correct Routing solution here. Multi-value answering records results in randomly picking one, which is not the requirement here (in this scenario, the region must be picked based on latency, not randomly). Similarly, geolocation-based routing honours country boundaries, not latency. If we use geolocation-based routing, an end-user in country A will always go to the AWS Region in country A, but that may not be the one with least latency, especially if the user is close to an international boundary, and the AWS data centre happens to be on the other end of his country.\nLambda@Edge can be used with Cloudfront to re-write origins for requests. This is an important usage of Lambda@Edge. Similarly, WAF can be activated with a Cloudfront web distribution for protection against malicious traffic.","links":[{"url":"https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/","title":"How to use Amazon DynamoDB global tables to power multiregion architectures"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html","title":"Search for - all DB instances in a multi-master cluster must be in the same AWS Region"},{"url":"https://aws.amazon.com/blogs/aws/latency-based-multi-region-routing-now-available-for-aws/","title":"Multi-Region Latency Based Routing now Available for AWS"}],"answers":[{"id":"c74e4146c5eb59b1a892f52ac51bfbd7","text":"For the database, use Amazon Aurora Cross-Region Replication, with instances in each AWS Region configured as master. For routing, use Route 53 geolocation-based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":false},{"id":"9faf11728364108db71381c81e091523","text":"For the database, use Amazon RDS with Cross Region Replication, with instances in each AWS Region configured as master. For routing, use Route 53 multi-region multi-value answering based routing. Use Cloudfront to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF with Cloudfront geo restriction feature.","correct":false},{"id":"25c4e1f274a66d3e2b5d67c75702ef4b","text":"In AWS, all cross-region replication solutions treat secondary instances as read-only, so multiple masters are not possible in a managed multi-region database solution. Use Cassandra on EC2 instances in different regions that are peered to each other and configure Cassandra in multi-master mode. For routing, use Route 53 multi-region latency based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":false},{"id":"32667dba126e63f9c8c6adb288419974","text":"For the database, use DynamoDB Global Tables, with instances in each AWS Region configured as master, and replication enabled using DynamoDB Streams. For routing, use Route 53 multi-region latency based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":true}]},{"id":"c9d5dfbb-8dda-4a84-bf04-49aa7f88d4db","domain":"awscsapro-domain2","question":"A healthcare provider has a recent history of failing to match patient clinical records from other providers with their own records. This has resulted in missed diagnoses and delayed treatments. To address the issue, they've begun an initiative to store patient records, both those generated in-house and those coming from other providers, in CSV format on an Amazon S3 data lake. They'll use other AWS services to aggregate and deduplicate patient information. Which HIPAA compliant solution will provide them with the highest level of accuracy to improve their level of patient care?","explanation":"An AWS Glue crawler can scan the raw patient CSV data and store the schemas in the Glue data catalog. A Glue ML Transform can be written to find patient information matches in tables defined in the Glue data catalog. If you don't want to write an ML Transform to find the matches, AWS provides a custom one called FindMatches. After training the ML Transform, you can use it as part of a scheduled Glue ETL job, which can write results back to the S3 data lake. Kinesis Data Analytics isn't capable of querying data in S3. It only provides SQL manipulation of the incoming stream. An EMR Hive job will be able to perform some basic record matching, but machine learning will provide greater matching accuracy over time. While SageMaker could provide a viable solution, the Caffe framework is most commonly used for computer vision use cases, not record matching.","links":[{"url":"https://aws.amazon.com/glue/","title":"AWS Glue"},{"url":"https://aws.amazon.com/about-aws/whats-new/2019/08/aws-glue-provides-findmatches-ml-transform-to-deduplicate/","title":"AWS Glue now provides FindMatches ML transform to deduplicate and find matching records in your dataset"},{"url":"https://aws.amazon.com/blogs/big-data/matching-patient-records-with-the-aws-lake-formation-findmatches-transform/","title":"Matching patient records with the AWS Lake Formation FindMatches transform"}],"answers":[{"id":"4745b6f6f2c711702b03c99faaab04b1","text":"Implement an AWS Glue crawler to determine patient record formats. Create and train a Glue ML Transform to match patient records. Execute a Glue ETL job using the ML Transform, and store results back in the data lake. Configure the Glue scheduler to run the crawler and the ETL jobs periodically","correct":true},{"id":"50bb504039515eef7c1372c744aa4d79","text":"As new patient records are ingested to S3, trigger an AWS Lambda function to start an AWS Glue crawler to update the Glue data catalog. Have the Lambda function then run an Amazon EMR Hive job to perform patient record matching and write the results back to the data lake","correct":false},{"id":"734eda6752df4074b7d1de2dc63055d6","text":"Create and train an Amazon SageMaker Caffe model to match patient records. Schedule an AWS Lambda function to periodically run the SageMaker model and deposit the results back into S3","correct":false},{"id":"7a5ba1c8cc4da315c99a4458891c790d","text":"Prior to depositing patient records in the S3 data lake, ingest them with Amazon Kinesis Data Streams and configure Amazon Kinesis Data Analytics as the consumer of the stream. Have Kinesis Data Analytics correlate records with those already ingested to S3 and write them to the corresponding patient folder","correct":false}]},{"id":"dc82c397-347d-4f69-bb06-03822238c7a0","domain":"awscsapro-domain1","question":"You are consulting for a large multi-national company that is designing their AWS account structure.  The company policy says that they must maintain a centralized logging repository but localized security management.  For economic efficiency, they also require all sub-account charges to roll up under one invoice.  Which of the following solutions most efficiently addresses these requirements?","explanation":"Service Control Policies are an effective way to broadly restrict access to certain features of sub-accounts.  Use of a single separate logging account is an effective way to create a secure logging repository.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"74a6c6df518100b16da3f16e870b5d5c","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Create localized IAM policies to restrict modification of CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create a centralized security account and establish trust relationships between each sub-account.","correct":false},{"id":"871870fa49beedfb95106595e4a1c9f4","text":"Configure billing for each account to load into a consolidated RedShift instance.  Create a centralized security account and establish trust relationships between each sub-account.  Configure admin roles within IAM of each sub-account for local administrators.  Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  ","correct":false},{"id":"0c9b5a803a99a3d2ef53869b6857c0e0","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use ACLs to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.  Establish trust relationships between the Consolidated Billing account and all sub-accounts.","correct":false},{"id":"cbec34b5388f7f183659e82c20fb3abf","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use an SCP to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.","correct":true}]},{"id":"1eb605a0-e0bc-4666-9fe9-aa249901bcb5","domain":"awscsapro-domain3","question":"Your company currently runs SharePoint as it's internal collaboration platform. It's hosted in the corporate data center on VMware ESXi virtual machines. To reduce costs, IT leadership has decided not to renew its VMware license agreement for the coming year. They've also decided on an AWS cloud-first approach going forward, and have ordered an AWS Direct Connect for connectivity back to the corporate network. On-premises Active Directory handles SharePoint authentication, and will continue to do so in the future. You've been tasked with determining the best way to deliver SharePoint to the company's users after the VMware agreement expires. How will you architect the solution in a cost effective and operationally efficient way?","explanation":"Deploying SharePoint Web Front Ends in separate Availability Zones behind a Network Load Balancer, with SharePoint App Servers in those same subnets, provides a highly reliable solution. RDS SQL Server supports Always On Availability Groups. Since RDS is a managed service, operational efficiency is achieved. Amazon Workspaces also provides managed service benefits for remote desktops, and gives the company the opportunity to have users use lower cost hardware. It can all be authenticated through an AWS Managed AD trust relationship with the on-premises Active Directory. The Managed AD managed service provides better operational efficiency than creating Domain Controllers on EC2. Introducing VMware Cloud on AWS for the database layer results in more networking complexity, and is not necessary since RDS supports Always On clusters. Remote Desktop Gateways will require higher cost end-user hardware.","links":[{"url":"https://d1.awsstatic.com/VMwareCloudonAWS/SharePoint-Hybrid_Reference-Architecture.pdf?did=wp_card&trk=wp_card","title":"SharePoint Reference Architecture - AWS and VMware Cloud on AWS"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html","title":"Multi-AZ Deployments for Microsoft SQL Server"},{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_microsoft_ad.html","title":"AWS Managed Microsoft AD"}],"answers":[{"id":"c2b436befa546d5de6bf07d1d3eb3766","text":"Configure an Application Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Run SQL Server Always On clusters on VMware Cloud on AWS. Use AWS Directory Service AD Connector for authentication from the on-premises Active Directory. Implement Remote Desktop Gateways in each subnet to provide connectivity for Windows desktops.","correct":false},{"id":"140b5814920d29ba818858f73c97577b","text":"Implement a Network Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers, SQL Server instances, and Active Directory Domain Controllers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Configure the SQL Server instances as Always On clusters. Join the Domain Controllers to the on-premises AD forest. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":false},{"id":"56ace4242f33e5150f0a40b3627f9568","text":"Use an Application Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers and SQL Server instances on EC2 instances in the same subnets as the SharePoint Web Front Ends. Configure the SQL Server instances as Always On clusters. Use AWS Directory Service AD Connector for authentication from the on-premises Active Directory. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":false},{"id":"1ff58ba530d63513506d1a8a58cb91b9","text":"Deploy a Network Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Run an Amazon RDS SQL Server Always On cluster. Use AWS Directory Service Managed AD for authentication in a trust relationship with the on-premises Active Directory. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":true}]},{"id":"b533b3c1-222f-4f33-99da-2c828e98ff91","domain":"awscsapro-domain5","question":"You have run out of root disk space on your Windows EC2 instance.  What is the most efficient way to solve this?","explanation":"We can easily increase the size of an EBS from the console or the CLI (using modify-volume) but then we also need to allow the OS to expand the resized volume so we can use it.  For Windows Server, we could use Disk Manager.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/expand-ebs-root-volume-windows/","title":"Expand the EBS Root Volume of Your EC2 Windows Instance"}],"answers":[{"id":"b893490015da5047b17b1220d43f4a1c","text":"From the AWS CLI, use the \"modify-instance\" command for EC2 to resize the volume to a larger size.  Using RDP, connect to the Windows instances and use Disk Manager to expand the volume.","correct":false},{"id":"346ac05b4353d34c630eb6d233f8a35d","text":"Compress all files on the root volume using the built-in zip utility.  Modern versions of Windows will automatically unzip the files when they are accessed.","correct":false},{"id":"229b013eff2d5f53df7b9c3a60bd2418","text":"From the AWS Console, select Modify Volume for the EBS volume.  Enter the new size and confirm the change.  Connect to your Windows instance and use Disk Manager to extend the newly resized volume.","correct":true},{"id":"38862a719074689f75df6a20a42f7df7","text":"Use AWS System Manager Run service to remotely execute a PowerShell script using AWS Tools for PowerShell to expand the volume using the ModifyInstance command.","correct":false}]},{"id":"05e085a9-4de3-46fe-9470-10c7f2faba57","domain":"awscsapro-domain5","question":"You are consulting with a client who is in the process of migrating over to AWS.  Their current on-prem Linux servers use RAID1 to provide redundancy.  One of the big benefits they are looking forward to with moving to AWS is the ability to create snapshots of EBS volumes without downtime.  Right now, they intend on migrating the servers over to AWS and retaining the same disk configuration.  What is your advice for them?","explanation":"Because RAID is based upon multiple volumes being in sync, taking snapshots of an individual volume that's part of a active and mounted RAID array would not create a proper backup.  You must first unmount the RAID volume and then create the snapshots of the component volumes.  This of course means any data on the RAID volume would be unavailable.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html","title":"RAID Configuration on Linux - Amazon Elastic Compute Cloud"}],"answers":[{"id":"73b207bc7a947de1eed26bc058b4b67b","text":"If snapshots without downtime are the priority, do not use RAID.","correct":true},{"id":"21a6a5218cf9778e0184eed7897c54ce","text":"Consider using RAID6 rather than RAID1 on AWS for performance reasons.","correct":false},{"id":"99a9d29ef15a0ced996c1510ff6d8f6a","text":"EC2 does not support RAID configurations.","correct":false},{"id":"ead9938f5d4fc4d2df30763406b6a8e5","text":"Consider using RAID10 when on AWS because it offers the best of both RAID0 and RAID1.","correct":false},{"id":"bee3a756d6bfddce4e9917e171a4b0e2","text":"Consider using RAID0 when on AWS for performance reasons.","correct":false}]},{"id":"29e18988-677f-4ce3-b70b-8c0ef6b45824","domain":"awscsapro-domain2","question":"You have a domain name (example.net) registered in an AWS Route53 hosted zone. You deploy multiple servers in different regions and each server has an elastic IP address. DNS record sets need to be created in Route53 for \"www.example.net\". You would like to return multiple IP addresses in response to DNS queries. Every IP address should have a custom health check and Route53 only replies to DNS queries with healthy IPs. Which of the following Route53 policy configuration should you use to achieve this?","explanation":"In this question, Route53 needs to return multiple IP addresses in response to DNS queries and each record should have a health check. The most suitable routing policy is Multivalue Answer. The traffic would be routed randomly to multiple resources. You cannot attach a health check to a Simple routing policy record. As the question does not mention the requirements of allocating weights or active-passive failover, Weighted Routing or Failover routing is inappropriate.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency","title":"Choosing a routing policy"}],"answers":[{"id":"565f475081d62dee89690c5bca688fc1","text":"Configure several Multivalue Answer routing policy records for \"www.example.net\". Each record has an IP address as its value and is associated with a Route53 health check. Clients only receive a DNS response that is reachable.","correct":true},{"id":"0b677fa1dbbeff3e55c5c27829133156","text":"Create several records in Route53 for \"www.example.net\". Configure separate health checks in Route53 and attach them to the new records. Choose Weighted Routing policy and allocate the same weight in the records.","correct":false},{"id":"a3fe29a9c966b12721c6ec7b57ebd146","text":"Configure a single record that contains multiple elastic IP addresses. The record has a Simple Routing policy and a health check is attached to the record. Route53 returns all healthy values to the resolver in random order.","correct":false},{"id":"06487f00d062ca6482a44ed515d0622c","text":"Create one primary record and one secondary record with the Failover routing policy. Allocate the same number of IP addresses to each record. Associate health checks to them. If the health check of the primary record fails, the traffic goes to the secondary record automatically.","correct":false}]},{"id":"9b8fcbdc-35da-425a-a77a-d5d7b2f6682b","domain":"awscsapro-domain5","question":"You have been asked to help develop a process for monitoring and alerting staff when malicious or unauthorized activity occurs.  Your Chief Security Officer is asking for a solution that is both fast to implement but also very low maintenance.  Which option best fits these requirements?","explanation":"AWS GuardDuty is a managed service that can watch CloudTrail, VPC Flow Logs and DNS Logs, watching for malicious activity.  It has a build-in list of suspect IP addresses and you can also upload your own lists of IPs.  GuardDuty can trigger CloudWatch events which can then be used for a variety of activities like notifications or automatically responding to a threat.  AWS Macie is a service to discovery and classify potentially sensitive information.  CloudWatch alone lacks the business rules that are provided with GuardDuty.","links":[{"url":"https://aws.amazon.com/guardduty/faqs/","title":"Amazon GuardDuty FAQs – Amazon Web Services (AWS)"}],"answers":[{"id":"8a07e363b2fe9a6ac84afe772979dec2","text":"Configure CloudWatch to create an event whenever malicious or unauthorized behavior is observed.  Trigger an SMS message via SNS to the Security Officer whenever the event happens. ","correct":false},{"id":"759d9a95c918d18810232d19f92f6d79","text":"Enable AWS Macie to monitor for malicious and unauthorized behavior.  Configure a custom whitelist for the IPs that were wrongly flagged.  Setup a Lambda function triggered from a CloudWatch event when anomalies are detected. ","correct":false},{"id":"069bd8db7abe860a4270a1b362abf0f0","text":"Enable AWS GuardDuty to monitor for malicious and unauthorized behavior.  Configure a custom blacklist for the IPs which you have seen suspect activity in the past.  Setup a Lambda function triggered from a CloudWatch event when anomalies are detected. ","correct":true},{"id":"68a4943d35341bc4e1fae5493607eaa0","text":"Use AWS Glue to direct all CloudTrail logs into Redshift.  Use QuickSight as a presentation layer with custom reports for visualizing malicious and unauthorized behavior.  Run the reports periodically and email them to the Security Officer. ","correct":false},{"id":"0809c23cd3f14044c6f063646502e2e7","text":"Use AWS SageMaker to implement a Linear Learner algorithm that periodically reviews CloudFront logs for malicious and unauthorized behavior.  When the ML model finds something suspicious, trigger an SES email to the Security Officer.","correct":false},{"id":"256dea4919c6b38545573ed3fe0fee5c","text":"Configure VPC Flow Logs to capture all traffic going in and out of the VPC.  Use ElastiSearch to process the logs and trigger a Lambda function whenever malicious or unauthorized behavior is found.","correct":false}]},{"id":"4eb19466-6d1a-4ccd-987c-3f8f0cc71479","domain":"awscsapro-domain1","question":"A client wants help setting up a way to manage access to the AWS Console and various services on AWS for their employees.  They are starting out small but expect to provide AWS-hosted services to their 20,000 employees within the year.  They currently have Active Directory on-premises, use VMware to host their VMs.  They want something that will allow for minimal administrative overhead and something that could scale out to work for their 20,000 employees when they have more services on AWS.  Due to audit requirements, they need to ensure that the solution can centrally log sign-in activity.  Which option is best for them?","explanation":"For userbases more than 5,000 and if they want to establish a trust relationship with on-prem directories, AWS recommends using AWS Directory Service for Microsoft Active Directory.  This is also compatible with AWS Single Sign-On which provides a simple way to provide SSO for your users across AWS Organizations.  Additionally, you can monitor and audit sign-in activity centrally using CloudTrail. ","links":[{"url":"https://aws.amazon.com/single-sign-on/faqs/","title":"AWS Single Sign-On FAQs – Amazon Web Services (AWS)"},{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ad_connector_best_practices.html","title":"Best Practices for AD Connector - AWS Directory Service"}],"answers":[{"id":"32479d3559d6ead0328c6419249c7859","text":"Connect the multiple accounts together using AWS Organizations.  Deploy AD Connector on AWS and configure their on-prem AD.  Create corresponding roles and groups in IAM and map those to their local AD groups.  Use STS to allow users to authenticate into AWS.","correct":false},{"id":"a3c1cc18aa02ff66c06476aa7b8e78ed","text":"Download and install the AWS ActiveDirectory Sync appliance and install it in vCenter.  Configure the Sync appliance to connect to the local AD and replicate to an instance of Simple AD on AWS.  In IAM, create corresponding roles and policies for the permissions you want to allow on AWS.  Assign these roles to the synchronized Simple AD users in IAM.","correct":false},{"id":"878c950f9d14c2d81185f1950edda98c","text":"Configure Cognito with web federation against the on-prem Active Directory.  In IAM, create corresponding users corresponding to the Cognito accounts you want to allow on AWS.  Assign these roles to the user pools within Cognito.  Distribute the Cognito SSO client to your users.","correct":false},{"id":"1eb0420847890f99ddd38092ac88c5f0","text":"Create a OAuth Identity Provider in IAM and create roles and policies with the appropriate level of permissions.  In AD, create groups which correspond to the roles you have created in IAM and populate the AD groups with the desired users.  Download and install the OAuth Identity Connector for AD.  Configure the connector for the OAuth Identity Provider on AWS.  ","correct":false},{"id":"b1b70c7c05b814cf9685b3a64175cd80","text":"Connect the multiple accounts with AWS Organizations.  Deploy AWS Directory Service for Microsoft Active Directory on AWS and configure a trust with your on-premises AD.  Configure AWS Single Sign-On with the users and groups who are permitted to log into AWS.  Give the users the URL to the AWS SSO sign-in web page.","correct":true}]},{"id":"a7c939f1-277e-469f-a209-9b290e8136c9","domain":"awscsapro-domain5","question":"Your company has contracted with a third-party Security Consulting company to perform some risk assessments on existing AWS resources.  As part of a routine list of activities, they inform you that they will be launching a simulated attack on one of your EC2 instances.  After the Security Group performed all their activities, they issue their report.  In their report, they claim that they were successful at taking the EC2 instance offline because it stopped responding soon after the simulated attack began.  However, you're quite certain that machine did not go offline and have the logs prove it.  What might explain the Security company's experience?","explanation":"AWS Shield and other counter-measure technologies work to protect all AWS customers from DDoS attacks.  Unless AWS was aware of the test time and expected duration, its likely the traffic was blocked as suspicious.  AWS Firewall Manager is used to manage WAF ACLs and not dynamically blacklist IPs.  Similarly, VPC Flow Logs cannot automatically implement NACL changes as described here. Despite being a permitted service, traffic suspected of being malicious will still be blocked","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/penetration-testing/","title":"Submit a Penetration Testing Request"}],"answers":[{"id":"e3facacbe52b6423f9cf2e700d8e0b81","text":"The Security Company's traffic was seen as a threat and blocked dynamically by AWS.  AWS must grant permission before any penetration testing is done.","correct":true},{"id":"82a05cb45adab6d248655e827de16c6f","text":"AWS Firewall Manager is dynamically adding a blacklist entry for the Security Company's testing machine because it sees the traffic as a threat.","correct":false},{"id":"f3f963b71e307f8c28109631df115418","text":"The EC2 instance is using an ENI and the Security Company temporarily exceeded the throughput limit resulting in a throttling of their connection.","correct":false},{"id":"7d4826f179b8dc854c9cfb6e43678373","text":"The VPC Flow Logs record the spike in suspicious traffic and implement an update to the inbound NACL to block the remote IP address.","correct":false}]},{"id":"6b6689f4-b150-482a-aa96-eab1674cb232","domain":"awscsapro-domain5","question":"Quality Auto Parts, Inc. has installed IoT sensors across all of their manufacturing lines. The devices send data to both AWS IoT Core and Amazon Kinesis Data Streams. Kinesis Data Streams triggers a Lambda function to format the data, and then forwards it to AWS IoT Analytics to perform monitoring and time-series analyses, and to take actions based on business processes. After an equipment failure on one of the manufacturing lines causes tens of thousands of dollars in revenue losses, it's determined that alarms for a specific piece of equipment where received seventy-five seconds after the issue originated, and that automated corrective action within a few seconds of the problem could have avoided the financial losses altogether. What changes should be made to the architecture to improve the latency of device alerts?","explanation":"AWS IoT Analytics is useful for understanding long-term device performance, performing business reporting, and identifying predictive fleet maintenance needs, but common latencies run from seconds to minutes. If you need to analyze IoT data in real-time for device monitoring, use Kinesis Data Analytics, which provides latencies in the millisecond to seconds range. A Lambda function can be used as the destination for Kinesis Data Analytics to perform corrective actions. IoT Core rules can write messages to a Kinesis stream, but not directly to Kinesis Data Analytics. Having a Lambda function perform anomaly detection will work, but will require more logic to be written for query setup and execution than using a specialized service like Kinesis Data Analytics. With Amazon CloudWatch Alarms, an alarm will watch a single metric over a period time, but will not provide the capabilities of SQL to detect complex anomaly conditions.","links":[{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/aws-reference-architecture-time-series-processing.pdf?did=wp_card&trk=wp_card","title":"Processing IoT Time Series Data on AWS"},{"url":"https://aws.amazon.com/iot-analytics/faq/","title":"AWS IoT Analytics FAQs"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/05/introducing-real-time-iot-device-monitoring-with-kinesis-data-analytics/","title":"Introducing Real-Time IoT Device Monitoring with Kinesis Data Analytics"}],"answers":[{"id":"7e2eb8f3a96a390aab88e66000821c26","text":"Create an AWS IoT Core rule to write the message to Amazon Kinesis Data Analytics to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":false},{"id":"71ade679994c0c1e6e55b3853194e4c5","text":"Add another AWS Lambda function as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Have the Lambda function write the anomalies to Amazon DynamoDB and perform device corrective action when needed.","correct":false},{"id":"fe8ff20697982ca413f81ea14472e603","text":"Add Amazon Kinesis Data Analytics as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":true},{"id":"522af3cd7d520d3e94e97c02d19c0672","text":"Create an AWS IoT Core rule to write the message to Amazon CloudWatch Alarms to detect anomalies in the data. Invoke another AWS Lambda function from CloudWatch Alarms to perform device corrective action when needed.","correct":false}]},{"id":"599dee9a-6ae7-4c85-a7c6-49edc6ae7d6b","domain":"awscsapro-domain5","question":"A development team is comprised of 20 different developers working remotely around the globe all in different timezones.  They are currently practicing Continuous Delivery and desperately want to mature to true Continuous Deployment.  Given a very large codebase and distributed nature of the team, enforcing consistent coding standards has become the top priority.  Which of the following would be the most effective to address this problem and get them closer to Continuous Deployment?","explanation":"Including an automated style check prior to the build can move them closer to a fully automated Continuous Deployment process.  A style check only before UI testing is too far in the SDLC.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"628453003287afe2200912bb38d0456b","text":"After integrating and load testing, run a code compliance check against the binary created during the build.","correct":false},{"id":"1f40591b9d9dbe7a2371e5e82ec05997","text":"Introduce a peer review step into their deployment pipeline during the daily stand-up, requiring sign off for each commit.","correct":false},{"id":"f4cd7f15eb32d8ddd77234b38d0b35b8","text":"Incorporate a code style check right before user interface testing to ensure standards are being followed.","correct":false},{"id":"ec61b60c7eeb3bf9ca9c4149c09c5f3d","text":"Issue a department directive that standards must be followed and require the developers to sign the document.","correct":false},{"id":"db4ecdbd1c7c8fda5d3e0792a15411ab","text":"Include code style check in the build stage of the deployment pipeline using a linting tool.  ","correct":true},{"id":"e60e97c4fb6cbf6c1dcf3e806624762f","text":"Require all developers to use the Pair Programming feature of Cloud9.  The commits must be signed by both developers before merging.","correct":false}]},{"id":"1db0c4e1-44ef-4a3a-8e71-6dce38e4a0bb","domain":"awscsapro-domain3","question":"You are considering a migration of your on-prem containerized web application and CouchBase database to AWS.  Which migration approach has the lowest risk and lowest ongoing administration requirements after migration?","explanation":"A lift-and-shift approach when containers are involved is often a very easy and low-risk way to migrate to the cloud.  ECS is a good option of you already have a container landscape.  Fargate provides more automated scale and management, but AWS wants users to treat Fargate as an ephemeral platform, so an application like CouchBase that requires persistent storage would not work well.  Our best option for least management is ECS on an EC2 cluster.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud | AWS Cloud Enterprise  Strategy Blog"},{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_GetStarted.html","title":"Getting Started with Amazon ECS using Fargate - Amazon Elastic Container  Service"}],"answers":[{"id":"ecd3b08ae8761671f56d59cca00dba2e","text":"Use Server Migration Service to migrate the on-prem servers into AWS as AMIs.  Configure data volume replication to synchronize both the web server and database AMIs.  Run in parallel for no longer than 90 days.  When the new environment is proven, change over the DNS entry to point to the new AWS landscape.","correct":false},{"id":"77591c2773a9058fb91669c7bb4104ac","text":"Provision sufficient sized EC2 instances to host the web application and Couchbase.  Manually install the web application and Couchbase on the EC2 instances and configure rsync and DMS to synchronize the web server and database respectively.  Once the AWS environment is proven, change the DNS entries to point to the new AWS landscape.  Mange the instances going forward with AWS Config.","correct":false},{"id":"e2ca91149ec9ad6a9ca978f52b63f9a7","text":"Use SCT to read the existing Couchbase schema and recreate it in DynamoDB.  Use DMS to initially migrate the data from Couchbase and keep it in sync.  Import the web application into ECS using a Fargate cluster.  Update the ECS web application to use DynamoDB.  Once the AWS landscape is proven, do a final commit from the web application container state to the latest version in the registry.  Wait until ECS completes the update of the new container and change DNS entries to point to the new AWS landscape.","correct":false},{"id":"e1b011698db39a5bd75f9561625d28a0","text":"Import the containers into Elastic Container Registry.  Deploy the web application and database on ECS using an EC2 cluster.  Once the AWS version is proven, do a final commit of the container state to the latest version in the registry and use Force New Deployment on the ECS console for the service.  Change over DNS entries to point to the new AWS landscape.","correct":true}]},{"id":"5af539b7-b132-4a3a-bc80-406c620e7325","domain":"awscsapro-domain1","question":"A food service business has begun an initiative to migrate all applications and data to the AWS cloud. Governance needs to be established before any migrations can occur. Business units such as sales, marketing, and product management have fluctuating infrastructure capacity and security requirements, while other business units like finance, operations, and human resources have more static demand. Security policies and compliance needs vary by project group within each business units. Each business unit is responsible for it's own cost center, and the finance group would like cost reporting to be as streamlined as possible. Which AWS account structure will best satisfy the company's governance needs?","explanation":"Leveraging AWS Organizations to manage an account structure with a core Organizational Unit and Organizational Units for each business unit provides flexibility for future organizational changes. Creating an account for each project group facilitates security policy differences within business units, and limits the exposure of a single security event. Managing differing security requirements by project group in a single account will require more governance maintenance. Creating billing, shared services, and log archive accounts in multiple Organizational Units will result in duplication of services, and can be done at the core level.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-laying-the-foundation/introduction.html","title":"Laying the Foundation: Setting Up Your Environment for Cost Optimization"},{"url":"https://aws.amazon.com/solutions/aws-landing-zone/?did=sl_card&trk=sl_card","title":"AWS Landing Zone"}],"answers":[{"id":"03705913700b8d76205d4203c58dc5e1","text":"Use AWS Organizations with a single Organizational Unit to consolidate costs. Create a billing account, a shared services account, and a log archive account in the Organizational Unit. Create individual accounts for each business unit. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs","correct":false},{"id":"bd400ff0d22480599228a0442d2bb8d4","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Create an Organizational Unit for each business unit that contains accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":true},{"id":"a66d8391267460b5800c5c3d07921767","text":"Use AWS Organizations to create Organizational Units for each business unit. Create a billing account, a shared services account, and a log archive account in each Organizational Unit. Create accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"a8f7d8fbb7c6c3a1a14c91577dff42e1","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Place business units with similar security requirements in shared Organizational Units. Create accounts for each business unit in the shared Organizational Units. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false}]},{"id":"73708d6f-e6cb-4b8f-90d9-723a2961496e","domain":"awscsapro-domain2","question":"Your team is architecting an application for an insurance company.  The application will use a series of machine learning methods encapsulated in an API call to evaluate claims submitted by customers.  Whenever possible, the claim is approved automatically but in some cases were the ML API is unable to determine approval, the claim is routed to a human for evaluation.  Given this scenario, which of the following architectures would most aligned with current AWS best practices?","explanation":"Formerly, AWS recommended SWF for human-involved workflows.  Now AWS recommends Step Functions be used as it requires less programmatic work to build workflows and is more tightly integrated into other AWS services.","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html","title":"Starting a State Machine Execution in Response to Amazon S3 Events - AWS  Step Functions"}],"answers":[{"id":"837fa3e7c6b347eddfa7fefd2a092017","text":"Create a workflow using Simple Workflow Service and an EC2 fleet to host worker and decider programs.  Create worker programs for each processing step and ML API call.  Create decider program to receive the output of the API and decide if the claim is approved.  For unapproved claims, create a worker program use the WorkMail SDK to place the unapproved claim into a mailbox to be reviewed by a human.","correct":false},{"id":"5298caaea3a55734efa8c62e637d0d40","text":"Use Kinesis to take in the claims and save them on S3 using Firehose.  Use Sagemaker to analyze the claims on S3 as a training set and devise a decider function.  Save the approved claims to another S3 bucket setup with an Event to trigger an SES message to a reviewer.","correct":false},{"id":"dfc86c259de4881886ffdac5b8106777","text":"Create a State Machine using Step Functions and a Lambda function for calling the API.  Intake the claims into an S3 bucket configured with a CloudWatch Event.  Trigger the Step Function from the CloudWatch Event.  Create an Activity Task after the API check to email an unapproved claim to a human.","correct":true},{"id":"fa669ed2f0666420f19ad9c8836509f6","text":"Take in the claims into an SQS queue.  Create a Lambda function to poll the SQS queue, fetch the claim and submit the API call.  Use another Lambda function to evaluate the API results and if the claim is not approved, place the claim in a dead letter queue.  Train a person to periodically log into the SQS console and read the dead letter queue for review.","correct":false}]}]}}}}
