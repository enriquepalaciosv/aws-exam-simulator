{"data":{"createNewExamAttempt":{"attempt":{"id":"dbe3aee4-5fc8-45b5-bfd7-57fdba7ac6ea"},"exam":{"id":"bdf654b8-ded5-40ca-b80f-c6bc9738a208","title":"AWS Certified DevOps Engineer - Professional 2019","duration":10800,"totalQuestions":75,"questions":[{"id":"b21d657a-8e37-4f2b-9641-6e37ff00825c","domain":"SDLCAutomation","question":"Your manager wants to implement a CI/CD pipeline for your new cloud-native project using AWS services, and would like you to ensure that it is performing the best automated tests that it can. He would like fast and cheap testing, where bugs can be fixed quickly. He suggests starting with individual units of your software and wants you to test each one, ensuring they perform how they are designed to perform. What kind of tests do you suggest implementing, and what part of your CI/CD pipeline will you implement them with?","explanation":"Unit tests are built to test individual units of your software and quickly identify bugs. These can be implemented with CodeBuild.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing CI/CD on AWS"}],"answers":[{"id":"962f2ea0d522b0054011f4e7935fd81e","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some unit tests for the existing code base and ensure that your developers produce unit tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeBuild","correct":true},{"id":"464e499150ad3f7270dbfb985c67bcb0","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some compliance tests for current code base and ensure that your developers produce component tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeBuild","correct":false},{"id":"437e248b28463ad899879dda97bea983","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some unit tests for the existing code base and ensure that your developers produce component tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeCommit","correct":false},{"id":"673e10c0ae89a727cede17ac866f1ae4","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build source tests for current code base and ensure that your developers produce source tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeCommit","correct":false}]},{"id":"dad35c13-8f81-4aac-a7b9-c2ed4a5f9335","domain":"HAFTDR","question":"With your company moving more internal services into AWS, your colleagues have started to complain about using different credentials to access different applications. Your team has started to plan to implement AWS SSO, connected to the corporate Active Directory system, but are struggling to implement a working solution.  Which of the following are not valid troubleshooting steps to confirm that SSO is enabled and working?","explanation":"The question states which are NOT valid troubleshooting steps, so we need to choose the ones which will not help us troubleshoot the issues. Firstly, you can use the User Principal Name (UPN) or the DOMAIN\\UserName format to authenticate with AD, but you can't use the UPN format if you have two-step verification and Context-aware verification enabled. Secondly, AWS Organisations and the AWS Managed Microsoft AD must be in the same account and the same region.  The answers which suggest the opposite are the ones which should be chosen.  The other answers are correct troubleshooting steps and therefore can not be chosen.","links":[{"url":"https://docs.aws.amazon.com/singlesignon/latest/userguide/prereqs.html","title":"AWS SSO PreRequisites"},{"url":"https://docs.aws.amazon.com/singlesignon/latest/userguide/troubleshooting.html","title":"Troubleshooting AWS SSO Issues"}],"answers":[{"id":"eb8551b7f4293f316685247af6fbe823","text":"Implement AWS Organisations with 'All Features' enabled, deploy the AD Connector residing in your master account.","correct":false},{"id":"3627bfa9871db5cdd41006b26ec5dbbe","text":"Implement AWS Organisations and deploy AWS Managed Microsoft AD in two separate accounts.  It does not matter which regions they are deployed in.","correct":true},{"id":"ff24f8f9a780de8999f077e354ae2eef","text":"To allow authentication using a User Principal Name, enable two-step verification in Context-aware verification mode.","correct":true},{"id":"fe88b553bb4a39e8268dcb243738d505","text":"AWS SSO with Active Directory only allows authentication using the DOMAIN\\UserName format.","correct":true},{"id":"0fa6f9fa1ae19cce6cead12014268484","text":"Ensure the number of AWS SSO permission sets are less than 500 and you have no more than 1500 AD groups.","correct":false}]},{"id":"781e2a28-ff7a-4712-bf0c-1ae69dec243f","domain":"SDLCAutomation","question":"You are part of a development team that has decided to compile release notes directly out of a CodeCommit repository, the version control system in use. This step is to be automated as much as possible. Standard GitFlow is used as the branching model with a fortnightly production deploy at the end of a sprint and occasional hotfixes. Select the best approach.","explanation":"Following GitFlow's standard release procedures, a release branch is merged into master. That commit on master must be tagged for easy future reference to this historical version. Both release and hotfix branches are temporary branches and would require ongoing updates of the CodeCommit trigger. Feature branches are used to develop new features for the upcoming or a distant future release and might be discarded (e.g. in case of a disappointing experiment). CodeCommit does not provide a generate release notes feature.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html","title":"Manage Triggers for an AWS CodeCommit Repository"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html#codecommit_event_type","title":"CodeCommit Events"},{"url":"https://aws.amazon.com/blogs/devops/build-serverless-aws-codecommit-workflows-using-amazon-cloudwatch-events-and-jgit/","title":"Build Serverless AWS CodeCommit Workflows using Amazon CloudWatch Events and JGit"},{"url":"https://nvie.com/posts/a-successful-git-branching-model/","title":"A successful Git branching model"},{"url":"https://docs.aws.amazon.com/codecommit/latest/APIReference/Welcome.html","title":"AWS CodeCommit API Reference"},{"url":"https://forums.aws.amazon.com/thread.jspa?messageID=756611","title":"CodeCommit Lambda triggers fire off separate events for each commit?"}],"answers":[{"id":"94ff89937241b8005e348ff0550a1947","text":"Setup up an Amazon CloudWatch Event rule to match CodeCommit repository events of type 'CodeCommit Repository State Change'. Look for 'referenceCreated' events with a 'tag' referenceType that are created when a production release is tagged after a merge into 'master'. In a Lambda function, use the CodeCommit API to retrieve that release commit message and store it in a static website hosting enabled S3 bucket.","correct":true},{"id":"601038d6d016fa6b4637092d2a68db75","text":"Configure a trigger by choosing the 'Delete branch or tag' repository event that invokes a Lambda function when development for a sprint is finished, i.e. the last feature-* branch has been deleted. In that Lambda, retrieve the latest git merge commit message before the deletion and append it to the release notes text file stored in an S3 bucket.","correct":false},{"id":"daf0713b7db34409528f3e191adf166e","text":"Use the 'generate release notes' feature of CodeCommit by running the 'create-release-notes' command with the --from <datetime> (use the start of the sprint) or --previousTag <tagName> option in the AWS CLI. Create a Lambda to execute this on a regular schedule (i.e. every 2 weeks) using CloudWatch Events with a cron expression.","correct":false},{"id":"625fdc89350c8e0422b2c105b66be63a","text":"Create a trigger for your CodeCommit repository using the 'Push to existing branch' event and apply that to any release and hotfix branch. Add an Amazon SNS topic as the target and have a Lambda listen to it. In that function, filter out specific commit type changes such as style, refactor and test that are not relevant for release notes. Store all other commit messages in a DynamoDB table and, at release time, run a query to collate the release notes.","correct":false}]},{"id":"c3e23238-408e-4b66-86f9-c652a8649dc5","domain":"MonitoringLogging","question":"Your organization deals with petabytes of data which needs to be shared with a vendor. They require full access to your private S3 buckets to perform their development work. They have extensive AWS experience already. How will you give them access?","explanation":"The best way to accomplish this is to create a cross-account IAM role with permission to access the bucket, and grant the vendor's AWS ID permission to use the role.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/","title":"Provide Cross-Account Access to Objects In S3 Buckets"}],"answers":[{"id":"9744a37e79095f8e3ca34aadf736f9b6","text":"Grant permission to vendor AWS ID to use the role","correct":true},{"id":"c81b77222ee909b09dcad4f625bfe641","text":"Enable cross region replication","correct":false},{"id":"10b2007be424d0cd17f0473bf718ef66","text":"Edit the bucket policy to allow the vendor AWS ID read access","correct":false},{"id":"087a96fb587651ee51b6ca0824c7b131","text":"Grant the role permission to the bucket","correct":true},{"id":"8f55be78212f11fd250d67174c0dc930","text":"Create an IAM Role","correct":false},{"id":"38cbb5ba1eb48a6fd20b712b0433a77d","text":"Grant permission to the vendors AWS ID in the S3 bucket policy","correct":false},{"id":"18e8d083cfe353b0d12b4ee66f546b75","text":"Create a cross-account IAM Role","correct":true}]},{"id":"977b9167-9509-4260-91b8-745ff4a10c16","domain":"ConfigMgmtandInfraCode","question":"You need to quickly test a proof of concept that your boss has given you. He's given you a zip file containing a php web application. You want to get it running in an AWS environment as fast a possible, however there's also a dependency on a library that must be installed as well. The library is available from a yum/apt repository.  Which service do you choose and how do you ensure dependencies are installed?","explanation":"AWS Elastic Beanstalk is a quick way to test the proof of concept, as webserver configuration is not required. Required Libraries can be installed quickly and automatically using ebextensions.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html","title":"Customizing Software on Linux Servers - AWS Elastic Beanstalk"}],"answers":[{"id":"4cc5c190534c4e744d352d96c3ebe2bc","text":"AWS CloudFormation, install dependency with custom resources.","correct":false},{"id":"adde411da8e54597432c35347938b2ec","text":"AWS Elastic Beanstalk for deployment, install dependency with ebextensions.","correct":true},{"id":"4f921aa28b46ed7642eb7a3dc8583844","text":"AWS OpsWorks for deployment, install dependency with chef.","correct":false},{"id":"c2f64a6cd63768d866b164a5c278f252","text":"AWS EC2 and apache2, install dependency with apt-get or yum.","correct":false}]},{"id":"a1b1f253-8d88-4b6a-882a-146b02e9e327","domain":"MonitoringLogging","question":"Your company has been moving its core application from a monolith to an ECS based, microservices architecture.  Although functionally the application is operational in its new environment, you randomly see spikes of latency throughout the day and you are concerned with the overall performance of the application.  How can you rapidly gain information about the requests each microservice is serving?","explanation":"X-Ray is the AWS service that can deal with this sort of scenario, therefore we can discount any option that doesn't include it.  The best solution would be to deploy a Docker image running the X-Ray agent as an ECS service and this could ingest data from all sources.  You could then track the path of the application and filter on this data to find the cause.","links":[{"url":"https://aws.amazon.com/xray/faqs/","title":"AWS X-Ray FAQs"},{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html","title":"AWS X-Ray Concepts"}],"answers":[{"id":"bfef3f324b5e71c44d859b637f210e99","text":"Deploy the X-Ray daemon to all ECS Services and view the traces and performance data that is generated, within the X-Ray console.","correct":false},{"id":"6cd41e60c1ed008cbabe084fc5f7c5f5","text":"Enable Enhanced Cloudwatch functionality in each microservice, push all performance information into Cloudwatch Logs and use Log Insights to identify poorly performing APIs.","correct":false},{"id":"92f0c9098189dbfb752878b2b946957f","text":"Utilise the AWS SDK to communicate with the AWS X-Ray API.  These will generate the JSON segments into an S3 bucket and the use SQL statements within AWS Athena to search through the traces stored in the bucket.","correct":false},{"id":"5a8dd485b25fce5ab3b460060dc89f4f","text":"Utilise AWS X-Ray by deploying a Docker image containing the X-Ray daemon to ECS, which will then gather segment data.  Then examine the traces generated to track the path of a request through the application and use Filter Expressions to find traces relating to specific paths.","correct":true}]},{"id":"b2d2d9d6-3ab6-4949-85a3-de49e4783ab9","domain":"ConfigMgmtandInfraCode","question":"In the past, your organisation has had security breaches due to unauthorised changes in infrastructure.  To reduce these, the InfoSec Team have implemented AWS Config so that all changes are recorded.  Any changes to the infrastructure are then monitored by the Service Desk.  However, the following errors; \"We are unable to complete the request at this time. Try again later or contact AWS Support\" are appearing in the console.  The Service Desk Manager has asked you for more information on why these errors are happening.  Choose the correct reasons from below.","explanation":"There are only two issues that would cause this error to appear in AWS Config.  The first is if you make multiple calls to the AWS Config API within a minute and the default rate limiting will then implement a temporary block.  The CloudTrail logs will show the following; \"You have exceeded the maximum request rate. Try again at a later time.\" and you should stop calling the API as frequently.  The second option will be if you exceed the set Aggregator limit (which is 50 by default).  CloudTrail will show; \"The configuration aggregator could not be created because the account already contains '50' configuration aggregators. Consider deleting configuration aggregators or contact AWS Config to increase the limit.\"  No other options listed accurately describe the message.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/config-console-error/","title":"How can I troubleshoot AWS Config console error messages?"},{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"}],"answers":[{"id":"72a656f0e09c8f5382bf1978c403720f","text":"This error is related to the Aggregator limit being reached, and you should contact AWS Support to increase the Configuration Aggregator limit from the default value.","correct":true},{"id":"bb66cca2e29dc015297e99a6c201a347","text":"This error is related to switching to a different region when Remediation is in Progress.","correct":false},{"id":"8d84028643f27b004e826c24087d49db","text":"This error is related to exceeding the rate limiting if you use the API call GetResourceConfigHistory or ListDiscoveredResources with a Lambda function.","correct":false},{"id":"92344dfafeacc6f8c91cc9d5159ce1ff","text":"This error is related to calling the StartConfigRulesEvaluation API more than once per minute.","correct":true},{"id":"c2b54e290aa3e1f5ba5b69b19e872a6a","text":"This error is related to the ConformancePackStatusReason, and you should check it to know more about the reason for failure.","correct":false}]},{"id":"6e252f1f-63d8-43b5-8df5-aaa7d83874d1","domain":"HAFTDR","question":"Your application has a multi-region architecture and database reading and writing is becoming an issue. You are currently storing flat file key-pairs on a shared EFS volume across all of your application servers but it is simply too slow to handle the growth your company is experiencing. Additionally, latency of static files delivered to your customers from S3 has been noted as an issue. Which solution will be not only fast but scalable as you move forward?","explanation":"DynamoDB is the only option that supports multi-region replication and multi-master writes, and it does this using Global Tables.","links":[{"url":"https://aws.amazon.com/dynamodb/global-tables/","title":"Global Tables"}],"answers":[{"id":"e1f2bdaf532480be3c4d53afd2d88645","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use a multi-region read-replica RDS configuration to create a multi-master, cross-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false},{"id":"cdfa2716b07fec163b72bf966bede22e","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use a multi-region write replica database for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false},{"id":"d6880a6902e45e48a98990a1df3f6a3d","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use multi-region rw-replica RDS configuration to create a multi-master, cross-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false},{"id":"cdc9c7113a0cd60f29d5d56c49e1819b","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use Amazon DynamoDB Global Tables to create a multi-master, multi-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":true}]},{"id":"10e64485-5c7d-44b1-adcd-23a1b1b60e7d","domain":"IncidentEventResponse","question":"Your organization is building millions of IoT devices that will track temperature and humidity readings in offices around the world. The data is then used to make automated decisions about ideal air conditioning settings based on that data, and then trigger some API calls to control the units. Currently, the software to accept the IoT data and make the decisions and API calls runs across a fleet of autoscaled EC2 instances. After just a few thousand IoT devices in production, you're noticing the EC2 instances are beginning to struggle and there's just too many being spun up by your autoscaler. If this continues you're going to hit your account service limits and costs will blow out way more than you budgeted for. How can you redesign this service to be more cost effective, more efficient and most importantly, scalable?","explanation":"In this instance, Kinesis Data Analytics with the data streamed with Kinesis Data Streams is the best choice. It's also completely serverless so you are able to save costs by shutting down your EC2 servers.","links":[{"url":"https://aws.amazon.com/kinesis/data-analytics/","title":"Amazon Kinesis Data Analytics"}],"answers":[{"id":"7af4964635eff76f7316e3605f75adaa","text":"Switch to CloudWatch Data Analytics. Stream the IoT data through Kinesis Data Streams and perform your decision making and API triggering with Lambda.","correct":false},{"id":"932e73a0f9bcccb0cb228111d6e91632","text":"Switch to Kinesis Data Analytics. Stream the IoT data with Kinesis Data Streams and perform your decision making and API triggering with Lambda. Shut down your EC2 fleet.","correct":true},{"id":"9e06f33f0b3d84a2d27b1e028103a035","text":"Switch to Kinesis Data Firehose. Stream the IoT data directly, perform your decision making and then store the results in S3. Analyze the S3 content to trigger your API calls with your EC2 fleet. Change the autoscaling metric to use the Firehose capacity.","correct":false},{"id":"ed5129b5b4a876e31ca0fe3383fa6b9e","text":"Switch to Kinesis Firehose Analytics. Stream the IoT data directly, perform your decision making and then store the results in S3. Analyze the S3 content to trigger your API calls with your EC2 fleet. Change the autoscaling metric to use the Firehose capacity.","correct":false}]},{"id":"3e8ee2fb-8946-43b2-ba85-78ce64b74626","domain":"ConfigMgmtandInfraCode","question":"You are discussing error scenarios and possible retry strategies for your Step Functions machine with your colleague. Which of her claims are incorrect?","explanation":"Errors can arise because of state machine definition issues, task failures or because of transient issues. When a state reports an error, the default course of action for AWS Step Functions is to fail the execution entirely.","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-errors.html","title":"AWS Step Functions: Amazon States Language - Errors"}],"answers":[{"id":"8c65f4ebde68eb7bc8a58c7747c9f843","text":"A Retry field with an 'IntervalSeconds' and 'MaxAttempts' value of 3 and 'BackoffRate' value of 1.5 will make three retry attempts after waits of 3, 4.5 and 6.75 seconds.","correct":false},{"id":"c1c080d749a342bf5e9fc2509c11fe40","text":"When a state reports an error, the default course of action for AWS Step Functions is to log the error and perform a single retry after 1 second. If that doesn't succeed, AWS Step Functions will fail the execution entirely.","correct":true},{"id":"5673ef332c854836ed6963374170526a","text":"Any state can encounter runtime errors. Examples are when a Lambda function throws an exception or if a transient network issue exists. AWS Step Functions distinguishes these clearly from Failures such as state machine definition issues that are handled differently.","correct":true},{"id":"51e213b667c0c51b9814eb2430f2d081","text":"A Retrier must contain the 'ErrorEquals' field which is a non-empty array of strings that match Error Names, e.g. 'States.Timeout'. When a state reports an error, Step Functions scans through the Retriers and, when the Error Name appears in this array, it implements the retry policy described in this Retrier.","correct":false}]},{"id":"b299b67c-9e64-46b9-8397-b3d92e5a8e9a","domain":"SDLCAutomation","question":"You have multiple teams of developers and at the moment they all have the ability to start and stop any EC2 instance that they can see in the EC2 console, which is all of them. You would really like to implement some security measures so they can only start and stop the instances based on their cost center. What AWS features would you use to achieve this?","explanation":"You can simplify user permissions to resources by using tags and policies attached to roles. the aws:PrincipalTag is a tag that exists on the user or role making the call, and an iam:ResourceTag is a tag that exists on an IAM resource. In this case we want the CostCenter tag on the resource to match the ConstCenter tag assigned to the developer.","links":[{"url":"https://aws.amazon.com/blogs/security/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles/","title":"Simplify granting access to your AWS resources by using tags"}],"answers":[{"id":"3c58e079022c3f48db6068b4e061539c","text":"Implement tags and restrict access by comparing the iam:PrincipalTag and the aws:ResourceTag in a policy attached to your developer role and seeing if they match.","correct":false},{"id":"3e3f004e8462587e3c0ee0b9692cde3b","text":"Implement EC2 policies which you can assign to each resource which will allow a developer to start or stop the instance if they are also assigned to it.","correct":false},{"id":"1597cc0291688fb61d2e070e6fefc038","text":"Implement tags and restrict access by comparing the aws:PrincipalTag and the iam:ResourceTag in a policy attached to your developer role and seeing if they match.","correct":true},{"id":"d106bcfe8a672539187a3453ec7498d0","text":"Implement roles which you can assign to each resource which will allow a developer to start or stop the instance if they are also assigned to it.","correct":false}]},{"id":"ea7992e6-08c7-4f1b-9391-67be322b64ac","domain":"MonitoringLogging","question":"Your company runs a fleet of spot instances to process large amounts of medical data which can be killed at any time. The processing results are extremely important, and you find that sometimes the servers shut down too quickly for the logs to be copied over to S3 when a batch has completed. It's OK if a batch doesn't complete processing because it will be re-run on another spot instance, but the small case of completing and not getting the logs for that completion is causing headaches. Which is the best solution to implement to fix this issue?","explanation":"The CloudWatch logs agent will stream your logs straight into CloudWatch Logs so nothing will be missed if the server is terminated.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html","title":"Install and Configure the CloudWatch Logs Agent on a Running EC2 Linux Instance"}],"answers":[{"id":"2b9f925fe8658d7ca17378e9d8006c98","text":"Configure the CloudWatch logs agent on the AMI the spot instance is using and stream your logs to CloudWatch Logs directly","correct":true},{"id":"2b8f5ce19a375c3062f4d5f2f3d46d59","text":"Configure the application to ingest the logs directly into Kinesis streams, and use a master logging server to process those logs and copy them into S3","correct":false},{"id":"7da59d6d572879b3f9de3a904c1ed08e","text":"Configure the server to send the logs to SQS on completion and use a Lambda function to poll the SQS queue and save the queue contents to S3","correct":false},{"id":"9808af87ddc6c3fed5dcb05510fc62b5","text":"Configure a cronjob to push the logs to S3 every minute","correct":false}]},{"id":"9404d153-05aa-4c0e-aeb4-5e38aa54f0d9","domain":"MonitoringLogging","question":"Your company has built an app called InstaFaceTube. It's incredibly successful. Your user-base is growing exponentially. Your manager has decided it's time to ensure the web application is as efficient as possible, running with the best possible performance. To achieve this you will have to monitor all aspects of the application and then analyze the issues to determine the root cause of any latencies, errors or issues that might be causing your application to slow down. How do you achieve this using AWS tools?","explanation":"Implementing the AWS X-Ray SDK will assist you in achieving this goal. It will help you understand how your application and its underlying services are performing and give you the information required to analyze issues and determine the root cause of issues.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-gettingstarted.html","title":"Getting Started with AWS X-Ray - AWS X-Ray"}],"answers":[{"id":"5dd4c06446d96cea277f4219bafed0c7","text":"Implement the Amazon QuickSight SDK, send partitions to QuickSight, view the service tables and traces in the QuickSight console.","correct":false},{"id":"1b1c00f2d92850ce3ef939b7be776c78","text":"Implement the AWS X-Ray SDK, send segments to X-ray, view the service graph and traces in the X-Ray console.","correct":true},{"id":"d68e824386322cec32abb02c7d311e66","text":"Implement the Amazon Athena SDK, send sectors to Athena, view the service tables and blocklines in the Athena console.","correct":false},{"id":"84b0ed98a0ac96d03ac7f24dbd70d85c","text":"Implement the AWS Macie SDK, send blocks to Macie, view the service graph and blocklines in the Macie console.","correct":false}]},{"id":"fbc8bae9-74cb-430a-aaf3-7c16cd49f9f9","domain":"MonitoringLogging","question":"You have built a serverless Node.js application which uses Lambda, S3 and a DynamoDB database. You'd like to log some simple metrics so you can possibly graph them at a later date, or analyze the logs for faults or errors. You aren't able to install the CloudWatch Logs agent into a Lambda function however. What do you do instead?","explanation":"console.log will work perfectly in Lambda and is the easiest way to log directly to CloudWatch Logs","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-logging.html","title":"AWS Lambda Function Logging in Node.js"}],"answers":[{"id":"f86fadfb6bf4f2aff4fc541fb9f962d9","text":"Use the log.console commands in Lambda, it will log your logs straight to CloudWatch Logs.","correct":false},{"id":"d694da5d9149fb6683cce0b6ec647e92","text":"Use an API gateway configured to log to CloudWatch Logs","correct":false},{"id":"32ad7a8632717703ab0efe26deafc538","text":"Use the CloudWatch Logs API, which will provide putLogEvents where you can upload logs to CloudWatch.","correct":false},{"id":"856b826225eea424a5e5472bb8242afa","text":"Use the console.log commands in Lambda, it will log your logs straight to CloudWatch Logs.","correct":true}]},{"id":"09effc45-51a5-418a-b1e5-b27430e92d3b","domain":"IncidentEventResponse","question":"Your organisation utilises Kinesis Data Streams to ingest large amounts of streaming data.  Each of the Kinesis Streams is consumed by a Java application running on multiple EC2 instances, utilising the KCL library.  The CloudWatch logs for the KCL application normally only show 'INFO' entries, but you have noticed that some errors have started to appear recently, and you need to investigate.  The errors are all of the form; \"getShard - Cannot find the shard given the shardId\" and include the ID of shard which still appears to exist.  What may have caused these errors to appear in the logs?","explanation":"This error usually appears just after a re-sharding event has completed when the consumer code is using an older version of KCL.  This appears to be caused when the KinesisProxy has a cache of shards which is not updated when leases change in the KCL DynamoDB table.  Although this error appears to have no affect on the data within the shards, it can usually be resolved by restarting the KCL consumers.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/building-consumers.html","title":"Reading Data from Amazon Kinesis Data Streams"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html","title":"Re-sharding, Scaling, and Parallel Processing"}],"answers":[{"id":"54eebd43d1e7d3fcae9de2fcadc3409b","text":"These errors can be seen in the Kinesis Consumer log usually after a re-sharding event and are generated by the KinesisProxy.","correct":true},{"id":"409329597050c414284e9b625163dbfe","text":"These errors are seen when in Enhanced Fan-Out mode and more than 20 consumers are registered per stream.","correct":false},{"id":"d8d212ca3330be75fe9cb3a00d49676d","text":"These errors are generated when migrating the Record Processor from version 1.x to version 2.x of the KCL.","correct":false},{"id":"45bc1b263334c69f60939ca82c41189a","text":"These errors are caused by the shard iterator expiring before your code is ready to utilise it.","correct":false}]},{"id":"e98496ed-60b9-4d96-8cbb-3d3e260da18e","domain":"IncidentEventResponse","question":"Your organisation is 75% through moving its core services from a Data centre and into AWS.  The AWS stacks have been working well in their new environment but you have been told that the Data centre contract will expire in 3 months and therefore there is not enough time to re-implement the remaining 25% of services before this date. As they are already managed by Chef, you decide to move them into AWS and manage them using OpsWorks for Chef. However, when configuring OpsWorks you have noticed the following errors have appeared; \"Not Authorized to perform sts:AssumeRole\" and \"FATAL Could not find pivotal in users or clients!\". Choose the correct options to resolve the errors.","explanation":"With the \"Not Authorized to perform sts:AssumeRole\" error, you can assume its policy/role related and therefore creating a role and attaching the AWSOpsWorksCMServiceRole policy should resolve this issue. Finally, any message which states that you 'cannot find a pivotal user', requires you to add one to the default location. All other answers will not resolve the problems listed.","links":[{"url":"https://docs.aws.amazon.com/en_pv/opsworks/latest/userguide/troubleshoot-opscm.html","title":"Troubleshooting AWS OpsWorks for Chef Automate"},{"url":"https://docs.aws.amazon.com/en_pv/opsworks/latest/userguide/welcome_opscm.html","title":"AWS OpsWorks for Chef Automate"}],"answers":[{"id":"0b86e189ee6ecc6aefa9be08d750c26f","text":"Ensure that the EC2 instance has the AWS service agent running and has outbound Internet access with DNS resolution enabled.","correct":false},{"id":"edd55a54eb6c02470c1f52081eed838d","text":"Create a new service role and attach the AWSOpsWorksCMServiceRole policy to the role. Verify that the service role is associated with the Chef server and it has that policy attached.","correct":true},{"id":"c565172b94c322361ca5823ed0126630","text":"Ensure you have at least one existing EIP address free by deleting unused addresses or by asking AWS Support for an increase.","correct":false},{"id":"0ddbd3212ebd514950bf4aafc59e898c","text":"Install the knife-opc command and then run the command; knife opc org user add default pivotal","correct":true}]},{"id":"bbe1fc54-7ed1-4156-930f-446e13ca5e59","domain":"ConfigMgmtandInfraCode","question":"One of your colleagues has developed a CloudFormation template which creates a VPC and a subnet for each Availability Zone within each Region the Stack is created in.  Although this template is currently functional, it requires the Region and Availability Zones to be passed into the Stack as parameters.  You have been asked by your manager to alter the template so that you can automate all of this functionality using Intrinsic Functions, and eliminate passing in parameters.  Choose options from the list which will meet the requirements of rewriting the template.","explanation":"Intrinsic functions can not be used within the Parameters section and so these options can be immediately ruled out. Using both Fn::GetAZs to return the Availability Zones and Fn:Select to choose from the list and also using Fn::FindInMap to pull back each Region from the Mappings section are both valid options to retrieve the Regions.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"Template Anatomy"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html","title":"Intrinsic Function Reference"}],"answers":[{"id":"9fe027bb7b742b6639a50bc38d88cd6f","text":"In the Mappings section, make a list of each Availability Zone in each Region.  Then in the Resources section use Fn::FindInMap to pull back each AZ using !Ref \"AWS::Region\" as the key.","correct":true},{"id":"c697110279709e26aa4a1219dcf5f6e2","text":"In the Resources section, use !Ref \"AWS::Region\" to return the Region, Fn::GetAZs to return a list of all Availability Zones and Fn:Select to choose each AZ from the list.","correct":true},{"id":"0f58a828daf6079e3560070d348c8d13","text":"In the Parameters section, create three individual parameters, one for each Availability Zone and use !Ref to retrieve the values in the Resources section.","correct":false},{"id":"3eca8a7377d618c7e76e66d97dddfd42","text":"In the Parameters section, use !Ref \"AWS::Region\" to return the Region for each parameter, Fn::GetAZs to return a list of all Availability Zones and Fn:Select to choose each AZ from the list.","correct":false}]},{"id":"5c90df95-736b-4b27-a7cf-095c4dc3d4c4","domain":"HAFTDR","question":"You run a load balanced, auto-scaled website in EC2. Your CEO informs you that due to an upcoming public offering, your website must not go down, even if there is a region failure. What's the best way to achieve this?","explanation":"A latency based routing policy will keep your website as fast as possible for your customers, and will act as redundancy should one of the regions go down.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy"}],"answers":[{"id":"f20603c9e3176b9a3bdd585343418443","text":"Deploy your load balancers and auto-scaled website in two different regions. Create a Route53 Latency Based Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":true},{"id":"560436ce37b94afeee434582179ddc95","text":"Deploy CloudFront in front of your instances. It will cache requests even if a region goes down and your users will not notice.","correct":false},{"id":"20703f2de437222f1e63b4b81216e053","text":"Deploy your load balancers and auto-scaled website in two different availability zones. Create a Route53 weighted Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":false},{"id":"230b3ec4e1a08538bc506cca11e9c1a1","text":"Deploy your load balancers and auto-scaled website in two different availability zones. Create a Route53 GeoProximity Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":false}]},{"id":"4cfee0f4-02a3-43bc-af10-a34d888a0086","domain":"HAFTDR","question":"The Marketing Team of Mega-Widgets Inc. have recently released an advertising campaign in the APAC region, in addition to their home market of Europe.  The CTO has told you that they have started receiving complaints from Australian customers that the site located in eu-west-1 is slow, and they think by utilising the existing Disaster Recovery infrastructure in ap-southeast-2, they can speed up response time for these customers.  As time is of the essence, they have asked you to assist.  You have already confirmed that data is synchronising between both sites.  What is the quickest way to reduce the latency for these customers?","explanation":"As we are not told otherwise, we can assume that the European site is fast for people within Europe and therefore we can assume a high latency is the cause of the problem.  Route 53 Latency-Based Routing sounds like the perfect candidate when utilising the Disaster Recovery site as the main site for Asia Pacific.  CloudFront could offer some help if we only had one site and the Edge locations could cache some content, but in this case the CTO wanted to use the Disaster Recovery infrastructure already in place.","links":[{"url":"https://aws.amazon.com/route53/faqs/","title":"Amazon Route 53 FAQs"},{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Tutorials.html","title":"Tutorials"}],"answers":[{"id":"6752c886fe176d4a7c3fd5d847abf946","text":"In Route 53, create two records for www.mega-widgets.com, a latency record pointing to the European IP and a latency record pointing to the Asia Pacific IP.","correct":true},{"id":"883bf6152c030fbe021fafb5fb55c8d4","text":"In Route 53, create two sub-domain A records of mega-widgets.com, one with the European IP and the other with the Asia Pacific IP. Create two aliases pointing www.mega-widgets.com to both sub-domains.","correct":false},{"id":"0e6075956527bb40627bfab8871eab13","text":"In Route 53, create eu.mega-widgets.com and ap.mega-widgets.com and point them to the same CloudFront endpoint.","correct":false},{"id":"d718f6cb2c4337528dfde78cbe94dfad","text":"Create a CloudFront endpoint with the origin pointing to the European site and then point www.mega-widgets.com to that endpoint.","correct":false}]},{"id":"7cd5551a-b9a0-4e59-bb53-d40d81dc8938","domain":"ConfigMgmtandInfraCode","question":"You have been ask to deploy a clustered application on a small number of EC2 instances.  The application must be placed across multiple Availability Zones, have high speed, low latency communication between each of the nodes, and should also minimise the chance of underlying hardware failure.  Which of the following options would provide this solution?","explanation":"Spread Placement Groups are recommended for applications that have a small number of critical instances which need to be kept separate from each other. Launching instances in a Spread Placement Group reduces the risk of simultaneous failures that might occur when instances share the same underlying hardware. Spread Placement Groups provide access to distinct hardware, and are therefore suitable for mixing instance types or launching instances over time. In this case, deploying the EC2 instances in a Spread Placement Group is the only correct option.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html","title":"Placement Groups"}],"answers":[{"id":"b1954190e825c200d890843b70d5cb38","text":"Create a new VPC with the tenancy type of host and deploy the instances in the VPC","correct":false},{"id":"72371c02b14e73370c1b01dd2523a1c1","text":"deploy the EC2 servers in a Spread Placement Group","correct":true},{"id":"112a2330d77c300b357edda7c17dddb6","text":"Deploy the EC2 servers in a Cluster Placement Group","correct":false},{"id":"f3d57c381ce0c53f5ff05f7a48d8ae15","text":"The application should deployed as a service in ECS","correct":false}]},{"id":"26b5e2d7-e00f-4d7b-b086-c09e029d1de9","domain":"HAFTDR","question":"The world wide cat news powerhouse, Meow Jones, has hired you as a DevOps Database consultant. They're currently using legacy in-house PostgreSQL databases which cost a considerable amount to maintain the server fleet, as well as operational costs for staff, and further hardware costs for scaling as the industry grows. You are tasked in finding an AWS solution which will meet their requirements. They require high throughput, push button scaling, storage auto-scaling and low latency read replicas. Any kind of automatic monitoring and repair of databases instances will also be appreciated. Which AWS service(s) do you suggest?","explanation":"Amazon Aurora will fit the needs perfectly, and the Database Migration Service can assist with the migration.","links":[{"url":"https://aws.amazon.com/rds/aurora/details/postgresql-details/","title":"Amazon Aurora Features: PostgreSQL-Compatible Edition"}],"answers":[{"id":"e96297fedff7f3062d962bbd0a387cb4","text":"An auto-scaled, load balanced EC2 fleet running PostgreSQL with data shared via EFS volumes.","correct":false},{"id":"0a663a4b84931c9af9abadcccde84f3f","text":"Keep the current PostgreSQL databases and implement an ElastiCache to cache common queries and reduce load on your in-house databases to save on upgrade costs.","correct":false},{"id":"6129915cbe4cbc9e1ddb0b74808beded","text":"Amazon Aurora, AWS Database Migration Service","correct":true},{"id":"9802017b44b6d60fc846a356c1fa9e9a","text":"A cluster of Amazon RDS PostgreSQL instances, AWS Database Migration Service.","correct":false}]},{"id":"10530be3-3d3d-4b54-875f-066cc6789c22","domain":"ConfigMgmtandInfraCode","question":"Your company APetGuru has been developing its website and companion application for a year, and everything is currently deployed manually. You have EC2 servers and load balancers running your website front end, your backend is a database using RDS and some Lambda functions. Your manager thinks it's about time you work on some automation so time isn't wasted on managing AWS resources. He would also like the added benefit of being able to deploy your entire environment to a different region for redundancy or disaster recovery purposes. Your RTO is an hour, and RPO is two hours.  Which AWS services would you suggest using to achieve this?","explanation":"AWS CloudFormation Stack Sets will suit the requirements, using the same S3 bucket for source code, and a recent RDS snapshots will allow a RPO of one hour.","links":[{"url":"https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/","title":"Use CloudFormation StackSets to Provision Resources Across Multiple AWS Accounts and Regions | AWS News Blog"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopyDBSnapshot","title":"Copying a Snapshot - Amazon Relational Database Service"}],"answers":[{"id":"3d27ef7d6046ef9df655199e0943af4c","text":"Automatically take an hourly snapshot and use a EventWatch event trigger to run a Lambda function which copies the snapshot to your DR region.  Use AWS CloudFormation Stack Sets to build your infrastructure in your Production region. Parameterise the stack set to use the snapshot ID to deploy RDS resources from snapshot.  Use Systems Manager to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CloudFormation StackSet to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.","correct":false},{"id":"530f3a5b92e4090b35ff7569343314b5","text":"Automatically take an hourly snapshot and use a CloudWatch event trigger to run a Lambda function which copies the snapshot to your DR region.  Use AWS CodeDeploy Deployment Sets to build your infrastructure in your Production region. Parameterise the deployment set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CodeDeploy Deployment Set to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.","correct":false},{"id":"0d3bb19cd25dd0111cf3edda16072d8e","text":"Automatically take an hourly snapshot using AWS Step functions which then invokes a Lambda function to copy the snapshot to your DR region.  Use AWS CodeFormation Sets to build your infrastructure in your Production region. Parameterise the CodeFormation Set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CodeFormation Set to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.","correct":false},{"id":"bd4c9a245ec35f4087247b3a463f5d41","text":"Automatically run an AWS Step function to take an hourly snapshot then run a Lambda function to copies the snapshot to your DR region.  Use AWS CloudFormation Stack Sets to build your infrastructure in your Production region. Parameterise the stack set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CloudFormation StackSet to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.","correct":true},{"id":"3373d70ebc5df7fa36010e5b49e31e69","text":"Automatically take an hourly snapshot and use Cloud Scheduler trigger to run a Lambda function which copies the snapshot to your DR region.  Use AWS CodePipeline Regional Deploy Set to deploy your infrastructure in your Production region. Parameterise the deploy set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CodePipeline Regional Deploy Set to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.","correct":false}]},{"id":"1c7dd43f-b2c7-49aa-b897-bd62f97ee183","domain":"ConfigMgmtandInfraCode","question":"A company has successfully deployed a two EC2 node Redis cluster, using CloudFormation.  In order for the cluster to work, Node one is started and populated with data before Node two is started. Some weeks later, a new Redis stack is created using the same template, but is populated by a different set of data.  In this case the stack consistantly fails to be created eventhough the template and parameters are identical to the original stack.  Which actions should be taken to ensure that the new cluster can successfully be created?","explanation":"If, as in this case, the order of resources is important, it should never be assumed that they will always stand up in the same order each time, the first stack may have deployed in the correct order by luck.  A 'DependsOn' resource should always be used to maintain order consistancy.  Also, the data that is populating Redis may be much larger and take more time in the second Stack.  Adding a Creation Policy with a large Timeout will ensure that data successfully completes the import and the EC2 instance is successfully created.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"Template Anatomy"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-creationpolicy.html","title":"CreationPolicy Attribute"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-dependson.html","title":"DependsOn Attribute"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html","title":"Working with Nested Stacks"}],"answers":[{"id":"9a01df5c5a78cacac35b87d8678c7d74","text":"Use a single CloudFormation template and add a Creation Policy and Deletion Policy with a Retain option to maintain both EC2 Redis nodes.","correct":false},{"id":"a7e04cbb4e145efffd811f7596f2fbdf","text":"Use two CloudFormation templates, one for each EC2 Redis node and manually run one at at time.","correct":false},{"id":"d1b2270b9abfe6f978128e05ec79268d","text":"Use a single CloudFormation template and add a Creation Policy with a large Timeout, to the first EC2 node.","correct":true},{"id":"af31ddbdf176fd17fce491dc05ba1388","text":"Use a CloudFormation root template which uses nested stacks.  Define each Redis node in a separate template which are linked to by the root template.","correct":false},{"id":"002f82cd165be0c2dd0d4bfba60bde24","text":"Use a single CloudFormation template and add a 'DependsOn' resource for the second EC2 node which points to the first EC2 node.","correct":true}]},{"id":"e742c07d-c719-4c53-9615-292147e77480","domain":"ConfigMgmtandInfraCode","question":"In the Amazon States Language, InputPath, Parameters, ResultPath, and OutputPath filter and control the flow of JSON from state to state. Which of the following definitions are correct?","explanation":"The definition of OutputPath and ResultPath are swapped around, i.e. an OutputPath can filter the JSON output to further limit the information that is passed to the output while the ResultPath selects what combination of the state input and the task result to pass to the output","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html","title":"Input and Output Processing in Step Functions"},{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/input-output-inputpath-params.html","title":"InputPath and Parameters"}],"answers":[{"id":"84df9b15c00389ac9ecd3ba94d81a165","text":"OutputPath selects what combination of the state input and the task result to pass to the output.","correct":false},{"id":"9d857343a25dab2067293a92f66a0fbf","text":"InputPath selects which parts of the JSON input to pass to the task of the Task state.","correct":true},{"id":"cfd8d208af0b74c2e7d0fdc920690032","text":"Parameters enable you to pass a collection of key-value pairs, where the values are either static values that you define in your state machine definition, or that are selected from the input using a path.","correct":true},{"id":"1063b9865beb745109392a1abe5501c8","text":"ResultPath can filter the JSON output to further limit the information that is passed to the output.","correct":false}]},{"id":"d1e18980-dcfc-4893-afce-cb19efacb6a9","domain":"IncidentEventResponse","question":"Your CEO has heard how an ELK stack would improve your monitoring, troubleshooting and ability to secure your AWS environment. Before letting you explain anything about it, he demands you get one up and running as soon as possible using whatever AWS services you need to use. How do you go about it?","explanation":"The Amazon Elasticsearch service will give you managed Elasticsearch, Logstash and Kibana without the requirement of installing, maintaining or scaling any of them and their associated infrastructure.","links":[{"url":"https://aws.amazon.com/elasticsearch-service/resources/articles/the-benefits-of-the-elk-stack/","title":"The benefits of the ELK stack without the operational overhead"}],"answers":[{"id":"2c6e86f82d079bd47ee2f084bbe0fa2d","text":"Install Elasticsearch, Logstash and Kibana on an EC2 instance.","correct":false},{"id":"c18a97d462ec5c25526d8376971d37b2","text":"Install CloudSearch, Logstash and Kibana on an EC2 instance.","correct":false},{"id":"de19d8eddb68265815d3495f5ee053dd","text":"Use CloudSearch, CloudWatch Logs and CloudKibana managed services to create your ELK stack.","correct":false},{"id":"04de12d4521b9dc826f59dc0d42f97f5","text":"Use the Amazon Elasticsearch Service.","correct":true}]},{"id":"85ec1f79-e327-44c4-a934-46e826ec0dd3","domain":"HAFTDR","question":"You currently run an autoscaled application which is database read heavy. Due to this, you are making use of a read replica for all application reads. It's currently running at around 60% load with your user base but you expect your company growth to double the amount of users every 6 months. You need to forward plan for this and determine methods to ensure the database doesn't become a bottleneck while still maintaining some redundancy. What is the best way to approach this issue?","explanation":"More read replicas will ease the load on your current ones, and load balancing them with a weighted routing policy will ensure they're not a single point of failure for your application.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/requests-rds-read-replicas/","title":"How can I distribute read requests across multiple Amazon RDS read replicas?"}],"answers":[{"id":"17ef3669be9b09f4623719b5771bcb40","text":"Create another read replica and deploy a second autoscaled version of your application. Point it at your second read replica.","correct":false},{"id":"43d1e6030522d9685ec499098adce943","text":"Monitor your database read replica usage in CloudWatch alerts. When it's close to 90% capacity perform an online resize to a larger instance type.","correct":false},{"id":"77851b21f9bb1d7557b828c6ad224a60","text":"Create more read replicas. Use a Route53 weighted routing policy to ensure the load is spread across your read replicas evenly.","correct":true},{"id":"c67fa8f440af68765fc377331fb22bb7","text":"Deploy an additional Multi-AZ RDS read replica and modify your application to use it instead.","correct":false}]},{"id":"69f350ad-db3a-4b83-bf8e-15ea0c0df866","domain":"PoliciesStandards","question":"Your company is preparing to become ISO 27001 certified and your manager has asked you to propose a comprehensive solution to log configuration and security changes in a separate audit account.  Specifically, the solution should ensure IAM users have MFA enabled, identify S3 buckets which aren't encrypted and enforce the addition of specific tagging.  Identify which option solves the problem.","explanation":"AWS Config is the only service that will meet all of the requirements in the question as it records configuration changes and snapshots the configuration at regular intervals set by you. Data aggregation means that AWS Config data from multiple accounts can be stored in a single account.  The following built in rules; s3-bucket-server-side-encryption-enabled and iam-user-mfa-enabled identify any S3 buckets not encrypted and any IAM accounts that do not have MFA enabled.  Tagging is also available for AWS Config resources that describe AWS Config rules.","links":[{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"},{"url":"https://aws.amazon.com/cloudtrail/faqs/","title":"AWS CloudTrail FAQs"}],"answers":[{"id":"0e1d676582953a5a30bcd07fa6996329","text":"Enable AWS Config and AWS Cloudtrail to track changes in all resources and to identify IAM user API calls with MFA enabled.","correct":false},{"id":"8c9a63c97e58123416200044d646218a","text":"Enable Enhance Logging in AWS Cloudwatch to track all security and configuration changes and view these using Cloudwatch Logs Insights.","correct":false},{"id":"efd3ca0f78115a7a32bffa7905edd17b","text":"Enable AWS Config and create three default rules to check whether IAM users have MFA enabled, S3 buckets have server side encryption and tagging is added to resources.","correct":true},{"id":"2f426c67cf1036d34927bdb5aac020ba","text":"Enable AWS Cloudtrail to check for MFA enabled IAM users, configure Server access logging in S3 to view the encryption status and use a CloudFormation Templates to add tagging.","correct":false}]},{"id":"5125f905-4372-437c-838f-63de5bf0354f","domain":"HAFTDR","question":"Your current application uses an Aurora database, however the speeds aren't as fast as you would like for your bleeding edge website. You are too deep into developing your application to be able to change the database you are using or to implement faster or larger read replicas. Your application is read-heavy, and the team has identified there are a number of common queries which take a long time to be returned from Aurora. What recommendations would you make to the development team in order to increase your read performance and optimise the application to use Aurora?","explanation":"ElastiCache will cache common queries by holding them in memory instead of on disk, and will speed up your application considerably","links":[{"url":"https://aws.amazon.com/elasticache/faqs/","title":"Amazon ElastiCache FAQs"}],"answers":[{"id":"d2f04a86c7627f46eed3695113f9d1d9","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should switch to Aurora Serverless.","correct":false},{"id":"c639fb8842c60b552eacb76b7583ebc3","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should create a read-optimised replica and redirect all application reads to that endpoint.","correct":false},{"id":"fd97848b273db8bc7178cc86c812aa15","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should increase the database instance size to a low-latency instance.","correct":false},{"id":"f9d6e50f24acc22852ddfc3f2dbfb914","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should Implement ElastiCache between your application and the database.","correct":true}]},{"id":"ee99751c-1258-4f81-adf9-31a8c57b2605","domain":"ConfigMgmtandInfraCode","question":"Your CloudFormation template is becoming quite large, so you have been tasked with reducing its size and coming up with a solution to structure it in a way that works efficiently. You have quite a few resources defined which are almost duplicates of each other, such as multiple load balancers using the same configuration. What CloudFormation features could you use to help clean up your template?","explanation":"Nested stacks should be used to reuse common template patterns.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#nested","title":"AWS CloudFormation Best Practices"}],"answers":[{"id":"b8d5d665b7d8b4abfc48daa77a64c021","text":"Use nested stacks. This will allow a dedicated templates to be defined for services that are used multiple times","correct":true},{"id":"242cb2d9063dee2cfd0a6b0cabacb580","text":"Use goto policies. These allow you to refer to different sections of your template for reuse.","correct":false},{"id":"86793dc47df7cca5d373cdb3f8fc8f61","text":"Use Intrinsic functions. This allows dedicated functions to be called and parameters passed to generate resources.","correct":false},{"id":"8a32bed4f4f47b1c84105ed6173ba8c4","text":"Use custom resources. They can be called by your stack to define resources without having to reuse their code.","correct":false}]},{"id":"d4166e07-35b5-4196-b355-a04793003f88","domain":"HAFTDR","question":"You currently have a lot of IoT weather data being stored in a DynamoDB database. It stores temperature, humidity, wind speed, rainfall, dew point and air pressure. You would like to be able to take immediate action on some of that data. In this case, you want to trigger a new high or low temperature alert and then send a notification to an interested party. How can you achieve this in the most efficient way?","explanation":"Using a DynamoDB stream is the most efficient way to implement this. It allows you to trigger the lambda function only when a temperature record is created, thus saving Lambda from triggering when other records are created, such as humidity and wind speed.","links":[{"url":"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/","title":"DynamoDB Streams Use Cases and Design Patterns"}],"answers":[{"id":"bfa4fc6ae9d9c31b4764750964fc2583","text":"Use CloudWatch custom metrics to plot your temperature readings and generate an event alert if it breaches your high and low thresholds.","correct":false},{"id":"295bff0a7cd575c34ecae937a4f12f8e","text":"Use a DynamoDB stream and Lambda trigger only on a new temperature reading. Send a SNS notification if a record is breached.","correct":true},{"id":"3cca31ba4defd9f00540ba540543c0af","text":"Modify your IoT devices to also log their data to Kinesis Data Firehose and trigger a Lambda function which will check for new high or low temperatures. Send a SNS notification.","correct":false},{"id":"86d964e70883ad4a58b196f134df686c","text":"Write an application to use a DynamoDB scan and select on your Sort key to determine the maximum and minimum temperatures in the table. Compare them to the existing records and send an SNS notification if they are breached. Run the application every minute.","correct":false}]},{"id":"dc3d128f-8cce-4541-b6e6-be1ec6cea96d","domain":"SDLCAutomation","question":"Your CI/CD pipeline generally runs well, but your manager would like a report of some CodeBuild metrics, such as how many builds were attempted, how many builds were successful and how many builds failed in an AWS account over a period of time. How would you go about gathering the data for you manager?","explanation":"These are default CloudWatch metrics that come with CodeBuild.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html","title":"Monitoring Builds with CloudWatch Metrics"}],"answers":[{"id":"19e6e35ed627bf15d10ddc88af1ab9be","text":"Configure a CloudWatch custom metric to track the build information, and create custom graphs in the CloudWatch console.","correct":false},{"id":"c5045cc818946fcb9c3f6d71744202b7","text":"Configure CodeBuild to log builds to CloudWatch Logs, and then write a metric filter which will graph the data points your manager requires.","correct":false},{"id":"a1f83b40240e6af928c6cab6eeb010c1","text":"CloudWatch metrics will report these metrics by default. You can view them in the CloudWatch console.","correct":true},{"id":"6149c7ca08a16f697f2988509bb61427","text":"Implement a lambda function to poll the CodeBuild API to gather the data and store it in CloudWatch Logs. Write a metric filter to graph the data and generate your report.","correct":false}]},{"id":"b8240606-8f74-4b39-ad43-d8374d4b275d","domain":"MonitoringLogging","question":"Your organization has been using AWS for 12 months. Currently, you store all of your custom metrics in CloudWatch. Per company policy, you must retain your metrics for 3 years before it is OK to discard or delete them. Is CloudWatch suitable?","explanation":"CloudWatch will retain metrics for 15 months, after which they will expire. If you need to keep metrics for longer you must pull them out of CloudWatch using their API and store them in a database somewhere else.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html","title":"CloudWatch Concepts"}],"answers":[{"id":"cf00bc1c445d1daf231db593c6f998d9","text":"Yes, CloudWatch will retain custom metrics for 5 years.","correct":false},{"id":"47ecaf50014e4dd77522488586ae375a","text":"Yes, CloudWatch will retain custom metrics for 3 years.","correct":false},{"id":"8faab65d1ba3ce5e0cb52e471493dd0b","text":"No, CloudWatch only retains metrics for 24 months. You will have to use the API to pull all metrics and store them somewhere else.","correct":false},{"id":"d829ad2942daa681d42ad4237f50827c","text":"No, CloudWatch only retains metrics for 15 months. You will have to use the API to pull all metrics and store them somewhere else.","correct":true}]},{"id":"d498c13f-5b8b-4612-a384-e2d4b6c32014","domain":"PoliciesStandards","question":"Your production website with over 10,000 viewers a day has been ridiculed because your SSL certificate expired, prompting a warning to be shown to every visitor over a period of 8 hours until you woke up and renewed the certificate. Your competitors jumped on this opportunity to claim your company doesn't care about it's customers and you've seen a 20% drop in new sign-ups since then. Obviously, management are furious. What can you do to ensure this never happens again?","explanation":"You will have to generate new certificates to use with AWS Certificate Manager. It will monitor the expiration date of a certificate you import, but it wont be able to automatically replace them when it's close to expiry.","links":[{"url":"https://aws.amazon.com/certificate-manager/features/","title":"AWS Certificate Manager Features"}],"answers":[{"id":"1940cd7673490afba20e52619195b14a","text":"Import your certificates to AWS Certificate Manager and let it handle auto renewal.","correct":false},{"id":"f5ff556c66dcb0eadc9184c5dc9bc8b3","text":"Implement a lambda function to run daily and check the expiry of your SSL certificates. Send an email to an SNS group that the entire System Administration team are signed up to when your certificate is one day out from expiry.","correct":false},{"id":"90133a460fe706b9055946a6042f1e33","text":"Implement a python script that checks if your certificates are going to expire","correct":false},{"id":"2011dbaa257893954cd9b6fcc98ffb0b","text":"Generate new certificates to use with AWS Certificate manager and let it handle auto renewal.","correct":true}]},{"id":"b1b39162-3af1-4aa5-9cd4-4e30338d07f6","domain":"HAFTDR","question":"Your CEO wants you to start future proofing your AWS environment, so he's asked you to look into IPv6 compatibility of your existing Load Balanced EC2 stack. You make use of both Application (ALB) and Network (NLB) load balancers in your EC2-VPC. What are your findings?","explanation":"At this moment in time only the Application Load Balancer supports IPv6 in the EC2-VPC environment. Classic Load balancers only support IPv6 if you are using an EC2-Classic environment.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/01/announcing-internet-protocol-version-6-ipv6-support-for-elastic-load-balancing-in-amazon-virtual-private-cloud-vpc/","title":"Announcing Internet Protocol Version 6 (IPv6) support for Elastic Load Balancing in Amazon Virtual Private Cloud (VPC)"}],"answers":[{"id":"4aff7beb08ea7fe4fa49ab96903dfc41","text":"Application Load balancers do not support IPv6, Network Load balancers do.","correct":false},{"id":"eda9ef0a0ac30b1db3a5bf8674687603","text":"Application and Network Load Balancers both support IPv6.","correct":false},{"id":"12d596f99a6d191c9c1aaae49901a9e1","text":"Application Load balancers support IPv6, Network Load Balancers do not.","correct":true},{"id":"2a74b0b6126f559f800f0f5cef044526","text":"No Load Balancers in EC2 support IPv6.","correct":false}]},{"id":"4c02fcad-e6c2-438b-927c-a343af2e4e89","domain":"SDLCAutomation","question":"About a dozen people collaborate on a company-internal side project using CodeCommit. The developer community is spread across multiple timezones and relies on repository notifications via email. Initially, it was configured for all event types but this resulted in a lot of emails and team members complained about too much 'noise'. Since then, commit comment notifications have been turned off. How can you improve this situation?","explanation":"You can create up to 10 triggers for each CodeCommit repository. The customData field is of type string and is used for information that you want included in the Lambda function such as the IRC channel used by developers to discuss development in the repository. It cannot be used to pass any dynamic parameters. This string is appended as an attribute to the CodeCommit JSON returned in response to the trigger. You can comment on an overall commit, a file in a commit, or a specific line or change in a file. For best results, use commenting when you are signed in as an IAM user. The commenting functionality is not optimized for users who sign in with root account credentials, federated access, or temporary credentials. By default, an Amazon SNS topic subscriber receives every message published to the topic. To receive a subset of the messages, a subscriber must assign a filter policy to the topic subscription","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html","title":"Manage Triggers for an AWS CodeCommit Repository"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda-cc.html","title":"Example: Create a Trigger in AWS CodeCommit for an Existing AWS Lambda Function"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-commit-comment.html","title":"Comment on a Commit in AWS CodeCommit"},{"url":"https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html","title":"Amazon SNS Message Filtering"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-repository-email.html","title":"Configuring Notifications for Events in an AWS CodeCommit Repository"}],"answers":[{"id":"a74a895989f0aff5ae55553849a46032","text":"For each team member, create an individual SNS topic and a CodeCommit trigger that uses a Lambda function to filter out notifications not authored by that developer. The remaining ones are send to that SNS topic. This allows users to selectivly subscribe to specific persons to follow their activities.","correct":false},{"id":"a00a4fd86554d04bbf089d9b1f302ca3","text":"Assign Amazon SNS subscription filter policies to the 'commit comment' topic subscriptions so that team members receive only a subset of the messages.","correct":true},{"id":"421e56d9693bf5c7a95f304f8be452c8","text":"Create a CodeCommit trigger that invokes a Lambda function when a new comment is added to a commit. Configure it so that its 'Custom data' field is populated with the email address of the user who authored the original commit, i.e. ${commit.author.email}. In the function, use SNS to send the notification to that address.","correct":false},{"id":"bb2ee7c9a6b15ed2c648ce414e7f5501","text":"Ask all team members to sign in to CodeCommit as IAM users.","correct":true}]},{"id":"dd926464-3558-4d65-8dff-ab54005ac1a3","domain":"ConfigMgmtandInfraCode","question":"ADogGuru is developing a new Node.js application which will require some servers, a mySQL database and a load balancer. You would like to deploy and maintain this in the easiest way possible, have the ability to configure it with basic configuration management code, which is a new concept for ADogGuru. You also need to allow non-technical people to deploy software to servers and for managers to administer access. A key component of this setup will also be the ability to rollback deploys should you need to. Which service(s) and configuration management system will you choose to achieve this with  the LEAST operational overhead?","explanation":"OpsWorks can achieve all of stated requirements, deployments are possible with a few clicks in the UI, and can be rolled back. It also supports Chef solo as a built-in configuration management system, which is the recommended solution for those not already using Puppet or Chef. Running a separate Chef Automate instance is unnecessary overhead and cost when Chef Solo will suffice.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/welcome.html","title":"What Is AWS OpsWorks? - AWS OpsWorks"}],"answers":[{"id":"e570ae2d7902cce0694b0d33367b527d","text":"Use AWS OpsWorks to create an application using pre-built layer templates to create your servers, mySQL RDS instances and load balancer. Use recipes running in Chef solo for configuration management. Grant your non-technical staff the 'deploy' permission level, and the administrator the 'Manage' permission level.","correct":true},{"id":"b2721cac2e6b9e28bd6af5140227bde4","text":"Use AWS Elastic Beanstalk to create an application using pre-built layer templates to create your servers, mySQL RDS instances and load balancer. Use the OpsWorks Puppet Enterprise server for configuration management. Grant your non-technical staff the 'deploy' permission level, and the administrator the 'Manage' permission level.","correct":false},{"id":"4b57fb90ae5763059a11fa1dd441ad86","text":"Use AWS OpsWorks to create an application using pre-built layer templates to create your servers, mySQL RDS instances and load balancer. Also deploy OpsWorks for Chef Automate and run recipes for configuration management. Grant your non-technical staff the 'deploy' permission level, and the administrator the 'Manage' permission level.","correct":false},{"id":"0df4ca579bacd9555bc9b14d5e3c3441","text":"Use AWS OpsWorks to create an application using pre-built layer templates to create your servers, mySQL RDS instances and load balancer. Use OpsWorks for Puppet Enterprise for configuration management. Grant your non-technical staff the 'deploy' permission level, and the administrator the 'Manage' permission level.","correct":false}]},{"id":"98efc6ca-1ded-4c51-94dd-7aa4ff3e835e","domain":"SDLCAutomation","question":"Your manager has asked you to investigate different deployment types for your application. You currently use Elastic Beanstalk so you are restricted to what that service offers. Your manager thinks it would be best to only deploy to a few machines at a time, so if there are any failures only the few that have been updated will be affected, making a rollback easier. To start with, your manager would like the roll-outs to occur on 2 machines at a time but would like the option to increase that to 3 at a time in the future. Deployments are done during quiet periods for your application, so reduced capacity is not an issue.  Which deployment type do you implement, and how can you get the most reliable indication that deployments have not introduced errors to the servers?","explanation":"The rolling deployment policy is suitable. Additional batch is not specified as a requirement.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html","title":"Elastic Beanstalk Deployment Policies and Settings"}],"answers":[{"id":"3ab638a2611327523fa489ffd5358081","text":"Implement Elastic Beanstalk's Rolling deployment policy. Within the Rolling update policy set Rolling based on Health with batch size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":true},{"id":"afd11d75ca714f8fd9bc5435eb0279b3","text":"Implement Elastic Beanstalk's Immutable deployment policy. Within the update policy set batch size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":false},{"id":"7257c3141e580ca3d648f8ccd2e3e77e","text":"Implement Elastic Beanstalk's Two-at-once deployment policy. Within the Two-at-once update policy set update based on Health with batch size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":false},{"id":"88b8bcbc9a5f8d43bd4e059cf5b1ee0f","text":"Implement Elastic Beanstalk's Rolling with additional batch deployment policy. Within the policy, set update based on Health with size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":false}]},{"id":"ec0b5238-f2d5-437c-9d06-6d0011f2ae8c","domain":"IncidentEventResponse","question":"Your organisation has dozens of AWS accounts owned and run by different teams and paying for their own usage directly in their account.  In a recent cost review it was noticed that your teams are all using on-demand instances. Your CTO wants to take advantage of any pricing benefits available to the business from AWS. Another issue that keeps arising involves authentication. It's difficult for your developers to use and maintain their logins all of their accounts, and it's also difficult for you to control what they have access to. What's the simplest solution which will solve both issues?","explanation":"AWS Single Sign-On allows you to centrally manage all of your AWS accounts managed through AWS Organizations, and it will also allow you to control access permissions based on common job functions and security requirements.","links":[{"url":"https://aws.amazon.com/single-sign-on/","title":"AWS Single Sign-On."}],"answers":[{"id":"398b5351a11512e388505258eb119be1","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Create a Billing account for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Switch to role-based authentication policies in IAM to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":false},{"id":"76f5be58299aec4cdb3dc958b35a5f6a","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Create a Billing account for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Use group-based authentication policies in IAM to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":false},{"id":"87c78562df56f4879a6773d20c92b308","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Create a Billing account for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Use AWS Single Sign-On to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":true},{"id":"e31c64ffefb763cb85b2e44f4a6f7bb6","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Raise a Support request with Amazon in order to link the accounts for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Use LDAP to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":false}]},{"id":"fddf85cb-72e6-4fa1-9c80-07536a84a6e8","domain":"HAFTDR","question":"You have a new website design you would like to test with a small subset of your users. If the test is successful, you would like to increase the amount of users accessing the new site to half your users. If that's successful and your infrastructure is able to scale up correctly, you would like to completely roll over to the new design and then decommission the servers hosting the old design. Which of these methods do you choose?","explanation":"A weighted routing policy combined with an auto-scaling group will meet your requirements and will continue to scale if your tests are successful and you completely roll over to the new design.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted","title":"Weighted Routing"}],"answers":[{"id":"07123797fd0770bb8b71a6b339d36bfc","text":"Install the new website design in a new AutoScaling group. Use a Weighted Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 50%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":true},{"id":"8d8f29ecca384ae73a4006c686df91b8","text":"Install the new website design in a new AutoScaling group. Create a Lambda function to modify your Route53 apex record to use the new AutoScaling group for 5% of the day. If that's successful then modify the function to change the apex record for half the day, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false},{"id":"ff9caa8ea679855cc9425d3e553bf303","text":"Install the new website design in a new AutoScaling group. Use an A/B Test Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 20%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false},{"id":"1a147be1638db0d199c9779bb5488bd7","text":"Install the new website design in a new AutoScaling group. Use a Weighted Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 20%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false}]},{"id":"1e2dcf0c-3e73-455f-a757-d704864b5194","domain":"ConfigMgmtandInfraCode","question":"Your CEO loves serverless. He wont stop talking about how your entire company is built on serverless architecture. He attends Serverlessconf to talk about it. Now, it's up to you to actually build the web application he's been talking about for 6 months. Which AWS services do you look at using to both create the application and orchestrate your components?","explanation":"AWS Lambda is the correct choice for compute in a serverless environment, as is DynamoDB for NoSQL databases and S3 for storage. AWS Step Functions are used to orchestrate your components, such as your lambda functions.","links":[{"url":"https://aws.amazon.com/serverless/","title":"Serverless Computing – Amazon Web Services"}],"answers":[{"id":"e9ce4547402d74b7fd336d09bf24e58b","text":"Create your application using AWS Lambda for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Serverless Application Framework can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":false},{"id":"f3b92e9d4214f817c65085013da23eb8","text":"Create your application using AWS Lambda for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":true},{"id":"a4414e5d2e313708a970e89063df27af","text":"Create your application using AWS Elastic Compute for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":false},{"id":"86fe4a20afac7056ea6e0f5dd7f2e70d","text":"Create your application using AWS Elastic Beanstalk for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Cold Store will allow you to archive old files cheaply.","correct":false}]},{"id":"21d1048a-2c33-4d84-af1a-ad6a039e0ddd","domain":"SDLCAutomation","question":"Your organization currently runs a hybrid cloud environment with servers in AWS and in a local data center. You currently use a cronjob and some bash scripts to compile your application and push it out to all of your servers via SSH. It's both difficult to log, maintain and extend to new servers when they are provisioned. You've been considering a move to CodePipeline to manage everything for you. Will it suit your requirements?","explanation":"CodeBuild is able to deploy to any server that can run the CodeDeploy agent, whether in AWS or in your own data centre.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/instances.html","title":"Working with Instances for CodeDeploy"}],"answers":[{"id":"f81f6e25dfbc48337f9ed9f263923302","text":"No, CodeDeploy wont be able to deploy to the non-AWS managed servers in your data center","correct":false},{"id":"2c74ebd4177ccf3f0749b4ecdcd73e7c","text":"No, CodePipeline wont be able to deploy to the non-AWS managed servers in your data center","correct":false},{"id":"beba600b1da886e2a2ad8713309745a2","text":"Yes, CodePipeline will be able to interface with any servers that can run the CodePipeline agent","correct":false},{"id":"299b7a966370e0bfb587d4c2d5ff774b","text":"Yes, CodeDeploy can deploy to any servers that can run the CodeDeploy agent","correct":true}]},{"id":"8c127356-141c-4199-bfd4-f1d8e0a8560d","domain":"IncidentEventResponse","question":"You have taken over a half finished project to implement Kinesis Data Streams.  Data is being successfully and efficiently placed onto the streams using KPL based code, but when running the KCL code to read off the shards, you are experiencing issues.  The main issues relate to slower than expected reading from the shards, but you are also seeing records being skipped.  Which options allow you to resolve both of these problems?","explanation":"Many of the above solutions answer Kinesis related questions, but we specifically need to know what could cause a slow down in reading from a shard, and why records would be skipped.  For this, there are only two correct answers.  For slow reading, it could be due to the maxRecords value being set too low or the code logic, which is calling processRecords, being inefficient and causing high CPU usage or blocking.  However, there is only one correct answer for the skipping of records and that is usually due to processRecords calls having un handled exceptions.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html","title":"Troubleshooting Amazon Kinesis Data Streams Consumers"}],"answers":[{"id":"989df0ef5880a136eb3db849ff45dc66","text":"Ensure the maxRecords value for the GetRecords call isn't set below the default setting.  Also, check that the processRecords call is not throwing an unhandled exception.","correct":true},{"id":"abd17cf52b5933e670c26b2caed0264c","text":"Choose a failover time that is less than 10 seconds, to ensure reading from a shard maintains a 1 to 1 relationship.  Also, make sure the consumers do not exceed the read per-shard limit.","correct":false},{"id":"b579dde17faaedc85ca4956edf187830","text":"Ensure the maxRecords value for the GetRecords call isn't set below the default setting.  Also, make sure you aren't seeing a sudden large increase in the GetRecords.IteratorAgeMilliseconds metric.","correct":false},{"id":"61bd51378999bcd6a2d6551c0cc12100","text":"Check the logic of the code to ensure the processRecords call isn't CPU intensive or I/O blocking.  Also, check that the processRecords call is not throwing an unhandled exception.","correct":true},{"id":"c6314cbc442d2cd7efa28180d7a5abcf","text":"Ensure you have made a GetRecords call at least every 5 minutes.  Also, check that the processRecords call is not throwing an unhandled exception.","correct":false}]},{"id":"dae6c09f-c2e7-4b01-b0ec-95ed07a7bab2","domain":"HAFTDR","question":"Due to some recent performance issues, you have been asked to move your existing Product Information System to Amazon Aurora.  The database uses the InnoDB storage engine, with new products being added regularly throughout the day. As the database is read heavy, the decision has also been made to add a Read Replica during the migration process.  The changeover completes successfully, but after a few hours you notice that lag starts to appear between the Read/Write Master and the Read Replica.  What actions could you carry out to reduce this lag?","explanation":"One of the most obvious causes of Replication lag between two Aurora databases is because of settings and values, so making the storage size comparable between the source DB and Read Replica is a good start to resolving the issue, as is ensuring compatible DB Parameter settings, such as with the max_allowed_packet parameter.  Turning off the Query Cache is good for tables that are modified often which causes lag, because the cache is locked and refreshed often. No other options are correct.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html","title":"Troubleshooting for Aurora"}],"answers":[{"id":"82a38022b7a8f6e84ec39753340df001","text":"Set query_cache_type=0 in the DB Parameter Group, to disable the query cache.","correct":true},{"id":"94692ead9b9079bf92302461566b94a5","text":"Add additional replicas and the alter the code to initiate Read/Write splitting of the Database.","correct":false},{"id":"09fee716d530e8dbb655cb9541bb4d5c","text":"Unlike Amazon RDS for MySQL, Amazon Aurora does not exhibit replication lag.","correct":false},{"id":"5b3b3ef613f42a00bbda6ed80b4f3683","text":"Change the Read Replica instance class to have the same storage size as the source instance.","correct":true},{"id":"05acabf0a4eea6213f78770124a81bcd","text":"Ensure the max_allowed_packet parameter value for the Read Replica is the same as the source DB instance.","correct":true}]},{"id":"61e75027-a31b-4647-9a9c-f5cb8c771339","domain":"IncidentEventResponse","question":"Your application uses Kinesis Data Streams to process incoming streaming data.  A colleague has noticed that GetRecords.IteratorAgeMilliseconds metric increases up to 10 minutes when traffic hits its peak during the day, and then reduces again when traffic gets less during the night.  This is a problem as it means data processing is being delayed during the times when the most amount of data is being ingested. What changes could you make to reduce the time lag in the stream?","explanation":"An increased GetRecords.IteratorAgeMilliseconds metric means that either the KCL consumers cannot keep up processing the data from the Kinesis stream or there aren't enough shards in the stream.  Choosing both of these options will satisfy the needs of the question.","links":[{"url":"https://aws.amazon.com/kinesis/data-streams/faqs/","title":"Amazon Kinesis Data Streams FAQs"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html","title":"Troubleshooting Amazon Kinesis Data Streams Consumers"}],"answers":[{"id":"88282ae94db4da5f7717813516387595","text":"Increase parallelism by adding more shards per stream.","correct":true},{"id":"3cef88c0f16832880d58c622e62f99f4","text":"Add more EC2 KCL consumers to allow each to process less shards per instance.","correct":true},{"id":"67a7d4be4c7d2f710e0f6a7ddc4f2086","text":"Increase the amount of KPL producers putting data onto the shards.","correct":false},{"id":"a3a32610b2f82f78ed6560f077bba3d2","text":"Increase the retention time of the stream from 24 hours to 7 days.","correct":false},{"id":"4e638cee5fe6dc1a0c8c05bbfc5d2192","text":"Rewrite code to handle all exceptions within processRecords.","correct":false}]},{"id":"e6b3fbb9-d87d-4dc7-b056-981294c46bdc","domain":"PoliciesStandards","question":"An error has occurred in one of the applications that your team looks after and you have traced it back to a DB connection issue. There have been no network outages and the database is up and running. A colleague tells you that all credentials are now stored in AWS Secrets Manager and suspects that the problem might be caused by a recent change in that area. Select all possible reasons for this.","explanation":"Although you typically only have one version of the secret active at a time, multiple versions can exist while you rotate a secret on the database or service. GetSecretValue has an optional VersionId parameter that specifies the unique identifier of the version of the secret that you want to retrieve. If you don't specify either a VersionStage or VersionId then the default is to perform the operation on the version with the VersionStage value of AWSCURRENT.","links":[{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/enable-rotation-other.html","title":"Enabling Rotation for a Secret for Another Database or Service"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_retrieve-secret.html","title":"Retrieving the Secret Value"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_overview.html","title":"Overview of Managing Access Permissions to Your Secrets Manager Secrets"}],"answers":[{"id":"ef3ec50fbea476c0443043b89119ef3b","text":"The GetSecretValue API call in the application didn't include the version of the secret to return.","correct":false},{"id":"67a9e9da426f34e831b5de27c46e3b56","text":"The AWS credentials that are used for the call to Secrets Manager in the client-side component embedded in the application to retrieve the database password don't have the secretsmanager:DescribeSecret permission on that secret.","correct":true},{"id":"4add2a29b600d7ccdc5e4633595bd593","text":"The DB credentials and connection details in Secrets Manager have been encrypted using the default Secrets Manager CMK for the account. The application and secret are in different AWS accounts though and no cross-account access has been granted.","correct":true},{"id":"8525c502b705804602b85cbe128c73a3","text":"When secret rotation is configured in Secrets Manager, it causes the secret to rotate once as soon as you store the secret. This can lead to a situation where the old credentials are not usable anymore after the initial rotation. It is possible that the team forgot to update the application to retrieve the secret from Secrets Manager.","correct":true}]},{"id":"0c271f46-7ab3-485d-a497-2387d04c151a","domain":"MonitoringLogging","question":"You're developing a Node.js application that uses the AWS Node.js API. You interact with lots of different AWS services, and would like to determine how long your API requests are taking in an effort to make your application as efficient as possible. It would also be useful to detect any issues that may be arising and give you an idea about how to fix them. Which AWS service can assist in this task and how would you go about achieving it?","explanation":"AWS X-Ray will produce an end to end view of requests made from your application where you can analyze the requests made as they pass through your application.","links":[{"url":"https://aws.amazon.com/xray/features/","title":"AWS X-Ray Features"}],"answers":[{"id":"4ca7aef91bd24ff0af398119f581d683","text":"Use Amazon QuickSight, inspect the client map, trace the segment path, determine the bottlenecks.","correct":false},{"id":"d920ccd5f4c9bc34ad4acb6ba48ac523","text":"AWS AWS X-Ray, inspect the client map, trace the segment path, determine the bottlenecks.","correct":false},{"id":"d58a545b79533928203dfa0e4a473547","text":"Use Amazon QuickSight, inspect the service map, trace the request path, determine the bottlenecks.","correct":false},{"id":"0d3184462bd7dde819bd2cb519ee4738","text":"Use AWS X-Ray, inspect the service map, trace the request path, determine the bottlenecks.","correct":true}]},{"id":"0e604476-a66d-48c8-b59d-dbb5efe66d38","domain":"SDLCAutomation","question":"AWS CodeBuild is used for a project that you look after which has a known issue with its build performance. You can trace it back to an environment variable named AWS_CODEBUILD_MAX_MEM_ALLOC that is defined in multiple places. It has a value of '4096' in your build spec declaration, '2048' in the build project definition and a value of '1024' in the start build operation call. Which of the following statements is correct?","explanation":"Its value is '1024'. Do not set any environment variable with a name that starts with 'CODEBUILD_' as this prefix is reserved for internal use.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html","title":"Build Specification Reference for CodeBuild: Build Spec Syntax: env"}],"answers":[{"id":"3e4a9e589e2cb112cdc5aff1387285fc","text":"The variable is undefined as the 'AWS_CODEBUILD_' prefix of its name is reserved for internal use.","correct":false},{"id":"a09a6c2e72b77a064358b20cbf7ffeea","text":"The value in the start build operation call takes highest precedence, followed by the value in the build project definition while the value in the build spec declaration takes lowest precedence. Therefore its value is determined as '1024'.","correct":true},{"id":"7201fd1c5d65552aba9eb7f9f1b7c846","text":"Its value is '4096' because the value in the build spec declaration takes highest precedence, followed by the value in the build project definition while the value in the start build operation call takes lowest precedence.","correct":false},{"id":"91e162eb52d2dd94ba6973eae901a468","text":"The value of AWS_CODEBUILD_MAX_MEM_ALLOC  is '2048' as the build project definition takes highest precedence, followed by the value in the build spec declaration while the value in the start build operation call takes lowest precedence.","correct":false},{"id":"6d6b3a85646a3254ab985c43b3a11a20","text":"The value in the build project definition takes highest precedence, followed by the value in the start build operation call while the value in the build spec declaration takes lowest precedence. Because of this, its value is '2048'.","correct":false}]},{"id":"68f29294-fe1b-46a6-8c84-d5076c7a126e","domain":"MonitoringLogging","question":"Your customers have recently reported that your Java web application stops working sometimes. Your developers have researched the issue and noticed that there appears to be a memory leak which causes the software to eventually crash. They have fixed the issue, but your CEO wants to ensure it never happens again. Which AWS services could help you detect future leaks so you're able to fix the issue before the application crashes?","explanation":"Pushing the custom cloudwatch metric is a good idea, you could add it to a dashboard but that wont alert your developers unless they're actively checking it, which you can't rely on.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"Publishing custom metrics"}],"answers":[{"id":"cc97d942d9ce31c3147db1f7555e8fb1","text":"Push your memory usage to a custom CloudWatch metric, set it to alert your developers if it crosses a threshold.","correct":true},{"id":"d4ed3b2231654ab7cfe611a280ed9fa8","text":"Create a CloudWatch dashboard to monitor the memory usage of your app from a custom metric you are pushing to CloudWatch.","correct":false},{"id":"c2991a8b12bcb9d0e41ed823d43ec6e8","text":"Push your memory usage to CloudWatch logs, have a lambda function monitor it and alert a SNS queue if it crosses a threshold.","correct":false},{"id":"b11a45539b5c62eef7c857b34b391f5e","text":"Push your memory usage to CloudTrail, have a lambda function monitor it and alert a SNS queue if it crosses a threshold.","correct":false}]},{"id":"7d87ad95-0cc6-4a66-8e2e-a2b02fe78f38","domain":"SDLCAutomation","question":"You receive a PermissionError from CodePipeline that complains about not being able to access your GitHub repository. What are possible reasons for that and how can you resolve this error?","explanation":"CodePipeline uses GitHub OAuth tokens and personal access tokens to access your GitHub repositories. CodePipeline doesn't automatically expire your GitHub personal access token.","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/troubleshooting.html?shortFooter=true#troubleshooting-gs2","title":"Troubleshooting CodePipeline"},{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html","title":"Managing Access Keys for IAM Users"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/GitHub-rotate-personal-token-CLI.html","title":"Use GitHub and the CodePipeline CLI to Create and Rotate Your GitHub Personal Access Token on a Regular Basis"}],"answers":[{"id":"7771d289cb96cfe426d515391a62d0a5","text":"The number of OAuth tokens is limited and if CodePipeline reaches that limit, older tokens will stop working, and actions in pipelines that rely upon that token will fail. To fix this issue, try to manually configure one OAuth token as a personal access token, and then configure all pipelines in your AWS account to use that token.","correct":true},{"id":"8b118ac224db32b1b232ec2b4f288564","text":"CodePipeline can use GitHub personal access tokens to access your GitHub repositories and retrieve the latest changes. As with all tokens, these should be stored securely and rotated or regenerated routinely. Token rotation is recommended by RFC-6819 (OAuth 2.0 Threat Model and Security Considerations). If not refreshed regularly, CodePipeline expires your GitHub personal access token after 90 days of inactivity. Regenerate a new token and run the get-pipeline command on the pipeline where you want to change the personal access token.","correct":false},{"id":"61e68f6697ca35f0212a17494ee8e14d","text":"The only way to authenticate CodePipeline in GitHub is to use your AWS access token. Use your IAM users' access key ID (not the secret!) to re-register CodePipeline as an OAuth application in GitHub.","correct":false},{"id":"ea42611005235ed3043a63d37da1be8c","text":"CodePipeline uses OAuth tokens to integrate with GitHub. You might have revoked the permissions of the OAuth token for CodePipeline. Try signing in to GitHub, go to Applications under Settings, and choose Authorized OAuth Apps. If you do not see CodePipeline in the list, open the CodePipeline console, edit the pipeline, and choose Connect to GitHub to restore the authorization.","correct":true}]},{"id":"0c1d0c55-7a4a-4f82-b5be-a93cd9be404a","domain":"ConfigMgmtandInfraCode","question":"Your team has been moving various legacy code projects over to AWS in the past few months.  Every application has been analysed to see whether it is best hosted on an EC2 instance or as a Lambda function. One particular software project has been rewritten to store an object in an S3 bucket, and then this action triggers a Lambda function to compress the file as a new object in the same bucket, and then delete the original. Unfortunately, due to the way you have deployed the solution the Lambda function appears to be constantly invoking itself and is stuck in a continuous loop.  How do you temporarily stop this from happening whilst you investigate?","explanation":"When architecting a solution, always ensure that you do not generate a recursive loop.  This is when something in one part of the infrastructure will trigger something elsewhere, and in turn trigger the first part once again.  In particular these scenarios are problematic when one AWS service triggers another AWS service and this in turn triggers the first.  In the example, this can only be rectified by enabling the Throttle option, which sets the reserved concurrency to zero and will throttle all future invocations of this function. This action should only be used in case of emergencies.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/lambda-troubleshooting.html","title":"Troubleshooting AWS Lambda"}],"answers":[{"id":"e9c4b55a14d04ed64e78c4852549c372","text":"Choose the 'Throttle' option on the function configuration page and then locate and resolve the error which caused the recursive invocation.","correct":true},{"id":"f709eae24d782eb5222b2128abfc354e","text":"You can constrain the memory or processing power available to the function. The Lambda function will stop and allow you to troubleshoot.","correct":false},{"id":"e2a75904e82f340ee969cbc6d6bdec5b","text":"There is no way to stop a Lambda function once in an infinite loop.  You need to delete the function, re-create it and re-deploy the code.","correct":false},{"id":"bdaac855074eb8ac866a362c5e7d6713","text":"Remove the IAM User or Role associated with the Lambda function to cease operation.  The Lambda function will stop and allow you to troubleshoot.","correct":false}]},{"id":"6e3db01a-b168-46b2-87a2-8ee861d18334","domain":"SDLCAutomation","question":"You have set up a new AWS CodeCommit repository for your company's development team. All but one member can connect to it from an SSH command line terminal on their local machines and you are now troubleshooting the problem for that user. What are some of the possible reasons for this not working and how can you resolve the issue?","explanation":"The simplest and easiest connection method to use with CodeCommit is HTTPS connections and Git credentials. It is supported by most IDEs and development tools. However, the SSH protocol is also supported. If you want to use SSH connections for your repository, you can connect to AWS CodeCommit without installing the AWS CLI.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html","title":"Setup for HTTPS Users Using Git Credentials"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-https-unixes.html","title":"Setup Steps for HTTPS Connections to AWS CodeCommit Repositories on Linux, macOS, or Unix with the AWS CLI Credential Helper"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-without-cli.html","title":"Setup for SSH Users Not Using the AWS CLI"}],"answers":[{"id":"6ebf3ce5aed5449aa5c0fc11fbd81b2b","text":"Instead of using the SSH protocol, the user tried unsuccessfully another connection method. With SSH connections, you create public and private key files on the users' local machine that Git and CodeCommit use for SSH authentication. You simply associate the public key with the IAM user and store the private key on the users' local machine. Besides the use of the CodeCommit console, this is currently the only supported connection method to use with CodeCommit.","correct":false},{"id":"a607ed26280fc1c3e5fe316de5339c82","text":"Because CodeCommit requires AWS Key Management Service, a policy might be attached to the already existing IAM user that expressly denies the KMS actions required by CodeCommit. If required, update the users' policies and ensure that all required permissions are granted. For a new IAM user, attach the AWSCodeCommitFullAccess or another managed policy for CodeCommit access.","correct":true},{"id":"c87085b06d5895f92cf3cc838ee49464","text":"The user didn't install the AWS CLI. You cannot connect to AWS CodeCommit without installing the AWS CLI when using SSH connections for your repository. You must install the AWS CLI first, then associate the public key in OpenSSH format with the IAM user and finally add CodeCommit to the SSH configuration.","correct":false},{"id":"136c49d59bc5936c51a8afe887d7fada","text":"The user might have previously configured his local computer to use the credential helper for CodeCommit. In this case, edit the .gitconfig file to remove the credential helper information from the file before using Git credentials. On macOS, you might need to clear cached credentials from Keychain Access.","correct":true}]},{"id":"47fefdcc-2b9b-40bc-ab6d-9c70bad210a3","domain":"SDLCAutomation","question":"CodeBuild is used to manage your project builds. In the past, quite a few builds completed successfully but you are currently dealing with a failed one after a source code change. Which of the below are INVALID statements about AWS CodeBuild build phase transitions?","explanation":"PROVISIONING comes before DOWNLOAD_SOURCE and a failed PRE-BUILD phase transitions to the FINALIZING phase.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/view-build-details.html#view-build-details-phases","title":"View Build Details in CodeBuild: Build Phase Transitions"}],"answers":[{"id":"38d29cdacb55007ab2949624e436d1c0","text":"The UPLOAD_ARTIFACTS phase is always attempted, even if the BUILD phase fails.","correct":false},{"id":"5e1175d25bc1d48d660e2eebac6142eb","text":"If a command fails during the INSTALL phase, CodeBuild transitions to the FINALIZING phase and none of the commands in the pre_build, build, and post_build phases are run for that build's lifecycle.","correct":false},{"id":"346f35bdae6107a46bb68a12559c154d","text":"CodeBuild transitions from a failed PRE-BUILD phase to the POST_BUILD phase.","correct":true},{"id":"8e289066c9ed3349987194d5056b8fe5","text":"This failed build could be a result of a failed PROVISIONING phase which comes after the DOWNLOAD_SOURCE phase.","correct":true}]},{"id":"fc10726c-c2c6-4c3e-a514-ef41e8e8ba30","domain":"IncidentEventResponse","question":"You work for a company who sells various branded products via their popular Website.  Every time a customer completes a purchase, streaming data is immediately sent back to your endpoint and is placed onto a Kinesis Data Stream.  Most of the year, traffic remains static and no scaling of the Stream is necessary.  However, during the Black Friday period, scaling is required and this is accomplished by increasing the KCL instances and also re-sharding the Kinesis Stream.  This year, when sharding some of the streams, it was noticed that an extra shard was left after the operation finished, and this meant that if an even number of shards was requested, the number of open shards became odd.  You have been asked to troubleshoot the issue and find the cause and resolution from the list below.","explanation":"This is an issue which occurs from time to time when re-sharding.  The difference between the StartingHashKey and EndingHashKey values is normally large (depending on the number of shards you have in the stream).  Occasionally, the difference can be a very low value such as 1 and this causes the UpdateShardCount to end up with an extra shard.  This can usually be resolved by finding the ShardID with next adjacent Hash Key value, and merging the small shard with the larger shard.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/introduction.html","title":"What Is Amazon Kinesis Data Streams?"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html","title":"Troubleshooting Amazon Kinesis Data Streams Consumers"}],"answers":[{"id":"c5d9d335c1cea4d9ab165da89fc4240f","text":"This occurs when an unhandled exception is thrown in the KCL from the processRecords code, and a record is skipped. This is resolved by handling all exceptions within processRecords appropriately.","correct":false},{"id":"4c33b9c64a3047658345b8abf257490c","text":"This occurs when the width of a shard is very small in size in relation to other shards in the stream. This is resolved by merging with any adjacent shard.","correct":true},{"id":"8d38887be863e96d47c5353c44733dd0","text":"This occurs when a producer application writes to an encrypted stream without permissions on the KMS master key. This is resolved by assigning the correct permissions to an application to access a KMS key.","correct":false},{"id":"0e5554f90414fbfc45af06379bc77584","text":"This occurs if a shard iterator expires immediately before you can use it.  This may be resolved by ensuring the DynamoDB storing the lease information has enough capacity to store this data, by increasing the write capacity assigned to the shard table.","correct":false}]},{"id":"cae0814f-fa70-443b-9c43-1cca04205b54","domain":"SDLCAutomation","question":"Your senior developer wants to be able to access any past version of the binaries that are being built as part of your CI/CD pipeline. You are using CodeBuild with CodePipeline to automate your build process. How will you achieve this?","explanation":"CodeBuild has an optional 'Namespace type', which will insert the build ID into the path to the build output zip file or folder, giving you a unique directory and binary artifact for each build that runs.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/create-project.html","title":"Create a Build Project in CodeBuild"}],"answers":[{"id":"4eed75febec27e9d78dc57a1fbbc70f8","text":"Change the artifact namespace type to Build ID, which will insert the build ID into path of the output folder in S3.","correct":true},{"id":"baa367f7cda7f088cdd1fbab6a72d39b","text":"Change the artifacts packaging to zip, which will append a version number to each build.","correct":false},{"id":"c52ea2b58f7db8902570649989645ace","text":"Implement a CodeBuild lambda trigger which will copy each build artifact to S3 with a unique ID.","correct":false},{"id":"8c3218ccd025dc9444babadf01808d86","text":"Nothing needs to change, by default all artifacts are given a unique filename in S3.","correct":false}]},{"id":"1793c7ec-eff4-4f0d-8252-d1a464d5cd02","domain":"ConfigMgmtandInfraCode","question":"A financial services company runs their application portfolio on EC2 Linux instances with Auto Scaling to provide elastic capacity. They need to increase compute resources based on demand during month-end processing, and decrease them after all reporting has completed to avoid unnecessary costs. Each instance must have a secondary network interface in an isolated subnet for administration tasks in order to meet compliance requirements. How will the company ensure that new instances provisioned by Auto Scaling meet the compliance requirement?","explanation":"Auto Scaling supports adding hooks to the launching and terminating stages of the instance lifecycle. These hooks can send an SNS notification and hold the instance in a pending state waiting for a callback to the API. You can trigger a Lambda function from the SNS topic to create an ENI and attach it to the instance. The lifecycle hook will abandon the instance after a timeout period or if the Lambda function fails. Auto Scaling does not allow for specifying a secondary ENI in the launch configuration. The user data shell script option may work, but there will only be a set number of ENIs available in the pool, whereas the lifecycle hook option creates ENIs as needed. The shell script could possibly create the ENIs on demand. aws:createENI is not a valid AWS Systems Manager automation document action.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html","title":"What Is Amazon EC2 Auto Scaling?"},{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html","title":"Amazon EC2 Auto Scaling Lifecycle Hooks"},{"url":"https://aws.amazon.com/blogs/compute/using-aws-lambda-with-auto-scaling-lifecycle-hooks/","title":"Using AWS Lambda with Auto Scaling Lifecycle Hooks"}],"answers":[{"id":"eba70943b29ff601c4b5f75eddbb07eb","text":"Set up the Auto Scaling launch configuration to include a second Elastic Network Interface for EC2 instances. Include the administrative subnet for the secondary ENI as part of the launch configuration. Allow Auto Scaling to create and attach the secondary ENI during the instance launch stage.","correct":false},{"id":"ad0e0d5cf6e1e64ab9d7089fc43559f7","text":"Include a shell script as user data to be run when Auto Scaling launches an instance. Have the shell script use the AWS CLI to call the AWS Systems Manager StartAutomationExecution API. Pass the aws:createENI action and the administrative subnet id as parameters to the API call.","correct":false},{"id":"40c2708f1afdec5ad8e8dbf1e224bd2c","text":"Create a pool of Elastic Network Interfaces in the administrative subnet. Pass a shell script as user data to be run when Auto Scaling launches an instance. Have the shell script use the AWS CLI to choose an ENI from the pool in the administrative subnet and attach it to the instance.","correct":false},{"id":"7479f465403b12319be40ec429085a28","text":"Add a lifecycle hook to the Auto Scaling launch stage which publishes to an Amazon Simple Notification Service topic. Configure SNS to trigger an AWS Lambda function that creates an Elastic Network Interface in the administrative subnet. Have the Lambda function attach the ENI to the EC2 instance.","correct":true}]},{"id":"79f293d1-a9b0-46ff-b2e2-bb8caa0f0ff1","domain":"IncidentEventResponse","question":"You are working in a creative industry where artists will upload various works of art to your servers before they are packaged up into a single archive each month. They are then sold to customers on a subscription basis where they receive the monthly collection from the artist(s) they support. Your servers are autoscaled, so it's difficult to know which one the artist will be uploading to at any one time. You have also decided to serve the collections straight out of S3 instead of storing them locally. What's the most convenient way to manage the uploading and packaging of the art?","explanation":"An EFS volume will ensure all files are included across all of your upload servers.","links":[{"url":"https://aws.amazon.com/efs/","title":"Amazon Elastic File System"}],"answers":[{"id":"6b6133799067d4557d61f78b2ae7e3e7","text":"Use an EFS volume across all upload servers. Package the art into a single zip or tar file with a cronjob at the end of the month and upload it to S3.","correct":true},{"id":"c81737cd1c2058ecb93efc5a2854e034","text":"Upload each file directly into S3, use S3's archival feature to package the artworks into a single file each month.","correct":false},{"id":"22aa5e2ae7e30890fad3160f0a4d2441","text":"Keep the art works on the server the artist is uploading to, run a cronjob which will tar the files and upload them to S3.","correct":false},{"id":"5ecfc0c066355a155ec75dd3eed9dff6","text":"Set up an FTP server for artists to upload to on all the EC2 instances. Package the art into a single zip or tar file with a cronjob at the end of the month and upload it to S3.","correct":false}]},{"id":"429835e9-1d99-4916-8888-4121a456edcb","domain":"PoliciesStandards","question":"You have been asked to configure Amazon Workspaces so that users can logon using the same set of credentials as they use when on the corporate Active Directory network.  Choose the easiest and most effective solution to implement this.","explanation":"There are many different ways of connecting and authenticating against an on-premise AD domain from within AWS, however in this question the easiest and most cost effective solution is to use the AD Connector to connect back to the on-premise Domain Controllers.  Using AWS Managed Microsoft AD with a Trust Relationship or creating Domain Controllers on EC2 instances are less cost effective and more complicated, whilst Simple AD does not support Trust Relationships and is therefore incorrect.","links":[{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html","title":"Active Directory Connector"},{"url":"https://aws.amazon.com/directoryservice/faqs/","title":"Amazon AWS Directory Service FAQs"}],"answers":[{"id":"9db640d1819586ed5c3cea31fd6712e8","text":"Create an AWS Managed Microsoft AD domain and then create a Trust Relationship between the AWS and on-premise domain.  Configure Amazon Workspaces to point to the domain created in AWS","correct":false},{"id":"5a3773f21f2cfd3f80207d651ed13619","text":"Create a Simple AD domain and then create a Trust Relationship between the AWS and on-premise domain.  Configure Amazon Workspaces to point to the domain created in AWS","correct":false},{"id":"dd58cc51f6b803b211b9e317432a2e01","text":"Create an AD Connector to act as an Active Directory proxy to your on-premise AD infrastructure and configure Amazon Workspaces to connect to the specified Directory","correct":true},{"id":"4f1c410737796b258b658cb1dcc907bd","text":"Create an Active Directory server on EC2, join it to the on-premise domain and then point the Amazon Workspaces Directory setting to the local Domain Controller","correct":false}]},{"id":"82e554d2-f867-4507-974f-04b0b9020cb9","domain":"MonitoringLogging","question":"Your company has a team of Windows Software Engineers which have recently switched from developing on-premise applications, to cloud native micro-services.  The Service Desk has been inundated with questions, but they can't troubleshoot because they don't know enough about Amazon CloudWatch.  The questions they have been receiving mainly revolve around why EC2 logs don't appear in log groups and how they can monitor .NET applications.  Choose the following options which will help troubleshoot the Amazon Cloudwatch issues.","explanation":"The question suggests we are utilising a Windows development environment, so we can discount any answers which have Linux only terms such as running shell scripts.  To run Amazon CloudWatch Application Insights for .NET and SQL Server, we will need to install the SSM Agent with the correct roles, IAM policies and Resource Groups.  We also need to ensure that the CloudWatch agent is running correctly, by starting it using the amazon-cloudwatch-agent-ctl.ps1 script, and as we are assuming defaults, Cloudwatch metrics can be found under the CWAgent namespace.  There are limits for many items in Cloudwatch, utilising Custom Metrics is not one of them.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/troubleshooting-CloudWatch-Agent.html","title":"Troubleshooting the CloudWatch Agent"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.html","title":"Amazon CloudWatch Application Insights for .NET and SQL Server"}],"answers":[{"id":"ffd7b413b965db5c4e66d6dc2c8a73e8","text":"Check the aws-agent.yaml file exists and is configured correctly, and then run the following shell command; aws-cw-agent.sh start.","correct":false},{"id":"33668d47f8774c94713045bff628ba97","text":"If you are utilising CloudWatch Custom Metrics, ensure that you have not reached the default limit.","correct":false},{"id":"aa7402fca9b873f6b49d144c5c67440e","text":"For each EC2 instance running .NET code, install the SSM Agent and attach the AmazonEC2RoleforSSM Role.  You will also need to create a Resource Group and IAM Policy.","correct":true},{"id":"815da42a7b90966156b2d66da1a16b9e","text":"Check the common-config.toml file exists and is configured correctly, and then run the following Powershell command; amazon-cloudwatch-agent-ctl.ps1 -m ec2 -a start.","correct":true},{"id":"ff3e53ec436317efcebcc4a0bf3b52a8","text":"In the Amazon Cloudwatch console, select Metrics, AWS Namespaces and then your metrics should appear under 'CWAgent'.","correct":true}]},{"id":"0917c160-74a4-439b-818b-248197d8fd54","domain":"IncidentEventResponse","question":"Your organization has a few million text documents in S3 that are stored in a somewhat random manner, and the amount of files is always growing. The developer that initially wrote the system in use stored everything with a random file name with some attempt at security through obscurity. Now your CEO and CFO both need to be able to search the contents of these documents, and they want to be able to do so quickly at a reasonable cost. What managed AWS services can assist with implementing a solution for your CEO and CFO, and what would the setup process involve?","explanation":"CloudSearch by itself is enough to fulfill the requirements put forward here. CloudSearch is managed, scalable can very quick to configure and get online. In comparison it would take some time to set up EC2 and install ElasticSearch or any other search tool, and would be much more difficult to scale. This involves creating a search domain and configuring the index as required, and then setting up the access policies.","links":[{"url":"https://aws.amazon.com/cloudsearch/","title":"AWS | Amazon CloudSearch - Search Service in the Cloud"}],"answers":[{"id":"705d488d75db0b38eb9811a38a863d57","text":"Implement MongoDB","correct":false},{"id":"a34f38b0356fb4da2ce3a29a66b4b415","text":"Configure your baseline","correct":false},{"id":"e26c5d07fc30848902a99c95897441b4","text":"Set up IAM roles","correct":false},{"id":"ab25d01b99c512f0cb03129afc920a07","text":"Implement ElasticSearch","correct":false},{"id":"8f9d7ad9c2c6d20b41851ce8e3572aaa","text":"Configure your index","correct":true},{"id":"280209643a35ef346f22e051eafca06e","text":"Create a search domain","correct":true},{"id":"38b79d58ed1b5a50d6bbc91173d0c745","text":"Create a search index","correct":false},{"id":"d912e8309bb58d3483307e1ea428bf8a","text":"Set up access policies","correct":true},{"id":"562f9538310fcd7c846445ad9768ab7c","text":"Implement Amazon CloudSearch","correct":true}]},{"id":"69875d52-fb7d-47de-b4a9-adaa60e7ff47","domain":"IncidentEventResponse","question":"Your company produces IoT enabled refrigerators and uses Kinesis Data Streams as part of its method to capture the streaming data coming back from all of the installed devices.  Your team has written code using KPL to process, sanitise and enrich the data and then put it onto a Kinesis Stream.  If the data cannot be added onto the stream after five tries, it pushes the data through a Kinesis Firehose and into an S3 bucket.  During the last week they have started to see the following exception appearing:  \"Slow down. Service - AmazonKinesisFirehose; Status Code - 500; Error Code - ServiceUnavailableException\". Why would you be seeing this error? Choose from the options below.","explanation":"Whenever a 'Slow down', 'Status Code 500' or 'ServiceUnavailableException' message is received from Kinesis Firehose, it's safe to assume that the error relates to a limit being reached.  There are a number of limits which could affect why data isn't being successfully sent into Firehose, and you should examine 'Amazon Kinesis Data Firehose Limits' to see which case matches your issue and request an increase from AWS Support.","links":[{"url":"https://docs.aws.amazon.com/firehose/latest/dev/limits.html","title":"Amazon Kinesis Data Firehose Limits"}],"answers":[{"id":"1ba3381bd83de693b9d3e6d4a0bf0882","text":"This message is shown when throughput limits for the delivery stream have been exceeded, they may be increased by contacting AWS Support.","correct":true},{"id":"d82d96094ba01a7e79f85057676ceb82","text":"Verify that your Kinesis Data Firehose delivery stream is located in the same Region as your other services, as some services can only send messages to Firehose located in the same Region.","correct":false},{"id":"a4ba42555f0d61efc6b9531130003027","text":"This message means the data delivery to Firehose is stale.  Check the 'DataFreshness' metric under the Monitoring tab to ensure that it is not increasing over time.","correct":false},{"id":"41846b65249adfc479e8f18f544d4245","text":"Firehose does not have the correct permissions to write to the S3 bucket.  Change the permissions allocated to the Role and try again.","correct":false}]},{"id":"74ff9710-cfb7-46df-a980-0811c45f45f0","domain":"HAFTDR","question":"For a number of years, your company has been running its Billing system using Amazon Aurora for MySQL, using additional Read Replicas to run reports and to generate invoices.  Your Manager has said that RDS is now showing a RDS-EVENT-0045 error after one of the team made a configuration change, then left to go on holiday.  After restarting the replication, you briefly get an RDS-EVENT-0046 notification which quickly returns another RDS-EVENT-0045 notification and replication stops again.  Your Manager wants you to troubleshoot the problem as soon as possible, because no bills can be generated.  What steps could you take to resolve the issue?","explanation":"All of the answers have something to do with failing replication, but we can immediately discount anything which requires a change in the code as the question states it was a configuration change which caused the initial issue.  It can't be a database engine issue, because replication will only run on a transactional engine like InnoDB.  Deleting and recreating the read replicas can resolve an issue, but only if there are data inconsistencies and performing only that operation, means that those errors will come back.  In this case, the only thing it could be, is that your colleague has changed the size of max_allowed_packet.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html","title":"Troubleshooting for Aurora"}],"answers":[{"id":"9ad744f1e763f7a5e0ede3939d14a3a3","text":"Switch the database engine from MyISAM to InnoDB and restart replication.","correct":false},{"id":"1ed2f7210119575d1441c76431e4ab4d","text":"Delete and recreate the Read replicas using the same DB instance identifier.","correct":false},{"id":"73f213792c22de894e9255b4caa4c13e","text":"Ensure the max_allowed_packet parameter on a Read Replica is the same as that of the source DB.","correct":true},{"id":"52956a328038ae53dea8c74318f72c79","text":"Remove any queries using SYSDATE() from the code, redeploy and restart replication.","correct":false},{"id":"da283d585bf5dbb555cb1e9207f19fae","text":"Change and redeploy the code to stop writing to a Read Replica, then set read_only to 1.","correct":false}]},{"id":"5fc99b47-e8ba-4a10-bb47-242c4bb839da","domain":"SDLCAutomation","question":"You have been tasked with the set up of AWS CodeStar project and need to decide on the roles for the team members. Peter and Marry are the front-end developers while John and Sue concentrate on the backend work. All of them are using CodeCommit. Stephanie is the assigned tester and uses JIRA while Fritz acts as the project manager. Linda (Ops team), Claudia (Security) and Ross (Architecture team) are all keen to see what's going on within the project and want to know about it's ongoing state. Which of the following statements are true about roles and permissions for this AWS CodeStar project?","explanation":"Each project in AWS CodeStar has three roles: Owner, Contributor and Viewer. If your project uses resources outside of AWS (for example, a GitHub repository or issues in Atlassian JIRA), access to those resources is controlled by the resource provider, not AWS CodeStar.","links":[{"url":"https://docs.aws.amazon.com/codestar/latest/userguide/working-with-teams.html","title":"Working with AWS CodeStar Teams"},{"url":"https://docs.aws.amazon.com/codestar/latest/userguide/working-with-user-info.html","title":"Working with Your AWS CodeStar User Profile"}],"answers":[{"id":"38559dc5760f8cc74e5ad5570016fda2","text":"Via the third-party connector, Stephanie can use AWS CodeStar to control access to JIRA.","correct":false},{"id":"9d01cb702340dd7f0fba3c3c0e0d4a6d","text":"Give Fritz the 'Owner' AWS CodeStar role while Linda, Claudia and Ross are assigned the 'Viewer' role in CodeStar so that they can view the code, project dashboard and, on the dashboard tiles, the state of the project and its resources. All others are given the 'Contributor' CodeStar role.","correct":true},{"id":"3eed526ae05522766fd69428352b38dc","text":"John and Sue can upload an SSH public key to their CodeStar user profile. This key is part of the SSH public-private key pair and can be used to connect to EC2 instances associated with the CodeStar project. CodeStar user profiles are associated with IAM users and contain additional details such as display name and email address that is used across AWS CodeStar projects.","correct":true},{"id":"9d3ac2af828cdbf3cf6de355064a0e71","text":"Make Peter,Marry, John and Sue 'Developers' so that they can commit their code to CodeCommit while Stephanie, Linda, Claudia and Ross become 'Team members' with read only permissions. Give Fritz the 'Administrator' role in CodeStar so that he can manage the project setup.","correct":false}]},{"id":"f6b4a9c9-32fd-433d-93fd-4b61bad8db8a","domain":"PoliciesStandards","question":"You are employee #2 in a new startup, right after the founders and their first developer. You've been given the task of checking over the AWS account that has been used so far and determining what service limits you are currently breaching (if any) and producing a report on the current service limit levels. How do you obtain this information?","explanation":"Trusted advisor makes Service Limit checks free for all users at any support level.","links":[{"url":"https://aws.amazon.com/blogs/mt/monitoring-service-limits-with-trusted-advisor-and-amazon-cloudwatch/","title":"Monitoring Service Limits"}],"answers":[{"id":"1812a1f14ddc45354e11146b11914c56","text":"Sign up for a business support plan to access the Service Limit Checks in Trusted Advisor","correct":false},{"id":"6ca7f9bfe1536150652a1e4ecaf6789c","text":"Enable detailed billing and look at the Service Limit reporting page, which is free for all users.","correct":false},{"id":"0ee62b6775e4ea63a1288cd941f273a5","text":"Sign up for a business support plan to access the detailed billing reports in your billings console.","correct":false},{"id":"de4b26655e419de8a97e82d3598b1eda","text":"Enable Trusted Advisor and look at the Service Limit page, which is free for all users.","correct":true}]},{"id":"2783a2d1-d7aa-42d9-8a75-041a2c73010e","domain":"MonitoringLogging","question":"During a compliance audit, a deficiency is identified stating that insufficient log monitoring and alarming exists for your entire application portfolio on AWS. You have workloads running on Amazon EC2 in ten different AWS accounts and in three AWS regions. Resolving the audit deficiency requires the creation of a centralized log monitoring capability for the AWS-based applications and infrastructure, and for certain on-premises systems that they interface with. You've been tasked with creating an automated solution that will satisfy the auditors. Which architecture will you implement for the solution?","explanation":"Amazon Elasticsearch is a fully managed service that lets you collect and analyze logs and metrics, giving you a comprehensive view into your applications and infrastructure, reducing mean time to detect (MTTD) and resolve (MTTR) issues. Amazon CloudWatch collects API events from CloudTrail, networking events from VPC Flow Logs, and application and system logs from EC2 instances via the CloudWatch Logs agent. Using a single CloudWatch log group ensures that system logs share the same retention, monitoring, and access control settings. Lambda functions in each account can move log data from CloudWatch into Elasticsearch for indexing and visualization with Kibana. CloudWatch log streams are sequences of events from the same source, whereas a log group is a collection of log streams. Logstash works well for collecting logs, but is usually paired with Elasticsearch for log monitoring and analysis. AWS Systems Manager agents send status and execution information back to Systems Manager, but not the application logs, which the CloudWatch Logs agents are capable of sending.","links":[{"url":"https://aws.amazon.com/cloudwatch/","title":"Amazon CloudWatch"},{"url":"https://aws.amazon.com/elasticsearch-service/","title":"Amazon Elasticsearch Service"},{"url":"https://aws.amazon.com/solutions/centralized-logging/","title":"Centralized Logging"}],"answers":[{"id":"9dd2205bea19fc01a45c85453e73a8a0","text":"Deploy CloudWatch Logs agents on all EC2 instances and set up a single log group. Also install CloudWatch Logs agents on the on-premises systems and send their logs to the same log group. Configure Amazon CloudWatch Events to trigger an AWS Lambda function on a regular schedule. Have the Lambda function load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data from CloudWatch into an Amazon Elasticsearch domain in the primary account. Use CloudWatch Events to trigger Lambda functions in each of the other accounts to send CloudWatch data to Elasticsearch in the primary account. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":true},{"id":"08a40cc3203d8aca0e7363189d9cff8e","text":"Install Logstash on an EC2 instance in an Auto Scaling Group in the primary account. Implement AWS Systems Manager agents on all EC2 instances and point them to Systems Manager in the primary account. Also install AWS Systems Manager agents on the on-premises systems and point them to Systems Manager in the primary account. Create an AWS Systems Manager Automation document in the primary account to load AWS CloudTrail, VPC Flow Log, and EC2 log data from all accounts into Logstash. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false},{"id":"b1ea8cd72d3e0e0233c2c1dd38b80375","text":"Configure Logstash on an EC2 instance in an Auto Scaling Group in the primary account. Implement CloudWatch Logs agents on all EC2 instances and set up a single log group. Also install CloudWatch Logs agents on the on-premises systems and send their logs to the same log group. Create an AWS System Manager Automation document in the primary account to load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data in all accounts into Logstash. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false},{"id":"bc1e38ce114c629b8597b80ee4242dc1","text":"Implement CloudWatch Logs agents on all EC2 instances and set up a single log stream. Also deploy CloudWatch Logs agents on the on-premises systems and send their logs to the same log stream. Configure Amazon CloudWatch Events to trigger an AWS Lambda function on a regular schedule. Have the Lambda function load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data from CloudWatch in all accounts into an Amazon Elasticsearch domain in the primary account. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false}]},{"id":"270ee558-9131-48be-937a-4c6711297ef0","domain":"PoliciesStandards","question":"You work for a medical imaging company, dealing with X-rays, MRI's, CT scans and so on. The images and other related patient reports and documents are stored in various S3 buckets in the US West region. Your organization is very security conscious and wants to ensure that while the S3 buckets are locked down, there's no other way that the documents are being shared internally or externally other than the approved methods already in place. Audits are also important, so whatever methods of data protection are in place must work together with this, as well as providing actionable alerts if there any observed issues. How do you best achieve this? Which AWS services can help?","explanation":"Amazon Macie is a security service that uses machine learning to discover personally identifiable information in your S3 buckets. It also provides you with dashboards and alerts that show how your private data is being accessed.","links":[{"url":"https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html","title":"What Is Amazon Macie? - Amazon Macie"}],"answers":[{"id":"4535ec14d07cb09bc414719290a94777","text":"Write a lambda function which is triggered when new data is uploaded into your S3 buckets to apply an S3 policy to ensure the data is secure.","correct":false},{"id":"2e08a3430fb173fad3d2a91519ecc6e5","text":"Don't store sensitive data in S3, the public cloud is not secure enough. Look into moving storage in-house.","correct":false},{"id":"2cd9c2c19e59cae06d93cba7c9861016","text":"Write a lambda function to monitor CloudTrail API calls to S3 and trigger an SNS notification if anything out of the ordinary is detected.","correct":false},{"id":"c41728096e81452a4abc0908a7e707db","text":"Enable Amazon Macie, Integrate Macie with S3.","correct":true}]},{"id":"5e50532a-5d17-4a6f-a367-db3eb2c87698","domain":"SDLCAutomation","question":"Your organization has been using CodePipeline to deploy software for a few months now and it has been smoothly for the majority of releases, but when something breaks during the build process it requires lots of man hours to determine what the problem is, roll back the deployment and fix it. This is frustrating both management and your customers. Your supervisor would like to assign one developer to test the build works successfully before your CodePipeline proceeds to the deploy stage so you don't encounter this issue again. How would you implement this?","explanation":"CodePipeline allows for manual approval steps to be implemented for exactly this reason","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html","title":"Add a Manual Approval Action to a Pipeline in CodePipeline"}],"answers":[{"id":"fb383cd098c54e0f94598a5029e3c99e","text":"Configure SES to email the assigned developer when CodePipeline has deployed to production. This will provide an immediate notification so they can check if there are any errors in testing. Then they can push the changes to the production git branch.","correct":false},{"id":"ca05b5864508a85819911de3b31a9c02","text":"Ask the assigned developer to run a local build first to test all changes, and then commit it to the repository which is then deployed to production only when they know there are no errors.","correct":false},{"id":"adac0bfeeb378f849ee1f8575b660f83","text":"Create a test deploy stage as well as a manual approval stage in CodePipeline. Once the assigned developer checks the testing deploy worked, they can authorize the pipeline to continue and deploy to production.","correct":true},{"id":"61692576c930a7b4f4bb412881fa1829","text":"Configure SQS to email the assigned developer when CodePipeline has deployed to production. This will provide an immediate notification so they can check if there are any errors in testing. Then they can push the changes to the production git branch.","correct":false}]},{"id":"243b97db-004d-4630-bae5-7020609c97d9","domain":"PoliciesStandards","question":"You have a web application in development which accesses a MySQL database in Amazon RDS. Your organization wants to securely store the credentials to the database so only the developers can retrieve them, and would also like the ability to rotate the passwords on a schedule. How would you best way store the credentials to ensure security and make sure no one else in your organisation has access to them?","explanation":"AWS secrets manager can use IAM policies to restrict access, as well as having the capability to automatically rotate passwords on a schedule.","links":[{"url":"https://aws.amazon.com/secrets-manager/","title":"AWS Secrets Manager"}],"answers":[{"id":"d3f8c3a26e2e90782dd1d1fb76b35173","text":"Store the passwords in AWS Secrets Manager, and use an AWS IAM policy to control the access permissions so only members of the developer group can access the credentials. Ensure your web application is also has permission to use this IAM policy.","correct":true},{"id":"b24e060733fd54d8805c389d724f62d4","text":"Store the passwords in AWS CodeCommit, and only give the developers the password to the repository. The developers with repository access can then code this into the application, and change it whenever the passwords need to be rotated","correct":false},{"id":"f87078b1bafcac4b31333b4212e2d96d","text":"Store the passwords in Amazon S3 and use an AWS IAM policy to restrict access. Implement a cronjob on an EC2 server which will update the password in MySQL and upload it to S3.","correct":false},{"id":"53fe0032f55d7f0c1026328f51bab301","text":"Store the passwords on an encrypted EBS instance attached to an EC2 instance where you can implement a cronjob to rotate the passwords on a schedule. Ensure only the developer SSH key is installed on the EC2 server.","correct":false}]},{"id":"100210fd-801f-4377-bbb1-131646b5bbf4","domain":"SDLCAutomation","question":"Your developers are currently storing their code in a private github repository, however your organization has recently introduced rules that everything must live within your AWS environment, so you need to find an alternative. What do you suggest that will both continue to work with your existing developer environments but also support a possible future transition into a CI/CD environment?","explanation":"CodeCommit is the best solution because it's compatible with CI/CD supporting services such as CodeBuild and CodePipeline","links":[{"url":"https://aws.amazon.com/codecommit/","title":"AWS CodeCommit | Managed Source Control Service"}],"answers":[{"id":"59bc8c49cd324cbe0767a72d0787fc19","text":"Move your repositories to CodeCommit.","correct":true},{"id":"efdd4ad61a489544e40886e3db7751db","text":"Move your repositories to S3.","correct":false},{"id":"dfa1a6e5db21b8bd2df6f5674aadbbd7","text":"Move your repositories to your own Git server running in EC2.","correct":false},{"id":"909d969c995f3c405df90a78ad1dc185","text":"Move your repositories to your own CodeCommit EC2 instance from the AWS marketplace.","correct":false}]},{"id":"141e78ef-c6e5-4575-baf2-489bb1f3b7f8","domain":"SDLCAutomation","question":"Your e-commerce platform sees tremendous spikes in transaction volume during new product launches. Each new product event requires that iterative code changes be made during pre-launch preparation based on marketing department feedback and testing results. Slow response times for users have occurred during previous launches, so leadership has directed that load tests be conducted on the e-commerce platform after each new code release. Load testing requires significant time to manually setup and execute, which slows down the development process. You've been tasked with creating an automated load testing service on AWS to shorten testing windows. Which architecture will you recommend to provide scalable load testing capabilities?","explanation":"Simulating thousands of connected users generating a select number of transactions per second can be done without having to provision servers. A test automation framework Docker image deployed on AWS Fargate can be scaled according to user-specified test scenarios. The test scenarios can be created in a web application, and written to S3 and DynamoDB by a Lambda function front-ended by API Gateway. These test scenarios can then be read by another Lambda function to manage Fargate's container scaling to generate the desired load on the e-commerce application. Hosting the web application on Elastic Beanstalk is overkill since test scenarios will probably be entered by a single user for only a short duration of time. S3 configured for static web hosting is a more economical option. Auto Scaling EC2 instances won't generate load according to the specified test scenarios - there's no way to get those scenarios into scaling policies. Running the test automation framework in a Lambda function will risk exceeding Lambda's maximum execution time. AWS Device Farm is used for functionality testing across different browsers and mobile devices, not load testing.","links":[{"url":"https://aws.amazon.com/fargate/","title":"AWS Fargate"},{"url":"https://aws.amazon.com/solutions/distributed-load-testing-on-aws/?did=sl_card&trk=sl_card","title":"Distributed Load Testing on AWS"}],"answers":[{"id":"2fe1750d28696e75b6795c06d03d883c","text":"Deploy a web application for creating test scenarios. Host the web application on an Amazon S3 bucket configured for static web hosting. Have the web application call Amazon API Gateway APIs which route requests to an AWS Lambda function. Have the Lambda function send test scenario parameters to AWS Device Farm in a JSON file to generate the desired load on the e-commerce application. Store test results in S3 and log output in Amazon CloudWatch.","correct":false},{"id":"18ad3b56af5e9d5741987077fd861aca","text":"Implement a web application for creating test scenarios. Host the web application on an Amazon S3 bucket configured for static web hosting. Have the web application call Amazon API Gateway APIs which route requests to AWS Lambda functions that store test scenarios on S3 and DynamoDB. Use a test automation framework to generate load to the e-commerce application. Deploy the test automation framework as a Docker image to Amazon Elastic Container Service. Use another Lambda function to scale AWS Fargate containers according to test scenario criteria. Store test results in S3 and log output in Amazon CloudWatch.","correct":true},{"id":"b8d86dc44ae5d62bf8966ca89db964b3","text":"Create a web application for entering test scenarios. Host the web application on AWS Elastic Beanstalk. Have the web application call Amazon API Gateway APIs which route requests to AWS Lambda functions that store test scenarios on S3 and DynamoDB. Use a test automation framework to generate load to the e-commerce application. Deploy the test automation framework as an Amazon Machine Image (AMI) for use on EC2 instances with Auto Scaling. Use another Lambda function to provision the initial number of EC2 instances according to test scenario criteria. Store test results in S3 and log output in Amazon CloudWatch.","correct":false},{"id":"2916315feefab0d67d74c91444be1fcf","text":"Build a web application for entering test scenarios. Host the web application on AWS Elastic Beanstalk. Have the web application call Amazon API Gateway APIs which route requests to an AWS Lambda function. Have the Lambda function use a test automation framework to generate load to the e-commerce application according to test scenario criteria. Store test results in S3 and log output in Amazon CloudWatch.","correct":false}]},{"id":"c08d5ff2-5be6-41bb-a904-5717d01b962e","domain":"IncidentEventResponse","question":"You've spent weeks building a production Linux AMI that your AutoScaling group is using. You realise that there's one configuration change you need to perform on all new servers that are created due to scaling. You don't have time to build a new AMI until next month due to other work requirements. What's the fastest way to implement the change?","explanation":"A lifecycle hook will be the fastest way to implement the change until you are able to create a new AMI.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html","title":"Amazon EC2 Auto Scaling Lifecycle Hooks"}],"answers":[{"id":"dcf9bba46aa9953c61b4987920b19a5f","text":"Create a cronjob to ping your subnet looking for new instances. If one is detected run a second bash script which will scp the configuration to the new server.","correct":false},{"id":"132083678842f894988594ad0310949b","text":"Switch to OpsWorks for Chef automate, write a cookbook which will implement your change and apply it to all servers and autoscaling groups.","correct":false},{"id":"c80347e18183928097b5db4067202089","text":"Create a Lambda function to monitor CloudTrail EC2 NewInstance events. If one is detected SSH to the server and make the change.","correct":false},{"id":"48dfbec198b5a359c205308dce9e2b13","text":"Use a lifecycle hook to make the change during the creation of the new instance.","correct":true}]},{"id":"3ab068b8-2849-4c61-8ffd-d4048963f8db","domain":"HAFTDR","question":"Due to the design of your application, your EC2 servers aren't treated as cattle as advised in the cloud world, but as pets. As such, you need DNS entries for each of them. Managing each DNS entry is taking a long time, especially when you have lots of servers, some of which may last a day, a week or a month. You don't want your Route53 records to be messy, and you would prefer some kind of automation to add and remove them. Which method would you choose to solve this in the best way?","explanation":"Tagging your instance with the required DNS record is a great way to help you automate the creation of Route53 records. A Lambda function can be triggered from a CloudWatch Events EC2 start/stop event and can add and remove the Route53 records on your behalf. This will meet your requirements and automate the creation and cleanup of DNS records.","links":[{"url":"https://aws.amazon.com/blogs/compute/building-a-dynamic-dns-for-route-53-using-cloudwatch-events-and-lambda/","title":"Building a Dynamic DNS for Route 53 using CloudWatch Events and Lambda"}],"answers":[{"id":"29aa731344e5dd9aad6885f0b8c7274c","text":"Make your instance ID the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use a CloudWatch Events rule to detect when an instance is started or stopped and trigger the Lambda function.","correct":false},{"id":"338d60d516c4c48ee9e575f75fe01c98","text":"Tag your instance with the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use a CloudWatch Events rule to monitor when an instance is started or stopped and trigger the Lambda function.","correct":true},{"id":"d6e2786b315c66a437523d7a8fb675c1","text":"Make your instance ID the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use CloudTrail API call logs to detect when an instance is started or stopped and trigger the Lambda function.","correct":false},{"id":"b1f933ed5c535439e1e097db065978eb","text":"Tag your instance with the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use CloudTrail API call logs to monitor when an instance is started or stopped and trigger the Lambda function.","correct":false}]},{"id":"9ec0b495-6913-472d-bf6c-7bdfc7a22bef","domain":"MonitoringLogging","question":"You have three EC2 instances running a core application, which has been performing sub-optimally since yesterday.  One of your colleagues said that they remember that the system appeared to perform in a similar way about 18 months ago, but they can't remember what the issue was.  You need to perform an indepth investigation of the current issue and you will need to view graphs of that period, with granular metrics.  Reading the logs from when the issue originally occurred would also help troubleshooting.  Which of the following options would give you the best chance of resolving the issue?","explanation":"You can immediately disregard any option with Cloudtrail as these will only contain API logs and will not record application issues.  For the remaining options, it's important to note that Cloudwatch Logs are available indefinitely by default, so any option stating that logs can't be kept can also be excluded.  Now we have to factor in the length of time in the past we are investigating.  15 months is the maximum amount of time we can retrieve metrics from the Console, this means that we need to retrieve the data from the API and process it locally.","links":[{"url":"https://aws.amazon.com/cloudwatch/faqs/","title":"Amazon CloudWatch FAQs"}],"answers":[{"id":"1c6bee332c95b0f8ec7ed30a6efaa04a","text":"View the Cloudwatch graphs from 18 months ago, setting the granularity at 1 minute. Unfortunately Cloudwatch logs cannot be kept for that length of time.","correct":false},{"id":"aa3088febe99800c543d7921893448b2","text":"View the Cloudtrail logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 1 minute.","correct":false},{"id":"3a9e3e96a4470855c66e7a3f8d0ad2ef","text":"View the Cloudwatch logs from 18 months ago and use the API to retrieve the datapoints and store in a local Database for analysis.","correct":true},{"id":"4adf59b6cbba3d749598c710e3671bdf","text":"View the Cloudwatch logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 15 minutes.","correct":false},{"id":"cb2703cd52452e6b2dd73d35a2beab1d","text":"View the Cloudwatch logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 60 minutes.","correct":false}]},{"id":"7182b5ef-3d25-41f6-8251-a554cdef2719","domain":"IncidentEventResponse","question":"Your team has been planning to move functionality from an on-premises solution into AWS.  New micro-services will be defined using CloudFormation templates, but much of the legacy infrastructure is already defined in Puppet Manifests and they do not want this effort to go to waste.  They have decided to deploy the Puppet-based elements using AWS OpsWorks but have received the following errors during configuration; \"Not authorized to perform sts:AssumeRole\" and also \"The following resource(s) failed to create [EC2Instance]\".  The team have been unable to resolve these issues and have asked for your help.  Identify the reasons why these errors occur from the options below.","explanation":"There are two answers which would resolve the errors in the question.  Any time a \"not authorized\" message is displayed, it is nearly always a permissions problem and in this case it can be resolved by attaching the AWSOpsWorksCMServiceRole policy to the instance profile role for EC2. opsworks-cm.amazonaws.com should also be listed in the Trust Relationships. For the second question, this error normally dictates that the EC2 instance doesn’t have sufficient network access, so we need to ensure that the instance has outbound Internet access, and that the VPC has a single subnet with DNS resolution and Auto-assign Public IP settings enabled. All other options will not resolve the errors.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/troubleshoot-opspup.html","title":"Troubleshooting OpsWorks for Puppet Enterprise"},{"url":"https://docs.aws.amazon.com/en_pv/opsworks/latest/userguide/welcome_opspup.html","title":"AWS OpsWorks for Puppet Enterprise"}],"answers":[{"id":"8bda36311bab2d5d74bccd6e3eac3e68","text":"You cannot use OpsWorks for managing Puppet based infrastructure.  OpsWorks only operates and integrates with Chef.","correct":false},{"id":"a4844f346bb4f746b9766b4608cd2b9a","text":"Ensure there are no errors within the Puppet Manifests.  Fix the syntax errors and restart the Puppet Master.","correct":false},{"id":"8d14b74d28ed11d3f4a78d4937d00b00","text":"Ensure that the 'AWSOpsWorksCMServerRole' policy is attached to the instance profile role.","correct":true},{"id":"fd22d4d4b177eefd2129262da6a99e82","text":"Deploy in a VPC which is set to non-default tenancy with an instance type that supports dedicated tenancy.","correct":false},{"id":"72c5c3db3a42b839c2ee0df93fadac14","text":"Ensure that the EC2 instance has the AWS service agent is running, has outbound Internet access and has DNS resolution enabled.","correct":true}]},{"id":"6a6f97fb-45c8-4509-91c7-4507629d60fa","domain":"HAFTDR","question":"Your current workload with DynamoDB is extremely latency bound, and you need it to be as fast as possible. You do not have time to look at other AWS services but instead have been instructed to use features and configuration changes of the services you are currently using. What do you do?","explanation":"Implementing a DAX cluster is the ideal solution here. It meets the requirement of using the existing DynamoDB service feature, while having the ability to reduce latency from milliseconds to microseconds. DynamoDB Scan times can also be optimised by reducing the number of attributes in your table and grouping attributes as JSON blobs within a single attribute.","links":[{"url":"https://aws.amazon.com/dynamodb/dax/","title":"Amazon DynamoDB Accelerator (DAX) – Fully managed in-memory cache for DynamoDB"},{"url":"https://aws.amazon.com/blogs/database/optimizing-amazon-dynamodb-scan-latency-through-schema-design/","title":"Optimize Amazon DynamoDB scan latency through schema design | AWS Database Blog"}],"answers":[{"id":"357461d5fd66eb609146fe0e65854901","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement Elasticache Redis for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS Insights to make available Insights tracing of their DynamoDB calls.","correct":false},{"id":"b5002075c11491df649c4c4f6e960472","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement DynamoDB Cached for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":false},{"id":"4e7fca450cdf45d6ded7827b5ef88c16","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement a DAX cluster for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":true},{"id":"3d69fc095075fdb4cc8e97f241e0fb76","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement Elasticache Memcached for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":false},{"id":"1c8d38560c0975ffecdfff42f01c5965","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement a DAX cluster for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS Insights to make available Insights tracing of their DynamoDB calls.","correct":false}]},{"id":"ffda164a-53b8-4683-82d5-422d4a4910b0","domain":"ConfigMgmtandInfraCode","question":"Your Manager has informed you that all 300 of the EC2 Linux instances in the company VPC will be audited next week and in preparation, a number of shell commands must be run on each of the servers, with results posted into a secure S3 bucket.  This task has to be completed by tomorrow.  What are the quickest ways to deploy and execute the commands and to retrieve the data?","explanation":"Systems Manager gives the ability to run a list of shell commands or a script, by choosing from a list of servers or by tags.  To do this you can run the Command Document called 'AWS-RunRemoteScript' to run a script located in an S3 bucket or GitHub repository or the Command Document 'AWS-RunShellScript' which will allow individual shell commands to be run on a set of servers.  Either of these options would be the quickest and most effective way of running the audit script on the 300 servers in the shortest time.","links":[{"url":"https://aws.amazon.com/systems-manager/faq/","title":"AWS Systems Manager FAQs"}],"answers":[{"id":"341a4cd5e00a3dfea4b7da604cc91b3f","text":"In AWS Systems Manager, run the Command Document called 'AWS-RunShellScript', supply the shell commands in a list and enable 'Write command output to an Amazon S3 bucket'","correct":true},{"id":"977b9ab61a47ae98fc0fd7f109d46b26","text":"In AWS Systems Manager, run the Automation Document called 'AWSSupport-DeployScript', add the shell command to a script and store this in S3 and enable 'Write results to Cloudwatch'","correct":false},{"id":"8a3910c5b9b55101e20ab80338a96fd8","text":"In AWS Systems Manager, run the Command Document called 'AWS-RunRemoteScript', add the shell command to a script and store this in S3. Enable 'Write command output to an Amazon S3 bucket'","correct":true},{"id":"9c4a809d938990c6c1f061c48223d0df","text":"In AWS Systems Manager, run the Automation Document called 'AWSSupport-TroubleshootSSH',  supply the shell commands in a list and enable 'Write results to Cloudwatch'","correct":false},{"id":"1a4604679274c8f0b28e31aa3958d321","text":"Run the script locally using the command 'ssh -i server.pem ec2-user@remoteserver-n \"bash -s\" < /scripts/audit.sh' replacing remoteserver-n with each IP of the 300 servers which will run the local script on the remote servers.","correct":false}]}]}}}}
