{"data":{"createNewExamAttempt":{"attempt":{"id":"3b485634-3079-48cd-9f36-12b841e122d6"},"exam":{"id":"eeac1950-5d19-447e-8042-1c761cbcb179","title":"AWS Certified DevOps Engineer - Professional 2019","duration":10800,"totalQuestions":75,"questions":[{"id":"141e78ef-c6e5-4575-baf2-489bb1f3b7f8","domain":"SDLCAutomation","question":"Your e-commerce platform sees tremendous spikes in transaction volume during new product launches. Each new product event requires that iterative code changes be made during pre-launch preparation based on marketing department feedback and testing results. Slow response times for users have occurred during previous launches, so leadership has directed that load tests be conducted on the e-commerce platform after each new code release. Load testing requires significant time to manually setup and execute, which slows down the development process. You've been tasked with creating an automated load testing service on AWS to shorten testing windows. Which architecture will you recommend to provide scalable load testing capabilities?","explanation":"Simulating thousands of connected users generating a select number of transactions per second can be done without having to provision servers. A test automation framework Docker image deployed on AWS Fargate can be scaled according to user-specified test scenarios. The test scenarios can be created in a web application, and written to S3 and DynamoDB by a Lambda function front-ended by API Gateway. These test scenarios can then be read by another Lambda function to manage Fargate's container scaling to generate the desired load on the e-commerce application. Hosting the web application on Elastic Beanstalk is overkill since test scenarios will probably be entered by a single user for only a short duration of time. S3 configured for static web hosting is a more economical option. Auto Scaling EC2 instances won't generate load according to the specified test scenarios - there's no way to get those scenarios into scaling policies. Running the test automation framework in a Lambda function will risk exceeding Lambda's maximum execution time. AWS Device Farm is used for functionality testing across different browsers and mobile devices, not load testing.","links":[{"url":"https://aws.amazon.com/fargate/","title":"AWS Fargate"},{"url":"https://aws.amazon.com/solutions/distributed-load-testing-on-aws/?did=sl_card&trk=sl_card","title":"Distributed Load Testing on AWS"}],"answers":[{"id":"18ad3b56af5e9d5741987077fd861aca","text":"Implement a web application for creating test scenarios. Host the web application on an Amazon S3 bucket configured for static web hosting. Have the web application call Amazon API Gateway APIs which route requests to AWS Lambda functions that store test scenarios on S3 and DynamoDB. Use a test automation framework to generate load to the e-commerce application. Deploy the test automation framework as a Docker image to Amazon Elastic Container Service. Use another Lambda function to scale AWS Fargate containers according to test scenario criteria. Store test results in S3 and log output in Amazon CloudWatch.","correct":true},{"id":"2fe1750d28696e75b6795c06d03d883c","text":"Deploy a web application for creating test scenarios. Host the web application on an Amazon S3 bucket configured for static web hosting. Have the web application call Amazon API Gateway APIs which route requests to an AWS Lambda function. Have the Lambda function send test scenario parameters to AWS Device Farm in a JSON file to generate the desired load on the e-commerce application. Store test results in S3 and log output in Amazon CloudWatch.","correct":false},{"id":"2916315feefab0d67d74c91444be1fcf","text":"Build a web application for entering test scenarios. Host the web application on AWS Elastic Beanstalk. Have the web application call Amazon API Gateway APIs which route requests to an AWS Lambda function. Have the Lambda function use a test automation framework to generate load to the e-commerce application according to test scenario criteria. Store test results in S3 and log output in Amazon CloudWatch.","correct":false},{"id":"b8d86dc44ae5d62bf8966ca89db964b3","text":"Create a web application for entering test scenarios. Host the web application on AWS Elastic Beanstalk. Have the web application call Amazon API Gateway APIs which route requests to AWS Lambda functions that store test scenarios on S3 and DynamoDB. Use a test automation framework to generate load to the e-commerce application. Deploy the test automation framework as an Amazon Machine Image (AMI) for use on EC2 instances with Auto Scaling. Use another Lambda function to provision the initial number of EC2 instances according to test scenario criteria. Store test results in S3 and log output in Amazon CloudWatch.","correct":false}]},{"id":"0c1d0c55-7a4a-4f82-b5be-a93cd9be404a","domain":"ConfigMgmtandInfraCode","question":"Your team has been moving various legacy code projects over to AWS in the past few months.  Every application has been analysed to see whether it is best hosted on an EC2 instance or as a Lambda function. One particular software project has been rewritten to store an object in an S3 bucket, and then this action triggers a Lambda function to compress the file as a new object in the same bucket, and then delete the original. Unfortunately, due to the way you have deployed the solution the Lambda function appears to be constantly invoking itself and is stuck in a continuous loop.  How do you temporarily stop this from happening whilst you investigate?","explanation":"When architecting a solution, always ensure that you do not generate a recursive loop.  This is when something in one part of the infrastructure will trigger something elsewhere, and in turn trigger the first part once again.  In particular these scenarios are problematic when one AWS service triggers another AWS service and this in turn triggers the first.  In the example, this can only be rectified by enabling the Throttle option, which sets the reserved concurrency to zero and will throttle all future invocations of this function. This action should only be used in case of emergencies.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/lambda-troubleshooting.html","title":"Troubleshooting AWS Lambda"}],"answers":[{"id":"bdaac855074eb8ac866a362c5e7d6713","text":"Remove the IAM User or Role associated with the Lambda function to cease operation.  The Lambda function will stop and allow you to troubleshoot.","correct":false},{"id":"e9c4b55a14d04ed64e78c4852549c372","text":"Choose the 'Throttle' option on the function configuration page and then locate and resolve the error which caused the recursive invocation.","correct":true},{"id":"f709eae24d782eb5222b2128abfc354e","text":"You can constrain the memory or processing power available to the function. The Lambda function will stop and allow you to troubleshoot.","correct":false},{"id":"e2a75904e82f340ee969cbc6d6bdec5b","text":"There is no way to stop a Lambda function once in an infinite loop.  You need to delete the function, re-create it and re-deploy the code.","correct":false}]},{"id":"1793c7ec-eff4-4f0d-8252-d1a464d5cd02","domain":"ConfigMgmtandInfraCode","question":"A financial services company runs their application portfolio on EC2 Linux instances with Auto Scaling to provide elastic capacity. They need to increase compute resources based on demand during month-end processing, and decrease them after all reporting has completed to avoid unnecessary costs. Each instance must have a secondary network interface in an isolated subnet for administration tasks in order to meet compliance requirements. How will the company ensure that new instances provisioned by Auto Scaling meet the compliance requirement?","explanation":"Auto Scaling supports adding hooks to the launching and terminating stages of the instance lifecycle. These hooks can send an SNS notification and hold the instance in a pending state waiting for a callback to the API. You can trigger a Lambda function from the SNS topic to create an ENI and attach it to the instance. The lifecycle hook will abandon the instance after a timeout period or if the Lambda function fails. Auto Scaling does not allow for specifying a secondary ENI in the launch configuration. The user data shell script option may work, but there will only be a set number of ENIs available in the pool, whereas the lifecycle hook option creates ENIs as needed. The shell script could possibly create the ENIs on demand. aws:createENI is not a valid AWS Systems Manager automation document action.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html","title":"What Is Amazon EC2 Auto Scaling?"},{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html","title":"Amazon EC2 Auto Scaling Lifecycle Hooks"},{"url":"https://aws.amazon.com/blogs/compute/using-aws-lambda-with-auto-scaling-lifecycle-hooks/","title":"Using AWS Lambda with Auto Scaling Lifecycle Hooks"}],"answers":[{"id":"7479f465403b12319be40ec429085a28","text":"Add a lifecycle hook to the Auto Scaling launch stage which publishes to an Amazon Simple Notification Service topic. Configure SNS to trigger an AWS Lambda function that creates an Elastic Network Interface in the administrative subnet. Have the Lambda function attach the ENI to the EC2 instance.","correct":true},{"id":"eba70943b29ff601c4b5f75eddbb07eb","text":"Set up the Auto Scaling launch configuration to include a second Elastic Network Interface for EC2 instances. Include the administrative subnet for the secondary ENI as part of the launch configuration. Allow Auto Scaling to create and attach the secondary ENI during the instance launch stage.","correct":false},{"id":"40c2708f1afdec5ad8e8dbf1e224bd2c","text":"Create a pool of Elastic Network Interfaces in the administrative subnet. Pass a shell script as user data to be run when Auto Scaling launches an instance. Have the shell script use the AWS CLI to choose an ENI from the pool in the administrative subnet and attach it to the instance.","correct":false},{"id":"ad0e0d5cf6e1e64ab9d7089fc43559f7","text":"Include a shell script as user data to be run when Auto Scaling launches an instance. Have the shell script use the AWS CLI to call the AWS Systems Manager StartAutomationExecution API. Pass the aws:createENI action and the administrative subnet id as parameters to the API call.","correct":false}]},{"id":"d330cf75-1b29-4c26-a96b-1bb878b00c5f","domain":"MonitoringLogging","question":"Your security conscious CEO has a strange request. She would like an email every time someone signs in to your organization's AWS Console. What is the simplest way to implement this?","explanation":"The AWS Console Sign-in is a valid event source in CloudWatch Events. This would be the simplest way to implement this requirement.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html","title":"What is Amazon CloudWatch Events?"}],"answers":[{"id":"1b70fcf29e1f7aaadbf314e9a69a0dd9","text":"Configure CloudTrail to stream events to CloudWatch Logs and configure a Lambda function to run on a 1 minute schedule to find logins. Send the results to an SNS queue your CEO is subscribed to.","correct":false},{"id":"623afa587682320a753c248c3d41f3c4","text":"Configure a CloudWatch event for the AWS Console Sign-in service and set the target to an SNS queue your CEO is subscribed to.","correct":true},{"id":"6497f488d9ea9251133be660f2896779","text":"Configure a CloudWatch logs Insight query to pull login attempts from the CloudTrail log and send the results to an SNS queue your CEO is subscribed to.","correct":false},{"id":"0bde2cccb1e8f94a030f2f445adae3fc","text":"Configure an IAM trigger on each user and set the target to an SNS queue your CEO is subscribed to.","correct":false}]},{"id":"9e3970ae-d3c6-4365-bd1d-ece6971ce4a0","domain":"ConfigMgmtandInfraCode","question":"Which of the following AWS services allow native encryption of data, while at rest?","explanation":"EBS, S3 and EFS all allow the user to configure encryption at rest using either the AWS Key Management Service (KMS) or, in some cases, using customer provided keys.  The exception on the list is ElastiCache for Memcached which does not offer a native encryption service, although ElastiCache for Redis does.","links":[{"url":"https://aws.amazon.com/ebs/faqs/","title":"Amazon EBS FAQs"},{"url":"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html","title":"Comparing Memcached and Redis"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html","title":"Protecting Data Using Server-Side Encryption"},{"url":"https://aws.amazon.com/efs/faq/","title":"Amazon EFS FAQs"}],"answers":[{"id":"4b222d59a012c5cc128037a5a473cde7","text":"Elastic Block Store (EBS)","correct":true},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true},{"id":"1e26da0c1f6b77f6ef2ad9a3b8cf5a98","text":"ElastiCache for Memcached","correct":false},{"id":"e9880f1e01aac341f6553ec8a6a7e622","text":"Elastic File System (EFS)","correct":true}]},{"id":"ee99751c-1258-4f81-adf9-31a8c57b2605","domain":"ConfigMgmtandInfraCode","question":"Your CloudFormation template is becoming quite large, so you have been tasked with reducing its size and coming up with a solution to structure it in a way that works efficiently. You have quite a few resources defined which are almost duplicates of each other, such as multiple load balancers using the same configuration. What CloudFormation features could you use to help clean up your template?","explanation":"Nested stacks should be used to reuse common template patterns.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#nested","title":"AWS CloudFormation Best Practices"}],"answers":[{"id":"86793dc47df7cca5d373cdb3f8fc8f61","text":"Use Intrinsic functions. This allows dedicated functions to be called and parameters passed to generate resources.","correct":false},{"id":"8a32bed4f4f47b1c84105ed6173ba8c4","text":"Use custom resources. They can be called by your stack to define resources without having to reuse their code.","correct":false},{"id":"242cb2d9063dee2cfd0a6b0cabacb580","text":"Use goto policies. These allow you to refer to different sections of your template for reuse.","correct":false},{"id":"b8d5d665b7d8b4abfc48daa77a64c021","text":"Use nested stacks. This will allow a dedicated templates to be defined for services that are used multiple times","correct":true}]},{"id":"1cce154e-46ef-4042-81de-54e423a7f043","domain":"MonitoringLogging","question":"Your chief security officer would like some assistance in producing a graph of failed logins to your Linux servers, located in your own data centers and EC2. The graph would then be used to trigger alert emails to investigate the failed attempts once it crosses a threshold. What would be your suggested method of producing this graph in the easiest way?","explanation":"The easiest way to produce these graphs would be to ensure you are streaming your logs to cloudwatch logs, create a metric filter and then graph the graph on your dashboard.","links":[{"url":"https://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/","title":"How to Monitor and Visualize Failed SSH Access Attempts to Amazon EC2 Linux Instances"}],"answers":[{"id":"fbb22151cccbae0d255548876414ad08","text":"Install the CloudWatch Logs agent on all of your servers, stream the logs to CloudWatch Logs. Create a Lambda function to parse the logs and detect failed logins, and push a custom metric data point to CloudWatch and then graph the metric on the dashboard.","correct":false},{"id":"a007e0393b7c2d1fc4b14392b98bab0b","text":"Install the CloudWatch Logs agent on all Linux servers, stream your logs to CloudWatch Logs and create a CloudWatch Logs insight query to create the graph.","correct":false},{"id":"76a62d761b77acd84a507a3000156072","text":"Install the CloudWatch Logs agent on all Linux servers, stream your logs to CloudWatch Logs and create a CloudWatch Logs Metric Filter. Create the graph on your Dashboard using the metric.","correct":true},{"id":"165852eeb9a4147840be82962d4a32c6","text":"Install the CloudWatch Logs agent on your Linux servers, stream the logs to CloudWatch Logs. Use cloudwatch events to trigger a lambda function to parse the log, and then push a custom metric data point to CloudTrail and then graph the metric on the dashboard.","correct":false}]},{"id":"cae0814f-fa70-443b-9c43-1cca04205b54","domain":"SDLCAutomation","question":"Your senior developer wants to be able to access any past version of the binaries that are being built as part of your CI/CD pipeline. You are using CodeBuild with CodePipeline to automate your build process. How will you achieve this?","explanation":"CodeBuild has an optional 'Namespace type', which will insert the build ID into the path to the build output zip file or folder, giving you a unique directory and binary artifact for each build that runs.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/create-project.html","title":"Create a Build Project in CodeBuild"}],"answers":[{"id":"8c3218ccd025dc9444babadf01808d86","text":"Nothing needs to change, by default all artifacts are given a unique filename in S3.","correct":false},{"id":"baa367f7cda7f088cdd1fbab6a72d39b","text":"Change the artifacts packaging to zip, which will append a version number to each build.","correct":false},{"id":"c52ea2b58f7db8902570649989645ace","text":"Implement a CodeBuild lambda trigger which will copy each build artifact to S3 with a unique ID.","correct":false},{"id":"4eed75febec27e9d78dc57a1fbbc70f8","text":"Change the artifact namespace type to Build ID, which will insert the build ID into path of the output folder in S3.","correct":true}]},{"id":"4c02fcad-e6c2-438b-927c-a343af2e4e89","domain":"SDLCAutomation","question":"About a dozen people collaborate on a company-internal side project using CodeCommit. The developer community is spread across multiple timezones and relies on repository notifications via email. Initially, it was configured for all event types but this resulted in a lot of emails and team members complained about too much 'noise'. Since then, commit comment notifications have been turned off. How can you improve this situation?","explanation":"You can create up to 10 triggers for each CodeCommit repository. The customData field is of type string and is used for information that you want included in the Lambda function such as the IRC channel used by developers to discuss development in the repository. It cannot be used to pass any dynamic parameters. This string is appended as an attribute to the CodeCommit JSON returned in response to the trigger. You can comment on an overall commit, a file in a commit, or a specific line or change in a file. For best results, use commenting when you are signed in as an IAM user. The commenting functionality is not optimized for users who sign in with root account credentials, federated access, or temporary credentials. By default, an Amazon SNS topic subscriber receives every message published to the topic. To receive a subset of the messages, a subscriber must assign a filter policy to the topic subscription","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html","title":"Manage Triggers for an AWS CodeCommit Repository"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda-cc.html","title":"Example: Create a Trigger in AWS CodeCommit for an Existing AWS Lambda Function"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-commit-comment.html","title":"Comment on a Commit in AWS CodeCommit"},{"url":"https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html","title":"Amazon SNS Message Filtering"},{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-repository-email.html","title":"Configuring Notifications for Events in an AWS CodeCommit Repository"}],"answers":[{"id":"421e56d9693bf5c7a95f304f8be452c8","text":"Create a CodeCommit trigger that invokes a Lambda function when a new comment is added to a commit. Configure it so that its 'Custom data' field is populated with the email address of the user who authored the original commit, i.e. ${commit.author.email}. In the function, use SNS to send the notification to that address.","correct":false},{"id":"bb2ee7c9a6b15ed2c648ce414e7f5501","text":"Ask all team members to sign in to CodeCommit as IAM users.","correct":true},{"id":"a00a4fd86554d04bbf089d9b1f302ca3","text":"Assign Amazon SNS subscription filter policies to the 'commit comment' topic subscriptions so that team members receive only a subset of the messages.","correct":true},{"id":"a74a895989f0aff5ae55553849a46032","text":"For each team member, create an individual SNS topic and a CodeCommit trigger that uses a Lambda function to filter out notifications not authored by that developer. The remaining ones are send to that SNS topic. This allows users to selectivly subscribe to specific persons to follow their activities.","correct":false}]},{"id":"fddf85cb-72e6-4fa1-9c80-07536a84a6e8","domain":"HAFTDR","question":"You have a new website design you would like to test with a small subset of your users. If the test is successful, you would like to increase the amount of users accessing the new site to half your users. If that's successful and your infrastructure is able to scale up correctly, you would like to completely roll over to the new design and then decommission the servers hosting the old design. Which of these methods do you choose?","explanation":"A weighted routing policy combined with an auto-scaling group will meet your requirements and will continue to scale if your tests are successful and you completely roll over to the new design.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted","title":"Weighted Routing"}],"answers":[{"id":"1a147be1638db0d199c9779bb5488bd7","text":"Install the new website design in a new AutoScaling group. Use a Weighted Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 20%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false},{"id":"07123797fd0770bb8b71a6b339d36bfc","text":"Install the new website design in a new AutoScaling group. Use a Weighted Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 50%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":true},{"id":"ff9caa8ea679855cc9425d3e553bf303","text":"Install the new website design in a new AutoScaling group. Use an A/B Test Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 20%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false},{"id":"8d8f29ecca384ae73a4006c686df91b8","text":"Install the new website design in a new AutoScaling group. Create a Lambda function to modify your Route53 apex record to use the new AutoScaling group for 5% of the day. If that's successful then modify the function to change the apex record for half the day, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false}]},{"id":"276f651e-fe53-4841-8545-38c00909e9c5","domain":"PoliciesStandards","question":"After the runaway success of their first website, ACatGuru is building out its IT services to handle its tremendous growth. The CEO of ACatGuru would like to centrally manage all of its commonly deployed IT services, while still giving its employees access to deploy only the services defined by company policy allows. For example, only a developer can spin up a test stack, only a database administrator can spin up a database, and so on. There should also be restrictions on which products can be launched, with the ability to create a custom user interface for you users so they can avoid the complex AWS Management Console. Which AWS service is ideal for implementing these requirements and which features are required?","explanation":"AWS Service Catalog enables organizations to create catalogs of what can be deployed, and create custom interfaces allowing users to deploy them. They are organised as Products which are added to Portfolios to which users can be given access.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/dg/what-is-service-catalog.html","title":"What Is AWS Service Catalog? - AWS Service Catalog"}],"answers":[{"id":"b1820ee1cf68e2e65f263ff7bb207626","text":"AWS Organizations","correct":false},{"id":"3ac62db74ae690ff5ba0cdff8b04efe0","text":"A Product","correct":true},{"id":"132976a248c1f5f10f99b4aa7d7fe9d1","text":"AWS System Catalog","correct":false},{"id":"725903c711beec9056a0f2891a402263","text":"A Project","correct":false},{"id":"96564dfa4633550e1df29d07c3dba125","text":"A Portfolio","correct":true},{"id":"526775c1622cd0e7b703eb8d4a83d657","text":"AWS Service Catalog","correct":true},{"id":"b8f528bf9294fdda9450ad077dfcfc66","text":"A Placement","correct":false}]},{"id":"6313796d-1496-4bfc-b9e4-fe35e00f883b","domain":"HAFTDR","question":"Your team is excited about embarking upon their first greenfield AWS project after months of lift-and-shift migration from your old datacenter into AWS.  This will be your first true infrastructure as code project.  Your team consists of eight people who all will be making changes to infrastructure over time. You would like to use native AWS tooling for writing the infrastructure templates however you are concerned about how team member changes will actually affect the resources you have running already. You don't want to accidentally destroy important AWS resources due to a developer or engineer changing a CloudFormation property whose update property requires replacement. How can you ensure that engineers are aware of the impact of their updates before they implement them, and protect important stateful resources such as EBS volumes and RDS instances against accidental deletion?","explanation":"CloudFormation Change sets will let you submit your modified stack template, it will compare it for you and show you which stack settings and resources will change. You can then execute that change set if you are happy with the changes that will occur.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"a0fa6661560c21c0020855fcc08e5013","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use diff in the Linux terminal to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":false},{"id":"08659d45b30150542b63689978cf172d","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they first create a CloudFormation Change Set. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":true},{"id":"3264bb68f07bc74ca58c6288d8f1074e","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use CloudFormation Modify Set to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":false},{"id":"1f2e9869f312d47386abc4a295c498d1","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use CloudFormation Drift Check to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":false}]},{"id":"79f293d1-a9b0-46ff-b2e2-bb8caa0f0ff1","domain":"IncidentEventResponse","question":"You are working in a creative industry where artists will upload various works of art to your servers before they are packaged up into a single archive each month. They are then sold to customers on a subscription basis where they receive the monthly collection from the artist(s) they support. Your servers are autoscaled, so it's difficult to know which one the artist will be uploading to at any one time. You have also decided to serve the collections straight out of S3 instead of storing them locally. What's the most convenient way to manage the uploading and packaging of the art?","explanation":"An EFS volume will ensure all files are included across all of your upload servers.","links":[{"url":"https://aws.amazon.com/efs/","title":"Amazon Elastic File System"}],"answers":[{"id":"22aa5e2ae7e30890fad3160f0a4d2441","text":"Keep the art works on the server the artist is uploading to, run a cronjob which will tar the files and upload them to S3.","correct":false},{"id":"5ecfc0c066355a155ec75dd3eed9dff6","text":"Set up an FTP server for artists to upload to on all the EC2 instances. Package the art into a single zip or tar file with a cronjob at the end of the month and upload it to S3.","correct":false},{"id":"6b6133799067d4557d61f78b2ae7e3e7","text":"Use an EFS volume across all upload servers. Package the art into a single zip or tar file with a cronjob at the end of the month and upload it to S3.","correct":true},{"id":"c81737cd1c2058ecb93efc5a2854e034","text":"Upload each file directly into S3, use S3's archival feature to package the artworks into a single file each month.","correct":false}]},{"id":"b1b39162-3af1-4aa5-9cd4-4e30338d07f6","domain":"HAFTDR","question":"Your CEO wants you to start future proofing your AWS environment, so he's asked you to look into IPv6 compatibility of your existing Load Balanced EC2 stack. You make use of both Application (ALB) and Network (NLB) load balancers in your EC2-VPC. What are your findings?","explanation":"At this moment in time only the Application Load Balancer supports IPv6 in the EC2-VPC environment. Classic Load balancers only support IPv6 if you are using an EC2-Classic environment.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/01/announcing-internet-protocol-version-6-ipv6-support-for-elastic-load-balancing-in-amazon-virtual-private-cloud-vpc/","title":"Announcing Internet Protocol Version 6 (IPv6) support for Elastic Load Balancing in Amazon Virtual Private Cloud (VPC)"}],"answers":[{"id":"12d596f99a6d191c9c1aaae49901a9e1","text":"Application Load balancers support IPv6, Network Load Balancers do not.","correct":true},{"id":"eda9ef0a0ac30b1db3a5bf8674687603","text":"Application and Network Load Balancers both support IPv6.","correct":false},{"id":"4aff7beb08ea7fe4fa49ab96903dfc41","text":"Application Load balancers do not support IPv6, Network Load balancers do.","correct":false},{"id":"2a74b0b6126f559f800f0f5cef044526","text":"No Load Balancers in EC2 support IPv6.","correct":false}]},{"id":"abfe783d-622d-4a39-9ee5-f210c96b3a6b","domain":"ConfigMgmtandInfraCode","question":"A company has created three Docker containers which need to be deployed.  The first is a core application for the company which will need to accommodate thousands of Websocket connections every minute.  The second is the main corporate Website which is based on a Node.js application running behind an Nginx sidecar. The final container is a small departmental application written by a single developer using the React framework.  You have been asked to deploy each container on the most suitable, cost effective and reliable AWS platform from the options below.","explanation":"In order to make the decision which options are the best, we should start with the deployment that has the most constraints and that is the application using Websockets as these require many network connections and should be installed as multiple tasks across the ECS Cluster.  With this constraint dealt with, we can eliminate all options that don't include the ECS Cluster.  Putting all containers on one ECS Cluster would work technically, but wouldn't be cost effective and would mean an internal departmental application lives on the same cluster as the main core production applications.  When we also exclude installing Docker directly on EC2 as there is no redundancy, this leaves the only option as using ECS for the core application, Fargate for the Website and Elastic Beanstalk for the internal application.","links":[{"url":"https://aws.amazon.com/ecs/faqs/","title":"Amazon Elastic Container Service FAQs"},{"url":"https://aws.amazon.com/elasticbeanstalk/faqs/","title":"AWS Elastic Beanstalk FAQs"},{"url":"https://aws.amazon.com/fargate/faqs/","title":"AWS Fargate FAQs"}],"answers":[{"id":"a09251d393d8cef1692aee1706d87dec","text":"Deploy the Websocket application on a managed ECS Cluster, the Corporate Website on Fargate and the Departmental application on Elastic Beanstalk.","correct":true},{"id":"d8af6733a20ec3d6677a1e9d0016f47a","text":"Deploy the Websocket application and the corporate Website on a managed ECS Cluster and deploy the departmental application in Fargate.","correct":false},{"id":"cc5e3933e49bdce4224537871a4f765b","text":"Deploy all three applications on a managed ECS Cluster.","correct":false},{"id":"37d838ae967ac13002610a1398c9ffd2","text":"Deploy all of the Docker containers in Fargate.","correct":false},{"id":"31d397d0bdcff1d57aeca0011626b830","text":"Deploy the Websocket application on a managed ECS Cluster, the corporate Website in Fargate and install Docker on a single EC2 instance to run the departmental application.","correct":false}]},{"id":"c3e23238-408e-4b66-86f9-c652a8649dc5","domain":"MonitoringLogging","question":"Your organization deals with petabytes of data which needs to be shared with a vendor. They require full access to your private S3 buckets to perform their development work. They have extensive AWS experience already. How will you give them access?","explanation":"The best way to accomplish this is to create a cross-account IAM role with permission to access the bucket, and grant the vendor's AWS ID permission to use the role.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/","title":"Provide Cross-Account Access to Objects In S3 Buckets"}],"answers":[{"id":"9744a37e79095f8e3ca34aadf736f9b6","text":"Grant permission to vendor AWS ID to use the role","correct":true},{"id":"c81b77222ee909b09dcad4f625bfe641","text":"Enable cross region replication","correct":false},{"id":"10b2007be424d0cd17f0473bf718ef66","text":"Edit the bucket policy to allow the vendor AWS ID read access","correct":false},{"id":"38cbb5ba1eb48a6fd20b712b0433a77d","text":"Grant permission to the vendors AWS ID in the S3 bucket policy","correct":false},{"id":"087a96fb587651ee51b6ca0824c7b131","text":"Grant the role permission to the bucket","correct":true},{"id":"8f55be78212f11fd250d67174c0dc930","text":"Create an IAM Role","correct":false},{"id":"18e8d083cfe353b0d12b4ee66f546b75","text":"Create a cross-account IAM Role","correct":true}]},{"id":"47fefdcc-2b9b-40bc-ab6d-9c70bad210a3","domain":"SDLCAutomation","question":"CodeBuild is used to manage your project builds. In the past, quite a few builds completed successfully but you are currently dealing with a failed one after a source code change. Which of the below are INVALID statements about AWS CodeBuild build phase transitions?","explanation":"PROVISIONING comes before DOWNLOAD_SOURCE and a failed PRE-BUILD phase transitions to the FINALIZING phase.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/view-build-details.html#view-build-details-phases","title":"View Build Details in CodeBuild: Build Phase Transitions"}],"answers":[{"id":"346f35bdae6107a46bb68a12559c154d","text":"CodeBuild transitions from a failed PRE-BUILD phase to the POST_BUILD phase.","correct":true},{"id":"5e1175d25bc1d48d660e2eebac6142eb","text":"If a command fails during the INSTALL phase, CodeBuild transitions to the FINALIZING phase and none of the commands in the pre_build, build, and post_build phases are run for that build's lifecycle.","correct":false},{"id":"38d29cdacb55007ab2949624e436d1c0","text":"The UPLOAD_ARTIFACTS phase is always attempted, even if the BUILD phase fails.","correct":false},{"id":"8e289066c9ed3349987194d5056b8fe5","text":"This failed build could be a result of a failed PROVISIONING phase which comes after the DOWNLOAD_SOURCE phase.","correct":true}]},{"id":"dae6c09f-c2e7-4b01-b0ec-95ed07a7bab2","domain":"HAFTDR","question":"Due to some recent performance issues, you have been asked to move your existing Product Information System to Amazon Aurora.  The database uses the InnoDB storage engine, with new products being added regularly throughout the day. As the database is read heavy, the decision has also been made to add a Read Replica during the migration process.  The changeover completes successfully, but after a few hours you notice that lag starts to appear between the Read/Write Master and the Read Replica.  What actions could you carry out to reduce this lag?","explanation":"One of the most obvious causes of Replication lag between two Aurora databases is because of settings and values, so making the storage size comparable between the source DB and Read Replica is a good start to resolving the issue, as is ensuring compatible DB Parameter settings, such as with the max_allowed_packet parameter.  Turning off the Query Cache is good for tables that are modified often which causes lag, because the cache is locked and refreshed often. No other options are correct.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html","title":"Troubleshooting for Aurora"}],"answers":[{"id":"82a38022b7a8f6e84ec39753340df001","text":"Set query_cache_type=0 in the DB Parameter Group, to disable the query cache.","correct":true},{"id":"5b3b3ef613f42a00bbda6ed80b4f3683","text":"Change the Read Replica instance class to have the same storage size as the source instance.","correct":true},{"id":"09fee716d530e8dbb655cb9541bb4d5c","text":"Unlike Amazon RDS for MySQL, Amazon Aurora does not exhibit replication lag.","correct":false},{"id":"05acabf0a4eea6213f78770124a81bcd","text":"Ensure the max_allowed_packet parameter value for the Read Replica is the same as the source DB instance.","correct":true},{"id":"94692ead9b9079bf92302461566b94a5","text":"Add additional replicas and the alter the code to initiate Read/Write splitting of the Database.","correct":false}]},{"id":"9a41a543-3d2a-4090-b97a-16d86d7a7ab0","domain":"SDLCAutomation","question":"CodeBuild has been configured as the Action provider for your Integration Test Action which has been added to your CodePipelines' 'Test' stage. These tests connect to your RDS test database that is isolated on a private subnet and because executing that stage alone takes nearly 2 hours, you want to run it during the night before developers come into the London office in the morning. How might you go about this?","explanation":"Enabling Amazon VPC access in your CodeBuild project requires the ID of the VPC, subnets, and security groups. You cannot use the internet gateway instead of a NAT gateway or a NAT instance because CodeBuild does not support assigning elastic IP addresses to the network interfaces that it creates, and auto-assigning a public IP address is not supported by Amazon EC2 for any network interfaces created outside of Amazon EC2 instance launches. The ordered cron expression fields are: minutes, hours, day of month, month, day of week, year.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/03/aws-codepipeline-adds-support-for-unit-testing/","title":"AWS CodePipeline Adds Support for Unit and Custom Integration Testing with AWS CodeBuild"},{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html","title":"Use CodePipeline with CodeBuild to Test Code and Run Builds"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-schedule-cli.html","title":"Create a CloudWatch Events Rule That Schedules Your Pipeline to Start (CLI)"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html","title":"Schedule Expressions for Rules"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/codebuild/start-build.html","title":"AWS CLI Command Reference: start-build"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_StartPipelineExecution.html","title":"AWS CodePipeline: API Reference: Actions: StartPipelineExecution"},{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/vpc-support.html","title":"Use CodeBuild with Amazon Virtual Private Cloud"}],"answers":[{"id":"7a12726d6a6e0d62c071bfa26f1daf46","text":"By default, CodeBuild projects can access VPCs in the same account. Use the AWS CLI CodeBuild command 'start-build' with the --repeat=true, --hours=3, --minutes=10 and --frequency=weekdays option.","correct":false},{"id":"680092a0588f13a4230a986961172023","text":"To allow CodeBuild access to your database, you have to specify the VPC ID and the ID of the subnet of your private RDS instance. Then create a CloudWatch events rule to schedule a CodeBuild project build using the following cron expression: 3 10 * * ? *","correct":false},{"id":"e49c56dbee3e27e947c2cda2784975fb","text":"CodeBuild supports assigning elastic IP addresses to the network interfaces that it creates. Therefore, you need to configure your build project with the allocated EIP and VPC details of your RDS instance. Call the AWS CodePipeline 'StartPipelineExecution' API with {'stage': 'Test', 'schedule': '5 23 ? * SUN-THU *' }.","correct":false},{"id":"f10014d4b7fb41a95a0f4102b36a8460","text":"Amazon VPC access in your CodeBuild project needs to be enabled. Specify your VPC ID, subnets, and security groups in your build project and set up a rule in Amazon CloudWatch Events to start the pipeline on a schedule. Use the following command for that: aws events put-rule --schedule-expression 'cron(10 3 ? * MON-FRI *)' --name IntegrationTests","correct":true}]},{"id":"860e8b11-9faf-4176-8c11-716aeccf799b","domain":"HAFTDR","question":"You're assisting a developer working on a very large and read-heavy application which uses an Amazon Aurora database cluster. The feature currently being worked on requires reading but no writing, however it will be called by the application frequently and from multiple different servers so the reads need to be load balanced. Additionally your reporting team need to make ad-hoc, expensive queries which need to be isolated so that reads for production are not affected by reporting.  Which Aurora configuration fulfils both needs with minimal extra configuration?","explanation":"The Reader endpoint is appropriate in this situation. The reader endpoint provides load balanced support for read only connections to the database cluster. A custom endpoint can be used to connect to an isolated replica for report generation or ad hoc (one-time) querying,","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html","title":"Amazon Aurora Connection Management - Amazon Aurora"}],"answers":[{"id":"af531b8d353cecad595506c5239ca5f2","text":"Use the Aurora Instance endpoint for your Production system to make load-balanced reads against a read replicas, and create another custom endpoint pointing to a dedicated reporting replica for isolation of ad-hoc reporting.","correct":false},{"id":"5ff29b324290cc87e7f79502c91662fd","text":"Use the Custom endpoint for your Production system to make its reads against a single high-capacity replica, and create another custom endpoint pointing to a dedicated reporting replica for isolation of ad-hoc reporting.","correct":false},{"id":"2b1242d9bda15b98989935f538280b8f","text":"Use the Cluster endpoint for your Production system to make its reads against high-capacity read replicas, and create an Aurora reporting endpoint pointing to a separate replica for isolation of ad-hoc reporting.","correct":false},{"id":"794b7ab3749de5b254f0dd3f2337bc74","text":"Use the Reader endpoint for your Production system to make its load-balanced reads against high-capacity read replicas, and create a custom endpoint pointing to a separate replica for isolation of ad-hoc reporting.","correct":true}]},{"id":"3ab068b8-2849-4c61-8ffd-d4048963f8db","domain":"HAFTDR","question":"Due to the design of your application, your EC2 servers aren't treated as cattle as advised in the cloud world, but as pets. As such, you need DNS entries for each of them. Managing each DNS entry is taking a long time, especially when you have lots of servers, some of which may last a day, a week or a month. You don't want your Route53 records to be messy, and you would prefer some kind of automation to add and remove them. Which method would you choose to solve this in the best way?","explanation":"Tagging your instance with the required DNS record is a great way to help you automate the creation of Route53 records. A Lambda function can be triggered from a CloudWatch Events EC2 start/stop event and can add and remove the Route53 records on your behalf. This will meet your requirements and automate the creation and cleanup of DNS records.","links":[{"url":"https://aws.amazon.com/blogs/compute/building-a-dynamic-dns-for-route-53-using-cloudwatch-events-and-lambda/","title":"Building a Dynamic DNS for Route 53 using CloudWatch Events and Lambda"}],"answers":[{"id":"29aa731344e5dd9aad6885f0b8c7274c","text":"Make your instance ID the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use a CloudWatch Events rule to detect when an instance is started or stopped and trigger the Lambda function.","correct":false},{"id":"338d60d516c4c48ee9e575f75fe01c98","text":"Tag your instance with the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use a CloudWatch Events rule to monitor when an instance is started or stopped and trigger the Lambda function.","correct":true},{"id":"b1f933ed5c535439e1e097db065978eb","text":"Tag your instance with the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use CloudTrail API call logs to monitor when an instance is started or stopped and trigger the Lambda function.","correct":false},{"id":"d6e2786b315c66a437523d7a8fb675c1","text":"Make your instance ID the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use CloudTrail API call logs to detect when an instance is started or stopped and trigger the Lambda function.","correct":false}]},{"id":"10e64485-5c7d-44b1-adcd-23a1b1b60e7d","domain":"IncidentEventResponse","question":"Your organization is building millions of IoT devices that will track temperature and humidity readings in offices around the world. The data is then used to make automated decisions about ideal air conditioning settings based on that data, and then trigger some API calls to control the units. Currently, the software to accept the IoT data and make the decisions and API calls runs across a fleet of autoscaled EC2 instances. After just a few thousand IoT devices in production, you're noticing the EC2 instances are beginning to struggle and there's just too many being spun up by your autoscaler. If this continues you're going to hit your account service limits and costs will blow out way more than you budgeted for. How can you redesign this service to be more cost effective, more efficient and most importantly, scalable?","explanation":"In this instance, Kinesis Data Analytics with the data streamed with Kinesis Data Streams is the best choice. It's also completely serverless so you are able to save costs by shutting down your EC2 servers.","links":[{"url":"https://aws.amazon.com/kinesis/data-analytics/","title":"Amazon Kinesis Data Analytics"}],"answers":[{"id":"932e73a0f9bcccb0cb228111d6e91632","text":"Switch to Kinesis Data Analytics. Stream the IoT data with Kinesis Data Streams and perform your decision making and API triggering with Lambda. Shut down your EC2 fleet.","correct":true},{"id":"9e06f33f0b3d84a2d27b1e028103a035","text":"Switch to Kinesis Data Firehose. Stream the IoT data directly, perform your decision making and then store the results in S3. Analyze the S3 content to trigger your API calls with your EC2 fleet. Change the autoscaling metric to use the Firehose capacity.","correct":false},{"id":"ed5129b5b4a876e31ca0fe3383fa6b9e","text":"Switch to Kinesis Firehose Analytics. Stream the IoT data directly, perform your decision making and then store the results in S3. Analyze the S3 content to trigger your API calls with your EC2 fleet. Change the autoscaling metric to use the Firehose capacity.","correct":false},{"id":"7af4964635eff76f7316e3605f75adaa","text":"Switch to CloudWatch Data Analytics. Stream the IoT data through Kinesis Data Streams and perform your decision making and API triggering with Lambda.","correct":false}]},{"id":"9ec0b495-6913-472d-bf6c-7bdfc7a22bef","domain":"MonitoringLogging","question":"You have three EC2 instances running a core application, which has been performing sub-optimally since yesterday.  One of your colleagues said that they remember that the system appeared to perform in a similar way about 18 months ago, but they can't remember what the issue was.  You need to perform an indepth investigation of the current issue and you will need to view graphs of that period, with granular metrics.  Reading the logs from when the issue originally occurred would also help troubleshooting.  Which of the following options would give you the best chance of resolving the issue?","explanation":"You can immediately disregard any option with Cloudtrail as these will only contain API logs and will not record application issues.  For the remaining options, it's important to note that Cloudwatch Logs are available indefinitely by default, so any option stating that logs can't be kept can also be excluded.  Now we have to factor in the length of time in the past we are investigating.  15 months is the maximum amount of time we can retrieve metrics from the Console, this means that we need to retrieve the data from the API and process it locally.","links":[{"url":"https://aws.amazon.com/cloudwatch/faqs/","title":"Amazon CloudWatch FAQs"}],"answers":[{"id":"3a9e3e96a4470855c66e7a3f8d0ad2ef","text":"View the Cloudwatch logs from 18 months ago and use the API to retrieve the datapoints and store in a local Database for analysis.","correct":true},{"id":"cb2703cd52452e6b2dd73d35a2beab1d","text":"View the Cloudwatch logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 60 minutes.","correct":false},{"id":"aa3088febe99800c543d7921893448b2","text":"View the Cloudtrail logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 1 minute.","correct":false},{"id":"1c6bee332c95b0f8ec7ed30a6efaa04a","text":"View the Cloudwatch graphs from 18 months ago, setting the granularity at 1 minute. Unfortunately Cloudwatch logs cannot be kept for that length of time.","correct":false},{"id":"4adf59b6cbba3d749598c710e3671bdf","text":"View the Cloudwatch logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 15 minutes.","correct":false}]},{"id":"5125f905-4372-437c-838f-63de5bf0354f","domain":"HAFTDR","question":"Your current application uses an Aurora database, however the speeds aren't as fast as you would like for your bleeding edge website. You are too deep into developing your application to be able to change the database you are using or to implement faster or larger read replicas. Your application is read-heavy, and the team has identified there are a number of common queries which take a long time to be returned from Aurora. What recommendations would you make to the development team in order to increase your read performance and optimise the application to use Aurora?","explanation":"ElastiCache will cache common queries by holding them in memory instead of on disk, and will speed up your application considerably","links":[{"url":"https://aws.amazon.com/elasticache/faqs/","title":"Amazon ElastiCache FAQs"}],"answers":[{"id":"c639fb8842c60b552eacb76b7583ebc3","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should create a read-optimised replica and redirect all application reads to that endpoint.","correct":false},{"id":"d2f04a86c7627f46eed3695113f9d1d9","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should switch to Aurora Serverless.","correct":false},{"id":"f9d6e50f24acc22852ddfc3f2dbfb914","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should Implement ElastiCache between your application and the database.","correct":true},{"id":"fd97848b273db8bc7178cc86c812aa15","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should increase the database instance size to a low-latency instance.","correct":false}]},{"id":"8cd81d91-8288-4830-ae8b-0aaa2c7f706c","domain":"SDLCAutomation","question":"You want to use Jenkins as the build provider in your CI/CD Pipeline. Is this possible, and if so how would you implement it?","explanation":"You can select Jenkins an action provider when creating a build stage in CodePipeline. You cannot select it as a source provider within CodeBuild.","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html","title":"Use CodePipeline with Jenkins"}],"answers":[{"id":"80ab8f614bf8d08d188690809f0dc8fa","text":"Yes it's possible. CodePipeline will let you select Jenkins as a destination build provider when you are creating your pipeline.","correct":false},{"id":"d27a4cc08449c797a8fd8b6c6b9d505e","text":"Yes it's possible. You can use a CodePipeline plugin for Jenkins and can configure a build stage which connects to your Jenkins instance.","correct":true},{"id":"2c74a9018e02367af56b8cd6036ae2a9","text":"No it's not possible.","correct":false},{"id":"c5a84ac110e8df3c1da4255e362f2066","text":"Yes it's possible. CodeBuild will let you select Jenkins as a source provider when you are creating your build project","correct":false}]},{"id":"9f7ba9e2-8030-4333-acf6-e2d27613e959","domain":"IncidentEventResponse","question":"You have an idea regarding your AWS account security. You would like to monitor your account for any possible attacks against your resources, such as port scans or brute force SSH and RDP attacks. If anything is detected, you want the report pushed to a Slack channel where anyone in your company can monitor and take action if it's their responsibility. How do you go about implementing this?","explanation":"Amazon GuardDuty is the best way to implement this. It can trigger Lambda functions on events which can be used to post to a Slack channel.","links":[{"url":"https://aws.amazon.com/guardduty/","title":"Amazon GuardDuty"}],"answers":[{"id":"2b3ed3e5fd2a948326831b73fe30bd4a","text":"Implement AWS Inspector. On detected events, trigger a Lambda function to post the information to your Slack channel.","correct":false},{"id":"e91bba567af0e13b250c7976157789ae","text":"Implement a Lambda function to monitor your VPC Flow Logs. For any odd requests, post the information to your Slack channel.","correct":false},{"id":"a594a49c80d0416db98a60a3f66adc4e","text":"Implement Amazon GuardDuty. On detected events, trigger a Lambda function to post the information to your Slack channel.","correct":true},{"id":"74245776eaafb5659f52d1c243b22462","text":"Implement a Lambda function to monitor your CloudTrail file. For any odd API calls or requests, post the information to your Slack channel","correct":false}]},{"id":"547f6880-c168-4fa5-91d6-6822a772af6b","domain":"ConfigMgmtandInfraCode","question":"Your team has been working with CloudFormation for a while and have become quite proficient at deploying and updating your stack and their associated resources. However one morning you notice that your RDS MySQL database is completely empty. You track this down to a simple port change request that came through, asking if the default MySQL port could be changed for security reasons. What do you suspect happened?","explanation":"The database was replaced due to the AWS:RDS:DBInstance Port attribute update requirement of 'Replacement', the database will have to be restored from backups.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html","title":"Update Behaviors of Stack Resources"}],"answers":[{"id":"2cb85ef53a3961bcf3c2798f04263e7e","text":"The 'Update' attribute for AWS:RDS has the update requirement of \"Replacement\".","correct":false},{"id":"aab0bf043065432156b116b7beed04c2","text":"The 'Port' attribute for AWS:RDS:DBInstance has the update requirement of \"Replacement\".","correct":true},{"id":"f3e35605b25b631ab990cfcba037091c","text":"The CloudFormation stack was updated with the Replacement stack value set to true.","correct":false},{"id":"0974d47e810ed46ba7723376856be841","text":"The developer changed the MySQL port to a port that was already in use in your VPC so it can no longer be connected to.","correct":false}]},{"id":"bbe1fc54-7ed1-4156-930f-446e13ca5e59","domain":"ConfigMgmtandInfraCode","question":"One of your colleagues has developed a CloudFormation template which creates a VPC and a subnet for each Availability Zone within each Region the Stack is created in.  Although this template is currently functional, it requires the Region and Availability Zones to be passed into the Stack as parameters.  You have been asked by your manager to alter the template so that you can automate all of this functionality using Intrinsic Functions, and eliminate passing in parameters.  Choose options from the list which will meet the requirements of rewriting the template.","explanation":"Intrinsic functions can not be used within the Parameters section and so these options can be immediately ruled out. Using both Fn::GetAZs to return the Availability Zones and Fn:Select to choose from the list and also using Fn::FindInMap to pull back each Region from the Mappings section are both valid options to retrieve the Regions.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"Template Anatomy"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html","title":"Intrinsic Function Reference"}],"answers":[{"id":"0f58a828daf6079e3560070d348c8d13","text":"In the Parameters section, create three individual parameters, one for each Availability Zone and use !Ref to retrieve the values in the Resources section.","correct":false},{"id":"c697110279709e26aa4a1219dcf5f6e2","text":"In the Resources section, use !Ref \"AWS::Region\" to return the Region, Fn::GetAZs to return a list of all Availability Zones and Fn:Select to choose each AZ from the list.","correct":true},{"id":"9fe027bb7b742b6639a50bc38d88cd6f","text":"In the Mappings section, make a list of each Availability Zone in each Region.  Then in the Resources section use Fn::FindInMap to pull back each AZ using !Ref \"AWS::Region\" as the key.","correct":true},{"id":"3eca8a7377d618c7e76e66d97dddfd42","text":"In the Parameters section, use !Ref \"AWS::Region\" to return the Region for each parameter, Fn::GetAZs to return a list of all Availability Zones and Fn:Select to choose each AZ from the list.","correct":false}]},{"id":"6a6f97fb-45c8-4509-91c7-4507629d60fa","domain":"HAFTDR","question":"Your current workload with DynamoDB is extremely latency bound, and you need it to be as fast as possible. You do not have time to look at other AWS services but instead have been instructed to use features and configuration changes of the services you are currently using. What do you do?","explanation":"Implementing a DAX cluster is the ideal solution here. It meets the requirement of using the existing DynamoDB service feature, while having the ability to reduce latency from milliseconds to microseconds. DynamoDB Scan times can also be optimised by reducing the number of attributes in your table and grouping attributes as JSON blobs within a single attribute.","links":[{"url":"https://aws.amazon.com/dynamodb/dax/","title":"Amazon DynamoDB Accelerator (DAX)  Fully managed in-memory cache for DynamoDB"},{"url":"https://aws.amazon.com/blogs/database/optimizing-amazon-dynamodb-scan-latency-through-schema-design/","title":"Optimize Amazon DynamoDB scan latency through schema design | AWS Database Blog"}],"answers":[{"id":"357461d5fd66eb609146fe0e65854901","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement Elasticache Redis for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS Insights to make available Insights tracing of their DynamoDB calls.","correct":false},{"id":"4e7fca450cdf45d6ded7827b5ef88c16","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement a DAX cluster for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":true},{"id":"3d69fc095075fdb4cc8e97f241e0fb76","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement Elasticache Memcached for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":false},{"id":"b5002075c11491df649c4c4f6e960472","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement DynamoDB Cached for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.","correct":false},{"id":"1c8d38560c0975ffecdfff42f01c5965","text":"Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement a DAX cluster for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS Insights to make available Insights tracing of their DynamoDB calls.","correct":false}]},{"id":"21d1048a-2c33-4d84-af1a-ad6a039e0ddd","domain":"SDLCAutomation","question":"Your organization currently runs a hybrid cloud environment with servers in AWS and in a local data center. You currently use a cronjob and some bash scripts to compile your application and push it out to all of your servers via SSH. It's both difficult to log, maintain and extend to new servers when they are provisioned. You've been considering a move to CodePipeline to manage everything for you. Will it suit your requirements?","explanation":"CodeBuild is able to deploy to any server that can run the CodeDeploy agent, whether in AWS or in your own data centre.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/instances.html","title":"Working with Instances for CodeDeploy"}],"answers":[{"id":"299b7a966370e0bfb587d4c2d5ff774b","text":"Yes, CodeDeploy can deploy to any servers that can run the CodeDeploy agent","correct":true},{"id":"beba600b1da886e2a2ad8713309745a2","text":"Yes, CodePipeline will be able to interface with any servers that can run the CodePipeline agent","correct":false},{"id":"f81f6e25dfbc48337f9ed9f263923302","text":"No, CodeDeploy wont be able to deploy to the non-AWS managed servers in your data center","correct":false},{"id":"2c74ebd4177ccf3f0749b4ecdcd73e7c","text":"No, CodePipeline wont be able to deploy to the non-AWS managed servers in your data center","correct":false}]},{"id":"0917c160-74a4-439b-818b-248197d8fd54","domain":"IncidentEventResponse","question":"Your organization has a few million text documents in S3 that are stored in a somewhat random manner, and the amount of files is always growing. The developer that initially wrote the system in use stored everything with a random file name with some attempt at security through obscurity. Now your CEO and CFO both need to be able to search the contents of these documents, and they want to be able to do so quickly at a reasonable cost. What managed AWS services can assist with implementing a solution for your CEO and CFO, and what would the setup process involve?","explanation":"CloudSearch by itself is enough to fulfill the requirements put forward here. CloudSearch is managed, scalable can very quick to configure and get online. In comparison it would take some time to set up EC2 and install ElasticSearch or any other search tool, and would be much more difficult to scale. This involves creating a search domain and configuring the index as required, and then setting up the access policies.","links":[{"url":"https://aws.amazon.com/cloudsearch/","title":"AWS | Amazon CloudSearch - Search Service in the Cloud"}],"answers":[{"id":"a34f38b0356fb4da2ce3a29a66b4b415","text":"Configure your baseline","correct":false},{"id":"ab25d01b99c512f0cb03129afc920a07","text":"Implement ElasticSearch","correct":false},{"id":"562f9538310fcd7c846445ad9768ab7c","text":"Implement Amazon CloudSearch","correct":true},{"id":"280209643a35ef346f22e051eafca06e","text":"Create a search domain","correct":true},{"id":"d912e8309bb58d3483307e1ea428bf8a","text":"Set up access policies","correct":true},{"id":"705d488d75db0b38eb9811a38a863d57","text":"Implement MongoDB","correct":false},{"id":"e26c5d07fc30848902a99c95897441b4","text":"Set up IAM roles","correct":false},{"id":"38b79d58ed1b5a50d6bbc91173d0c745","text":"Create a search index","correct":false},{"id":"8f9d7ad9c2c6d20b41851ce8e3572aaa","text":"Configure your index","correct":true}]},{"id":"68f29294-fe1b-46a6-8c84-d5076c7a126e","domain":"MonitoringLogging","question":"Your customers have recently reported that your Java web application stops working sometimes. Your developers have researched the issue and noticed that there appears to be a memory leak which causes the software to eventually crash. They have fixed the issue, but your CEO wants to ensure it never happens again. Which AWS services could help you detect future leaks so you're able to fix the issue before the application crashes?","explanation":"Pushing the custom cloudwatch metric is a good idea, you could add it to a dashboard but that wont alert your developers unless they're actively checking it, which you can't rely on.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"Publishing custom metrics"}],"answers":[{"id":"d4ed3b2231654ab7cfe611a280ed9fa8","text":"Create a CloudWatch dashboard to monitor the memory usage of your app from a custom metric you are pushing to CloudWatch.","correct":false},{"id":"b11a45539b5c62eef7c857b34b391f5e","text":"Push your memory usage to CloudTrail, have a lambda function monitor it and alert a SNS queue if it crosses a threshold.","correct":false},{"id":"c2991a8b12bcb9d0e41ed823d43ec6e8","text":"Push your memory usage to CloudWatch logs, have a lambda function monitor it and alert a SNS queue if it crosses a threshold.","correct":false},{"id":"cc97d942d9ce31c3147db1f7555e8fb1","text":"Push your memory usage to a custom CloudWatch metric, set it to alert your developers if it crosses a threshold.","correct":true}]},{"id":"26b5e2d7-e00f-4d7b-b086-c09e029d1de9","domain":"HAFTDR","question":"The world wide cat news powerhouse, Meow Jones, has hired you as a DevOps Database consultant. They're currently using legacy in-house PostgreSQL databases which cost a considerable amount to maintain the server fleet, as well as operational costs for staff, and further hardware costs for scaling as the industry grows. You are tasked in finding an AWS solution which will meet their requirements. They require high throughput, push button scaling, storage auto-scaling and low latency read replicas. Any kind of automatic monitoring and repair of databases instances will also be appreciated. Which AWS service(s) do you suggest?","explanation":"Amazon Aurora will fit the needs perfectly, and the Database Migration Service can assist with the migration.","links":[{"url":"https://aws.amazon.com/rds/aurora/details/postgresql-details/","title":"Amazon Aurora Features: PostgreSQL-Compatible Edition"}],"answers":[{"id":"e96297fedff7f3062d962bbd0a387cb4","text":"An auto-scaled, load balanced EC2 fleet running PostgreSQL with data shared via EFS volumes.","correct":false},{"id":"9802017b44b6d60fc846a356c1fa9e9a","text":"A cluster of Amazon RDS PostgreSQL instances, AWS Database Migration Service.","correct":false},{"id":"6129915cbe4cbc9e1ddb0b74808beded","text":"Amazon Aurora, AWS Database Migration Service","correct":true},{"id":"0a663a4b84931c9af9abadcccde84f3f","text":"Keep the current PostgreSQL databases and implement an ElastiCache to cache common queries and reduce load on your in-house databases to save on upgrade costs.","correct":false}]},{"id":"b8ae52d2-9638-47a9-b2b0-69c471a35eab","domain":"MonitoringLogging","question":"Your organization runs a large amount of workloads in AWS and has automated many aspects of its operation, including logging. As the in-house devops engineer, you've received a ticket asking you to log every time an EC2 instance state changes. Normally you would use CloudWatch events for something like this, but CloudWatch logs aren't a valid target in CloudWatch Events. How will you solve this?","explanation":"CloudWatch Events can use a Lambda function as a target, which will solve this issue.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html","title":"Tutorial: Log the State of an Amazon EC2 Instance Using CloudWatch Events"}],"answers":[{"id":"a68e57ea4a6bd4cfa8f565ef7cbc637e","text":"Create a Lambda function and run it on a schedule with CloudWatch Events. Make the Lambda function parse your CloudTrail logs for EC2 instance state changes and log them to another CloudWatch Logs log.","correct":false},{"id":"b482ab43e71b8137c1bb08d0904d0576","text":"Use a CloudWatch dashboard, which will log EC2 state changes to CloudWatch logs if you create a text widget.","correct":false},{"id":"a4e104c9dd0f3e6a03765a0680648eed","text":"Use CloudWatch Events, but use a Lambda function target. Write a Lambda function which will perform the logging for you.","correct":true},{"id":"1fc1f373fd2b33a9a55daeb93aa03e7b","text":"Parse the EC2 CloudWatch changelog with a Lambda function each minute, and log the results to a separate log for instance state changes","correct":false}]},{"id":"7bb8566a-c105-47ad-9848-105db61926ef","domain":"PoliciesStandards","question":"Your CTO, in a very urgent manner, wants to know the uptime of all of your EC2 servers with an 'environment' tag set to 'prod'. How do you achieve this quickly?","explanation":"The Run Command feature of AWS Systems Manager is suitable for this task. All roles must be configured correctly and the SSM agent needs to be installed.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html","title":"AWS Systems Manager Run Command - AWS Systems Manager"}],"answers":[{"id":"d165cfe0af0f1d1de31dc1657219abe7","text":"Use AWS Systems Manager","correct":true},{"id":"5435e85a50449d33e87b3cf25e3dcf23","text":"Ensure required roles are configured","correct":true},{"id":"898c6c270ab3457638bd087cfd1247e8","text":"Use the Batch Command feature","correct":false},{"id":"23c84837eb29f8d7c5906f9701407929","text":"Use the Run Command feature","correct":true},{"id":"2b2248d6940ab012e3eb6750edcac3fd","text":"Ensure required agent is installed","correct":true},{"id":"9d336b9d0d3b3fc25b4610e2a7a2f90d","text":"Ensure required users are configured","correct":false},{"id":"92292ff4cfdeb0c7acd7901a12866431","text":"Use AWS Service Catalog","correct":false}]},{"id":"f6f2e5d0-a72e-4d44-aaea-2e2e8fa264c3","domain":"HAFTDR","question":"Your company utilizes EC2 and various other AWS services for its workloads. As the DevOps engineer it is your responsibility to ensure all company policies are implemented. You have noticed that while you are using S3 for data archival and backups, your company policy is that your backups need to reside on company owned servers, such as those you run in your local Equinix data center. You also have another company policy that backup and archival cannot traverse the internet. What do you do?","explanation":"AWS Direct Connect is the only way to access your AWS resources from a Data Center without traversing the internet, despite the encryption offered by the other solutions.","links":[{"url":"https://aws.amazon.com/directconnect/","title":"AWS Direct Connect"}],"answers":[{"id":"008894eae706c0e60a40a59c79342658","text":"Implement an AWS Client VPN with a company owned server in your data center. Push backups to your data center backup NAS through SSH via vpn.","correct":false},{"id":"b51577fa11c831eba7efec0c73952602","text":"Implement a Site to site VPN with a company owned server in your data center. Push backups to your data center backup NAS through SSH via vpn.","correct":false},{"id":"f42f4a5866921fe7f0288f667c043aac","text":"Provision an AWS Direct Connect connection to your local router in your data center and your local VPC. Push backups via the Direct Connect connection.","correct":true},{"id":"32f6b2f1cc26546abf7ce835ae778156","text":"Install the EFS agent on your data center backup NAS, mount the volume on an EC2 server and copy the backups to the volume.","correct":false}]},{"id":"a1b1f253-8d88-4b6a-882a-146b02e9e327","domain":"MonitoringLogging","question":"Your company has been moving its core application from a monolith to an ECS based, microservices architecture.  Although functionally the application is operational in its new environment, you randomly see spikes of latency throughout the day and you are concerned with the overall performance of the application.  How can you rapidly gain information about the requests each microservice is serving?","explanation":"X-Ray is the AWS service that can deal with this sort of scenario, therefore we can discount any option that doesn't include it.  The best solution would be to deploy a Docker image running the X-Ray agent as an ECS service and this could ingest data from all sources.  You could then track the path of the application and filter on this data to find the cause.","links":[{"url":"https://aws.amazon.com/xray/faqs/","title":"AWS X-Ray FAQs"},{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html","title":"AWS X-Ray Concepts"}],"answers":[{"id":"6cd41e60c1ed008cbabe084fc5f7c5f5","text":"Enable Enhanced Cloudwatch functionality in each microservice, push all performance information into Cloudwatch Logs and use Log Insights to identify poorly performing APIs.","correct":false},{"id":"5a8dd485b25fce5ab3b460060dc89f4f","text":"Utilise AWS X-Ray by deploying a Docker image containing the X-Ray daemon to ECS, which will then gather segment data.  Then examine the traces generated to track the path of a request through the application and use Filter Expressions to find traces relating to specific paths.","correct":true},{"id":"bfef3f324b5e71c44d859b637f210e99","text":"Deploy the X-Ray daemon to all ECS Services and view the traces and performance data that is generated, within the X-Ray console.","correct":false},{"id":"92f0c9098189dbfb752878b2b946957f","text":"Utilise the AWS SDK to communicate with the AWS X-Ray API.  These will generate the JSON segments into an S3 bucket and the use SQL statements within AWS Athena to search through the traces stored in the bucket.","correct":false}]},{"id":"7bce21b9-f986-4147-b616-c58ac141d19f","domain":"PoliciesStandards","question":"You work for a global service provider, deploying critical software 24 hours a day.  You have 3 AWS Accounts; 'POC' allows for Developers to stand up new technology and try out new ideas on an adhoc basis. 'QA' allows for automated builds and testing to be carried out as part of your CI/CD Pipeline and any problems here mean that software will not get pushed into production, so it's important that any issues are resolved quickly. The final account is 'Live' which is the main Production account.  It's the most critical and requires the best response times.  You need to choose the most appropriate and cost effective Support Plan which will satisfy your support needs but also allow for the full range of AWS Trusted Advisor Checks and Recommendations.","explanation":"The important phrase in the question is that any choice must \"...allow for the full range of AWS Trusted Advisor Checks and Recommendations\", only two support plans have all the Trust Advisor checks and these are Business and Enterprise.  Anything that contains the Basic or Developer plans will not be correct.  Also, it specifies that the 'Live' account is \"critical and requires the best possible response times\", so we could use the Enterprise plan for this, but we'll only get better response time for mission critical business applications such as Microsoft, SAP and Oracle tools, which is not the software we are deploying.  Therefore taking all of this into account, along with cost, 'POC', 'QA' and 'Live' should all be allocated the Business plan.","links":null,"answers":[{"id":"92b9da164af4d9a8f9d10be3a3964a68","text":"The 'POC', 'QA' and 'Live' Accounts should all be allocated the Business Support Plan.","correct":true},{"id":"eec4f4e8aab13e184a6c7abc550b5ded","text":"The 'POC' and 'QA' Accounts should use the Basic Support Plan, while 'Live' should use the Enterprise Support Plan.","correct":false},{"id":"e52ec269985d97d389e8e39403ec69fc","text":"The 'POC' Account should be allocated the Basic Support Plan, 'QA' the Business Support Plan and 'Live' the Enterprise Support Plan.","correct":false},{"id":"b802f5ecd5a395f2c899ccdbf041d677","text":"The 'POC' Account should be allocated the Basic Support Plan and both 'QA' and 'Live' the Business Support Plan.","correct":false},{"id":"4dbc42a7817c285587307fc6102d43d2","text":"The 'POC' Account should be allocated the Developer Support Plan, 'QA' the Business Support Plan and 'Live' the Enterprise Support Plan.","correct":false}]},{"id":"e98496ed-60b9-4d96-8cbb-3d3e260da18e","domain":"IncidentEventResponse","question":"Your organisation is 75% through moving its core services from a Data centre and into AWS.  The AWS stacks have been working well in their new environment but you have been told that the Data centre contract will expire in 3 months and therefore there is not enough time to re-implement the remaining 25% of services before this date. As they are already managed by Chef, you decide to move them into AWS and manage them using OpsWorks for Chef. However, when configuring OpsWorks you have noticed the following errors have appeared; \"Not Authorized to perform sts:AssumeRole\" and \"FATAL Could not find pivotal in users or clients!\". Choose the correct options to resolve the errors.","explanation":"With the \"Not Authorized to perform sts:AssumeRole\" error, you can assume its policy/role related and therefore creating a role and attaching the AWSOpsWorksCMServiceRole policy should resolve this issue. Finally, any message which states that you 'cannot find a pivotal user', requires you to add one to the default location. All other answers will not resolve the problems listed.","links":[{"url":"https://docs.aws.amazon.com/en_pv/opsworks/latest/userguide/troubleshoot-opscm.html","title":"Troubleshooting AWS OpsWorks for Chef Automate"},{"url":"https://docs.aws.amazon.com/en_pv/opsworks/latest/userguide/welcome_opscm.html","title":"AWS OpsWorks for Chef Automate"}],"answers":[{"id":"0ddbd3212ebd514950bf4aafc59e898c","text":"Install the knife-opc command and then run the command; knife opc org user add default pivotal","correct":true},{"id":"c565172b94c322361ca5823ed0126630","text":"Ensure you have at least one existing EIP address free by deleting unused addresses or by asking AWS Support for an increase.","correct":false},{"id":"edd55a54eb6c02470c1f52081eed838d","text":"Create a new service role and attach the AWSOpsWorksCMServiceRole policy to the role. Verify that the service role is associated with the Chef server and it has that policy attached.","correct":true},{"id":"0b86e189ee6ecc6aefa9be08d750c26f","text":"Ensure that the EC2 instance has the AWS service agent running and has outbound Internet access with DNS resolution enabled.","correct":false}]},{"id":"46d6ca80-c3ad-4319-8a9e-b1ed0047f73a","domain":"SDLCAutomation","question":"You have been asked to deploy two, small internal applications on Elastic Beanstalk.  One is a Python application which requires Nginx, and the other is a Java application with the need to have special java parameters passed to it.  Which Platform option should you use as the Base Configuration to run each application?  Choose the option that is the quickest and easiest to deploy in each case.","explanation":"To choose the quickest and easiest deployment mechanism, you must understand what options the preconfigured solutions give you.  In the case of Python, it comes with Apache HTTP by default, but we required Nginx, so we cannot choose a preconfigured option for the Python application and we must build it as a contaniner.  For the Java application however, it will happliy be deployed as the preconfigured option with the java parameters specified in the 'Environment properties' section.","links":[{"url":"https://aws.amazon.com/elasticbeanstalk/faqs/","title":"AWS Elastic Beanstalk FAQs"},{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options-specific.html","title":"Platform Specific Options"}],"answers":[{"id":"bcc14bebc6f2b9e9dc6564979d981d56","text":"Create the Python application as a Docker container, and on the 'Platform' drop down, choose 'Generic - Docker' and upload the container and for the Java application, carry out the same process.","correct":false},{"id":"0df59da0902f3797038a0c11f8eb5874","text":"Create the Python application as a Docker container, and on the 'Platform' drop down, choose 'Generic - Docker' and upload the container.  For the Java application, choose 'Preconfigured - Node.js' and upload the code.","correct":false},{"id":"8291c776c87c1530f5f2e159d308b3ed","text":"Create the Python application as a Docker container, and on the 'Platform' drop down, choose 'Generic - Docker' and upload the container.  For the Java application, choose 'Preconfigured - Java' and upload the code.","correct":true},{"id":"77e071eb26d8852300b66b6c64c84698","text":"On the 'Platform' drop down, choose 'Preconfigured - Python' for the Python application and then upload the code.  For the Java application, choose 'Preconfigured - Java' and upload the code.","correct":false}]},{"id":"09effc45-51a5-418a-b1e5-b27430e92d3b","domain":"IncidentEventResponse","question":"Your organisation utilises Kinesis Data Streams to ingest large amounts of streaming data.  Each of the Kinesis Streams is consumed by a Java application running on multiple EC2 instances, utilising the KCL library.  The CloudWatch logs for the KCL application normally only show 'INFO' entries, but you have noticed that some errors have started to appear recently, and you need to investigate.  The errors are all of the form; \"getShard - Cannot find the shard given the shardId\" and include the ID of shard which still appears to exist.  What may have caused these errors to appear in the logs?","explanation":"This error usually appears just after a re-sharding event has completed when the consumer code is using an older version of KCL.  This appears to be caused when the KinesisProxy has a cache of shards which is not updated when leases change in the KCL DynamoDB table.  Although this error appears to have no affect on the data within the shards, it can usually be resolved by restarting the KCL consumers.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/building-consumers.html","title":"Reading Data from Amazon Kinesis Data Streams"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html","title":"Re-sharding, Scaling, and Parallel Processing"}],"answers":[{"id":"d8d212ca3330be75fe9cb3a00d49676d","text":"These errors are generated when migrating the Record Processor from version 1.x to version 2.x of the KCL.","correct":false},{"id":"54eebd43d1e7d3fcae9de2fcadc3409b","text":"These errors can be seen in the Kinesis Consumer log usually after a re-sharding event and are generated by the KinesisProxy.","correct":true},{"id":"409329597050c414284e9b625163dbfe","text":"These errors are seen when in Enhanced Fan-Out mode and more than 20 consumers are registered per stream.","correct":false},{"id":"45bc1b263334c69f60939ca82c41189a","text":"These errors are caused by the shard iterator expiring before your code is ready to utilise it.","correct":false}]},{"id":"8c127356-141c-4199-bfd4-f1d8e0a8560d","domain":"IncidentEventResponse","question":"You have taken over a half finished project to implement Kinesis Data Streams.  Data is being successfully and efficiently placed onto the streams using KPL based code, but when running the KCL code to read off the shards, you are experiencing issues.  The main issues relate to slower than expected reading from the shards, but you are also seeing records being skipped.  Which options allow you to resolve both of these problems?","explanation":"Many of the above solutions answer Kinesis related questions, but we specifically need to know what could cause a slow down in reading from a shard, and why records would be skipped.  For this, there are only two correct answers.  For slow reading, it could be due to the maxRecords value being set too low or the code logic, which is calling processRecords, being inefficient and causing high CPU usage or blocking.  However, there is only one correct answer for the skipping of records and that is usually due to processRecords calls having un handled exceptions.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html","title":"Troubleshooting Amazon Kinesis Data Streams Consumers"}],"answers":[{"id":"b579dde17faaedc85ca4956edf187830","text":"Ensure the maxRecords value for the GetRecords call isn't set below the default setting.  Also, make sure you aren't seeing a sudden large increase in the GetRecords.IteratorAgeMilliseconds metric.","correct":false},{"id":"989df0ef5880a136eb3db849ff45dc66","text":"Ensure the maxRecords value for the GetRecords call isn't set below the default setting.  Also, check that the processRecords call is not throwing an unhandled exception.","correct":true},{"id":"c6314cbc442d2cd7efa28180d7a5abcf","text":"Ensure you have made a GetRecords call at least every 5 minutes.  Also, check that the processRecords call is not throwing an unhandled exception.","correct":false},{"id":"abd17cf52b5933e670c26b2caed0264c","text":"Choose a failover time that is less than 10 seconds, to ensure reading from a shard maintains a 1 to 1 relationship.  Also, make sure the consumers do not exceed the read per-shard limit.","correct":false},{"id":"61bd51378999bcd6a2d6551c0cc12100","text":"Check the logic of the code to ensure the processRecords call isn't CPU intensive or I/O blocking.  Also, check that the processRecords call is not throwing an unhandled exception.","correct":true}]},{"id":"eef970b2-0cae-40bb-a9bc-a2fedbd2bc02","domain":"ConfigMgmtandInfraCode","question":"Your manager wants you to look into including and provisioning a new authentication service in your CloudFormation templates. Generally this would be simple, but the authentication service he has signed a 3 year deal with is not an AWS service at all, which could make things difficult. The authentication service has an API you can use, which will hopefully ease the pain of solving this problem. How do you solve it?","explanation":"This is an excellent use case for a CloudFormation custom resource","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html","title":"Custom Resources"}],"answers":[{"id":"ccb593ba89acf906fb6a643256d93f44","text":"Use the API to provision the service with an AWS Lambda function. Use a CloudFormation custom resource to trigger the Lambda function.","correct":true},{"id":"6200195512a7053624054d12f6d6d0c7","text":"Provision the authentication service externally to your AWS resources. You will have to maintain them in parallel.","correct":false},{"id":"2bbfc07a49181a40ef8a9ecaefe0c6f9","text":"Use the API to provision the service with an AWS Lambda function. Use a CloudFormation wait conditions to trigger the Lambda function during the provisioning process.","correct":false},{"id":"8c7dd37aaf9be0d28c45046384cb9056","text":"Use the API to provision the service with a python script using the boto framework. Run the python script with a CloudWatch Events schedule.","correct":false}]},{"id":"69f350ad-db3a-4b83-bf8e-15ea0c0df866","domain":"PoliciesStandards","question":"Your company is preparing to become ISO 27001 certified and your manager has asked you to propose a comprehensive solution to log configuration and security changes in a separate audit account.  Specifically, the solution should ensure IAM users have MFA enabled, identify S3 buckets which aren't encrypted and enforce the addition of specific tagging.  Identify which option solves the problem.","explanation":"AWS Config is the only service that will meet all of the requirements in the question as it records configuration changes and snapshots the configuration at regular intervals set by you. Data aggregation means that AWS Config data from multiple accounts can be stored in a single account.  The following built in rules; s3-bucket-server-side-encryption-enabled and iam-user-mfa-enabled identify any S3 buckets not encrypted and any IAM accounts that do not have MFA enabled.  Tagging is also available for AWS Config resources that describe AWS Config rules.","links":[{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"},{"url":"https://aws.amazon.com/cloudtrail/faqs/","title":"AWS CloudTrail FAQs"}],"answers":[{"id":"8c9a63c97e58123416200044d646218a","text":"Enable Enhance Logging in AWS Cloudwatch to track all security and configuration changes and view these using Cloudwatch Logs Insights.","correct":false},{"id":"2f426c67cf1036d34927bdb5aac020ba","text":"Enable AWS Cloudtrail to check for MFA enabled IAM users, configure Server access logging in S3 to view the encryption status and use a CloudFormation Templates to add tagging.","correct":false},{"id":"0e1d676582953a5a30bcd07fa6996329","text":"Enable AWS Config and AWS Cloudtrail to track changes in all resources and to identify IAM user API calls with MFA enabled.","correct":false},{"id":"efd3ca0f78115a7a32bffa7905edd17b","text":"Enable AWS Config and create three default rules to check whether IAM users have MFA enabled, S3 buckets have server side encryption and tagging is added to resources.","correct":true}]},{"id":"927aeb0d-23f7-48f5-9ca7-aa76c6403385","domain":"SDLCAutomation","question":"You are contracting for APetGuru, an online pet food store. Their website works fine, but you really want to update the look and feel to be more modern. They have given you strict guidelines that their website can not be offline at all or they will lose sales. You are thinking of using a rolling deployment so some of their servers are always online during the rollout. Just before you trigger the roll out, you receive a phone call asking if you can only install your updates on a few servers first for testing purposes. It is suggested that a few customers can be redirected to the updated site and everyone else can still use the old site until you confirm the new one is working properly. Once you're happy with the operation of the updated website, you can complete the rollout to all servers. What do you do?","explanation":"A canary deployment will allow you to complete the roll out in the way you want to.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing continuous integration / continuous delivery on AWS"}],"answers":[{"id":"a93e067980f0010b2f812e5f3eef9b9f","text":"Use a Blue/Green deployment. This allows you to install the new website to the blue servers, and once you're happy with it working you can finalise the install on the green servers. If there's an issue you can roll back the blue installation.","correct":false},{"id":"807b35728975296049d217f0dfbb3bc8","text":"Use an In-Place Deployment. You can deploy to the \"In-Place\" servers first for testing, then once testing is verified you can continue the deployment to the \"Out-Place\" externally facing servers.","correct":false},{"id":"562add4453cef6369a9860f6a26dbe36","text":"Use a Batch Deployment. This allows you to install your new website to the small batch which traffic will be directed to, and will allow you to roll back the small batch if there's an error. Otherwise, you can complete the rollout on the large batch.","correct":false},{"id":"864cd30cc708bcfb524df58f775ae226","text":"Use a Canary Deployment. This allows deployment to a few servers where you can observe how the website is running while still receiving a small amount of customers. If there's an error it can be rolled back. Otherwise, the rollout will continue on new servers.","correct":true}]},{"id":"69875d52-fb7d-47de-b4a9-adaa60e7ff47","domain":"IncidentEventResponse","question":"Your company produces IoT enabled refrigerators and uses Kinesis Data Streams as part of its method to capture the streaming data coming back from all of the installed devices.  Your team has written code using KPL to process, sanitise and enrich the data and then put it onto a Kinesis Stream.  If the data cannot be added onto the stream after five tries, it pushes the data through a Kinesis Firehose and into an S3 bucket.  During the last week they have started to see the following exception appearing:  \"Slow down. Service - AmazonKinesisFirehose; Status Code - 500; Error Code - ServiceUnavailableException\". Why would you be seeing this error? Choose from the options below.","explanation":"Whenever a 'Slow down', 'Status Code 500' or 'ServiceUnavailableException' message is received from Kinesis Firehose, it's safe to assume that the error relates to a limit being reached.  There are a number of limits which could affect why data isn't being successfully sent into Firehose, and you should examine 'Amazon Kinesis Data Firehose Limits' to see which case matches your issue and request an increase from AWS Support.","links":[{"url":"https://docs.aws.amazon.com/firehose/latest/dev/limits.html","title":"Amazon Kinesis Data Firehose Limits"}],"answers":[{"id":"41846b65249adfc479e8f18f544d4245","text":"Firehose does not have the correct permissions to write to the S3 bucket.  Change the permissions allocated to the Role and try again.","correct":false},{"id":"a4ba42555f0d61efc6b9531130003027","text":"This message means the data delivery to Firehose is stale.  Check the 'DataFreshness' metric under the Monitoring tab to ensure that it is not increasing over time.","correct":false},{"id":"d82d96094ba01a7e79f85057676ceb82","text":"Verify that your Kinesis Data Firehose delivery stream is located in the same Region as your other services, as some services can only send messages to Firehose located in the same Region.","correct":false},{"id":"1ba3381bd83de693b9d3e6d4a0bf0882","text":"This message is shown when throughput limits for the delivery stream have been exceeded, they may be increased by contacting AWS Support.","correct":true}]},{"id":"1e2dcf0c-3e73-455f-a757-d704864b5194","domain":"ConfigMgmtandInfraCode","question":"Your CEO loves serverless. He wont stop talking about how your entire company is built on serverless architecture. He attends Serverlessconf to talk about it. Now, it's up to you to actually build the web application he's been talking about for 6 months. Which AWS services do you look at using to both create the application and orchestrate your components?","explanation":"AWS Lambda is the correct choice for compute in a serverless environment, as is DynamoDB for NoSQL databases and S3 for storage. AWS Step Functions are used to orchestrate your components, such as your lambda functions.","links":[{"url":"https://aws.amazon.com/serverless/","title":"Serverless Computing  Amazon Web Services"}],"answers":[{"id":"e9ce4547402d74b7fd336d09bf24e58b","text":"Create your application using AWS Lambda for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Serverless Application Framework can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":false},{"id":"f3b92e9d4214f817c65085013da23eb8","text":"Create your application using AWS Lambda for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":true},{"id":"a4414e5d2e313708a970e89063df27af","text":"Create your application using AWS Elastic Compute for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Glacier will allow you to archive old files cheaply.","correct":false},{"id":"86fe4a20afac7056ea6e0f5dd7f2e70d","text":"Create your application using AWS Elastic Beanstalk for compute functions within the application. Data storage can be provided using Amazon DynamoDB for a NoSQL database.  File storage can be provided using Amazon S3. AWS Step Functions can orchestrate workflows and AWS Cold Store will allow you to archive old files cheaply.","correct":false}]},{"id":"a5ef00b0-c312-49fc-9553-6e7217f93561","domain":"HAFTDR","question":"Your organization has multiple weather stations installed around your state. You would like to move the data from your old relational database to a newer and hopefully faster NoSQL database. Which AWS solution would you choose for storing and migrating the data, and which keys do you use for your weather station ID and the timestamp of the reading?","explanation":"DynamoDB is the AWS NoSQL database. The Weather Station ID will be your Partition Key and the timestamp will be your Sort key.","links":[{"url":"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/","title":"Choosing the Right DynamoDB Partition Key"}],"answers":[{"id":"52f544e451b4aef2951330297f1b4235","text":"Use Amazon DMS to migrate data to an Amazon DynamoDB table. Select your RDS instance as the source and DynamoDB as destination.  Create a task with an object mapping rule to copy the required relational database tables to DynamoDB.  Within your object-mapping set your weather station ID as the Primary Key of your table and timestamp of the reading as the Sort key.","correct":false},{"id":"3399272a0609e8a363a59878ea5960ef","text":"Use Amazon DMS to migrate data to an Amazon ElasticCache table. Select your RDS instance as the source and ElasticCache as destination.  Create a task with an object mapping rule to copy the required relational database tables to DynamoDB.  Within your object-mapping set your weather station ID as the Primary Key of your table and timestamp of the reading as the Sort key.","correct":false},{"id":"a0d196957142835b951087cae863939c","text":"Use Amazon DMS to migrate data to an Amazon DynamoDB table. Select your RDS instance as the source and DynamoDB as destination.  Create a task with an object mapping rule to copy the required relational database tables to DynamoDB.  Within your object-mapping set your weather station ID as the Partition Key of your table and timestamp of the reading as the Sort key.","correct":true},{"id":"4ea68f2e23cb18631ed94c5aefa19cf8","text":"Use Amazon DMS to migrate data to an Amazon DataTables table. Select your RDS instance as the source and DataTables as destination.  Create a task with an object mapping rule to copy the required relational database tables to DynamoDB.  Within your object-mapping set your weather station ID as the Partition Key of your table and timestamp of the reading as the Sort key.","correct":false}]},{"id":"781e2a28-ff7a-4712-bf0c-1ae69dec243f","domain":"SDLCAutomation","question":"You are part of a development team that has decided to compile release notes directly out of a CodeCommit repository, the version control system in use. This step is to be automated as much as possible. Standard GitFlow is used as the branching model with a fortnightly production deploy at the end of a sprint and occasional hotfixes. Select the best approach.","explanation":"Following GitFlow's standard release procedures, a release branch is merged into master. That commit on master must be tagged for easy future reference to this historical version. Both release and hotfix branches are temporary branches and would require ongoing updates of the CodeCommit trigger. Feature branches are used to develop new features for the upcoming or a distant future release and might be discarded (e.g. in case of a disappointing experiment). CodeCommit does not provide a generate release notes feature.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html","title":"Manage Triggers for an AWS CodeCommit Repository"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html#codecommit_event_type","title":"CodeCommit Events"},{"url":"https://aws.amazon.com/blogs/devops/build-serverless-aws-codecommit-workflows-using-amazon-cloudwatch-events-and-jgit/","title":"Build Serverless AWS CodeCommit Workflows using Amazon CloudWatch Events and JGit"},{"url":"https://nvie.com/posts/a-successful-git-branching-model/","title":"A successful Git branching model"},{"url":"https://docs.aws.amazon.com/codecommit/latest/APIReference/Welcome.html","title":"AWS CodeCommit API Reference"},{"url":"https://forums.aws.amazon.com/thread.jspa?messageID=756611","title":"CodeCommit Lambda triggers fire off separate events for each commit?"}],"answers":[{"id":"601038d6d016fa6b4637092d2a68db75","text":"Configure a trigger by choosing the 'Delete branch or tag' repository event that invokes a Lambda function when development for a sprint is finished, i.e. the last feature-* branch has been deleted. In that Lambda, retrieve the latest git merge commit message before the deletion and append it to the release notes text file stored in an S3 bucket.","correct":false},{"id":"94ff89937241b8005e348ff0550a1947","text":"Setup up an Amazon CloudWatch Event rule to match CodeCommit repository events of type 'CodeCommit Repository State Change'. Look for 'referenceCreated' events with a 'tag' referenceType that are created when a production release is tagged after a merge into 'master'. In a Lambda function, use the CodeCommit API to retrieve that release commit message and store it in a static website hosting enabled S3 bucket.","correct":true},{"id":"daf0713b7db34409528f3e191adf166e","text":"Use the 'generate release notes' feature of CodeCommit by running the 'create-release-notes' command with the --from <datetime> (use the start of the sprint) or --previousTag <tagName> option in the AWS CLI. Create a Lambda to execute this on a regular schedule (i.e. every 2 weeks) using CloudWatch Events with a cron expression.","correct":false},{"id":"625fdc89350c8e0422b2c105b66be63a","text":"Create a trigger for your CodeCommit repository using the 'Push to existing branch' event and apply that to any release and hotfix branch. Add an Amazon SNS topic as the target and have a Lambda listen to it. In that function, filter out specific commit type changes such as style, refactor and test that are not relevant for release notes. Store all other commit messages in a DynamoDB table and, at release time, run a query to collate the release notes.","correct":false}]},{"id":"82e554d2-f867-4507-974f-04b0b9020cb9","domain":"MonitoringLogging","question":"Your company has a team of Windows Software Engineers which have recently switched from developing on-premise applications, to cloud native micro-services.  The Service Desk has been inundated with questions, but they can't troubleshoot because they don't know enough about Amazon CloudWatch.  The questions they have been receiving mainly revolve around why EC2 logs don't appear in log groups and how they can monitor .NET applications.  Choose the following options which will help troubleshoot the Amazon Cloudwatch issues.","explanation":"The question suggests we are utilising a Windows development environment, so we can discount any answers which have Linux only terms such as running shell scripts.  To run Amazon CloudWatch Application Insights for .NET and SQL Server, we will need to install the SSM Agent with the correct roles, IAM policies and Resource Groups.  We also need to ensure that the CloudWatch agent is running correctly, by starting it using the amazon-cloudwatch-agent-ctl.ps1 script, and as we are assuming defaults, Cloudwatch metrics can be found under the CWAgent namespace.  There are limits for many items in Cloudwatch, utilising Custom Metrics is not one of them.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/troubleshooting-CloudWatch-Agent.html","title":"Troubleshooting the CloudWatch Agent"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.html","title":"Amazon CloudWatch Application Insights for .NET and SQL Server"}],"answers":[{"id":"ffd7b413b965db5c4e66d6dc2c8a73e8","text":"Check the aws-agent.yaml file exists and is configured correctly, and then run the following shell command; aws-cw-agent.sh start.","correct":false},{"id":"aa7402fca9b873f6b49d144c5c67440e","text":"For each EC2 instance running .NET code, install the SSM Agent and attach the AmazonEC2RoleforSSM Role.  You will also need to create a Resource Group and IAM Policy.","correct":true},{"id":"815da42a7b90966156b2d66da1a16b9e","text":"Check the common-config.toml file exists and is configured correctly, and then run the following Powershell command; amazon-cloudwatch-agent-ctl.ps1 -m ec2 -a start.","correct":true},{"id":"ff3e53ec436317efcebcc4a0bf3b52a8","text":"In the Amazon Cloudwatch console, select Metrics, AWS Namespaces and then your metrics should appear under 'CWAgent'.","correct":true},{"id":"33668d47f8774c94713045bff628ba97","text":"If you are utilising CloudWatch Custom Metrics, ensure that you have not reached the default limit.","correct":false}]},{"id":"ed3b02bb-13e4-4115-a289-c04aaea670bc","domain":"PoliciesStandards","question":"Your AWS account usage is getting out of hand. All of your developers are using their own accounts which have been set up to bill directly the company credit card. It's difficult to track how everything is working together and to govern access to services, resources and regions across the accounts. How could you configure your account environment to work more efficiently, consolidate billing, implement access control and take advantage of some pricing benefits from aggregated usage?","explanation":"Using AWS Organizations you can invite all accounts and implement service control policies to govern access to services, resources and regions.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_tutorials_basic.html","title":"Creating and Configuring an Organization"}],"answers":[{"id":"11d108737ee2f69b03cb68e55fe22498","text":"Configure AWS Organizations, invite all the developer accounts and implement organization-wide service control policies.","correct":true},{"id":"e1ec013e391381c086d998e9377f2153","text":"Configure AWS consolidated billing, invite all the developer accounts and implement organization-wide service control policies.","correct":false},{"id":"67a80ccd0c94c32cf5bd26f79e01e7ea","text":"Configure AWS consolidated billing, invite all the developer accounts and implement cross-account IAM roles.","correct":false},{"id":"99356453991ae98cfef32e007616cacd","text":"Configure AWS Organizations, invite all the developer accounts and implement cross-account IAM roles.","correct":false}]},{"id":"d9e654d8-7cf1-4822-b834-308402b8ea9d","domain":"IncidentEventResponse","question":"You currently host a website from an EC2 instance. Your website is entirely static content. It contains images, static html files and some video files. You would like to make the site as fast as possible for everyone around the world in the most cost effective way. Which solution meets these requirements?","explanation":"S3 and CloudFront is the cheapest solution which will ensure the fastest content delivery around the world, but will also be cheaper due to no ongoing EC2 costs.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/","title":"How do I use CloudFront to serve a static website hosted on Amazon S3?"}],"answers":[{"id":"f4f5a1b2cd8a58cfae42965d8f070404","text":"Move your dynamic assets into S3. Serve the website on your EC2 instance with Amazon CloudFront as your content delivery network.","correct":false},{"id":"08d5998199f3298235e31cd4c01b2e25","text":"Move the website into an S3 bucket and serve it through Amazon CloudFront.","correct":true},{"id":"5851d3ee6cc5b6445f30a454af7d8d08","text":"Keep your website hosting on EC2. Add Amazon CloudFront as your content delivery network.","correct":false},{"id":"67c4d281d5d766f7f403fce78108d67d","text":"Move your static assets into S3. Serve the website on your EC2 instance with Amazon CloudFront as your content delivery network.","correct":false}]},{"id":"3e8ee2fb-8946-43b2-ba85-78ce64b74626","domain":"ConfigMgmtandInfraCode","question":"You are discussing error scenarios and possible retry strategies for your Step Functions machine with your colleague. Which of her claims are incorrect?","explanation":"Errors can arise because of state machine definition issues, task failures or because of transient issues. When a state reports an error, the default course of action for AWS Step Functions is to fail the execution entirely.","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-errors.html","title":"AWS Step Functions: Amazon States Language - Errors"}],"answers":[{"id":"51e213b667c0c51b9814eb2430f2d081","text":"A Retrier must contain the 'ErrorEquals' field which is a non-empty array of strings that match Error Names, e.g. 'States.Timeout'. When a state reports an error, Step Functions scans through the Retriers and, when the Error Name appears in this array, it implements the retry policy described in this Retrier.","correct":false},{"id":"5673ef332c854836ed6963374170526a","text":"Any state can encounter runtime errors. Examples are when a Lambda function throws an exception or if a transient network issue exists. AWS Step Functions distinguishes these clearly from Failures such as state machine definition issues that are handled differently.","correct":true},{"id":"c1c080d749a342bf5e9fc2509c11fe40","text":"When a state reports an error, the default course of action for AWS Step Functions is to log the error and perform a single retry after 1 second. If that doesn't succeed, AWS Step Functions will fail the execution entirely.","correct":true},{"id":"8c65f4ebde68eb7bc8a58c7747c9f843","text":"A Retry field with an 'IntervalSeconds' and 'MaxAttempts' value of 3 and 'BackoffRate' value of 1.5 will make three retry attempts after waits of 3, 4.5 and 6.75 seconds.","correct":false}]},{"id":"977b9167-9509-4260-91b8-745ff4a10c16","domain":"ConfigMgmtandInfraCode","question":"You need to quickly test a proof of concept that your boss has given you. He's given you a zip file containing a php web application. You want to get it running in an AWS environment as fast a possible, however there's also a dependency on a library that must be installed as well. The library is available from a yum/apt repository.  Which service do you choose and how do you ensure dependencies are installed?","explanation":"AWS Elastic Beanstalk is a quick way to test the proof of concept, as webserver configuration is not required. Required Libraries can be installed quickly and automatically using ebextensions.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html","title":"Customizing Software on Linux Servers - AWS Elastic Beanstalk"}],"answers":[{"id":"4cc5c190534c4e744d352d96c3ebe2bc","text":"AWS CloudFormation, install dependency with custom resources.","correct":false},{"id":"4f921aa28b46ed7642eb7a3dc8583844","text":"AWS OpsWorks for deployment, install dependency with chef.","correct":false},{"id":"adde411da8e54597432c35347938b2ec","text":"AWS Elastic Beanstalk for deployment, install dependency with ebextensions.","correct":true},{"id":"c2f64a6cd63768d866b164a5c278f252","text":"AWS EC2 and apache2, install dependency with apt-get or yum.","correct":false}]},{"id":"100210fd-801f-4377-bbb1-131646b5bbf4","domain":"SDLCAutomation","question":"Your developers are currently storing their code in a private github repository, however your organization has recently introduced rules that everything must live within your AWS environment, so you need to find an alternative. What do you suggest that will both continue to work with your existing developer environments but also support a possible future transition into a CI/CD environment?","explanation":"CodeCommit is the best solution because it's compatible with CI/CD supporting services such as CodeBuild and CodePipeline","links":[{"url":"https://aws.amazon.com/codecommit/","title":"AWS CodeCommit | Managed Source Control Service"}],"answers":[{"id":"dfa1a6e5db21b8bd2df6f5674aadbbd7","text":"Move your repositories to your own Git server running in EC2.","correct":false},{"id":"909d969c995f3c405df90a78ad1dc185","text":"Move your repositories to your own CodeCommit EC2 instance from the AWS marketplace.","correct":false},{"id":"efdd4ad61a489544e40886e3db7751db","text":"Move your repositories to S3.","correct":false},{"id":"59bc8c49cd324cbe0767a72d0787fc19","text":"Move your repositories to CodeCommit.","correct":true}]},{"id":"836be367-5c22-4432-bde3-9be7e7c18889","domain":"SDLCAutomation","question":"Your application is being built with CodeBuild. You would like your artifacts to be stored with the region they were built in added to the filename so you can easily locate them later on. The CodeBuild documentation refers to an environment variable called $(AWS_REGION) that you could use. How and where will you implement this?","explanation":"You can specify the artifact name in the artifacts section of the buildspec.yaml file.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html","title":"Build Specification Reference for CodeBuild"}],"answers":[{"id":"56b7a49bf330e1e6de9009853f296dcb","text":"In the artifacts section of the post_build phase of the buildspec.yaml file. Specify 'name: build-$(AWS_REGION)'","correct":false},{"id":"45f22a55f14531223e9a323369b431d3","text":"In the artifacts section of the buildspec.yaml file. Specify 'name: build-$(AWS_REGION)'","correct":true},{"id":"0cd9fb161ae6c87e00e659ccb22d878e","text":"In the post_build phase in the phases section of the buildspec.yaml file. Specify 'name: build-$(AWS_REGION)'","correct":false},{"id":"63a8a2011e441323f7461896a7d71e16","text":"In the artifacts phase of the buildspec.yaml file. Specify 'name: build-$(AWS_REGION)'","correct":false}]},{"id":"19cc0253-1dad-4ed2-bc2a-95515248a9ac","domain":"SDLCAutomation","question":"You are developing a completely serverless application and store your code in a git repository. Your CEO has instructed you that under no circumstances are you allowed to spin up an EC2 instance. In fact, he's blocked access to ec2:* company-wide with an IAM policy. He does however still want you to completely automate your development process, you just can't use servers to do so. Which AWS services will you make use of to meet these requirements?","explanation":"CodeDeploy can deploy a serverless lambda function straight from your CodeCommit repository.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html","title":"What Is CodeDeploy?"}],"answers":[{"id":"e3ce2b50bb87a7fe18adff65e0320c08","text":"Use AWS Lambda for your compute functions, edit them directly in the Cloud9 console in the browser. Once installed they don't need to redeployed.","correct":false},{"id":"fbc7f2f64b4e3a232325800045b1725f","text":"Move your compute functions into S3, and use CodePipeline to deploy your code from S3 into AWS Lambda.","correct":false},{"id":"93bfcd1da129b57627d8753c0b2d65ef","text":"Use AWS Lambda for your compute functions, use CodeDeploy to deploy the functions for you. Store your code in CodeCommit and use a CodePipeline to automatically deploy the functions when you commit your code to the repository.","correct":true},{"id":"d385958d5064373b0d639a8c16f6ab09","text":"Use AWS Lambda for your compute functions, use CodeDeploy to deploy the functions for you. Store your code in GitCommit and use a CodePipeline to automatically deploy the functions when you commit your code to the repository.","correct":false}]},{"id":"6edec40a-5186-484f-84e3-d7bd3df44eb8","domain":"IncidentEventResponse","question":"Your company has recently switched from an external Email Service Provider to utilising SES to send marketing email.  You have been asked to record all bounce and complaint information from each email sent, in an S3 bucket so that it can be processed later.  You know that Kinesis Firehose can perform this task.  You have also been asked to anonymise the email addresses that are returned, in order to comply with GDPR and you will use Lambda to perform this task.  After a number of attempts, you have failed to receive any data into the S3 bucket.  Which of the following troubleshooting methods are valid in resolving your issues?","explanation":"We can immediately reject any answers that contain RedShift, as there is no need to use it to meet the needs of the question.  All of the remaining answers are possible methods of troubleshooting your issues.","links":[{"url":"https://docs.aws.amazon.com/firehose/latest/dev/troubleshooting.html","title":"Troubleshooting Amazon Kinesis Data Firehose"}],"answers":[{"id":"9c2908c869a796ee883dbbd0f643986a","text":"Make sure the Firehose DeliveryToRedshift.Success metric is available to confirm that Firehose has tried to copy data from your S3 bucket to the Redshift cluster.","correct":false},{"id":"b0f65e9497f5a0ee31b86e2dfd462e31","text":"Make sure the Firehose IncomingBytes and IncomingRecords metrics are available, to confirm that data is being delivered to your Firehose delivery stream successfully.","correct":true},{"id":"bc1a53efea21f6531d0b1a523da1b07f","text":"Make sure the Amazon Redshift STL_LOAD_ERRORS table is available, to verify the reason for the COPY failure.","correct":false},{"id":"196fe528acbe4658755d261eab3182d4","text":"Make sure that the Firehose DeliveryToS3.Success metric is available, to confirm that Firehose has tried putting data into your S3 bucket.","correct":true},{"id":"f71cd26ef170d12dafd69c0d3db6ddd9","text":"Make sure the Firehose ExecuteProcessingSuccess metric is available, to confirm that an attempt has been made to invoke your Lambda function.","correct":true}]},{"id":"85ec1f79-e327-44c4-a934-46e826ec0dd3","domain":"HAFTDR","question":"You currently run an autoscaled application which is database read heavy. Due to this, you are making use of a read replica for all application reads. It's currently running at around 60% load with your user base but you expect your company growth to double the amount of users every 6 months. You need to forward plan for this and determine methods to ensure the database doesn't become a bottleneck while still maintaining some redundancy. What is the best way to approach this issue?","explanation":"More read replicas will ease the load on your current ones, and load balancing them with a weighted routing policy will ensure they're not a single point of failure for your application.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/requests-rds-read-replicas/","title":"How can I distribute read requests across multiple Amazon RDS read replicas?"}],"answers":[{"id":"77851b21f9bb1d7557b828c6ad224a60","text":"Create more read replicas. Use a Route53 weighted routing policy to ensure the load is spread across your read replicas evenly.","correct":true},{"id":"17ef3669be9b09f4623719b5771bcb40","text":"Create another read replica and deploy a second autoscaled version of your application. Point it at your second read replica.","correct":false},{"id":"c67fa8f440af68765fc377331fb22bb7","text":"Deploy an additional Multi-AZ RDS read replica and modify your application to use it instead.","correct":false},{"id":"43d1e6030522d9685ec499098adce943","text":"Monitor your database read replica usage in CloudWatch alerts. When it's close to 90% capacity perform an online resize to a larger instance type.","correct":false}]},{"id":"7cd5551a-b9a0-4e59-bb53-d40d81dc8938","domain":"ConfigMgmtandInfraCode","question":"You have been ask to deploy a clustered application on a small number of EC2 instances.  The application must be placed across multiple Availability Zones, have high speed, low latency communication between each of the nodes, and should also minimise the chance of underlying hardware failure.  Which of the following options would provide this solution?","explanation":"Spread Placement Groups are recommended for applications that have a small number of critical instances which need to be kept separate from each other. Launching instances in a Spread Placement Group reduces the risk of simultaneous failures that might occur when instances share the same underlying hardware. Spread Placement Groups provide access to distinct hardware, and are therefore suitable for mixing instance types or launching instances over time. In this case, deploying the EC2 instances in a Spread Placement Group is the only correct option.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html","title":"Placement Groups"}],"answers":[{"id":"b1954190e825c200d890843b70d5cb38","text":"Create a new VPC with the tenancy type of host and deploy the instances in the VPC","correct":false},{"id":"112a2330d77c300b357edda7c17dddb6","text":"Deploy the EC2 servers in a Cluster Placement Group","correct":false},{"id":"72371c02b14e73370c1b01dd2523a1c1","text":"deploy the EC2 servers in a Spread Placement Group","correct":true},{"id":"f3d57c381ce0c53f5ff05f7a48d8ae15","text":"The application should deployed as a service in ECS","correct":false}]},{"id":"e94bdca3-b267-4c84-8485-4e53c9319db4","domain":"ConfigMgmtandInfraCode","question":"You have an application built on docker. You would like to not only launch your docker instances at scale but load balance them as well. Your director has also enquired as to whether it is possible to build and store the containers programmatically using AWS API calls in future or if additional products are required. Which AWS services will enable you to store and run the containerized application and what do you tell the director?","explanation":"Amazon ECS and ECR are the Amazon Elastic Container Service and Registry, which will meet all of the requirements specified. It also has an API which can be used to implement your requirements.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html","title":"What is Amazon Elastic Container Service? - Amazon Elastic Container Service"}],"answers":[{"id":"361a8c7414d01c019db585e4f410d8c0","text":"Build containers and store them using AWS ECR (Elastic Container Registry). Run the containers using Amazon ECS. Automated container builds are not possible with AWS services and APIs however third-party build engines such as Jenkins and BuildKite will achieve your director's goal.","correct":false},{"id":"2b2965d7b7b01e772fd99e1883745cc6","text":"Build EC2 instances with docker using EC2 and AutoScaling and store them using AWS ECR (Elastic Container Registry). Run the containers on docker desktop on EC2. Yes automated container builds are possible with AWS CodeBuild pushing to ECR and using ECS APIs.","correct":false},{"id":"2786f753087512c515526c6e2f9909a3","text":"Build EC2 instances with docker using EC2 and AutoScaling and store them using AWS ECR (Elastic Container Registry). Run the containers using Autoscaling EC2. Automated container builds are not possible with AWI services and APIs however third-party build engines such as Jenkins and BuildKite will achieve your director's goal.","correct":false},{"id":"9d3a19960e725c6a8f4133c71337a88e","text":"Build containers and store them using AWS ECR (Elastic Container Registry). Run the containers using Amazon ECS. Yes automated container builds are possible with AWS CodeBuild pushing to ECR and using ECS APIs.","correct":true}]},{"id":"e96d9cdb-1a97-4abe-9aea-0f5c2016fd0f","domain":"IncidentEventResponse","question":"You run a cat video website, and have an EC2 instance running a webserver which serves out the video files to your visitors who watch them. On the same EC2 instance, you also perform some transcoding to make the files smaller or to change their resolution based on what the website visitor is requesting. You have found that as your visitors grow, this is getting harder to scale. You would like to add more web servers and some autoscaling, as well as moving the transcoding to its own autoscaling group so it autoscales up when you have a lot of videos to convert. You are also running out of disk space fairly frequently and would like to move to a storage system that will scale too, with the least amount of effort and change in the way your entire system works. What do you do?","explanation":"This is a great case for using EFS. With minimal effort you can move your cat video website to an automatically scaling storage solution which can be used by all of your EC2 instances.","links":[{"url":"https://aws.amazon.com/efs/","title":"EFS"}],"answers":[{"id":"c9acb0ccafd771fdd4739d4cb5146111","text":"Implement S3 Glacier to save on storage costs. Use the S3 Glacier API to retrieve and store video files as required.","correct":false},{"id":"b908baa5c72de0c770b146c5103275e0","text":"Implement S3. Modify your applications to serve videos from S3 as well as downloading files from S3, transcoding them and storing them there.","correct":false},{"id":"fc82534fc7c30a5bbc7dd66625b426ae","text":"Implement Storage Gateway on each EC2 instance. Use the gateway to quickly move files between instances as required.","correct":false},{"id":"0369b9bbb9fc65a7ad7ed7be897b6824","text":"Implement EFS. Mount your volume to each server for serving content and transcoding.","correct":true}]},{"id":"b8240606-8f74-4b39-ad43-d8374d4b275d","domain":"MonitoringLogging","question":"Your organization has been using AWS for 12 months. Currently, you store all of your custom metrics in CloudWatch. Per company policy, you must retain your metrics for 3 years before it is OK to discard or delete them. Is CloudWatch suitable?","explanation":"CloudWatch will retain metrics for 15 months, after which they will expire. If you need to keep metrics for longer you must pull them out of CloudWatch using their API and store them in a database somewhere else.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html","title":"CloudWatch Concepts"}],"answers":[{"id":"d829ad2942daa681d42ad4237f50827c","text":"No, CloudWatch only retains metrics for 15 months. You will have to use the API to pull all metrics and store them somewhere else.","correct":true},{"id":"47ecaf50014e4dd77522488586ae375a","text":"Yes, CloudWatch will retain custom metrics for 3 years.","correct":false},{"id":"8faab65d1ba3ce5e0cb52e471493dd0b","text":"No, CloudWatch only retains metrics for 24 months. You will have to use the API to pull all metrics and store them somewhere else.","correct":false},{"id":"cf00bc1c445d1daf231db593c6f998d9","text":"Yes, CloudWatch will retain custom metrics for 5 years.","correct":false}]},{"id":"1c7dd43f-b2c7-49aa-b897-bd62f97ee183","domain":"ConfigMgmtandInfraCode","question":"A company has successfully deployed a two EC2 node Redis cluster, using CloudFormation.  In order for the cluster to work, Node one is started and populated with data before Node two is started. Some weeks later, a new Redis stack is created using the same template, but is populated by a different set of data.  In this case the stack consistantly fails to be created eventhough the template and parameters are identical to the original stack.  Which actions should be taken to ensure that the new cluster can successfully be created?","explanation":"If, as in this case, the order of resources is important, it should never be assumed that they will always stand up in the same order each time, the first stack may have deployed in the correct order by luck.  A 'DependsOn' resource should always be used to maintain order consistancy.  Also, the data that is populating Redis may be much larger and take more time in the second Stack.  Adding a Creation Policy with a large Timeout will ensure that data successfully completes the import and the EC2 instance is successfully created.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"Template Anatomy"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-creationpolicy.html","title":"CreationPolicy Attribute"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-dependson.html","title":"DependsOn Attribute"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html","title":"Working with Nested Stacks"}],"answers":[{"id":"002f82cd165be0c2dd0d4bfba60bde24","text":"Use a single CloudFormation template and add a 'DependsOn' resource for the second EC2 node which points to the first EC2 node.","correct":true},{"id":"d1b2270b9abfe6f978128e05ec79268d","text":"Use a single CloudFormation template and add a Creation Policy with a large Timeout, to the first EC2 node.","correct":true},{"id":"af31ddbdf176fd17fce491dc05ba1388","text":"Use a CloudFormation root template which uses nested stacks.  Define each Redis node in a separate template which are linked to by the root template.","correct":false},{"id":"9a01df5c5a78cacac35b87d8678c7d74","text":"Use a single CloudFormation template and add a Creation Policy and Deletion Policy with a Retain option to maintain both EC2 Redis nodes.","correct":false},{"id":"a7e04cbb4e145efffd811f7596f2fbdf","text":"Use two CloudFormation templates, one for each EC2 Redis node and manually run one at at time.","correct":false}]},{"id":"353f2086-4a2b-4c7a-b40d-8199509f0f8f","domain":"PoliciesStandards","question":"You have an antiquated piece of software across your Linux virtual machines that you are attempting to move away from. Unfortunately the process of extracting the data from this software is difficult and requires a lot of commands to be run on the command line. The final process of moving away from this software is executing 90 commands on each database server. Which AWS services will make this as painless as possible and easiest to set up?","explanation":"The Systems Manager Run Command on the database servers is the best way to achieve this. You do not need to run commands on all servers.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/rc-console.html","title":"Running Commands from the Console"}],"answers":[{"id":"77c20d947ea3cdba35cde43dc31f265a","text":"Use the scp command to copy the bash script to each server and then execute it manually.","correct":false},{"id":"b524668957c324a9210f76db4fc4995f","text":"Create an EC2 instance and a bash script with the commands. Run the bash script which will ssh to all of your servers and execute the commands.","correct":false},{"id":"16101d7bc0420494d4449f62f5980f41","text":"Use the Systems Manager Run Command feature to run a shell script containing the 90 commands on each database server.","correct":true},{"id":"fdcba29bfa345864d3e47b322ca54389","text":"Create a bash script with the commands and store it in S3. Use a Lambda function to copy the script to all of your database servers and execute the script.","correct":false}]},{"id":"50424d57-02c2-4af2-84ff-10d935637fc6","domain":"IncidentEventResponse","question":"Your company has been producing Internet-enabled Microwave Ovens for two years.  These ovens are constantly sending streaming data back to an on-premises endpoint behind which sit multiple Kafka servers ingesting this data.  The latest Microwave has sold more than expected and your Manager wants to move to Kinesis Data Streams in AWS, in order to make use of its elastic capabilities.  A small team has deployed a Proof of Concept system but are finding throughput lower than expected, they have asked for your advice on how they could put data on the streams quicker.  What information can you give to the team to improve write performance?","explanation":"You should always use the Kinesis Producer Library (KPL) when writing code for Kinesis where possible, due the Performance Benefits, Monitoring and Asynchronous performance.  You should choose any answers which include KPL as a solution.  You should also check all relevant Service Limits to ensure no throttling is occurring.  Only three of these options are shown in the question, but there are many more possibilities.  The remaining options will work, but generally will give slower performance.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-producers.html","title":"Troubleshooting Amazon Kinesis Data Streams Producers"}],"answers":[{"id":"7080b30b6deaa847a5edb074ea4be9d8","text":"Use a Large Producer without the Kinesis Producer Library, but using the PutRecord operation.","correct":false},{"id":"8309467932d5a97196aca6a575a318af","text":"Develop code using the Kinesis Producer Library to put data onto the streams.","correct":true},{"id":"19d55c732ada47508097e33c6e04a120","text":"Check the GetShardIterator, CreateStream and DescribeStream Service Limits.","correct":true},{"id":"95940bef689407d24dcd08ee0da18ca5","text":"Develop code using the SDK to put data onto the streams with the Kinesis Data Streams API.","correct":false},{"id":"c6a098ae823197ee09ed9de745ce2cc8","text":"Use a Small Producer with the Kinesis Producer Library, but using the PutRecords operation.","correct":true}]},{"id":"d1e18980-dcfc-4893-afce-cb19efacb6a9","domain":"IncidentEventResponse","question":"Your CEO has heard how an ELK stack would improve your monitoring, troubleshooting and ability to secure your AWS environment. Before letting you explain anything about it, he demands you get one up and running as soon as possible using whatever AWS services you need to use. How do you go about it?","explanation":"The Amazon Elasticsearch service will give you managed Elasticsearch, Logstash and Kibana without the requirement of installing, maintaining or scaling any of them and their associated infrastructure.","links":[{"url":"https://aws.amazon.com/elasticsearch-service/resources/articles/the-benefits-of-the-elk-stack/","title":"The benefits of the ELK stack without the operational overhead"}],"answers":[{"id":"04de12d4521b9dc826f59dc0d42f97f5","text":"Use the Amazon Elasticsearch Service.","correct":true},{"id":"c18a97d462ec5c25526d8376971d37b2","text":"Install CloudSearch, Logstash and Kibana on an EC2 instance.","correct":false},{"id":"de19d8eddb68265815d3495f5ee053dd","text":"Use CloudSearch, CloudWatch Logs and CloudKibana managed services to create your ELK stack.","correct":false},{"id":"2c6e86f82d079bd47ee2f084bbe0fa2d","text":"Install Elasticsearch, Logstash and Kibana on an EC2 instance.","correct":false}]},{"id":"6c131b00-36b0-41c4-a384-b2865d782a9f","domain":"PoliciesStandards","question":"The security officer of the company you work for has mandated that from now on all production database credentials are to be changed every 90 days. You've decided to automate this for a cluster of Db2 instances and want to use AWS Secrets Manager. This database is used by several of your applications but owned by an external party who has given you three separate DB users for different environments (DEV, TEST, PROD). You can change their passwords yourself but not create new users. Select the correct answer that describes the best way to proceed.","explanation":"Secrets Manager already natively knows how to rotate secrets for supported Amazon RDS databases. However, it also can enable you to rotate secrets for other databases or third-party services. It provides a secure API that enables the programmatic retrieval of secrets to ensure that it can't be compromised by someone examining your code and configurations stored in a version control system. Secrets Manager invokes the secrets rotation Lambda function each time with the same secretId and clientTokenRequest. Only the Step parameter changes with each call. This helps prevent you from having to store any state between steps.","links":[{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html","title":"Rotating Your AWS Secrets Manager Secrets"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets-one-user-one-password.html","title":"Rotating AWS Secrets Manager Secrets for One User with a Single Password"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets-lambda-function-overview.html","title":"Overview of the Lambda Rotation Function"}],"answers":[{"id":"2c8938e46fe2a94adbb22a51927db17e","text":"You can implement the DB credentials change with an AWS Lambda function that changes the DB user's password and updates the application's DB connection configurations. Use CloudWatch events to trigger this every 3 months.","correct":false},{"id":"0698b583ffe36fca819f464eaaffe7a9","text":"AWS Secrets Manager allows you to automatically rotate secrets for some Amazon RDS databases, Redshift and DocumentDB. Db2 is not a supported RDS database engine and therefore, you cannot use AWS Secrets Manager to rotate your secrets and should use the AWS Systems Manager Parameter Store instead.","correct":false},{"id":"e30a5db26a1d4130fbfdc496ea15db8f","text":"Update the apps to retrieve the DB credentials from AWS Secrets Manager. You will also need to configure Secrets Manager with a custom Lambda function that is called several times by it when rotation is triggered, each time with different parameters. It is expected to perform several tasks throughout the process of rotating a secret. The task to be performed for each request is specified by the 'Step' parameter in the request. Every step is invoked with a unique 'clientTokenRequest' parameter.","correct":false},{"id":"857599feabd6fa9df995006ef2a15739","text":"You should ask the external party for a DB user with at least two credential sets or the ability to create new users yourself. Otherwise, you might encounter client sign-on failures. The risk is because of the time lag that can occur between the change of the actual password and - when using Secrets Manager - the change in the corresponding secret that tells the client which password to use.","correct":true}]},{"id":"5c90df95-736b-4b27-a7cf-095c4dc3d4c4","domain":"HAFTDR","question":"You run a load balanced, auto-scaled website in EC2. Your CEO informs you that due to an upcoming public offering, your website must not go down, even if there is a region failure. What's the best way to achieve this?","explanation":"A latency based routing policy will keep your website as fast as possible for your customers, and will act as redundancy should one of the regions go down.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy"}],"answers":[{"id":"230b3ec4e1a08538bc506cca11e9c1a1","text":"Deploy your load balancers and auto-scaled website in two different availability zones. Create a Route53 GeoProximity Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":false},{"id":"560436ce37b94afeee434582179ddc95","text":"Deploy CloudFront in front of your instances. It will cache requests even if a region goes down and your users will not notice.","correct":false},{"id":"20703f2de437222f1e63b4b81216e053","text":"Deploy your load balancers and auto-scaled website in two different availability zones. Create a Route53 weighted Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":false},{"id":"f20603c9e3176b9a3bdd585343418443","text":"Deploy your load balancers and auto-scaled website in two different regions. Create a Route53 Latency Based Routing Record. Point the record to each of your Elastic LoadBalancers.","correct":true}]},{"id":"e7f37856-b31b-4997-872b-99ede9470dda","domain":"SDLCAutomation","question":"Your organization is currently using CodeDeploy to deploy your application to 20 EC2 servers which sit behind a load balancer. It's making use of the CodeDeployDefault.OneAtATime deployment configuration. Your manager has decided to speed up deployment by deploying to as many servers as possible at once, as long as at least five of them remain in service at any one time. How do you achieve this, ensuring that it will scale?","explanation":"Setting the minimum healthy host as a Number and 5 will work as desired. A percentage set to 25% will work for the current 20 servers, but it will not scale down if any servers are removed.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html","title":"Working with Deployment Configurations in CodeDeploy"}],"answers":[{"id":"09c0a847baf9482b080d3e333ecca413","text":"Create a custom deployment configuration, specifying the maximum healthy host as a \"Number\" and set it to 5","correct":false},{"id":"8b33708bd673179fc3d5e2da7d7eed70","text":"Create a custom deployment configuration, specifying the maximum healthy host as a \"Percentage\" and set it to 25%","correct":false},{"id":"9cad758b5b1009c5dcdc6df71e58852b","text":"Create a custom deployment configuration, specifying the minimum healthy host as a \"Number\" and set it to 5","correct":true},{"id":"c58aab48dd8ec648df3d39b4264533d8","text":"Create a custom deployment configuration, specifying the minimum healthy host as a \"Percentage\" and set it to 25%","correct":false}]},{"id":"0c271f46-7ab3-485d-a497-2387d04c151a","domain":"MonitoringLogging","question":"You're developing a Node.js application that uses the AWS Node.js API. You interact with lots of different AWS services, and would like to determine how long your API requests are taking in an effort to make your application as efficient as possible. It would also be useful to detect any issues that may be arising and give you an idea about how to fix them. Which AWS service can assist in this task and how would you go about achieving it?","explanation":"AWS X-Ray will produce an end to end view of requests made from your application where you can analyze the requests made as they pass through your application.","links":[{"url":"https://aws.amazon.com/xray/features/","title":"AWS X-Ray Features"}],"answers":[{"id":"0d3184462bd7dde819bd2cb519ee4738","text":"Use AWS X-Ray, inspect the service map, trace the request path, determine the bottlenecks.","correct":true},{"id":"d58a545b79533928203dfa0e4a473547","text":"Use Amazon QuickSight, inspect the service map, trace the request path, determine the bottlenecks.","correct":false},{"id":"d920ccd5f4c9bc34ad4acb6ba48ac523","text":"AWS AWS X-Ray, inspect the client map, trace the segment path, determine the bottlenecks.","correct":false},{"id":"4ca7aef91bd24ff0af398119f581d683","text":"Use Amazon QuickSight, inspect the client map, trace the segment path, determine the bottlenecks.","correct":false}]},{"id":"d4166e07-35b5-4196-b355-a04793003f88","domain":"HAFTDR","question":"You currently have a lot of IoT weather data being stored in a DynamoDB database. It stores temperature, humidity, wind speed, rainfall, dew point and air pressure. You would like to be able to take immediate action on some of that data. In this case, you want to trigger a new high or low temperature alert and then send a notification to an interested party. How can you achieve this in the most efficient way?","explanation":"Using a DynamoDB stream is the most efficient way to implement this. It allows you to trigger the lambda function only when a temperature record is created, thus saving Lambda from triggering when other records are created, such as humidity and wind speed.","links":[{"url":"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/","title":"DynamoDB Streams Use Cases and Design Patterns"}],"answers":[{"id":"295bff0a7cd575c34ecae937a4f12f8e","text":"Use a DynamoDB stream and Lambda trigger only on a new temperature reading. Send a SNS notification if a record is breached.","correct":true},{"id":"bfa4fc6ae9d9c31b4764750964fc2583","text":"Use CloudWatch custom metrics to plot your temperature readings and generate an event alert if it breaches your high and low thresholds.","correct":false},{"id":"3cca31ba4defd9f00540ba540543c0af","text":"Modify your IoT devices to also log their data to Kinesis Data Firehose and trigger a Lambda function which will check for new high or low temperatures. Send a SNS notification.","correct":false},{"id":"86d964e70883ad4a58b196f134df686c","text":"Write an application to use a DynamoDB scan and select on your Sort key to determine the maximum and minimum temperatures in the table. Compare them to the existing records and send an SNS notification if they are breached. Run the application every minute.","correct":false}]},{"id":"00ff039c-bad1-448c-8d17-e6ad8b1491f6","domain":"SDLCAutomation","question":"You manage a team of developers who currently push all of their code into AWS CodeCommit, and then CodePipeline automatically builds and deploys the application. You think it would be useful if everyone received an email when a pull request is created or updated. How would you achieve this in the simplest way?","explanation":"Notifications in the CodeCommit console is the simplest way to implement this requirement. Triggers only trigger when someone pushes to a repository, not when a pull request is created.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-repository-email.html","title":"Configuring Notifications for Events in an AWS CodeCommit Repository"}],"answers":[{"id":"15c5b59dd67cddfeb10567fef856e7b1","text":"Enable triggers in the CodePipeline console. Create a trigger stage after your source stage and select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.","correct":false},{"id":"d5af6b2263080d905efb836b99bf8b19","text":"Enable notifications in the CodePipeline console. Create a notification stage after your source stage and select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.","correct":false},{"id":"ee242f6e3c35045661e6306dbd2e0153","text":"Enable notifications in the CodeCommit console. Select 'Pull request update events' as the event type and choose or create a new SNS topic for the notifications. Subscribe everyone to the SNS topic.","correct":true},{"id":"439fdcb8b7e144ddacda7b944536f82b","text":"Enable triggers in the CloudCommit console. Select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.","correct":false}]},{"id":"b299b67c-9e64-46b9-8397-b3d92e5a8e9a","domain":"SDLCAutomation","question":"You have multiple teams of developers and at the moment they all have the ability to start and stop any EC2 instance that they can see in the EC2 console, which is all of them. You would really like to implement some security measures so they can only start and stop the instances based on their cost center. What AWS features would you use to achieve this?","explanation":"You can simplify user permissions to resources by using tags and policies attached to roles. the aws:PrincipalTag is a tag that exists on the user or role making the call, and an iam:ResourceTag is a tag that exists on an IAM resource. In this case we want the CostCenter tag on the resource to match the ConstCenter tag assigned to the developer.","links":[{"url":"https://aws.amazon.com/blogs/security/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles/","title":"Simplify granting access to your AWS resources by using tags"}],"answers":[{"id":"3c58e079022c3f48db6068b4e061539c","text":"Implement tags and restrict access by comparing the iam:PrincipalTag and the aws:ResourceTag in a policy attached to your developer role and seeing if they match.","correct":false},{"id":"d106bcfe8a672539187a3453ec7498d0","text":"Implement roles which you can assign to each resource which will allow a developer to start or stop the instance if they are also assigned to it.","correct":false},{"id":"3e3f004e8462587e3c0ee0b9692cde3b","text":"Implement EC2 policies which you can assign to each resource which will allow a developer to start or stop the instance if they are also assigned to it.","correct":false},{"id":"1597cc0291688fb61d2e070e6fefc038","text":"Implement tags and restrict access by comparing the aws:PrincipalTag and the iam:ResourceTag in a policy attached to your developer role and seeing if they match.","correct":true}]},{"id":"279ef48b-4c8c-4cf0-a14a-8144644ac2f1","domain":"MonitoringLogging","question":"You work for a company that uses a serverless architecture to process up to 4,000 events per second for its usage analytics service. At its core, it consists of a multitude of Lambdas that have been given various resource configurations, i.e. memory/CPU settings. For each function invocation, you want to monitor that allocation against the actual memory usage. Select the simplest feasible approach to achieve that.","explanation":"AWS Lambda doesnt provide a built-in metric for memory usage but you can set up a CloudWatch metric filter. The AWS Lambda console provides monitoring graphs for Invocations, Duration, Error count and success rate (%), Throttles, IteratorAge and DeadLetterErrors.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html","title":"AWS Lambda Function Configuration"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html","title":"Monitoring and troubleshooting Lambda applications using Amazon CloudWatch"},{"url":"https://forums.aws.amazon.com/thread.jspa?threadID=226674","title":"Profiling memory usage for Lambda functions"},{"url":"https://gist.github.com/cgoering/4de674e3e3ca8d6255ea708997cca3b0","title":"AWS Lambda memory usage metric in CloudFormation"}],"answers":[{"id":"65883e3f656c8f5b6f91c895b44649ff","text":"Use the 'Monitoring' tab in the AWS Lambda console and add the 'Resource' monitoring graph to your dashboard.","correct":false},{"id":"2f5f4e15a960c888b86e315b993df292","text":"You can set up a custom CloudWatch metric filter using a pattern that includes 'REPORT', 'MAX', 'Memory' and 'Used:'.","correct":true},{"id":"e2113fd25833a45a179807bc633f635a","text":"Log entries written into the log group associated with a Lambda function don't include profiling info such as memory usage. You need to use AWS X-Ray for that.","correct":false},{"id":"55b98a256995ac4e4c6f7afc1e1a5845","text":"AWS Lambda provides a built-in CloudWatch metric for memory usage.","correct":false}]}]}}}}
