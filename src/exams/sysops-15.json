{"data":{"createNewExamAttempt":{"attempt":{"id":"4b14a7c4-c8ca-4136-9171-aea08bfc3e08"},"exam":{"id":"28a81e57-f5bf-40f0-8284-d14162637163","title":"AWS Certified SysOps Administrator - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"6eb6dee0-9456-492e-afc7-b7c860b04c92","domain":"mon-rep","question":"You have a fleet of EC2 webservers behind an application load balancer. Your web application had some down time which involved some 5XX errors during a very important time in your business 1 week ago. Although you maintain application logs on individual EC2 instances, you do not store these logs anywhere central and unfortunately the EC2 instances that experienced the downtime have since been terminated. How could you review this log data?","explanation":"Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. These logs are encrypted and stored in S3.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html","title":"Application Load Balancer Access Logs"}],"answers":[{"id":"d53665e5cde1c8f661130ca2882d789a","text":"Open the AWS artifact service. Create a new artifact job and point the AWS artifact agents at the terminated EC2 instances. Download the metrics and review in CloudWatch.","correct":false},{"id":"49774faac3030c4425f2aa345cb89dc0","text":"If access logs is turned on for your application load balancer you could review this data by reviewing the logs in S3.","correct":true},{"id":"ae59c599efa75d438daecb43cc08ad41","text":"Use AWS X-ray to restore the logs from the terminated EC2 instances","correct":false},{"id":"c13c66278433ebb606e21e9c9f5250fd","text":"Create a new AWS inspector job to pull the snapshots of the EC2 instances from S3 and run a report in conjuction with AWS Athena.","correct":false}]},{"id":"076713dd-bc2b-4460-ae94-9d3df45eaa08","domain":"dep-prov","question":"You are an engineer for a large publishing firm.  You have been asked to deploy a new EC2 instance in your VPC to act as a bastion host. After running the EC2 launch wizard and selecting your public subnet you received an error: InstanceLimitExceeded. What could be the cause of this issue?","explanation":"When the InstanceLimitExceeded error is returned, you have reached the limit on the number of instances that you can launch in a region. When you create your AWS account, AWS set default limits on the number of instances you can run on a per-region basis.  The solution to this would be to raise a Support request with AWS to raise the instance limit.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html","title":"Troubleshooting EC2 Launch"}],"answers":[{"id":"2b0e392d011048a6e9bffa6106ce57cf","text":"You have not enabled Source/Destination Checks in the public subnet","correct":false},{"id":"80c5d5df6225f5583c39e71966ddad02","text":"You have exceeded the limit for the number of that instance type which you can launch in that Region","correct":true},{"id":"242e116ec33eeede66cc4f85212ce4b7","text":"Your account does not have the EC2 Run Instance permission for that Region","correct":false},{"id":"a96267034b308f86823a30ab2cb3de84","text":"You have selected an EC2 instance type that is not available in that Availability Zone","correct":false}]},{"id":"4553a766-ae3d-4a15-a0cd-8f7617510415","domain":"security-comp","question":"You work at a pharmaceutical company who has just deployed a new web application in to their development environment. The webapp sits inside a single Availability Zone inside a public subnet and runs on PHP. It connects in to an RDS instance which is in a private subnet running MySQL. You load the webapp’s DNS address in your browser and you get the following error “Database Connection Error” what might be the cause of the problem?","explanation":"You need to configure the Security Group to allow ingress access to allow inbound connections to the RDS instance","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html","title":"Security Groups"}],"answers":[{"id":"c9f3da4211e1e93ebb9d49ee8c2a0bd6","text":"The security group for the Webserver does not allow egress SSH access to the instance","correct":false},{"id":"30e5bf736c605d975c83d2bc39ba2e63","text":" The security group for the RDS instance does not allow ingress access to the RDS instance","correct":true},{"id":"c343ffd845f05787efb4ada77e096f99","text":"The security group for the RDS instance does not allow egress access to the RDS instance","correct":false},{"id":"4cbe0aae95d4b87bb274b250394fcd6c","text":"The security group for the Webserver does not allow ingress SSH access to the instance","correct":false}]},{"id":"ff6e1b5e-e383-4b55-bf94-6721270ee9b5","domain":"security-comp","question":"The engineering team of a FinTech company has migrated their on-premise application to AWS and has decided to use AWS DynamoDB to store the records and a combination of EC2 instances and Lambda functions for the data processing requirements. The Chief Security Officer of the company has mandated that the DynamoDB table is accessed without the use of access keys and secrets. How can the engineering team accomplish this?","explanation":"IAM roles allow EC2 instances and similar resources such as Lambda functions to perform operations on other resources without the need for access keys and secrets. There is no such thing as Cognito roles.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg//access-control-identity-based.html","title":"Access Control Identity Based"}],"answers":[{"id":"c026f0685e3e2bdc391b7f1d5d2036c2","text":"Create and associate IAM roles to the EC2 Instances and Lambda functions.","correct":true},{"id":"2d476ce7d034e0bb6e4bbf75b24aec29","text":"Create and associate IAM resource policies to the EC2 Instances. Create and associate IAM roles to the Lambda functions.","correct":false},{"id":"cc59ffeba9257938c9ae467e4e7ea6ee","text":"Create and associate IAM roles to the EC2 Instances. Create and associate IAM resource policies to the Lambda functions.","correct":false},{"id":"e81471d72607716ab24cf65e93a13bc6","text":"Create and associate IAM roles to the EC2 Instances. Create and associate Cognito roles to the Lambda functions.","correct":false}]},{"id":"11a06ef4-bb74-456d-ba26-e7852eba3c7e","domain":"mon-rep","question":"A custom web application has been deployed to AWS.  The development team want to know when a specific .NET counter hits a threshold. Which service allows this?","explanation":"Custom operating system metrics can be pushed to AWS by using CloudWatch Custom Metrics. CloudWatch events refer to events around AWS resources but not within an EC2 instance.  X-Ray allows transaction tracing throughout applications but would not push .NET metrics to AWS.","links":[{"url":"https://aws.amazon.com/blogs/mt/query-for-the-latest-windows-ami-using-systems-manager-parameter-store/","title":"Query for the Latest Windows AMI using Systems Manager Parameter Store"}],"answers":[{"id":"0c810b936071c67e7bc9845d93416bc2","text":"AWS X-Ray and Alarms","correct":false},{"id":"957649c9aaeda66468ced97b30d4ce29","text":"CloudWatch Events and Alarms","correct":false},{"id":"ce8086df6e4acbf892ecfe8075412a33","text":"AWS CloudWatch Insights and Alarms","correct":false},{"id":"9d53eb372c233c0124733016585613fc","text":"CloudWatch custom metrics and Alarms","correct":true}]},{"id":"3f043cde-54b2-42c7-9a19-7153fe5b6e95","domain":"high-avail","question":"Your customer has asked about cost-savings opportunities with AWS. They've noted that their EC2 instances are on most, if not all, the time but metrics show that aggregate CPU utilization is low. Demand for their application is also unpredictable. They want to cut costs around their EC2 fleet. Which of the below suggestions would you recommend to your customer to maximize savings?","explanation":"AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. AWS Auto Scaling makes scaling simple with recommendations that allow you to optimize performance, costs, or balance between them. With auto scaling you can scale-out Amazon EC2 instances seamlessly and automatically when demand increases, shed unneeded Amazon EC2 instances automatically and save money when demand subsides, and scale dynamically based on your Amazon CloudWatch metrics, or predictably according to a schedule that you define. Purchasing reserved instances, although cheaper than on-demand, would not necessarily cut costs. Since demand is unpredictable you may be purchasing a commitment that you may not use. There is no indication that the application will be running for at least a year. This is true even after right-sizing. Storing snapshots of EBS in S3 is indeed cheaper than storing EBS volumes but that does not address the issue of the EC2 instances themselves.","links":[{"url":"https://aws.amazon.com/autoscaling/","title":"AWS Auto Scaling"}],"answers":[{"id":"99040729979583a2eb85e3502b484989","text":"Purchase convertible reserved instances for your EC2 fleet. They will experience up to 66% savings compared to on-demand costs and will have the option to change their instance types if the application needs change.","correct":false},{"id":"a8dd2caf5c16147c7b1b6bd321b6558d","text":"Take snapshots of the EBS volumes attached to the EC2 instances and store them in S3. Delete the EBS volumes as storing in S3 is a cheaper alternative than EBS storage costs.","correct":false},{"id":"89a84b7404990525469a3025ab3b5116","text":"Decrease the instance sizes for those instances with low CPU utilization. Purchase standard reserved instances after right-sizing the instances.","correct":false},{"id":"76a2f03f07c2ade7d896075a501c6bc5","text":"Utilize auto scaling groups for the EC2 fleet. Set up a scaling policy that will launch EC2 instances when CUP utilization is above a threshold, and release instances when CPU utilization is below a threshold.","correct":true}]},{"id":"1ac9a5ce-d1dd-4b67-963e-c329dab20cce","domain":"dep-prov","question":"You've been tasked with the design of a disaster recovery solution that will allow a backup application in eu-west-1 to assume the duties of your main application running in eu-west-2. Which of the following processes will you need to follow first to get the backup server up-and-running in the backup region?","explanation":"To use an AMI in a region other than the one in which it was created, the AMI must be *copied* to the new region. From there, you can build a new server based on that AMI.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html#copy-amis-across-regions","title":"Cross-Region AMI Copying"}],"answers":[{"id":"f5d75f5446b082c6621fe586b3b19e52","text":"Since AMIs are global, all you need to do is create a backup instance from the AMI of the original server.","correct":false},{"id":"33f9cce716c72b0ee6990957bf7a55a2","text":"Image the primary server and copy its AMI from eu-west-2 to eu-west-1; use that image as the template for your backup server.","correct":true},{"id":"b3b57590edfded4925d17c01250cd2a4","text":"Image the primary server, open its permissions to 'any region' and share the AMI with a new instance in eu-west-1; use that image as the template for your backup server.","correct":false},{"id":"52f25ffef53f0dae9b6c3ea47606a040","text":"Use cross-region replication to copy the server to the backup Region.","correct":false}]},{"id":"17b1cb9e-6c30-4cb5-8122-ac0799b51470","domain":"mon-rep","question":"Your team uses a CloudFormation template to configure a CloudWatch dashboard. The dashboard includes metrics of a classic load balancer such as HTTPCode_Backend_5XX and HTTPCode_ELB_5XX. The development team changes the type of the load balancer to network load balancer. However, after you recreate the CloudFormation stack, the metrics in the CloudWatch dashboard do not report any data. What is the cause of it?","explanation":"CloudFormation uses the resource type AWS::CloudWatch::Dashboard to create a CloudWatch dashboard. And the resource configures the metrics names in the DashboardBody. Metrics: HTTPCode_Backend_5XX and HTTPCode_ELB_5XX are valid only for the Classic Load Balancer, and are not supported in the Network Load Balancer (NLB), since NLBs operate in layer 4. The DashboardBody in the template needs to be modified to include the appropriate NLB metrics.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-cloudwatch-metrics.html","title":"CloudWatch Metrics for network load balancer"}],"answers":[{"id":"0c719bf2f29ec9b673c0092df8426d3c","text":"The target groups of the network load balancer are unhealthy so that they do not report any data.","correct":false},{"id":"fee717ce407145a9caf9135e8b046e55","text":"Network load balancers do not support reporting metrics to CloudWatch dashboard. Only Classic load balancers do.","correct":false},{"id":"959ff1c7a797aea82688e7a5b2468b64","text":"The selected metrics: HTTPCode_Backend_5XX and HTTPCode_ELB_5XX are not supported in a network load balancer.","correct":true},{"id":"fb1a3d94cf187bb86bd85a8a4d584c4d","text":"It takes longer time for the network load balancer to send metrics data to the CloudWatch dashboard. Wait at least half an hour.","correct":false}]},{"id":"e9a2a106-8596-4e7c-bd0b-7a274c9b65f1","domain":"mon-rep","question":"Your organization is growing and your CISO is concerned with the increasing risk of users accessing resources they shouldn't have permissions for. What is the most effective solution to track requests for access to S3 buckets?","explanation":"To track requests for access to your bucket, you can enable access logging. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and error code, if any. Access log information can be useful in security and access audits. AWS Config does not collect logs. AWS CloudWatch aggregates logs into one place. You could collect AWS CloudTrail logs for API calls to an S3 bucket but the most effective method is to turn on access logging for the bucket.","links":[{"url":"https://d0.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf","title":"Amazon Storage Services Overview"}],"answers":[{"id":"dcd1426526e6edaa4108cf53b54f5886","text":"Turn on access logging for the bucket","correct":true},{"id":"aa39a2d244d1792f0833837683ceaba8","text":"Turn on AWS CloudWatch logs on the bucket","correct":false},{"id":"ff1f340f3014857175b4708e7eefa686","text":"Turn on AWS Config access for the bucket","correct":false},{"id":"5e65dc82f972821c704a45d2cb78368e","text":"Turn on AWS CloudTrail for the bucket","correct":false}]},{"id":"571b9603-45b6-45b1-aa47-68849ac814bb","domain":"automation","question":"A big-box retailer runs their in-store point-of-sale system on EC2 linux instances. All of the infrastructure is managed as part of a CloudFormation stack. The web servers are part of an Auto Scaling Group. The application only needs to be available during business hours from 9:00am until 6:00pm. What would be the best way to scale the web servers cost efficiently based on demand?","explanation":"Authoring the CloudFormation template to include an AutoScaling:ScheduledAction resource to increase the Auto Scaling Group's MinSize and MaxSize values at 9:00am, and another AutoScaling:ScheduledAction resource to decrease the Auto Scaling Group's MinSize and MaxSize values at 6:00pm will save costs for the retailer during non-business hours. CloudFormation conditions control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update, but don't control the actions of an Auto Scaling Group. Using an Auto Scaling Group scheduled action provides more streamlined automation than using a Lambda function. CloudFormation mappings are key/value pairs that can be used to specify conditional parameter values, but they have no impact on the Auto Scaling Group unless they are used to create a resource.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html","title":"What is AWS CloudFormation?"},{"url":"https://s3-us-west-2.amazonaws.com/cloudformation-templates-us-west-2/AutoScalingScheduledAction.template","title":"Cloud Formation Sample Template for Time-based Auto Scaling"}],"answers":[{"id":"725933a9f7ea7ef281e2decf6cefadae","text":"Configure AutoScaling:ScheduledAction mappings in the CloudFormation template with Maxsize, MinSize, and Recurrence values based on business hours","correct":false},{"id":"0db6203397832efddcbca893f96ba1b6","text":"Include AutoScaling:ScheduledAction resources in the CloudFormation template that change Maxsize, MinSize, and Recurrence values based on business hours","correct":true},{"id":"765dccb8332e2036eb70a0b6276e7285","text":"Use CloudWatch Events to trigger a Lambda function at business opening and closing that adjusts the Auto Scaling Group's MinSize and MaxSize accordingly","correct":false},{"id":"a0f99978bb65910b10b75a32e1b92a2a","text":"Create AutoScaling:ScheduledAction conditions in the CloudFormation template that change Maxsize and MinSize values based on business hours","correct":false}]},{"id":"f3ec51e7-232c-4d1d-9d28-d0dcc089732","domain":"networking","question":"A prescription drug company runs its in-house developed fulfillment application on EC2 with multiple instances behind an ELB Network Load Balancer in a single AWS region. They'd like to be able to failover to a different AWS region if issues arise with the application in the primary region. Security policy requires that both the primary and failover EC2 instances run in private subnets. Which architecture will provide the most reliable solution for failover between regions?","explanation":"The monitored resources of Route 53 health checks must have public IP addresses, so standard Route 53 health checks won't work for the private subnet instances in this use case. You can deploy a Lambda function in the private subnet where the instances reside, and use it to check an application's health. You can then use CloudWatch Events and alarms to inform Route 53 of the need for a failover to another region using a failover routing policy. The Route 53 API doesn't support invoking a routing policy. Route 53 Resolver supports hybrid clouds, not cross-region health check resolution.","links":[{"url":"https://aws.amazon.com/route53/","title":"Amazon Route 53"},{"url":"https://aws.amazon.com/blogs/networking-and-content-delivery/performing-route-53-health-checks-on-private-resources-in-a-vpc-with-aws-lambda-and-amazon-cloudwatch/","title":"Performing Route 53 health checks on private resources in a VPC with AWS Lambda and Amazon CloudWatch"}],"answers":[{"id":"d9bc7673b4bf5a5ed631d877bb205d21","text":"Schedule an AWS Lambda function to check the health of the primary application. Call the Amazon Route 53 API from the Lambda function to invoke a failover routing policy that re-routes the traffic to the load balancer of the failover instances.","correct":false},{"id":"6a65fb4fb046619eeae442309bbf8d7e","text":"Implement an Amazon Route 53 health check to monitor the primary application. Deploy a failover routing policy in both regions and use Route 53 Resolver to re-route traffic to the load balancer in the failover region when an issue is discovered.","correct":false},{"id":"9436bd62455fac582d39528d0fbfaa03","text":"Deploy Amazon CloudWatch Events to invoke an AWS Lambda function at regular intervals to check the health of the primary application. Trigger a CloudWatch alarm when the Lambda function returns an unhealthy status. Attach the alarm to an Amazon Route 53 failover routing policy that re-routes the traffic to the load balancer of the failover instances.","correct":true},{"id":"beb25447af410abf3aaa61057b471b4c","text":"Implement an Amazon Route 53 health check to monitor the primary application. Use a failover routing policy to re-route the traffic to the load balancer of the failover instances when an issue is discovered.","correct":false}]},{"id":"b84511ce-eca6-4a78-8b45-e76b4a0f37af","domain":"data-man","question":"An organization is moving its existing data lake from on-premises SAN storage to AWS.  You have 30TB of data to move. Your new cloud footprint includes an AWS Direct Connect link of 200Mbps, which is used for the mission critical link between your offices and the new systems in Production in AWS. Which of the below options would you recommend for the fastest transfer to AWS S3?","explanation":"With such a large amount of data to transfer it makes sense to use the AWS Snowball service.  Additionally flooding the Direct Connect connection with such a lot of traffic for several weeks would be a high risk to Production.  Snowball will provide a reliable and quick way to move the data. AWS DataSync is another possible candidate but it will place a heavy load on the Direct Connect link.","links":[{"url":"https://aws.amazon.com/snowball/faqs/#when-to-use","title":"AWS Snowball FAQs"}],"answers":[{"id":"c2d838ea2b9043be2161255da328dc60","text":"AWS Snowball","correct":true},{"id":"0d9da8d932c0312c10d819af40de0ba8","text":"AWS Snowball Edge","correct":false},{"id":"56169776bdeba6f7384a8ad473b0fc72","text":"AWS Data Sync","correct":false},{"id":"64bf8ac94115dbfb0847b75ceda67d5a","text":"AWS S3 Accelerated Transfer","correct":false}]},{"id":"f1f36583-612f-4946-ac77-a6e78aede912","domain":"networking","question":"You have an Amazon VPC with one private subnet, one public subnet and one network address translation (NAT) server. You are creating a group of EC2 instances that configure themselves to deploy an application via GIT. Which of the following setups provides the highest level of security?","explanation":"You should use EC2 instances in private subnet; no EIPs; and route outgoing traffic via the NAT.","links":[{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_NAT_Instance.html#basics","title":"NAT Instance Basics"}],"answers":[{"id":"1f9416e0df4342a7c5298d9f43d4d392","text":"Amazon EC2 instances in a private subnet; assign EIPs; route outgoing traffic via the internet gateway (IGW).","correct":false},{"id":"6a084c8ddbef4573cc627373b1a04456","text":"Amazon EC2 instances in public subnet; assign EIPs; route outgoing traffic via the NAT.","correct":false},{"id":"03808744fcc2ec07927563e827ba457a","text":"Amazon EC2 instances in private subnet; no EIPs; route outgoing traffic via the NAT.","correct":true},{"id":"9767e7ce6f652652e1ec42f2d2a48af1","text":"Amazon EC2 instances in public subnet; no EIPs; route outgoing traffic via the internet gateway (IGW).","correct":false}]},{"id":"b4e4d4f8-b9af-47da-9f90-2b63cab25ddf","domain":"automation","question":"As a SysOps Administrator you are managing your company's infrastructure as code. You have a number of CloudFormation templates that automate the provisioning of AWS resources for disaster recovery purposes. Your CISO have asked you for additional insights into the changes that teams are making to the CloudFormation templates in order to see when templates are updated with what changes. How would you build a solution that fulfills the CISO's ask?","explanation":"Change sets allow you to preview how proposed changes to a stack might impact your running resources. For example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. Config and Lambda would be complicated to configure and unnecessary as you would be able to directly do this using change sets. Amazon Inspector is used for EC2 instances.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"a374362c79930669cc8b737ca45f03cb","text":"Create a Lambda function that parses through CloudWatch logs for any changes made to a CloudFormation stack. Ensure CloudFormation has a role assigned that sends logs to CloudWatch.","correct":false},{"id":"44fab4e192c83f7e36b2143702b9958e","text":"Configure an AWS Config rule to detect changes to a CloudFormation stack. Send an SNS notification to the CISO for any changes.","correct":false},{"id":"00c32ce29ae8b3d7291d321ea5a8c6ba","text":"Run Amazon Inspector report periodically to identify changes made to a CloudFormation stack. Forward these reports to your CISO.","correct":false},{"id":"a4e164938d62b8c500a3c3bc4680f546","text":"Create a change set by submitting changes against the stack you want to update.","correct":true}]},{"id":"6fd27cf3-e84d-4a26-875c-7695a786a998","domain":"networking","question":"Two EC2 instances in two VPCs cannot ping each other. Both instances are located in private subnets. Which of the following would help you troubleshoot the problem? Select two.","explanation":"You can only peer VPCs, not individual subnets. There must also exist a route between the peered VPCs. Subnet security policy isn't a real thing although Network ACLs could be considered as such. VPC endpoint policy controls access to S3 or DynamoDB, not to a peered VPC.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html","title":"What is VPC Peering?"}],"answers":[{"id":"d6d0388ad6ed9fa8508ce01fce126328","text":"Check that there is a peering connection between the subnets","correct":false},{"id":"088c1791b9d25f53b479820769d14995","text":"Check that there is a peering connection between the VPCs","correct":true},{"id":"b60b61736af41d60dfcadf8fde5d83b6","text":"Check that a route exists between the VPCs","correct":true},{"id":"19b59f579598d8721b6507a457658f92","text":"Check that a subnet security policy allows ping","correct":false},{"id":"e88fa7d7390ad4d45912151c4e94b106","text":"Check that a VPC endpoint policy allows ping","correct":false}]},{"id":"1ccc6579-f892-435f-a85c-5b976ac4f5cc","domain":"networking","question":"You have a stateless web application running on a several EC2 instances behind a Classic Load Balancer. As a SysOps Administrator you check the CPU utilization in CloudWatch for the instances and see that only one of the instances is running at 90% CPU utilization. The other EC2 instances are running at 10% CPU utilization. How would you troubleshoot this issue?","explanation":"With sticky sessions, the load balancer binds a user's session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance. Disabling the sticky sessions would evenly distribute the load. Cross-zone load balancing and connection draining would not solve the issue - Cross-zone load balancing distributes requests evenly across instances in all Availability Zones while connection draining stops sending requests to instances that are de-registering or unhealthy. Changing the instance sizes would not solve the issue.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html","title":"Configure Sticky Sessions for Your Classic Load Balancer"}],"answers":[{"id":"b0e61186c84d6eb0c683ca1c20263258","text":"Decrease the instance size of the instances at 10% CPU utilization while increasing the instance size of the instance at 90% CPU utilization. Add additional instances if the load is still unbalanced.","correct":false},{"id":"b98d3b1c376f2c2a52d9e06913946d0e","text":"Disable sticky sessions on the Classic Load Balancer. Check CloudWatch to see if the issue is resolved.","correct":true},{"id":"3d06f0cd6554a14e6822464e6f4a1697","text":"Decrease the connection draining duration to allow the load to move from the instance with high CPU utilization sooner.","correct":false},{"id":"a7088c71d4d9d933ef7db7bc61e57957","text":"Enable cross-zone load balancing to evenly distribute the load across all EC2 instances.","correct":false}]},{"id":"374ba54c-c9fb-4438-92d0-b002e0cd8e58","domain":"data-man","question":"An application has an Aurora single-master cluster as its database. As users keep growing, there are more and more read requests to the database and throughput is nearing capacity. You need to increase the read capacity of the database to ensure the user experience is not impacted as user numbers continue to grow. Which of the following options is the most appropriate to address the read capacity issue?","explanation":"There are two ways to scale Aurora MySQL DB instances, instance scaling and read replica scaling. As the read capacities increase, Aurora Replicas should be added. A single-master Aurora DB cluster can have up to 15 Aurora Replicas and they can offload read workloads from the primary DB instance. Unlike DynamoDB, there is no Accelerator feature for Aurora. There is not option to modify a DB instance to be Read optimized, it is either a Primary DB instance or a Read Replica DB instance. CloudFront also does not help as CloudFront distribution cannot configure an Aurora cluster endpoint as its origin.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html","title":"Aurora read replica"}],"answers":[{"id":"1b8cb94dc108c58fe7bc5ee63f8ee45a","text":"Configure several Read Replicas in the Aurora cluster to share the read traffic.","correct":true},{"id":"53a2145209a8955575577b2e91e93aeb","text":"Modify the DB instance class for each DB instance in the DB cluster to be a read optimized one.","correct":false},{"id":"5b235156cd7d71262846409b34661073","text":"Enable Aurora Accelerator for the cluster.","correct":false},{"id":"b1780d2940a1e174629e1c00182059ad","text":"Configure a CloudFront distribution with the Aurora cluster endpoint as the origin.","correct":false}]},{"id":"9f2d0dd1-cac2-4cff-9166-005b4746f186","domain":"networking","question":"Your developers are coming to you for advice on how to make their website content better available to customers in Asia. There is a growing customer base in Asia Pacific (Singapore) but customers are complaining about poor latency. You discover that customers in Asia are being directed to an S3 endpoint in EU (Ireland). What would you do to ensure customers are directed to the appropriate region to improve performance?","explanation":"Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. This would be the best way to improve latency. Creating a customer domain name is not relevant to the question. Amazon WAF is a web application firewall that would serve no purpose toward performance improvement. An ELB cannot span regions.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy"}],"answers":[{"id":"3f2402f6feb904363b29958f853b0b75","text":"Configure a custom domain name by creating a new DHCP option set.","correct":false},{"id":"e691592cfcfc0a33ddeac2529c433659","text":"Configure Amazon WAF to only allow traffic from the Asia Pacific (Singapore) region to filter out unnecessary traffic to improve performance.","correct":false},{"id":"d2964a2b6eb3dadd409e8e4870f8e686","text":"Configure an Elastic Load Balancer to direct traffic to instances located in the Asia Pacific (Singapore) region to improve performance.","correct":false},{"id":"aea00e8d5238909a0f741554d4179219","text":"Configure a geolocation routing policy in Amazon Route 53 to route traffic based on the location of your users.","correct":true}]},{"id":"ed2ywbaa-95e6-2i0x-jzmq-kdvksdj16nxi","domain":"security-comp","question":"As an administrator, which of the following IAM tasks are *critical* to the security of your AWS environment?","explanation":"While all of these things are important, it's critical that you delete the root access keys, activate MFA on the root account and create an IAM password policy.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html","title":"IAM Best Practices"}],"answers":[{"id":"5911b1e6aadb27a95ade3c5e00f03ea8","text":"The application of an IAM password policy","correct":true},{"id":"18ca1c0dbfc5485da49fcec4853501ee","text":"The use of groups to assign permissions","correct":false},{"id":"c436360232888720d87f4d9d3b2a8147","text":"The activation of MFA on the root account","correct":true},{"id":"d4f7829d16d41ce16ff31c2d31aeba6a","text":"The creation of individual IAM users","correct":false},{"id":"878a3402611421642a5100d0177d21fb","text":"The deletion of root access keys","correct":true}]},{"id":"ab519717-12c2-4bb8-8c4d-b9a7c37fc4ea","domain":"dep-prov","question":"You work at an international organization which has its headquarters in New York and a Satellite Office in London. You primary production environment is based in New York and you have hardened your EC2 instances to meet the CIS (Center For internet Security) standards as well as your own strict corporate security policies. This has taken many hours but now meets all the standards required by both CIS and your organization. You wish to use the same EC2 instances for your office in London, however the London office has a separate AWS account to the New York Office. How can you share this EC2 instance to your London office only?","explanation":"Creating an AMI and sharing it with the other account is the only way to do this. Remember that AMIs are regional so you will also need to copy it to EU-WEST-2 so that it can be used in the London region","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html","title":"Sharing An AMI"}],"answers":[{"id":"a15e566637980374e13999125aed430c","text":"Create an AMI of the EC2 Instance","correct":true},{"id":"4cd72c1043a90ccf72eaa1673eb8afae","text":"Make the AMI Public","correct":false},{"id":"318b415e5bea55c97d24d02ffc7228ee","text":"Use Cross Region Replication to copy the AMI from S3 in US-EAST-1 to EU-WEST2","correct":false},{"id":"304cbf6ddbf9e8c0abaa16154558edd3","text":"Copy the AMI to the EU-West-2 region","correct":true},{"id":"b7491c4eeabd9a1471cb71631b412612","text":"Share the AMI with the London office using the AWS account number of the London Office","correct":true},{"id":"a26f1652c6e2c012d915a666416686d5","text":"Use the EC2 migrate API call to migrate the EC2 instance from US-EAST-1 to EU-WEST-2","correct":false}]},{"id":"e105d8a7-6333-48d8-9610-7c2f9ad73991","domain":"mon-rep","question":"There has been a steady rise in costs with your AWS bill, and the security team has noticed that there has been an increase in the number of requests, even though the number of IAM users has decreased due to employee turnover and down-sizing. The CISO has tasked you with identifying whether recent requests to the AWS account's environment were made with temporary security credentials for a role or federated user. How would you go about identifying this?","explanation":"A trail enables CloudTrail to deliver log files to an Amazon S3 bucket. By default, when you create a trail in the console, the trail applies to all AWS Regions. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs. The userIdentity element contains details about the type of IAM identity that made the request, and which credentials were used. If temporary credentials were used, the element shows how the credentials were obtained. Identify Federation is used for access to an account. Config will not log API activity. Lambda cannot scrape an entire account; it needs to access an S3 bucket of CloudTrail logs to take any action.","links":[{"url":"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-user-identity.html","title":"CloudTrail userIdentity Element"},{"url":"https://docs.aws.amazon.com/config/latest/developerguide/log-api-calls.html","title":"Logging AWS Config API Calls with AWS CloudTrail"}],"answers":[{"id":"41e73582d973a777c1838270973c1a5a","text":"Create a Lambda function that is triggered daily that scrapes the account for requests from any assumed roles. Have the Lambda function revoke access if any requests are non-compliant.","correct":false},{"id":"73e4bf9add9c239534c52f8a03fb95fb","text":"Create a CloudTrail log to deliver files to an Amazon S3 bucket. Use Amazon Athena to query the logs to search for the userIdentity element.","correct":true},{"id":"97ddb7778b0f9f5decefb8013df4d710","text":"Set up Identity Federation with SAML. Create IAM roles and create policies for employees to assume with the correct permissions. Log all requests using the Credential report.","correct":false},{"id":"4ffef50c6af5fe0e395fb58ca8319f05","text":"Record all ongoing events in the AWS account using AWS Config. Create a CloudWatch alarm to send an SNS topic when an identity under AssumeRole makes a request to the account.","correct":false}]},{"id":"b5cf6800-4ce3-4d24-8eaa-a1279c1c6409","domain":"mon-rep","question":"An insurance company has a monolithic application hosted in an EC2 instance and a serverless application hosted in AWS Lambda. After a few months of running the application, the customers have raised multiple delays and performance issues from the applications. The Operations Engineer responsible has mentioned that the latency issues might have been caused by code-level performance issues and the Head of Operations has instructed the team to add code-level monitoring support. How can the team accomplish this?","explanation":"X-Ray can be used for adding code tracing support for both monolithic application code (e.g. a large Django monolithic project) and serverless (Lambda function) code. CloudTrail is used for auditing API call logs. CloudWatch is used for monitoring resource usage and metrics. X-Ray is a distributed tracing system.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html","title":"AWS X-Ray"}],"answers":[{"id":"359f4dd3d7689aca37514e23a8781431","text":"Use AWS X-Ray for both the monolithic application code and the serverless application code.","correct":true},{"id":"2eb097422fa45d569c37d094428d9a9d","text":"Use AWS CloudWatch for the monolithic application code. Use AWS X-Ray for the serverless application code.","correct":false},{"id":"ab066c8d1096ed9b7b49b4637589b201","text":"Use AWS X-Ray for the monolithic application code. Use AWS CloudTrail for the serverless application code.","correct":false},{"id":"7d3a7bc0301958a4e3ad624f70b462d4","text":"Use AWS CloudTrail for the monolithic application code. Use AWS X-Ray for the serverless application code.","correct":false}]},{"id":"a8d6d4f3-6877-4e89-9e7c-c6f83b83e1de","domain":"automation","question":"A small business has a monolithic application utilizing several EC2 instances. The technology consultant has suggested to the engineering team of the business that the application is containerized to improve the deployment and scaling processes of the team. The engineering team processes both critical and non-critical workloads within the application. Given the size of the application and the number of transactions being processed by the system, the CFO has instructed the engineering team to ensure that the setup must be as cost-effective as possible without introducing the risk of data loss for critical workloads. How can the engineering team accomplish this?","explanation":"On-demand EC2 instances should be used for critical workloads and spot instances can be used for non-critical workloads. ECR (Elastic Container Registry) is not used to run container processes. Instead, it is used to store container images. For containerization requirements, ECS (Elastic Container Service) is used to manage container workloads. Step Functions is used for orchestrating different processes and can not be used by itself to manage the workloads.","links":[{"url":"https://aws.amazon.com/blogs/compute/running-high-scale-web-on-spot-instances/","title":"Running High-Scale Web Applications on Spot Instances"}],"answers":[{"id":"ddf2c2e29f2118061f6a160054593338","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use ECR and spot EC2 instances for non-critical workloads.","correct":false},{"id":"1d3682fdb0beec13b13a37502cfb3108","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use Step Functions and on-demand EC2 instances for non-critical workloads.","correct":false},{"id":"b8864524b96a467a8f1023b5a2c9f003","text":"Use Step Functions and on-demand EC2 instances for the critical workloads. Use Step Functions and spot EC2 instances for non-critical workloads.","correct":false},{"id":"47e42a6e665f0a913236e5580553e3ad","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use ECS and spot EC2 instances for non-critical workloads.","correct":true}]},{"id":"y895ku45-wsg2-9rye-087a-zdmu2wd7qtr8","domain":"mon-rep","question":"Which AWS service can be used to log API calls from the AWS console, the EC2 CLI, the AWS CLI, or the AWS SDKs.","explanation":"CloudTrail captures API calls and delivers the log files to an Amazon S3 bucket.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/APIReference/using-cloudtrail.html","title":"Logging API Calls Using AWS CloudTrail"}],"answers":[{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"a8c600be214ced26950e704d39c3ca21","text":"CloudWatch Logs","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":true}]},{"id":"463d990a-0af1-4e11-ad8a-c379444c02fd","domain":"automation","question":"A government agency currently runs all of their AWS workloads in a single region. Services utilized include S3, EC2, ECS, RDS, CloudFront, and Route 53 in multiple accounts. Their new business continuity plan calls for readiness in a separate AWS region in case disaster recovery is needed. Which approach will provide them with the most efficient way to manage their primary and business continuity environments?","explanation":"By default, CloudFormation templates are tied to a single account and a single region. However, CloudFormation allows for the creation of a custom resource that can launch a nested stack in another account or region with the appropriate IAM roles setup. The CloudFormation parameter and mappings sections don't provide cross-region functionality. Creating templates in each account would also work, but will require significant maintenance to keep resource definitions in-sync across all the templates.","links":[{"url":"https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/","title":"Multiple-account, multiple-Region AWS CloudFormation"}],"answers":[{"id":"b3128e59899200446191df0bbc118793","text":"Create AWS CloudFormation templates in each account that provision stacks with resources into the primary and business continuity regions using the CloudFormation 'Region' parameter","correct":false},{"id":"21d658a249d3a95d1de0860519bd585b","text":"Create an AWS CloudFormation template in a single account that defines custom resources to launch nested stacks into the other accounts and regions","correct":true},{"id":"12eb00153c980c0e9e1fec8d03035f78","text":"Create AWS CloudFormation templates in each account and each region that provision stacks with the resources for their specific accounts/regions","correct":false},{"id":"fabf8fa8f7205c00448ae603a1d1af90","text":"Create AWS CloudFormation templates in a single account and configure the CloudFormation mappings section for the appropriate primary and business continuity regions","correct":false}]},{"id":"44eb75dc-1cf8-47bc-a6c4-7eb3551d39ad","domain":"dep-prov","question":"Your team uses Docker to build and install a new application. You need to deploy it with Amazon Elastic Container Service (Amazon ECS). You create an Auto Scaling group using an ECS optimized AMI as the ECS cluster. You also define a task definition and specify the number of tasks that will run on the cluster. Which of the following is responsible for starting and stopping tasks when it receives a request from Amazon ECS?","explanation":"The ECS optimized AMI has the ECS container agent installed and it communicates with the ECS service. It sends information about the current running tasks to Amazon ECS, and schedules tasks whenever receiving requests from Amazon ECS. ECS task definition does not schedule tasks. Fargate agent is used for Fargate instead of ECS. Elastic Container Registry saves Docker images but does not start/stop ECS tasks.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html","title":"ECS agent"}],"answers":[{"id":"1f773c4abe01e9ac9ea69a8467588e92","text":"Amazon Elastic Container Registry.","correct":false},{"id":"69c4feff2c34858d768595502051dddb","text":"AWS ECS Fargate agent.","correct":false},{"id":"77d3861f206ed0b395e521eef335a70e","text":"The ECS container agent running on ECS container instances.","correct":true},{"id":"6da0c2a81f282023e97f7cb2e6d2afde","text":"The Amazon ECS task definition.","correct":false}]},{"id":"225c564a-15fc-45f8-b734-2bd99e1fbb77","domain":"dep-prov","question":"You are experiencing issues with HTTP traffic going through a Network Load Balancer you have recently set up. Access logging is enabled on the NLB and VPC Flow Logs have been configured. In this situation, which of the following monitoring tools is the most appropriate to help with troubleshooting HTTP client requests?","explanation":"You can use VPC Flow Logs to capture detailed information about the traffic going to and from your Network Load Balancer. You can use access logs to capture detailed information about TLS requests made to your load balancer, which you can use to analyze traffic patterns and to troubleshoot issues with your targets. On Network Load Balancers, access logs only capture TLS requests, not HTTP requests, so they are not appropriate in this case. Request tracing is not a valid monitoring tool for Network Load Balancers. You can use AWS CloudTrail to capture detailed information about the calls made to the Elastic Load Balancing API and store them as log files in Amazon S3. You can use these CloudTrail logs to determine which calls were made, the source IP address where the call came from, who made the call, when the call was made, and so on. CloudTrail logs are not appropriate for troubleshooting client requests.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-monitoring.html","title":"Monitor Your Network Load Balancers"}],"answers":[{"id":"6a292fdb897093b64ef80b39e7db0a4a","text":"Request tracing","correct":false},{"id":"6c5c81f47915de5f03d2577e8fae1c34","text":"Access logs","correct":false},{"id":"8db8c2b0bbd0ff71d1d15bb32f69e3b8","text":"VPC Flow Logs","correct":true},{"id":"8c6ded942a243b91e65d037ab4e21f7d","text":"CloudTrail logs","correct":false}]},{"id":"ab86f355-ca8c-4f03-bc21-bd2b6add379f","domain":"high-avail","question":"You need to deploy a DynamoDB table in the production environment for an application. The read and write traffic is low during weekdays. On weekends, the traffic becomes much higher due to the increasing number of users. The traffic pattern is very predictable and there is no unexpected spike. Which kind of capacity mode would you configure for the DynamoDB table?","explanation":"In this scenario, DynamoDB Auto Scaling should be used as the pattern is predictable. On-demand mode is more suitable for unknown workloads and development environments. Reserved capacity would cause a waste of resources. And users cannot configure a schedule to automatically adjust the capacity in DynamoDB.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html","title":"DynamoDB read/write capacity mode"}],"answers":[{"id":"b799c024f6af20489d00692976f1bfd1","text":"On demand mode without the need to configure the capacity.","correct":false},{"id":"fe576b38a59cfeaa466623938285521e","text":"Provisioned capacity on weekdays with a schedule to automatically increase the capacity on weekends.","correct":false},{"id":"35ba3d07134765e8d2b001befc79e653","text":"Reserved capacity that is able to cover the high traffic on weekends.","correct":false},{"id":"4fdafc1c54632989128146ba919a0e07","text":"Provisioned capacity with auto scaling.","correct":true}]},{"id":"17885db5-c61d-4edf-b0e3-e9d449d8e618","domain":"mon-rep","question":"Which of the following EC2 instance metrics are sent to Amazon CloudWatch by default? Select three.","explanation":"CPU utilization, disk I/O and network traffic are visible to the hypervisor running the instance and are sent to CloudWatch by default. For the others, you would need to install CloudWatch Agent on the instance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html","title":"Available CloudWatch Metrics for Your Instances"}],"answers":[{"id":"613b1188dd73dfdb768f39cfad3cc9a3","text":"Memory utilization","correct":false},{"id":"b4e5bb2b6842990e919682b3d6d5726c","text":"Volume of incoming and outgoing network traffic","correct":true},{"id":"2105454033539f83d3b07265aac88d7a","text":"The amount of swap space currently in use","correct":false},{"id":"ec1e54ae04652319df5c011f228c07ac","text":"Free disk space","correct":false},{"id":"fb8326e1edbd06b1bf6ea0332e089055","text":"CPU utilization","correct":true},{"id":"c4903df1e41e0ba0b4636e753d8c7661","text":"Disk read and write operations","correct":true}]},{"id":"f8a73a53-7c3b-4203-abda-2a0df772b8c0","domain":"data-man","question":"Your company's on-premises CRM application captures all lead tracking activities performed by the account team. Leadership would like to leverage machine learning to increase the lead-to-customer conversion ratio. They direct you to create ML models on Amazon SageMaker to predict which lead activities should be applied to specific customers based on previous successes. How will you architect the solution to get the CRM data to SageMaker in the most operationally efficient and cost effective way?","explanation":"AWS Storage Gateway File Gateway provides the capability to process hybrid cloud workflows through a file interface that seamlessly stores files as objects on S3 for AWS services such as machine learning, big data analytics, and serverless functions. Storage Gateway Volume Gateway stores data on premises and writes EBS Snapshots to S3 for backup. The EBS Snapshots would not be readable by SageMaker. Maintaining a script to write full files will require less effort than maintaining a program that parses CRM files and writes individual records to a Kinesis stream. Using a Snowball device will add time to the process due to shipping.","links":[{"url":"https://aws.amazon.com/storagegateway/?nc=sn&loc=0&whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc","title":"AWS Storage Gateway"},{"url":"https://aws.amazon.com/storagegateway/faqs/?nc=sn&loc=6","title":"What Can I Do With File Gateway?"}],"answers":[{"id":"d5351d8c29a3e0729ea2f1502a512b6f","text":"Implement an AWS Storage Gateway Volume Gateway in stored mode. Use a script to periodically write the CRM files to the Volume Gateway volume. Ingest the data into SageMaker from S3.","correct":false},{"id":"2d79bea0e750fdc444dd765b28cd49a4","text":"Install an AWS Storage Gateway File Gateway. Use a script to periodically write the CRM files to the File Gateway file system. Ingest the data into SageMaker from S3.","correct":true},{"id":"e5d1933264a00d252115c375f7202d99","text":"Write the files to an AWS Snowball device. Ship the Snowball device to AWS each week. Ingest the data into SageMaker from S3.","correct":false},{"id":"038b6b51cd73fe4e7ebdf85ad0cf26d9","text":"Use an AWS SDK to programmatically write CRM activity records to an Amazon Kinesis Data Firehose stream. Ingest the data into SageMaker from S3.","correct":false}]},{"id":"524af9a6-94c5-4c0b-a6d1-8c29011045df","domain":"automation","question":"Which AWS service allows you to consolidate billing across multiple AWS accounts, automate account creation and control access to AWS services?","explanation":"AWS Organizations offers policy-based management for multiple AWS accounts as well as consolidated billing. Personal Health Dashboard provides alerts when AWS is experiencing outages and other events that may impact you. Inspector is used for vulnerability scanning of applications running on EC2. IAM is used for policy based access control for users under a single AWS account","links":[{"url":"https://aws.amazon.com/organizations/","title":"AWS Organizations"}],"answers":[{"id":"41dff7155cc7aeb11c06434f6a450bb3","text":"IAM","correct":false},{"id":"9deb03cd21d41a691cdc24bfaab2820c","text":"Inspector","correct":false},{"id":"b1820ee1cf68e2e65f263ff7bb207626","text":"AWS Organizations","correct":true},{"id":"cf4db9f312542c8284a6ccdefcd98544","text":"Personal Health Dashboard","correct":false}]},{"id":"fa546ba1-3d10-4f56-8cca-03f69c7bb141","domain":"mon-rep","question":"You have a task to maintain a CloudFormation template for a new application. The template contains resources for an AutoScaling Group, a Launch Configuration and a Classic Load Balancer. To monitor the application running status, you need to create a new CloudWatch Log Group. The Launch Configuration user data in the template is modified to install the CloudWatch Agent and configuration files. Which resource should you also create in the CloudFormation template for the CloudWatch Log Group to work?","explanation":"The CloudFormation stack should create a CloudWatch Log Group resource. A Log Group is a group of log streams. The correct resource type is AWS::Logs::LogGroup. Other resources types such as AWS::CloudWatch::Dashboard, AWS::Logs::Destination or AWS::Logs::LogStream are unnecessary or invalid.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-loggroup.html","title":"Create CloudWatch log groups in CloudFormation"}],"answers":[{"id":"753205d9356936ff0505312dd82f15ed","text":"A resource with the resource type as AWS::Logs::LogStream.","correct":false},{"id":"b763f5ea4415de324920f7c7c639e4ac","text":"An AWS::Logs::Destination resource where the logs are sent to.","correct":false},{"id":"546e068e392aafa083829c464e843ed2","text":"A CloudWatch dashboard with the resource type AWS::CloudWatch::Dashboard.","correct":false},{"id":"5d8b7a94aecd66edf0bbbac7a39b9010","text":"A resource with the resource type of AWS::Logs::LogGroup.","correct":true}]},{"id":"eee921f8-2761-41ca-9e3a-d92097e53bf9","domain":"automation","question":"You would like to run a Lambda function at the same time every night. Which of the following tools could you use to configure this?","explanation":"You can create rules that self-trigger on an automated schedule in CloudWatch Events using cron or rate expressions.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html","title":"CloudWatch Scheduled Events"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html","title":"Using Lambda with CloudWatch Events"}],"answers":[{"id":"6cee3cba6e3661d0f97640c21953e6e2","text":"Use SQS to send a notification every night to trigger the function","correct":false},{"id":"9535623ef961c06aa4c1567e4caf2fdb","text":"Use SES to send a notification every night to trigger the function","correct":false},{"id":"0d22c1f845fb2bb5740847afadb3d4f6","text":"Use SNS to send a notification every night to trigger the function","correct":false},{"id":"afdfd739747e5925ab6d039107cf70ba","text":"Schedule an event in CloudWatch to trigger the function","correct":true}]},{"id":"1e148869-35dd-465b-962c-780c6d32b8b8","domain":"security-comp","question":"Your main application currently stores its credentials as a text file on an EC2 server.  Your Manager has informed you that this is an insecure practice and has told you to store these credentials in an AWS managed service instead.  AWS Systems Manager Parameter Store and AWS Secrets Manager can be used for the secure storage of credentials.  Of the below features, which apply to both Secrets Manager and Parameter Store?","explanation":"Many aspects of Parameter Store and Secrets Manager appear very similar, but Secrets Store charges you for storing each secret and also provides a secret rotation service whereas Parameter Store does not.  Therefore these are the only two answers related to both services.","links":[{"url":"https://aws.amazon.com/systems-manager/faq/#Parameter_Store","title":"AWS Systems Manager FAQs"},{"url":"https://aws.amazon.com/secrets-manager/faqs/","title":"AWS Secrets Manager FAQs"}],"answers":[{"id":"c954aba0b83727b5f6892234e837686d","text":"Integrated with Identity and Access Management","correct":true},{"id":"5a6c9e66d676a1b933ff0f18e23874d1","text":"Can store credentials in hierarchical form","correct":true},{"id":"da5f35495ba99b6d624c0dadf1bd7626","text":"Available at no additional charge providing you store less than 10,000 credentials","correct":false},{"id":"a6c8669080382c473c6c3331a4d3e832","text":"Supports encryption at rest using customer-owned KMS keys","correct":true},{"id":"12697ea16ebbdb808c191988acdbecba","text":"Manages rotation and lifecycle of credentials","correct":false}]},{"id":"723a6cb2-65a0-454f-b391-8647401fa54d","domain":"networking","question":"A photo sharing application is growing in popularity.  The application uses S3 to store photographs. Your boss has asked you how you can improve the upload and download times for your end users?","explanation":"S3 upload and download times can be improved by enabling Transfer Acceleration. This leverages Points of Presents (PoPs) in the CloudFront network to provide connection points closer to users, thereby improving transfer speeds.  Disabling Object Versioning or enabling Intelligent Tiering would not affect upload or transfer speeds.  Lastly, BitTorrent support is for publicly-available files and is designed to increase availability of files and reduce S3 costs but does not improve upload and download times from the S3 service itself.","links":[{"url":"https://docs.aws.amazon.com/en_pv/AmazonS3/latest/dev/transfer-acceleration.html","title":"Amazon S3 Transfer Acceleration"}],"answers":[{"id":"792c81205244e2382feec0be4a8a8716","text":"Enable S3 BitTorrent support","correct":false},{"id":"3992a5af710f81ade0226fff00fedf2d","text":"Enable S3 Intelligent Tiering","correct":false},{"id":"26b2f3dfb7078bf49a2aafd952d45a13","text":"Disable S3 Object Versioning","correct":false},{"id":"5c6679012efaeb73c2123036ba676303","text":"Enable S3 Transfer Acceleration on your buckets","correct":true}]},{"id":"f253314b-765f-471a-a5de-bc5f6095164a","domain":"dep-prov","question":"An application server running in an autoscaling group is terminating and relaunching every few minutes. What is the most likely cause?","explanation":"When an instance is seen to be terminating and relaunching regularly (commonly known as 'flapping' or 'thrashing'), the most likely cause is that the Autoscaling group is marking the instance as unhealthy to trigger a replacement.  This can be caused if the Load Balancer health check has been improperly configured- for instance if a missing security group rules means the ALB cannot perform health checks. or the health check too aggressively marks instances as unhealthy before launch has completed (i.e. before userdata has finished.  Termination of an autoscaling instance when using Spot fleet is common, but to see the regular launch and termination would suggest this is health-check related rather than due to spot price changes.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html#health-check-grace-period","title":"EC2 AutoScaling - Replacing Unhealthy Instances"}],"answers":[{"id":"22b774b395936efd71fe7ddaf41a3f8a","text":"The autoscaling health check is marking the instance as unhealthy before it has time to initalise fully.","correct":false},{"id":"ceb2643dc2123b2807f446e103bad7ff","text":"The price for the EC2 spot instance has increased to above the maximum price for your autoscaling group Launch Configuration.","correct":false},{"id":"ef10fce024d97295098b2e7c8b9de34e","text":"The Launch Configuration is using an unsupported AMI for your Availability Zone.","correct":true},{"id":"18d69cdccb048f832573792064f79345","text":"There is a temporary outage in the AWS Autoscaling service in that region.","correct":false}]},{"id":"7d7ac6a5-ba31-4418-9d4c-6da76657dbf2","domain":"networking","question":"You are designing a network with a bastion host (jump box) for security. Your network admins will SSH in to the bastion host and then on to other EC2 instances in a private subnet. You need your bastion host to be highly available. How should you build this environment?","explanation":"There has been is a much discussion about resilient Bastion design. The ELB does not add much value in this situation. Although you can get around it the ELB session timeouts will cause an SSH session to become disconnected if idle.  The answer with two AZs is a trap of the type you will see on the exam.  While 2x AZs would be ideal, the WHOLE answer must be correct, not just part of it. You should never use an ELB IP address for business as it is ephemeral and may change at any time.  DNS convectional round-robin will achieve the resiliency needed, as would an R53 Failover policy. The answer with two subnets does not exclude 2x AZs even though it does not stipulate it. Another design option you might see is EC2 Auto-Recovery or an Autoscaling group of Max=1 & Min=1 so that if the Bastion Host fails it is recreated automatically. ","links":[{"url":"https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html","title":"Bastion Hosts on AWS - Architecture"},{"url":"https://en.wikipedia.org/wiki/Round-robin_DNS","title":"DNS convectional round-robin"},{"url":"hhttps://aws.amazon.com/blogs/aws/new-auto-recovery-for-amazon-ec2/","title":"Auto Recovery for Amazon EC2"}],"answers":[{"id":"d5cfd03aad43f4cb7329ceb07c3b288d","text":"Create 2 Bastion EC2 instances in the same subnet. Create a DNS entry in Route53 which uses Round Robin DNS and points to each instance. Tell your SysAdmins to connect using the new DNS entry.","correct":false},{"id":"cb402e295d67552ab9dfcffabcfb0dcb","text":"Create 1 Bastion EC2 instance in a private subnet. Connect to this EC2 instance using a site to site VPN. Configure your router to automatically reconnect if the VPN is dropped.","correct":false},{"id":"01a3b5d266f7455e0d0cfc25070de365","text":"Create 2 Bastion EC2 instances in separate availability zones. Place these instances behind an elastic load balancer, and ask your SysAdmins to connect to the ELB's public IP Address.","correct":false},{"id":"c87a02a4d83b0d5310563e4b700e303a","text":"Create 2 Bastion EC2 instances in different subnets. Create a DNS entry in Route53 which uses Round Robin DNS and points to each instance. Tell your SysAdmins to connect using the new DNS entry.","correct":true}]},{"id":"4871ce12-9da0-4e28-b1e4-373560dbdffa","domain":"automation","question":"A telecommunications company sends out monthly bills to their customers. Usage is accumulated during the month by nightly batch jobs that process call details. The company is in the process of migrating the billing system to AWS to reduce costs. What approach will provide them with the most cost effective solution for the compute portion of their nightly batch runs?","explanation":"AWS Batch provides allocation strategies to consider capacity and throughput in addition to cost when provisioning instances for jobs. This is a newer feature that provides more flexibility than the previous scheme that chose an instance that was the best fit based on vCPU, memory, and GPU requirements. Creating a pool of EC2 Reserved Instances might result in unused capacity if workload requirements change. Lambda is not currently available as a compute resource for AWS Batch.","links":[{"url":"https://aws.amazon.com/batch/","title":"AWS Batch"},{"url":"https://aws.amazon.com/blogs/compute/optimizing-for-cost-availability-and-throughput-by-selecting-your-aws-batch-allocation-strategy/","title":"Optimizing for cost, availability and throughput by selecting your AWS Batch allocation strategy"}],"answers":[{"id":"3220afaa7c6fd31e7a8d35ce1e2df1fa","text":"Configure AWS Batch to choose an instance type for each job based on vCPU, memory, and GPU requirements at the lowest cost.","correct":false},{"id":"8633cd86b6633324660fa073362c2f98","text":"Schedule jobs with AWS Batch into a pool of EC2 Reserved Instances that contains enough servers for the minimum number of jobs that will be run on any one night. Use an Auto Scaling Group to provision Spot Instances to handle any additional demand.","correct":false},{"id":"96ef18a4d93470acb7dbd558eb666ca3","text":"Specify AWS Lambda as the compute resource for AWS Batch. Invoke the appropriate Lambda functions for each job.","correct":false},{"id":"0bb4d60773589240e55a5a506ee84275","text":"Use AWS Batch allocation strategies to define capacity, throughput, and cost priorities for instance type provisioning.","correct":true}]},{"id":"5dddbe8f-a62f-4692-b8a5-61e1ff70f722","domain":"high-avail","question":"You run a popular desktop application which is hosted on a fleet of EC2 instances behind an autoscaling group. After three years you are about to release version 9, which millions of people have been waiting for with excitement. When you released version 8, 8 years ago your website crashed from the demand. You need to prevent this from happening on this release. What AWS service could you use to assist with this?","explanation":"CloudFront works together with your website and speeds up delivery of your content by caching it at Edge Locations local to your users","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html","title":"CloudFront"}],"answers":[{"id":"3ea178bab502b44ed89d0a22b2fcfa78","text":" Host the update on a single T2 nano instance on EC2 and publish the public IP address to your customers to download","correct":false},{"id":"e994c20e96f7f92075129815a13774c3","text":"Host the update in AWS Aurora and turn on Aurora Accelerator to keep pace with the demand","correct":false},{"id":"e5e2e10e37ff47a42272a2c0335fca65","text":"Use Cloudfront to cache the software update at Edge Locations so as to keep up with the demand","correct":true},{"id":"52c99d94be86aed6ff572f5dd85fc5c9","text":"Use AWS Shield to protect the software update from too many users attempting to download it at once","correct":false}]},{"id":"fe87b3bd-e952-4f75-9b0e-3583fe879ad6","domain":"data-man","question":"Your manager has informed you that due to compliance issues, all data stored in company S3 buckets must be encrypted as soon as possible.  What is the quickest way to ensure all of this data is encrypted to meet the requirements?","explanation":"The easiest and quickest way to encrypt data in a bucket is to use Server Side Encryption, because Client Side Encryption will encrypt the files before sending to S3 and therefore will only work on newly uploaded files, we can discount any Client Side Encryption options first.  Of the remaining Server Side Encryption options, we can remove any method of managing keys ourselves, as this creates an overhead, so using S3 Managed Keys (SSE-S3) will be the quickest way to encrypt objects in a bucket.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html","title":"Protecting Data Using Server-Side Encryption"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html","title":"Protecting Data Using Client-Side Encryption"}],"answers":[{"id":"873764e12ed3764f29ba07e9fcdb622a","text":"Enable Server-Side Encryption with S3-Managed Keys on each S3 bucket","correct":true},{"id":"56f875bcb7f1ca2bd789c15f1cdc5c37","text":"Enable Server-Side Encryption with AWS KMS-Managed Keys on each S3 bucket","correct":false},{"id":"1a4d7384bee05f19390352598fbc5fb6","text":"Enable Server-Side Encryption with Customer-Provided Keys on each S3 bucket","correct":false},{"id":"d16f34464ccb19afe4b04ca69703c59e","text":"Encrypt new data using AWS KMS–Managed Customer Master Key and add to S3 bucket","correct":false}]},{"id":"d585a94b-6bfb-47bf-83b2-d92525fa925b","domain":"security-comp","question":"Your CFO would like to hire a consulting company called CostControl Corp to monitor your AWS account and help optimize costs. In order to track your daily spending, CostControl Corp needs to access your AWS resources. CostControl Corp also monitors many other AWS accounts for other customers. How would you grant CostControl Corp access to your account in a secure and administratively efficient way?","explanation":"Do not give CostControl Corp access to an IAM user and its long-term credentials in your AWS account. Creating an IAM Group would still require long-term credentials. Instead, use an IAM role and its temporary security credentials. An IAM role provides a mechanism to allow a third party to access your AWS resources without needing to share long-term credentials. You can use an IAM role to establish a trusted relationship between your AWS account and a third party. After this relationship is established, a member of the exteranl account can call the AWS STS AssumeRole API to obtain temporary security credentials. Cognito is a web service that delivers scoped temporary credentials to mobile devices and other untrusted environments.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html","title":"How to Use an External ID When Granting Access to Your AWS Resources to a Third Party"}],"answers":[{"id":"511bbeb0af589b7478846f2bc4488bb7","text":"Identify a secure user within CostControl Corp. Create an IAM user in your AWS account with the necessary permissions and grant access to that securely identified user.","correct":false},{"id":"ef9e894add84e3a1607217b2cfa0f98d","text":"Use Amazon Cognito User Pools to enable authentication with the external party. Cognito will create a unique identifier for each user in CostControl Corp to access temporary, limited-privilege AWS credentials.","correct":false},{"id":"2df4700c329bc33dc8133ca8de8c37ad","text":"Create an IAM role. Allow CostControl Corp users to assume this role for access to AWS resources in your account.","correct":true},{"id":"0e0f124a17ea65e797f45765d7fac5a1","text":"Create an IAM Group for CostControl Corp users. Give this group limited AWS permissions. Create CloudTrail log for this group to ensure their API calls in your AWS are within the realm of their scope of work.","correct":false}]},{"id":"0a940a2a-74b1-40ec-ac16-7af1c431571e","domain":"data-man","question":"You work at a media company who distributes movie posters for upcoming block busters to a select group of users. Your company has won the rights to design the poster for a very popular sci-fi franchise which releases movies every year around Christmas. The nature of the poster contains some “spoilers” and therefore you must restrict who can view the posters. The image files are stored on S3. In what way can you restrict who can view this file?","explanation":"You can make the files available over the internet to only those who you want to see them using pre-signed URLs","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html","title":"Pre-Signed URLs"}],"answers":[{"id":"61715b5882933c94dff8bc456f9836ab","text":"Use S3 Server Side Encryption so that the file is encrypted at rest.","correct":false},{"id":"f7b3da6c3f41cb76aad42404df716798","text":" Use HTTPS so that the file is encrypted in transit to it’s location.","correct":false},{"id":"7cadcfc2524b9fca4365698cc447dcd7","text":"Make the files public in S3 and post the link to reddit.","correct":false},{"id":"fe248fbd23d34c81ec9244bb549f6467","text":"Use pre-signed URL’s so that only those who are authorized to view the files can view them. Share the URL with the authorized party","correct":true}]},{"id":"5dc94849-7cfe-45fd-ae52-8e634c5e7d04","domain":"mon-rep","question":"Which of the following services does CloudWatch use to send you an email following an alarm event?","explanation":"Amazon EC2, S3 and CloudWatch, can all publish messages to your SNS topics to trigger event-driven computing and workflows. Amazon CloudWatch uses SNS to send email.","links":[{"url":"https://docs.aws.amazon.com/sns/latest/dg/welcome.html","title":"About SNS"}],"answers":[{"id":"8513f757701b24dbadad3df74e817df5","text":"SES","correct":false},{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":true},{"id":"ef4ba2f338cdf4b615ed280ff0b2777d","text":"SQS","correct":false},{"id":"f62772d94b939126ee608465cf5e0881","text":"SWF","correct":false}]},{"id":"ba52dee8-0d6c-4faf-9121-0e64c18bbf1a","domain":"high-avail","question":"Your website is evenly distributed across 10 EC2 instances in 5 AWS regions. How could you configure your site to maintain high-availability with minimum downtime if one of the 5 regions was to lose network connectivity for an extended period of time?","explanation":"If you are designing to check for loss of contact with the instances you need to use \"Evaluate Target Health\" to confirm connectivity.  The Latency policy will eventually detect the unavailability; however it is not a real-time test.","links":[{"url":"http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html","title":"How Health Checks Work in Complex Amazon Route 53 Configurations"}],"answers":[{"id":"90875d19db265ba7b3931ecbd5a5d813","text":"Establish VPN Connections between the instances in each region. Rely on BGP to failover in the case of a region-wide connectivity outage.","correct":false},{"id":"0e2faa25f8eb6371b513d8d442513b10","text":"Create a Route 53 Latency-based Routing Record Set that resolves to Elastic Load Balancers in each region and has the Evaluate Target Health flag set to \"True\".","correct":true},{"id":"dc1f9d897e82bca789bb2983fa7ce22d","text":"Create a Route 53 Latency-based Routing Record Set that resolves to an Elastic Load Balancer in each region. Set an appropriate health check on each ELB.","correct":false},{"id":"b5494c49d052d62119e11eab7d8499c7","text":"Create an Elastic Load Balancer to place in front of each EC2 instance. Set an appropriate health check on each ELB.","correct":false}]},{"id":"95b5fb6b-4742-4c0a-8563-4dc9dc116700","domain":"mon-rep","question":"You have been asked to monitor custom metrics generated by your own applications and services. There is a need for these metrics to be collated and returned to the product owner each week so they can plan for the following sprint's requirements. What services would you use to be able to provide this automated report?","explanation":"You can set up monitoring of custom metrics in CloudWatch, from this you can use CloudWatch also to trigger a Lambda Function at a scheduled time, which will collect use AWS SDK to pull the metrics into a report that will be sent via email using the SNS service","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"Publishing Custom Metrics"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html","title":"Using AWS Lambda with Amazon CloudWatch Events"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-sns-example.html","title":"Tutorial: Using AWS Lambda with Amazon Simple Notification Service"}],"answers":[{"id":"f79368a88745f4dba5e0fc92aa545c61","text":"EKS","correct":false},{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":true},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":true},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":true}]},{"id":"739bcd50-ad59-4cbe-a911-17cff575b80d","domain":"security-comp","question":"You have been asked by your company's CISO to create and manage an S3 bucket with highly confidential company information. Only two business-critical individuals within the company should have read and write access to the bucket. All other personnel should not have access. How would you go about ensuring the security and confidentiality of the bucket in the most efficient manner?","explanation":"Bucket policies provide centralized access control to buckets and objects based on a variety of conditions, including Amazon S3 operations, requestors, resources, and aspects of the request (for example, IP address). The policies are expressed in the access policy language and enable centralized management of permissions. The permissions attached to a bucket apply to all of the objects in that bucket. AWS Config would only report on if buckets are in compliance to any rules set. An explicitly deny for all IAM users would work but it isn't the most efficient solution. Bucket access control lists apply to bucket objects, not the bucket itself.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html","title":"Bucket Policies"}],"answers":[{"id":"fdadf86a8030fc5cbfd2cdb0b16b95f6","text":"Create a bucket policy explicitly granting access to the two principals.","correct":true},{"id":"ebf041183040a34a7220734ce5497478","text":"Create an IAM policy for all non-business-critical individuals that explicitly denies access to the bucket.","correct":false},{"id":"48f34a3e8e8a95fcbe3abee586e0f775","text":"Create a bucket access control list to explicitly grant access to the two individuals.","correct":false},{"id":"06899090182266f944ef7ff8a34750ed","text":"Create an AWS Config rule that denies access to the S3 bucket except for the business-critical users. Set an alarm whenever someone attempts to access the bucket.","correct":false}]},{"id":"4680fe6f-7f9c-4af7-b8da-5cc0933ebe57","domain":"dep-prov","question":"A company is migrating several workloads using different languages from their on-premise setup to AWS. The migration team of the company has been given a short period of time to perform the migration for the first phase of the project. How can the team accomplish this?","explanation":"Given the time constraints and the need to support multiple languages (and potentially multiple frameworks and environments), the use of containers will solve the problem. Out of all the options, only Elastic Beanstalk will support the preparation of environments with Docker container processes running.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/single-container-docker.html","title":"Elastic Beanstalk - Single Container Docker"}],"answers":[{"id":"9144ac49f4f40c6c26bdebe22d465fed","text":"Wrap the application code and processes inside Docker containers. Deploy the containers behind an AppSync API.","correct":false},{"id":"2bbb835baf49b3cc461d4cde5adab575","text":"Wrap the application code and processes inside Docker containers. Deploy the containers to Elastic Beanstalk environments.","correct":true},{"id":"677dbd1b57d7ca7a67cb5212ddcf07f0","text":"Wrap the application code and processes inside Docker containers. Deploy the containers to ECR.","correct":false},{"id":"91a3239475f30c6a9593a60c67f617da","text":"Wrap the application code and processes inside Docker containers. Deploy the containers behind a Kinesis Stream.","correct":false}]},{"id":"31081113-ca90-46fd-b9de-b0d861af38ab","domain":"automation","question":"A sports company has migrated systems to AWS but has not implemented any patching policy. A SysOps administrator has been hired to understand the current state of patching and help plan remediation. Which service can they use to understand the patch level of the EC2 instances?","explanation":"AWS Systems Manager provides a centralised location to view patching status of all Managed EC2 and on-prem instances. AWS Inspector, AWS Config and Macie are not services that can provide patch status reports.","links":[{"url":"https://docs.aws.amazon.com/en_pv/systems-manager/latest/userguide/systems-manager-patch.html","title":"AWS Systems Manager Patch Manager"}],"answers":[{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":false},{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false},{"id":"113b1ad9ce6cdc3a37ad8475bc9bb2b2","text":"AWS Systems Manager","correct":true},{"id":"7c90c8f2a24f3a1a28525f19fb2c75ab","text":"AWS Inspector","correct":false}]},{"id":"56b58c2e-0c37-4b6e-a9e5-79d362200641","domain":"networking","question":"You are a network administrator for your organization that is working to move into the Cloud. You are tasked with connecting your on-premises corporate network with AWS network. You have created a VPC in AWS, with an IP CIDR range of 10.0.1.0/16. You have launched an EC2 instance within a subnet in your VPC. Your external corporate network has a CIDR range of 172.16.0.0/12. How would you set up the destination on the route table so that the subnet in your AWS VPC connects to your external corporate network?","explanation":"A route table contains a set of rules that you can direct where network traffic in your subnet is directed to. If you are setting up a route to your external corporate network from your VPC, the destination must be the CIDR range of your external corporate network. 0.0.0.0/16 is the destination for the open Internet. 10.0.1.0/16 is the CIDR range for your own subnet, which would not work. 192.168.192.168/32 is the instance metadata and would not connect to your corporate network.","links":[{"url":"https://docs.aws.amazon.com/en_pv/vpc/latest/userguide/VPC_Route_Tables.html","title":"Route Tables"}],"answers":[{"id":"7b02a9ed0632530a998a9a41da156695","text":"Set the destination CIDR as 172.16.0.0/12 for the traffic in your subnet to go.","correct":true},{"id":"0b4c0e7099a39c83801d4570c9dc6c27","text":"Set the destination CIDR as 0.0.0.0/16 for the traffic in your subnet to go.","correct":false},{"id":"14eecdfdcb09e543dbbaf8bfb2df4e0f","text":"Set the destination CIDR as 192.168.192.168/32 for the traffic in your subnet to go.","correct":false},{"id":"83f6f90763fb37e1a55fafd4ca6ad17c","text":"Set the destination CIDR as 10.0.1.0/16 for the traffic in your subnet to go.","correct":false}]},{"id":"95096463-bf44-4a55-a938-134e17096ace","domain":"security-comp","question":"Instance 'A' and instance 'B' are running in two different subnets 'A' and 'B' of a VPC. Instance 'A' is not able to ping instance 'B'. Which of the following is a possible reason for this failure?","explanation":"The NACL on subnet B does not allow outbound ICMP traffic; and the security group attached to instance B does not allow inbound ICMP traffic.","links":[{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html#ACLs","title":"Network ACL Basics"}],"answers":[{"id":"48173a79d09d626e62e08c080f3dafcb","text":"The route table of subnet 'A' has no target route to subnet 'B'; and the security group attached to instance B does not allow inbound ICMP traffic.","correct":false},{"id":"87877573324ed4201e723046eb1d8409","text":"The security group attached to instance 'B' does not allow inbound ICMP traffic; the policy linked to the IAM role on instance 'A' is not configured correctly.","correct":false},{"id":"b683f2644731441134b6c0db51d38f6e","text":"The policy linked to the IAM role on instance 'A' is not configured correctly; and the NACL on subnet 'B' does not allow outbound ICMP traffic.","correct":false},{"id":"788e0ab5d1b2c372b5d80c26c17c6a71","text":"The NACL on subnet B does not allow outbound ICMP traffic; and the security group attached to instance B does not allow inbound ICMP traffic.","correct":true}]},{"id":"86596bb1-b46e-449f-889d-b5d830ca4f2e","domain":"dep-prov","question":"The development team in your company has finished developing a proof-of-concept of a new Ruby web application. The management wants to show it to a customer quickly, so they ask you to deploy it as soon as possible. You choose to use Elastic Beanstalk. What are the steps required to deploy the application with it?","explanation":"The benefit of deploying with Elastic Beanstalk is that you don't have to worry about provisioning and managing the infrastructure and application stack. You don't have to create EC2 instances or load balancers. Elastic Beanstalk does it all for you. You simply upload your code.","links":[{"url":"https://aws.amazon.com/elasticbeanstalk/faqs/","title":"AWS Elastic Beanstalk FAQs"}],"answers":[{"id":"4cefa75ab7d2e02f84528f56fe7b9646","text":"Provision an EC2 instance and an Elastic Load Balancer. Upload your code.","correct":false},{"id":"845d1a709577cc630a50cd331d24103c","text":"Provision an Elastic Load Balancer. Upload your code.","correct":false},{"id":"4bc0f60972b5b7a69a1cca6e12facb39","text":"Provision an EC2 instance. Upload your code.","correct":false},{"id":"5057183b7131b6d33dab6404e3169e16","text":"Upload your code.","correct":true}]},{"id":"2ed81e01-78af-4ca1-8cc9-2ecfeeff8984","domain":"data-man","question":"Which of the following is not a use case for read replicas?","explanation":"Providing greater redundancy via automatic failovers is not a use case for read replicas. They're not useful in this case.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.Overview","title":"Overview: Read Replicas"}],"answers":[{"id":"865c6a9aca74af0e2c37ab4e50ca6013","text":"Providing greater redundancy via automatic failovers.","correct":true},{"id":"5224fa97bf0d7d77ee80e90a695f1e40","text":"Serving read traffic while the source DB instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your read replicas.","correct":false},{"id":"82d85c0b4a19e372bf779bb4908cd64a","text":"Business reporting or data warehousing scenarios; you may want business reporting queries to run against a read replica, rather than your primary DB Instance.","correct":false},{"id":"61cb6fc4fb9c00d23b126d6e5c0d8905","text":"Scaling beyond the compute or I/O capacity of a single DB Instance for read-heavy database workloads. This excess read traffic can be directed to one or more read replicas.","correct":false}]},{"id":"de2610da-0b20-4258-b215-146e96c134d3","domain":"automation","question":"Which section of a CloudFormation template allows you to set up differing instance types based on environment type (e.g. 'Production' or 'QA')?","explanation":"","links":[{"url":"http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"CloudFormation Template Anatomy"}],"answers":[{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":false},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false},{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false},{"id":"229eb04083e06f419f9ac494329f957d","text":"Conditions","correct":true}]},{"id":"0ac7251e-689e-4cd6-bfb9-992628fb3e3c","domain":"mon-rep","question":"There has been a major outage of S3 in US-East-1 where many of your company’s AWS assets are. Your boss wants to know what effect this will have on your organization. What dashboard can you use to help diagnose how this will affect your individual organisation?","explanation":"AWS Personal Health Dashboard provides alerts and remediation guidance when AWS is experiencing events that may impact you. Inspector is an automated security assessment service. AWS X-Ray helps developers analyze and debug production, distributed applications.","links":[{"url":"https://aws.amazon.com/premiumsupport/phd/","title":"AWS Personal Health Dashboard"}],"answers":[{"id":"3473fa31769f9b170662878d3f67fc8c","text":"AWS Inspector Dashboard","correct":false},{"id":"6bd8c280d2d212f5f6338714620001a4","text":"AWS Service Dashboard","correct":false},{"id":"0863526e514d88cd2d00b39f8dda0920","text":"X-Ray","correct":false},{"id":"cf4db9f312542c8284a6ccdefcd98544","text":"Personal Health Dashboard","correct":true}]},{"id":"f0bf5535-d6d8-4188-aa8c-9cefd40e2218","domain":"security-comp","question":"You have just created an IAM group for your development team. You want to give them access to be able to access an S3 bucket  - Which policy type is best used to achieve this?","explanation":"IAM Identity based policies can be in-line or managed policies that enable you to restrict user, role or group access to services and actions within the VPC","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html","title":"Policies and Permissions"}],"answers":[{"id":"8e2ddf5878aac8b5d22a6acab856040d","text":"Access Policy","correct":false},{"id":"33b02b9157e2a10758df1edb0fa38865","text":"Service Control Policy","correct":false},{"id":"eeec60437e6675cbd890b5f482628cfc","text":"Identity-based Policy","correct":true},{"id":"2d53d9afa60c8a03ddae8baa0cb72fb2","text":"Resource-based Policy","correct":false}]},{"id":"cde5021e-5b37-406b-b25a-1bdb489d3b24","domain":"security-comp","question":"Following a recent security event, a SysOps administrator has been asked to provide details of source IP addresses of requests to a website which is hosted on EC2 instances behind an Application Load Balancer. Where can the administrator find these details?","explanation":"Application Load Balancer (ALB) Access Logs record details of client connections which include the client's IP address and port.  CloudTrail logs AWS API calls so will not provide client IP addresses, neither will CloudWatch Custom Metrics which are for logging performance metrics.  EC2 instances will log the ALB's internal IP address as the source unless specifically configured to record the true source IP by using the x-forwarded-for header, which is not enabled by default.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html#access-log-entry-format","title":"ALB Access Log Entries"}],"answers":[{"id":"6a219c87573826721cb51b987140a267","text":"CloudTrail Event Trail","correct":false},{"id":"9a19403fef543b4899ec570ad1a29ca6","text":"CloudWatch Custom Metrics","correct":false},{"id":"43bc48b76d9918101fb772b4cfc58235","text":"/var/log/httpd/","correct":false},{"id":"8480ea5c787566c9a8bd2a608c725a06","text":"ALB Access Logs","correct":true}]},{"id":"ff1ea769-7316-4292-9cb6-efc556c4af5d","domain":"networking","question":"You have an application running on an EC2 instance using IPv4. The EC2 instance is in a public subnet with a route to an Internet Gateway with target 0.0.0.0/0. As a SysOps Administrator you've been tasked with editing the network to accommodate an upgrade to the application. The upgrade adds an IPv6 address to the instance and it requires only outbound Internet access. What changes would you make?","explanation":"If you have an existing VPC that supports IPv4 only, and resources in your subnet that are configured to use IPv4 only, you can enable IPv6 support for your VPC and resources. Your VPC can operate in dual-stack mode — your resources can communicate over IPv4, or IPv6, or both. IPv4 and IPv6 communication are independent of each other. An egress-only Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances. You cannot use a NAT instance nor a NAT Gateway using the IPv6 protocol; you can only use an egress-only Internet Gateway.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html","title":"Egress-Only Internet Gateways"}],"answers":[{"id":"e83dc6bf30ada297b10ae6b7d524fb08","text":"Set up a NAT Gateway. Create a route from your public subnet to the NAT Gateway.","correct":false},{"id":"754f95d8f4bb6371c0314ad0727947ff","text":"Create an egress-only Internet gateway and associate it to your subnet. Specify ::/0 in the destination box and select the egress-only Internet Gateway ID in the Target list.","correct":true},{"id":"453bf5bd268d1213b9aa8e0d45ba876a","text":"This is not possible. You will need to create a new VPC that supports IPv6.","correct":false},{"id":"ed594610a3ab11624c962c4b7b60381c","text":"Set up a NAT instance and place it in the public subnet. As the NAT instance is in the same subnet as your EC2 instance, no route changes are necessary.","correct":false}]},{"id":"0a54c671-954d-4e52-8a46-83eadcf029cd","domain":"mon-rep","question":"Which of the following are valid alarm statuses in CloudWatch?","explanation":"The three alarm statuses are OK, INSUFFICIENT_DATA and ALARM.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html","title":"About CloudWatch Alarms"}],"answers":[{"id":"e0aa021e21dddbd6d8cecec71e9cf564","text":"OK","correct":true},{"id":"f5d0aa0db6ffc40d938f1412b89d946c","text":"INSUFFICIENT_DATA","correct":true},{"id":"320f86f60f25459ba5550e000b2c3929","text":"ALERT","correct":false},{"id":"9de6d0a670ae5a0dee31a6318aa00e8d","text":"ALARM","correct":true}]},{"id":"414ac705-4dc4-4261-9523-5439c70ee19e","domain":"data-man","question":"You are running a relational database on a provisioned IOPS volume that is set to handle a moderate amount of traffic during the month. You know to expect a 10x spike in traffic during the final three days of each month due to month-end processing. How would you architect your storage environment to meet this expected increase in demand?","explanation":"With Elastic Volumes, you can increase volume size, adjust performance, or change the volume type while the volume is in use. You can continue to use your application while the change takes effect. Using CloudWatch alarms to automate these workflows is best practice. A reserved instance would not help in scaling to meet increased demand. Multi-AZ is for failover/disaster recovery purposes. Attaching multiple volumes is not best practice as it would increase cost and waste storage space during periods of low demand.","links":[{"url":"https://aws.amazon.com/blogs/aws/amazon-ebs-update-new-elastic-volumes-change-everything/","title":"Amazon EBS Update â€“ New Elastic Volumes Change Everything"}],"answers":[{"id":"8df9d9e9da10ed671385be6f7b1a0ffc","text":"Use a CloudWatch alarm to watch for a volume that is running at or near its IOPS limit. Initiate a workflow and approval process that could provision additional IOPS or change the type of the volume.","correct":true},{"id":"d49d9044d5ba871697b4be060e80701e","text":"Enable multi-AZ for your RDS instance to account for the increased demand and spread the load between at least two instances.","correct":false},{"id":"afe5d1141b5bb2e9227d1d7d8adeb777","text":"Attach multiple Provisioned IOPS volumes for your relational database in order to meet the expected increase in demand.","correct":false},{"id":"870c3d2a2d0fd0b784490ec327392d9e","text":"Purchase an AZ-specific reserved instance for your relational database in order to gain the benefit of reserved capacity.","correct":false}]},{"id":"3xre6hrv-j02a-kj8k-nkyn-5951wwipzpzd","domain":"automation","question":"Which service can you use to enable configuration management using Chef or Puppet?","explanation":"OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Config allows you to record and evaluate configuration but doesn't use Chef or Puppet, Systems Manager is an operational insights tool and Athena is used to run SQL queries on data held in S3.","links":[{"url":"https://aws.amazon.com/opsworks/","title":"OpsWorks"}],"answers":[{"id":"582ca45acfd3e21caca8b786c1413850","text":"Athena","correct":false},{"id":"8e75b153e61c22a8ea4e14aadc7cb4ee","text":"Systems Manager","correct":false},{"id":"fa535ffb25e1fd20341652f9be21e06e","text":"Config","correct":false},{"id":"c42aaccedc51aac929c8ae313066f320","text":"OpsWorks","correct":true}]},{"id":"2e877def-25d5-45d3-92eb-431238138055","domain":"security-comp","question":"After 5 years of continuous growth, an e-commerce startup has started to experience application level attacks from multiple sources. The web application is hosted inside a VPC where the EC2 instances are hosted inside a public subnet behind an Application Load Balancer. The RDS instances are hosted inside a private subnet. The cloud security team has suggested the use of a managed service to protect the application from application level attacks. How can the team accomplish this?","explanation":"AWS WAF (Web Application Firewall) provides application level cloud security. It can easily be attached to an existing ALB. NACL provides network level security. A Jumpbox/Bastion Host does not protect the application from application-level attacks. There is no such thing as CloudFront firewall rules.","links":[{"url":"https://aws.amazon.com/waf/","title":"WAF"}],"answers":[{"id":"1b19cca862178c0eb9a70c8d18e729b1","text":"Set up AWS WAF","correct":true},{"id":"343d46869dd9c79eea5ee27e480b6e37","text":"Configure CloudFront firewall rules","correct":false},{"id":"d70befb134b011bc2ef74354837ad33d","text":"Configure NACL rules","correct":false},{"id":"adb3a7040f4ab6ab7552c7c53e082bae","text":"Set up a Jumpbox/Bastion Host EC2 instance in the public subnet","correct":false}]},{"id":"2a16f1cd-530d-4e30-ad08-d89afae34484","domain":"mon-rep","question":"You create a new DynamoDB table with the provisioned read and write capacity units set to 5. The auto scaling feature is enabled for both read and write. And the target utilization is set as 70%. After monitoring the table for some time, you notice that there are two CloudWatch alarms related to the table. The description of one alarm is \"ConsumedWriteCapacityUnits < 150 for 15 datapoints within 15 minutes\". Which action do you need to take to address the alarm?","explanation":"DynamoDB manages the throughput capacities automatically with the auto scaling feature. The alarms are used for the feature and no action is required. There is no need to disable the feature or modify the provisioned capacities.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html","title":"Manage throughput capacity automatically with DynamoDB Auto Scaling"}],"answers":[{"id":"252cd5486e4ae9685450c3e9ad208b05","text":"No action is required as the alarms are used for auto scaling for the DynamoDB table.","correct":true},{"id":"3a5208c005673a46313dce67b9e2dd57","text":"You should disable the auto scaling feature for the table.","correct":false},{"id":"2131aeffdfb01948cc4943432a680327","text":"You need to adjust the provisioned read and write capacities to a higher value such as 10.","correct":false},{"id":"859412aa0d0666ab4c2b8d20707c8f41","text":"They are fake alarms. You can manually delete them from the console.","correct":false}]},{"id":"5b71bc80-b7ef-4fde-86f0-1a73d1d63e4c","domain":"high-avail","question":"You are an AWS administrator and have set up an Elastic Load Balancer inside a VPC. The ELB spans several Availability Zones. The ELB sits in front of a web application running on Amazon EC2. You notice that incoming traffic is not being evenly distributed across the AZs. How would you solve this issue?","explanation":"Traffic not evenly distributed across the instances in multiple AZs means the traffic is going to only specific EC2 instances. This happens when either the instances which are not receiving the traffic are unhealthy, or the instances that are receiving the traffic are holding on to the session. Since there is no mention of unhealthy instances, disabling sticky sessions on the ELB is the best answer. Increasing the number of subnets and/or instances will not solve the problem as users will remain stuck to the original instance. Increasing the frequency of health checks will have no impact to force even distribution of traffic.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html","title":"Configure Sticky Sessions for Your Classic Load Balancer"}],"answers":[{"id":"e23eea1cecbc63f7423de00e65f29614","text":"Increase the frequency of the health checks to the EC2 instances running your application.","correct":false},{"id":"d532ac3048a69a5902a79b51bc219bd3","text":"Increase the number of EC2 instances behind the ELB.","correct":false},{"id":"b7116d2a43d8462c83e8b93fda085c71","text":"Disable sticky sessions on the ELB.","correct":true},{"id":"c0d9e44a3e92d6d91864058257e9922c","text":"Add additional subnets within your ELB and configure your ELB to span the new subnets.","correct":false}]},{"id":"3f14908a-fb13-4a6a-a777-7eeb6722447d","domain":"dep-prov","question":"You are running a legacy application on an EC2 instance and have created and attached an EBS volume with default settings. The volume occasionally encounters data consistency errors making the EBS volume inaccessible to the instance. How can this be prevented?","explanation":"When Amazon EBS determines that a volume's data is potentially inconsistent, it disables I/O to the volume from any attached EC2 instances by default. This causes the volume status check to fail, and creates a volume status event that indicates the cause of the failure. Switching on Enable Volume IO will allow the instance to access the volume. Switching on Auto-Enabled IO will also achieve this outcome automatically when enabled. All other options are incorrect.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html#work_volumes_impaired","title":"Working with an Impaired Volume"}],"answers":[{"id":"1775e91227da79ae13b5522ccf280ca0","text":"Switch on Auto-Consistency IO","correct":false},{"id":"58bcefbd4466f36f955a0383bd253082","text":"Switch on Auto-Enabled IO","correct":true},{"id":"628b701156eef0c168e27dc6d58a9d15","text":"Switch off Auto-Enabled IO","correct":false},{"id":"65e65e38f33598416e8a272b6ad29b2a","text":"This cannot be prevented","correct":false},{"id":"bf737cfd17a3b913cb405f896e5e9b7d","text":"Switch on Enable Volume IO","correct":true}]},{"id":"4fa7de17-2f03-4b83-a519-e00b011ca644","domain":"security-comp","question":"You are a SysOps Administrator for your organzation. The organization has one AWS account that all users share. You are asked by the CISO to make access to AWS secure and manageable. What would be the best solution?","explanation":"AWS Single Sign-On makes it easy to centrally manage access to all of your AWS accounts. It supports Security Assertion Markup Language (SAML) 2.0 so you don't have to create IAM users for every individual in your organization. The other solutions would be an administrative and unnecessary burden, while encrypted login credentials using KMS is not used for AWS access.","links":[{"url":"https://docs.aws.amazon.com/en_pv/singlesignon/latest/userguide/iam-auth-access.html &https://docs.aws.amazon.com/cli/latest/reference/sts/assume-role-with-saml.html","title":"AWS Single Sign-On"}],"answers":[{"id":"b18f21a1b9a12f45b180d3ba09d175a0","text":"Configure a SAML federation between AWS and your organization's Active Directory. Set up Active Directory groups with AWS IAM groups to manage user permissions.","correct":true},{"id":"0949ef9615ffc62f343f10d27d0bea2e","text":"Encrypt Active Directory logins using AWS KMS. Store encrypted logins in an RDS table. When users access AWS, set up permissions to decrypt the credentials to securely access AWS resources.","correct":false},{"id":"48371df9efaa3423dc8c9153f0af03c3","text":"Group Active Directory users and mirror their permissions with IAM policies. Create IAM users in AWS and group them into IAM groups that correspond to the Active Directory Group.","correct":false},{"id":"678d5bab1e8e14c5f9a7055f83d2aa56","text":"Set up an AWS Organization to manage accounts and apply permissions boundaries. Set up IAM users and group them in the appropriate OU.","correct":false}]}]}}}}
