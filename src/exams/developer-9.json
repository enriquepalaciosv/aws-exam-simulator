{"data":{"createNewExamAttempt":{"attempt":{"id":"659ed000-655c-41ab-bbdb-738fac3107f1"},"exam":{"id":"57acaf0b-3937-44fb-9685-03aab2921285","title":"AWS Certified Developer - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"9feaacea-e1e9-4fc6-9602-e7d8c50faa0d","domain":"development","question":"You have a simple, stateless Python application which processes the contents of an S3 bucket once an hour and takes about 6 minutes to complete once started. Which AWS service should you run this application on in order to provide the most reliable and cost effective solution?","explanation":"The question states that the application is to run once every hour, which immediately means we should be looking for an option which only utilizes resources when needed.  Lambda is the only option in the above list which bills you only when an application is running. The EC2 and ECS options will bill for all resources, regardless of whether the application is running or not.  S3 should be discounted because although you can host static Websites, you cannot run Python applications from it.","links":[{"url":"https://aws.amazon.com/lambda/faqs/","title":"Amazon Lambda FAQs"}],"answers":[{"id":"4a155d7bbaa5f82abb58d55f1274a1cd","text":"Create a CloudFront endpoint and point it to an S3 bucket. Run the application from that bucket","correct":false},{"id":"640c9577ed0b4efae9a09e7f32b9f36d","text":"Create an ECS Cluster and run the application as a service on that cluster","correct":false},{"id":"f31120d65a588f0acc604ac80cf32c3c","text":"Make the application a Lambda function and create a Scheduled Event Trigger set to 1 hour","correct":true},{"id":"c0ee3176dc24aa0728823bd03bd0936b","text":"Create an Auto Scaling Group for an EC2 instance. Run the application on that instance with crontab set to 1 hour","correct":false}]},{"id":"d4258c06-e769-46cc-b5ab-1571ea57879b","domain":"development","question":"You are pair programming with another senior developer in your team and you have been tasked with writing a number of different Lambda functions. Your colleague recommends that you separate the Lambda handler from the core business logic of your code. What is the rationale for this?","explanation":"At the time you create a Lambda function, you specify a handler, which is a function in your code, that AWS Lambda can invoke when the service executes your code. Separating the handler from the core business logic is best practice as it enables code re-use as well as making unit testing easier. See the URL below for best practices for developing Lambda functions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html","title":"Best Practices for Working with AWS Lambda Functions"}],"answers":[{"id":"0f56edddb67c69aafb238d531d6abdfb","text":"To reduce the size of your deployment package","correct":false},{"id":"79c80a290b9838caa5e24e1f96a2a8f7","text":"To make the code easier to re-use","correct":true},{"id":"1763988c31a85fbf689902e6d3e1a185","text":"To reduce complexity caused by dependencies","correct":false},{"id":"6a5519ec7f0685c57d611a5c6731c5a4","text":"To improve function performance","correct":false}]},{"id":"fb5e139f-6556-4841-94ce-b9801f9b88e4","domain":"refactoring","question":"You are migrating a restaurant booking application from your own data center to AWS. The application currently runs on a number of virtual machines running web and application servers as well as a shared database server. The applications need to access a large number of shared images and documents containing drinks and food menus. Which of the following could you use as a shared storage solution for this application so that the application servers can still access the shared files?","explanation":"ElastiCache is a temporary in memory data store, and is not for persisting shared files. DynamoDB is a noSQL database and not a suitable place to store images and text documents. SQS is a messaging system and not a data store. S3 is a storage solution suitable for images, documents and other files or objects which can be accessed by multiple users and services. The recommended way to enable EC2 instances to access S3 is by using an Instance Role.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html","title":"Using an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances"}],"answers":[{"id":"1da781de965981ece00cf74ff1369c3e","text":"Generate a pre-signed URL to grant access","correct":false},{"id":"1c032bc585b67b0da68ac7711f6903dc","text":"Store the files in SQS","correct":false},{"id":"cfef61a88093b57cdfb2b8f688437a31","text":"Store the files in S3","correct":true},{"id":"7b5afb2448ce66d49ccd20687b03fb3d","text":"Store the files in DynamoDB","correct":false},{"id":"c82a0db1b2bb65a785fb8195491de836","text":"Store the files in ElastiCache","correct":false},{"id":"2412dfdf36b33ac6269617fc20e8cc04","text":"Use an IAM instance role to grant access","correct":true},{"id":"31ad78ce89dc92d90e742d7ddf764a6d","text":"Embed IAM credentials in the EC2 instance metadata","correct":false}]},{"id":"d193b6e5-b33d-429a-8334-2295b5032bf0","domain":"mon-trb","question":"You are responsible for a number of different applications which are hosted across multiple regions. You would like to use CloudWatch to view all system metrics data in one place. Which of the following approaches should you choose?","explanation":"CloudWatch dashboards are customizable home pages in the CloudWatch console that you can use to monitor your resources in a single view, even those resources that are spread across different Regions.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Dashboards.html","title":"Using Amazon CloudWatch Dashboards"}],"answers":[{"id":"e87ab0fb6d9ac17e828c3df9b699fe9a","text":"Create a single dashboard to cover all the regions and include metrics for each application","correct":true},{"id":"7456dbc94be76b5a2f0b98bc81920d8f","text":"Create a different dashboard for each resource type on a regional basis","correct":false},{"id":"ac051baaf2ee5b9115dd035726ab1f51","text":"Create a dashboard to cover all the applications and use separate namespace for each region","correct":false},{"id":"9f39dac5a56fd07170a15a0ee5a2e338","text":"Create a dashboard for each region and group the applications together on a regional basis","correct":false}]},{"id":"97b2f9cc-962b-4c75-82c3-a44815644776","domain":"mon-trb","question":"You are developing a new application using Lambda, API Gateway, S3 and DynamoDB. You would like to record information about incoming and outgoing HTTP requests as well as latency incurred by each component. You have multiple versions of the application to cater for your Development, UAT, Performance Test and Production environments. What is the most efficient way to collect this information and group it according to which environment it relates to?","explanation":"AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, micro-services, databases and HTTP web APIs. When you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segment document as annotations and metadata. Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations","title":"X-Ray Annotations and Metadata"},{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html","title":"Searching for Traces in the AWS X-Ray Console with Filter Expressions"}],"answers":[{"id":"d2ae15c70b6c5a546c571203b1a18474","text":"Use CloudTrail to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false},{"id":"7596c1ed59077a183e47b002a53bb84a","text":"Use CloudFormation to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false},{"id":"c57540f9c7ee62b718518b73d8ea536c","text":"Use X-Ray to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":true},{"id":"8f0f2ab2fa58b6ddef1f41eb2fe56872","text":"Use CloudWatch to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false}]},{"id":"b1a8a6e6-0eb1-4c01-8d8b-4baebf067f88","domain":"development","question":"An application uses DynamoDB table as its backend data store. Each item has size of 10KB.  The application needs to perform 100 eventually consistent read operations per second, and 50 write operations per second.  What is the provisioned RCU value required to meet these requirements?","explanation":"1 RCU is equivalent to two eventually consistent reads per second of an item up to 4KB in size.  Thus, to calculate the required RCU in this scenario we need to: 1) Round up the item size to the nearest 4KB (12KB). 2) Divide by 4KB to calculate number of read units (12/4 = 3). 3) Divide by 2 to calculate the number of eventually consistent read units per item (3/2 = 1.5). 4) Multiple by operations per second to get the total RCU required (1.5*100 = 150 RCU).","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","title":"Provisioned Mode"}],"answers":[{"id":"a9b7ba70783b617e9998dc4dd82eb3c5","text":"1000","correct":false},{"id":"cee631121c2ec9232f3a2f028ad5c89b","text":"500","correct":false},{"id":"94f6d7e04a4d452035300f18b984988c","text":"300","correct":false},{"id":"7ef605fc8dba5425d6965fbd4c8fbe1f","text":"150","correct":true}]},{"id":"427e94b9-4fba-461b-aea9-d1a0c40a150e","domain":"development","question":"A DynamoDB table is configured in provisioned throughput mode with 500 RCU and 100 WCU.  How much data can be read and written to the table each second?","explanation":"One read capacity unit is equivalent to one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. One write capacity unit is equivalent to one write per second for an item up to 1 KB in size.  Therefore, 500 RCU is equivalent to: 1) 500 RCU * 4KB = 2000 KB per second for strongly consistent read operations; 2) 500 RCU * 4KB = 2000 KB per second * 2 = 4000 KB per second for eventually consistent read operations; 3) 100 WCU * 1KB = 100 KB per second for write operations.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","title":"Provisioned Mode"}],"answers":[{"id":"3d31cbf18bacabdeabf60932252ab4e0","text":"2000 KB for strongly consistent read operations, 4000 KB for eventually consistent read operations, 400 KB for write operations.","correct":false},{"id":"28e9c06ef3d9789d4e86426c1a59bbcf","text":"2000 KB for strongly consistent read operations, 4000 KB for eventually consistent read operations, 100 KB for write operations.","correct":true},{"id":"3e1674995e5d9212840ca5a62310ccf9","text":"500 KB for strongly consistent read operations, 1000 KB for eventually consistent read operations, 100 KB for write operations.","correct":false},{"id":"c233b8f618b2e1721b960acd0f8a9c81","text":"500 KB for strongly consistent read operations, 1000 KB for eventually consistent read operations, 400 KB for write operations.","correct":false}]},{"id":"a0b993eb-5210-4269-b055-8cb0f8ee0192","domain":"security","question":"Your mobile application needs to read data from DynamoDB. What is the best way to give mobile devices permissions to read from DynamoDB?","explanation":"Web identity federation removes the need for creating individual IAM users. Instead, users can sign in to an identity provider and then obtain temporary security credentials from the AWS Security Token Service.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WIF.html","title":"Using Web Identity Federation"}],"answers":[{"id":"8b6a4ec1dbecce8cd70cbe48c62b5c6c","text":"Create an IAM role for your users.","correct":false},{"id":"baa876447171bf9fb69ccfb1811d73e1","text":"Issue an access key and secret access key to each user.","correct":false},{"id":"85b1805591d84001d38b3d9942fee4fe","text":"Connect your application to an EC2 instance with permission to read from DynamoDB.","correct":false},{"id":"9fb0f3fc8b9734347e97c0b47d69bbba","text":"Create an IAM role that can be assumed by an app that allows federated users.","correct":true}]},{"id":"e466b1d0-1858-40d8-aeca-e48e5e44ae2d","domain":"deployment","question":"Your application is using Kinesis to ingest data from a number of environmental sensors which continuously monitor for pollution within a 1 mile radius of a local primary school. An EC2 instance consumes the data from the stream using the Kinesis Client Library. You have recently increased the number of shards in your stream to 6 and your project manager is now suggesting that you need to add at least 6 additional EC2 instances to cope with the new shards. What do you recommend?","explanation":"Re-sharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. You should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances. When re-sharding increases the number of shards in the stream, the corresponding increase in the number of record processors increases the load on the EC2 instances that are hosting them. If the instances are part of an Auto Scaling group, and the load increases sufficiently, the Auto Scaling group adds more instances to handle the increased load.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html","title":"Kinesis Data Streams Terminology"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html","title":"Resharding, Scaling, and Parallel Processing"}],"answers":[{"id":"7561e4f93183899bc8757da9db13ae60","text":"One worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances","correct":true},{"id":"c11271a6921d20363c624ddde1790781","text":"You should decrease the number of shards to match the number of consumer instances","correct":false},{"id":"1c8f531c39885147b4935b0e64880b97","text":"The number of instances should be greater than number of shards","correct":false},{"id":"6c565b117fa64cb3b8ddaf38c670efed","text":"You should increase the number of instances to match the number of shards","correct":false}]},{"id":"4ebf4856-d4c7-4ea6-85c6-602dba6571a3","domain":"development","question":"You are developing a latency-sensitive application which stores a lot of data in DynamoDB. Each item is 3.5KB in size. Which of the following DynamoDB settings would give you the greatest read throughput?","explanation":"A read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. Eventually consistent reads provide greater throughput than strongly consistent.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ProvisionedThroughput.CapacityUnits.Read","title":"DynamoDB Provisioned throughput"}],"answers":[{"id":"85f208ed09607ef8653aef5841e25848","text":"Configure the table with 15 read capacity units and configure the application to use a scan operation","correct":false},{"id":"58dedf407e5eb2c525e6732d526eeca0","text":"Configure the table with 10 read capacity units and use strongly consistent reads","correct":false},{"id":"882e4855b7af9b2ffa948de81b75cf47","text":"Configure the table with 10 read capacity units and use high-performance reads","correct":false},{"id":"53f0f96b15844d8d110ee8bc7174411d","text":"Configure the table with 10 read capacity units and use eventually consistent reads","correct":true},{"id":"fa75717ea7d6fc295eb8d4b34d6ac4f6","text":"Configure the table with 15 read capacity units and use strongly consistent reads","correct":false}]},{"id":"6a5ea0b3-2527-4c20-9410-d8807d16fcd3","domain":"security","question":"An organization is hosting their static website on S3, using a custom domain name. Users have started reporting that their web browsers' are alerting them to the fact that the organization's website is \"Not Secure\" because it is not served via a secure HTTPS connection.\n\nWhat is the easiest way to start serving the website via HTTPS?","explanation":"S3 buckets do not directly support HTTPS with a custom domain name. The simplest solution is to create a CloudFront distribution and set its origin to the S3 bucket. CloudFront allows you to specify a custom domain name, and supports managed certificates via Amazon Certificate Manager.\n\nEnabling AES-256 Default Encryption on the S3 bucket only affects the object at rest.\n\nApplication Load Balancers do support SSL termination but do not support S3 as a target.\n\nAWS Shield relates to Distributed Denial of Service protection, not encryption over the wire.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-https-requests-s3/","title":"Use CloudFront to serve HTTPS requests for Amazon S3"}],"answers":[{"id":"674d0590e06799926f232e63b73894a8","text":"Add a CloudFront distribution in front of the S3 static website, which supports HTTPS with a custom domain name.","correct":true},{"id":"6d63f6dae98a20c7db6446021c83e7f5","text":"Enable AWS Shield on the S3 bucket. Browsers automatically detect that Shield is enabled and report that the website is secure.","correct":false},{"id":"d76d398f54d04a531867ae84eda26050","text":"Add an Application Load Balancer in front of the S3 bucket and enable SSL termination.","correct":false},{"id":"27631f4c194c5ea5116e308788e945f2","text":"Enable AES-256 Default Encryption on the S3 bucket, which ensures all content is delivered via HTTPS.","correct":false}]},{"id":"6641f65a-c837-49a2-bbeb-11af158d44e0","domain":"mon-trb","question":"What is the maximum execution duration for a Lambda request?","explanation":"As of Oct 2018 the maximum execution duration has been increased from 300 seconds to 900 seconds (15 minutes)","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/limits.html","title":"Lambda Limits"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/","title":"Update notice- Oct 2018"}],"answers":[{"id":"8d15ed7d27d83ed6229a66b1f44b7696","text":"3 minutes","correct":false},{"id":"a51534ea662db3ce23238035e25859e2","text":"900 seconds","correct":true},{"id":"4234838a99b7912e550babb083c205c4","text":"60 seconds","correct":false},{"id":"7ed53d277129b356be62369ec930e3b8","text":"500 seconds","correct":false},{"id":"533f546e5ddb63fb2c810f7cca06678f","text":"300 seconds","correct":false}]},{"id":"43f82a5f-84eb-4bda-9424-b03f27fa79ab","domain":"security","question":"What is the recommended approach to configuring a mobile application to allow users to sign-in and sign-up to your application via Facebook?","explanation":"Cognito is the preferred Web ID Federation mechanism in AWS","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html","title":"Cognito For Mobile Apps"}],"answers":[{"id":"a9d9e07070663e9217c3fcc296f07259","text":"Use Cognito as an Identity Broker between your application and the Web Identity Provider","correct":true},{"id":"51e84803699eae0c33a7fd3c998881df","text":"Use a custom Lambda function to act as an Identity Broker between your application and the Web Identity Provider","correct":false},{"id":"09bea5739247e8d082cd79bd90905562","text":"Use encrypted AWS credentials within your application code and store them locally on the device","correct":false},{"id":"0aed15971d7cc88de6e66d66abc629f5","text":"Use IAM as an Identity Broker between your application and the Web Identity Provider","correct":false}]},{"id":"952d36e5-535a-4b3c-a107-e834a2126477","domain":"development","question":"You have developed a web application running on a number of EC2 instances running Tomcat, you are using an S3 bucket to store product data, with customer transaction data held in an RDS database. You anticipate that the number of connections into your website will grow considerably over the next year and you want to configure a scalable place to store session state data so that multiple web servers can share the session state. Which of the following are suitable options for this application?","explanation":"DynamoDB and ElastiCache are both great options for storing session data. Both are scalable and resilient. RDS is more suited to relational data, whereas DynamoDB is far more flexible and better suited to storing session state data. Storing the data on the EC2 instances is not scalable or resilient. Lambda cannot store session state.","links":[{"url":"http://docs.amazonaws.cn/sdk-for-java/v1/developer-guide/java-dg-tomcat-session-manager.html","title":"Storing Web Server Session State in DynamoDB"},{"url":"https://aws.amazon.com/caching/session-management/","title":"Session management in AWS"}],"answers":[{"id":"bc7071d9896321da24f137567ef7e97c","text":"Store the session state data in a DynamoDB table","correct":true},{"id":"87bdddfb8d5f7cc0273d9b48a667d260","text":"Store the data in the same RDS database used for customer transactions","correct":false},{"id":"ba003a66972966fcb9cc5503341c8a59","text":"Use ElastiCache to store session state","correct":true},{"id":"be80d5651d3ee2a3c4e8736eb5082177","text":"Store session state locally on each EC2 instance","correct":false},{"id":"690b4209ea43175d9fb511ffba4d6e52","text":"Store the session state data inside a Lambda function","correct":false}]},{"id":"3eb49e2e-25b6-4fe6-8bbe-e3ccedcd1efd","domain":"security","question":"A developer is looking to implement a load balancing solution for web-based service oriented application deployed in AWS EC2. The solution must support path based routing and all communication to the users must be encrypted. What is the most performant method to achieve these requirement?","explanation":"The application requirement states support for path based routing. This means that we must use an Application Load Balancer as Network Load Balancer does not have this feature. It is best practice to deploy the SSL certificates on the Load Balancer. This implements SSL termination on the load balancer and off-loads this task from the application, thus reducing the load on EC2 instances. Additionally, it removes the requirement of distributing the certificate to all target EC2 instances.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html","title":"Create an HTTPS Listener for Your Application Load Balancer"}],"answers":[{"id":"206131d0e41aa9e13214ac701d6f08e2","text":"Use Application Load Balancer. Deploy SSL certificate on the Application Load Balancer.","correct":true},{"id":"b97b895cbdc513c48c5bfb7564a38aa7","text":"Use Network Load Balancer. Deploy SSL certificates on the EC2 instances.","correct":false},{"id":"7ebcbebf7f00c8f17f377813b31cc76e","text":"Use Network Load Balancer. Deploy SSL certificates on the Network Load Balancer.","correct":false},{"id":"a5a020608230cb06058ddd6291c7886a","text":"Use Application Load Balancer. Deploy SSL certificates on the EC2 instances.","correct":false}]},{"id":"f7fd0d7a-5d45-4f72-aa8e-d9a65270360f","domain":"refactoring","question":"You are developing an online hotel booking application which makes an number of requests to different back end applications to get quotes for travel related add-on services. You are using API gateway handle all the API calls and you notice that the majority of requests are for the same 5 or 6 services. How can you optimize the configuration to ensure the best performance for your application?","explanation":"You can enable API caching to cache your endpoint's responses, this reduces the number of calls made to your endpoint and improves the latency of requests to your API.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"API Caching"}],"answers":[{"id":"59a69a5bb20a3dbe9214ef93c38041f7","text":"Configure auto-scaling for the API Gateway","correct":false},{"id":"336a729744a0b4682222e3a4a1cdc750","text":"Implement API Caching to cache the endpoint's response for the most popular requests","correct":true},{"id":"9a86454fefc71c7430655ce2e18ac716","text":"Configure a CloudFront CDN in front of the API Gateway to cache the most frequent HTTP requests","correct":false},{"id":"b255bca6117b1096664ab7b1415fae80","text":"Add an ElastiCache cluster in front of your database to cache the most frequently accessed data","correct":false}]},{"id":"787db7cd-ff6a-46b1-83e7-9fe990e22c5e","domain":"security","question":"A developer is implementing an image sharing website hosted on AWS. Images are stored in a separate S3 bucket. During testing, it is discovered that the website does not load properly because the images are being blocked by the browser. How can the developer resolve this issue?","explanation":"As a security feature, resources in one domain cannot be access by a web-page resources in a different domain. Cross-origin resource sharing (CORS) is a mechanism that enables resources on a web page to be requested from a different domain outside the domain from which the first resource was served. If we are hosting images in a separate S3 bucket, this bucket will have a different domain. To allow these resources to be accessed by the web page domain, we need to configure CORS on the S3 bucket.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html","title":"Cross-Origin Resource Sharing (CORS)"}],"answers":[{"id":"d89d59d8223823c1f20371c88b73fc27","text":"Configure CORS on the S3 bucket.","correct":true},{"id":"1a29c427267cabba7bd3106de626aa13","text":"Make the S3 bucket public.","correct":false},{"id":"8f55313a69544c54c5bf4c4a71c9d935","text":"Enable versioning on the S3 bucket.","correct":false},{"id":"216ba600e977ed0524a78ecba4335037","text":"Enable S3 Transfer Acceleration.","correct":false}]},{"id":"d36289fe-be3b-4a73-8cc2-c216cfd47b05","domain":"security","question":"You're part of a developer team which is building an application that requires access to S3. Everyone on your team requires the same IAM permissions. As your team grows, how would you manage IAM policies and access to the right AWS resources in the most efficient manner?","explanation":"IAM groups are collections of IAM users in one AWS account. You can create IAM groups on a functional, organizational, or geographic basis, or by project, or on any other basis where IAM users need to access similar AWS resources to do their jobs. You can provide each IAM group with permissions to access AWS resources by assigning one or more IAM policies. All policies assigned to an IAM group are inherited by the IAM users who are members of the group. Creating IAM Users for each team member is not the most efficient manner; IAM Groups is more efficient. You cannot log into the AWS Management Console using an IAM role, nor can you do the same with Amazon Cognito. Amazon Cognito is best suited for mobile applications.","links":[{"url":"https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf?refid=em_","title":"AWS Security Best Practices"}],"answers":[{"id":"d81d323b3e2be35dfba6061c653ce5d1","text":"Create an Amazon Cognito user pool for each user and a corresponding S3 bucket. Grant S3 bucket GET requests for each bucket to each Cognito user. Require users to log into the Console using their Cognito credentials.","correct":false},{"id":"a834e2bd501ea84ca860e5213b4290ea","text":"Create one IAM role with the necessary permissions. Have all team members log into the AWS Management Console using that role. Rotate the password regularly.","correct":false},{"id":"7c9871a327396074304930d0dbe0fcee","text":"Create IAM Users for each team member. Attach an IAM policy to each user. Edit the IAM policy for each user adhering to the Principle of Least Privilege. Create new IAM policies for new team members as appropriate.","correct":false},{"id":"9ef5172ef2f91f7f79640375254443b1","text":"Create an IAM Group called 'Developers'. Attach an IAM policy to the group with the appropriate permissions. Associate your IAM user and your team members' users to the Group. Add new team members to the group as appropriate.","correct":true}]},{"id":"ee1f9799-b2a7-445f-bb89-932b725a8374","domain":"deployment","question":"A business-critical application is deployed using CloudFormation. The team would like to prevent accidental deletion of the stack. How can this be achieved most efficiently?","explanation":"Termination Protection stack option can be enabled to prevent accidental deletion of an entire CloudFormation stack. It is possible to use IAM policy to prevent deletion of a CloudFormation stack, however, this is not the optimal solution from operations and management perspective. The DeletionPolicy CloudFormation attribute applies to individual resources, not an entire stack. There is no DeletionProtection attribute in CloudFormation.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html","title":"Protecting a Stack From Being Deleted"}],"answers":[{"id":"6983aeb4c0d42bb293c8ec93966aedbb","text":"Set the DeletionProtection to True in the CloudFormation template.","correct":false},{"id":"558cc27970e2b64a29bdc85f381b8cb9","text":"Set the DeletionPolicy to Retain in the CloudFormation template.","correct":false},{"id":"8093dacb1a766d0f91fa60e48baa87ed","text":"Set Stack Termination Protection to Enable.","correct":true},{"id":"eee064b70f79422e8511f18b251253ef","text":"Create IAM Policy with Effect of Deny for 'cloudformation:DeleteStack' Action.","correct":false}]},{"id":"802be8c8-07cc-48a7-94c8-21d785d18f5a","domain":"deployment","question":"Your organization is developing a CI/CD environment to improve software delivery of your applications. It has already adopted a plan to execute the various phases of the CI/CD pipeline from continuous integration to continuous deployment. There are now discussions around restructuring the team make-up to implement a CI/CD environment. How would you recommend creating developer teams as a best practice to support this change in the long run?","explanation":"AWS recommends organizing three developer teams for implementing a CI/CD environment: an application team, an infrastructure team, and a tools team. This organization represents a set of best practices that have been developed and applied in fast-moving startups, large enterprise organizations, and in Amazon itself. The teams should be no larger than groups that two pizzas can feed, or about 10-12 people. This follows the communication rule that meaningful conversations hit limits as group sizes increase and lines of communication multiply. Hiring an external consulting firm will not be beneficial in the long run. Setting up a single team is not best practice. AWS CodePipeline is a continuous integration and continuous delivery service for fast and reliable application and infrastructure updates and not used for team structuring.","links":[{"url":"https://d0.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"f3054a7a017a533ace8f081538d0b664","text":"Set up an application team to develop applications. Set up an infrastructure team to create and configure the infrastructure to run the applications. Set up a tools team to build and manage the CI/CD pipeline.","correct":true},{"id":"d163f1cc494b3cde77122207a42d9de1","text":"Set up one team to own an operate all components of the CI/CD pipeline to consolidate tasks and improve efficiency.","correct":false},{"id":"b943ddf4265197477c4036cdd230b033","text":"Hire an external consulting firm to build and manage the pipeline. Provide them with the proper IAM roles to access your AWS environment.","correct":false},{"id":"3e514092970cfe2b9a3ea7331a0828cf","text":"Use CodePipeline to manage your CI/CD environment and assign team members to own different phases within your CodePipeline.","correct":false}]},{"id":"21efb969-377d-45b1-a907-8994e94aa26b","domain":"development","question":"An organization is considering making use of AWS Fargate in their next project. Which of the following statements best describes AWS Fargate?","explanation":"AWS Fargate is a compute engine for Amazon ECS that allows you to run containers without having to manage servers or clusters.","links":[{"url":"https://aws.amazon.com/fargate/","title":"AWS Fargate"}],"answers":[{"id":"940863780b5c57669b92b1fc551543ca","text":"Automates management of the control plane within a Kubernetes cluster.","correct":false},{"id":"fbe3eadaa96811fdb58081b395eb6164","text":"Stores Docker containers within a registry, making them available for use by AWS ECS.","correct":false},{"id":"9996f92567fdefae4123ba718e107bbf","text":"Deploys Compute logic to an AWS Edge location.","correct":false},{"id":"5c10facdecd8cb3ff129afb77d315739","text":"Deploys Docker containers within AWS, without having to manage underlying EC2 instances.","correct":true}]},{"id":"c73f812b-373b-4429-9a32-a3d71186c137","domain":"deployment","question":"Your application needs 100 strongly consistent reads on items that are 9KB in size every second. How many units of read capacity units should you provision?","explanation":"9KB rounds up to 12KB. 12KB/4KB=3 strongly consistent read capacity units each. 3*100=300 strongly consistent read capacity units.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html","title":"Throughput Capacity for Reads and Writes"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html","title":"DDB - Read Consistency"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CapacityUnitCalculations.html","title":"Calculating CU"}],"answers":[{"id":"94f6d7e04a4d452035300f18b984988c","text":"300","correct":true},{"id":"9de6d14fff9806d4bcd1ef555be766cd","text":"350","correct":false},{"id":"3644a684f98ea8fe223c713b77189a77","text":"200","correct":false},{"id":"7ef605fc8dba5425d6965fbd4c8fbe1f","text":"150","correct":false}]},{"id":"ea9dc476-332a-424f-a806-8a535a8b516e","domain":"deployment","question":"You want to quickly deploy and manage an application in the AWS Cloud without having to learn about the infrastructure that runs the application. Elastic Beanstalk is the first service that comes to mind. You have written you application with C#. How would you launch your application on Elastic Beanstalk in the most efficient manner?","explanation":"AWS Elastic Beanstalk supports custom platforms which lets you develop an entire new platform from scratch, customizing the operating system, additional software, and scripts that Elastic Beanstalk runs on platform instances. This flexibility enables you to build a platform for an application that uses a language or other infrastructure software, for which Elastic Beanstalk doesn't provide a managed platform. In addition, with custom platforms you use an automated, scripted way to create and maintain your customization, whereas with custom images you make the changes manually over a running instance. Rewriting your application would not be the most efficient way if you can create your own platform. Launching an EC2 instance would still require you to manage your own infrastructure. OpsWorks manages infrastructure deployment by organizing applications into layers to provision EC2 instances and resources for an application.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/custom-platforms.html","title":"Elastic Beanstalk Custom Platforms"}],"answers":[{"id":"a55638d3d79efef4debfe4f43fcf4c40","text":"Rewrite your application in Python as C# is not a supported programming language.","correct":false},{"id":"0993d2ef16ec086413357c635d131d1c","text":"Create your own Elastic Beanstalk platform using Packer. Use this platform for your application.","correct":true},{"id":"15978d42b93994c5fa1bdaaa66df8564","text":"Use AWS OpWorks instead to launch your application that will help automate deployment and configurations for your application.","correct":false},{"id":"5d283fee57c43b00226df6388edd5bb7","text":"Use EC2 instance. Launch the application on an EC2 instance and use CloudFormation to automate infrastructure provisioning.","correct":false}]},{"id":"9abb4110-4b7b-11ea-b77f-2e728ce88125","domain":"development","question":"You want to add a cross-origin resource sharing (CORS) configuration to one of your S3 buckets. Which of the following tabs should you choose to do so?","explanation":"To add a CORS configuration to your S3 bucket, you have to click the 'Permissions' tab and choose 'CORS configuration'. The 'Properties' tab is for configuring object settings such as versioning, transfer acceleration, and logging. The 'Management' tab is for managing object replication, analytics, and storage lifecycle. If you want to simplify bucket access by creating endpoints, you choose 'Access points'.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-cors-configuration.html","title":"How Do I Add Cross-Domain Resource Sharing with CORS?"}],"answers":[{"id":"902c305d02b28c2f6199601e7128ed36","text":"Access points","correct":false},{"id":"d08ccf52b4cdd08e41cfb99ec42e0b29","text":"Permissions","correct":true},{"id":"9fc2d28c05ed9eb1d75ba4465abf15a9","text":"Properties","correct":false},{"id":"fe4dbcab9b910577e5035e97ac068dae","text":"Management","correct":false}]},{"id":"8b887631-86bd-436d-adee-4e2ba3b02111","domain":"security","question":"You have an application running on multiple EC2 instances, however every time an instance fails, your users complain that they lose their session. What can you do to prevent this from happening?","explanation":"There are various ways to manage user sessions including storing those sessions locally to the node responding to the HTTP request or designating a layer in your architecture which can store those sessions in a scalable and robust manner. Common approaches used include utilizing Sticky sessions or using a Distributed Cache for your session management. In order to address scalability and to provide a shared data storage for sessions that can be accessed from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution for this is to leverage an In-Memory Key/Value store such as ElastiCache.","links":[{"url":"https://aws.amazon.com/caching/session-management/","title":"Session management in AWS"}],"answers":[{"id":"b225818943ba4680b8e7dc9d9c376359","text":"Store session state in RDS","correct":false},{"id":"76fc6bddee6b0f8d088ea5cbe4e57160","text":"Store session state in S3","correct":false},{"id":"89230492f141a4f85234c624287bb96a","text":"Store session state in ElastiCache","correct":true},{"id":"b193b1caff1bda86125cc326ca1058ac","text":"Store session state on a dedicated EC2 instance","correct":false},{"id":"ef4fd36fa55c3c499f3fffa82a0c95e8","text":"Store session state in on the Elastic Load Balancer","correct":false}]},{"id":"9554e10d-8fab-4545-b3ea-a756133ffab3","domain":"refactoring","question":"You need to retrieve some data from your DynamoDB table, which of the following methods would consume the least provisioned Capacity Units?","explanation":"A Query is generally more efficient than a Scan operation. Eventual consistency reads use up fewer Read Capacity Units than strongly consistent reads","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html","title":"Query Vs Scan"},{"url":"https://aws.amazon.com/dynamodb/pricing/provisioned/","title":"Provisioned Capacity"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html","title":"DynamoDB Read Consistency "}],"answers":[{"id":"ac9570514b07db98ad6cc89ea1cc8741","text":"Query with strong consistency","correct":false},{"id":"5d682b15ed6ff4d0b5100242fa11d4ea","text":"Scan with eventual consistency","correct":false},{"id":"d87fc0cde8df11c904ff6fe25cac6bfc","text":"Query with eventual consistency","correct":true},{"id":"8747d18821697e99310487487007af03","text":"Scan with strong consistency","correct":false}]},{"id":"8856df48-5866-4ee3-a5a7-2033444e21eb","domain":"security","question":"You have provisioned an RDS database and then deployed your application servers using Elastic Beanstalk. You now need to connect your application servers to the database. What should you do?","explanation":"As you are connecting to a database that was not created within your Elastic Beanstalk environment, you will need to create the Security Group yourself and also provide connection string and credentials to allow your application servers to connect to the database","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html#rds-external-defaultvpc","title":"Elastic Beanstalk And RDS"}],"answers":[{"id":"24e204c62e09f215959e144cbb8611d4","text":"Configure a security group allowing access to the database and add it to your environments auto-scaling group","correct":true},{"id":"50021ae41e50f1b10069ebcccab3c0ef","text":"Provide the database connection information to your application","correct":true},{"id":"1aa1798c4c20c22832e9a949af74ada3","text":"Configure Elastic Beanstalk to install a database client on your application servers","correct":false},{"id":"26b897f7474a08158506b32e7c566323","text":"Provide the ip address of the RDS instance to Elastic Beanstalk","correct":false}]},{"id":"7d99fa38-f31e-4527-95a6-a88611f7731c","domain":"mon-trb","question":"A developer deployed a serverless application consisting of an API Gateway and Lambda function using CloudFormation. Testing of the application resulted in a 500 status code and 'Execution failed due to configuration' error. What is a possible cause of the error?","explanation":"When you build an API Gateway API with standard Lambda integration using the API Gateway console, the console automatically adds the required permissions. However, when you set up a stage variable to call a Lambda function through your API, you must manually add these permissions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html","title":"Invoke"},{"url":"https://docs.aws.amazon.com/apigateway/api-reference/handling-errors/#api-error-codes","title":"Error Codes (Client and Server Errors)"}],"answers":[{"id":"dddca3f487612188440bbfe93d1ef829","text":"IAM policy is restricting the user from invoking the API Gateway API endpoint.","correct":false},{"id":"8fa26152f6981113cb014274ea900af1","text":"Too many API Gateway requests were created exceeding the allowed limit.","correct":false},{"id":"eef7bf395052a4cb6bf0286cc9578779","text":"API Gateway is not authorized to invoke the Lambda function.","correct":false},{"id":"d69e5b40405e5538753a24c15bb80a0e","text":"The Lambda function's resource-based policy doesn't include permission for your API to invoke the function.","correct":true}]},{"id":"3c98a94a-9140-4c40-8524-2f936ad71ca2","domain":"mon-trb","question":"Your API Gateway endpoint is experiencing a Throttling Exception due to too many requests. How can you avoid this in the most cost effective way?","explanation":"Numerous components on a network, such as DNS servers, switches, load balancers, and others can generate errors anywhere in the life of a given request. The usual technique for dealing with these error responses in a networked environment is to implement retries in the client application. Each AWS SDK implements automatic retry logic with an exponential backoff algorithm for better flow control. If you're not using an AWS SDK, you should retry original requests that receive server (5xx) or throttling errors. However, client errors (4xx) indicate that you need to revise the request to correct the problem before trying again.","links":[{"url":"https://docs.aws.amazon.com/general/latest/gr/api-retries.html","title":"Error Retries and Exponential Backoff in AWS"}],"answers":[{"id":"f642a919ac2888a46eed52e71d2cfa5a","text":"This indicates an internal problem with your API. Fix the issue in your API and try the request again","correct":false},{"id":"24417628a02050b9e0bf5a0f4c91681b","text":"Increase capacity by scaling the back-end of your API","correct":false},{"id":"6e6c16caa30d9157417c24e0997fa08c","text":"Distribute the load by adding an Elastic Load Balancer in front of the API Gateway","correct":false},{"id":"57040cb76b810ed9248085009af077f7","text":"Configure your application to use retry logic with exponential backoff","correct":true}]},{"id":"0d9472d0-587b-4961-9457-e37e75979b7c","domain":"development","question":"You are working on a Serverless application written in Python and running in Lambda. You have uploaded multiple versions of your code to Lambda, but would like to make sure your test environment always utilizes the latest version. How can you configure this?","explanation":"When you create a Lambda function, there is only one version: $LATEST. You can refer to the function using its Amazon Resource Name (ARN). There are two ARNs associated with this initial version, the qualified ARN which is the function ARN plus a version suffix e.g. $LATEST. Or the unqualified ARN which is the function ARN without the version suffix. The function version for an unqualified function always maps to $LATEST, so you can access the latest version using either the qualified ARN with $LATEST, or the unqualified function ARN. Lambda also supports creating aliases for each of your Lambda function versions. An alias is a pointer to a specific Lambda function version, aliases will not be updated automatically when a new version of the function becomes available.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-intro.html","title":"Introduction To Lambda Function Versioning"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda Function Versioning and Aliases"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/how-to-manage-versioning.html","title":"Lambda Versioning"}],"answers":[{"id":"02f663ae252fea90c711171a5572d211","text":"Reference the function using a qualified ARN and the $LATEST suffix","correct":true},{"id":"0311fc386b14ce5afd0843ecf4a6ff16","text":"Create another function to automatically update your function alias to point to the latest version of the code every time it is updated","correct":false},{"id":"d483464867986a2d830ac23858944891","text":"Create an alias to point to the correct version of your code","correct":false},{"id":"07c1095b47175c42406ba59e91205281","text":"Reference the function using an unqualified ARN","correct":true},{"id":"cc67dedc36d230ab3574dc07b41b7443","text":"Configure the alias to automatically update to point to the latest version of the code every time it is updated","correct":false}]},{"id":"95a7c517-4b4e-4996-b4d4-8ae11f359065","domain":"development","question":"An organization is considering performing canary deployments with their application. Which of the following statements best describes a canary deployment?","explanation":"A canary deploy allows us to gain confidence in an application update by initially just releasing it to a subsection of users. Once we are satisfied that the update is working as expected, the update is then rolled out to the remaining users.\n\nThe concept of a canary deployment is covered in the AWS Well-Architected Framework, and is a feature of API Gateway.\n\nIt can also be performed manually using Route53 Weighted Records, or via an Application Load Balancer with a Forward Action and Weighted Target Groups.","links":[{"url":"https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html","title":"Canary deployment - Well-Architected Framework"}],"answers":[{"id":"0beefb480486ea595083b6d39062c27b","text":"All instances of the original application are stopped, after which new instances of the application are started up in their place. During the transition, there is a short period of downtime.","correct":false},{"id":"18c6683d5f9bd9718e5549310c8647e2","text":"A new version of the application is deployed alongside the existing version. A proportion of application’s traffic is directed to the new application. If, after a given number of minutes, metrics demonstrate that the new version is performing correctly, the remainder of the traffic is moved to the new version.","correct":true},{"id":"03ff12365c5a8fc5fe5dfdbd2f1f00b6","text":"Each instance of the original application is taken out of service one at a time and replaced with the new version of the application. During the deployment, traffic is sent to a mix of the original and new versions.","correct":false},{"id":"d593ae8cd7b7dc0ff31e7a286abc4a60","text":"A new version of the application is deployed alongside the existing version. Once the new version is ready to handle traffic, all traffic is redirected to it.","correct":false}]},{"id":"031bc6f7-17b3-47aa-b449-a97b2ae63aa6","domain":"deployment","question":"Which section of the AWS Serverless Application Model template would you use to describe the configuration of a Lambda function and an API Gateway endpoint, if you were deploying your application using AWS SAM?","explanation":"Use the Transform section to describe your Serverless functions when using the serverless application model. Under the Transform section, you define the resources you want to deploy.","links":[{"url":"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/appendix-appendix-sam-templates-and-cf-templates.html","title":"CloudFormation Resources"},{"url":"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-template-basics.html","title":"AWS SAM Template Concepts"}],"answers":[{"id":"7df96b18c230f90ada0a9e2307226338","text":"Templates","correct":false},{"id":"2ff4148554480a37f85efd299df04850","text":"Transform","correct":true},{"id":"e93acb146e114b5dfa6ce2d12dcb96e4","text":"Functions","correct":false},{"id":"ba0e0cde1bf72c28d435c89a66afc61a","text":"Sam","correct":false}]},{"id":"339adcf9-1e4b-4680-89d7-c123c6b2c310","domain":"deployment","question":"Which DynamoDB feature allows you to set an expiry on table items so that they can automatically be deleted to reduce storage costs?","explanation":"Time To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database. TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","title":"DynamoDB TTL"}],"answers":[{"id":"1f60690ba7c488a02416a7bf195f900b","text":"DynamoDB Streams","correct":false},{"id":"52d5592ad24a2cda379ce38e9c218d65","text":"DynamoDB TTL","correct":true},{"id":"61cc6306baa4c0f3c5fe422a835c2455","text":"DynamoDB AutoDelete","correct":false},{"id":"23b12446c9410404041f4de88841a9c3","text":"DynamoDB Provisioned Throughput","correct":false}]},{"id":"1afa6661-6523-49f7-912c-46b740714117","domain":"deployment","question":"You are deploying a number of Lambda functions using CloudFormation. Which section of the CloudFormation template should you use to define your Lambda resources?","explanation":"Use the Resources section of the CloudFormation template to define the resources you are going to deploy, e.g. EC2 instances, S3 buckets, IAM roles, Lambda functions etc.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"CloudFormation Template Anatomy"},{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-lambda-state-machine-cloudformation.html#lambda-state-machine-cfn-step-2","title":"Example CloudFormation Template"}],"answers":[{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false},{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":true},{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":false}]},{"id":"f89c8e58-528b-49fd-aa15-79de8988d6b3","domain":"refactoring","question":"A developer has just finished amending a Lambda function. Originally, the function ran outside of a VPC, but after the update, it now connects to a VPC. Since the change, part of the function that accesses a HTTPS endpoint on a third-party website has stopped working.\n\nWhat is the most likely cause of the Lambda function no longer being able to access the third-party endpoint?","explanation":"If a Lambda function is connected to a VPC, it must have a route out to the internet via a NAT Gateway or NAT instance in order to connect to external services. You must also ensure that the relevant Security Groups and Network Access Control Lists are configured to allow the required ports.\n\nFrom the perspective of the third-party web server, a request that originates from a Lambda function is no different from any other request, so no changes are needed.\n\nLambda functions connect to a VPC via an Elastic Network Interface. Security controls are therefore managed by network Security Groups and NACLs, not IAM policies. The function's execution role does, however, need permissions to initially connect to the VPC. This can be done with the AWS Managed AWSLambdaVPCAccessExecutionRole policy, for example.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html","title":"Configuring a Lambda Function to Access Resources in a VPC"}],"answers":[{"id":"addf6deaedc46deb808c104b5a9d2cb0","text":"It is not possible to access external web services from a Lambda function that is connected to a VPC. The amend should be rolled back.","correct":false},{"id":"305feb7678afbbe3cc3591f084988eaf","text":"The third-party web server does not support access from a Lambda function that is connected to a VPC. Contact the third-party to request they create a VPC endpoint.","correct":false},{"id":"acf36c68916ef9dff04cf50b1c90ede7","text":"The Lambda's execution role does not have the required permissions. Attach the AWS Managed AWSLambdaInternetAccess policy to the Lambda's execution role.","correct":false},{"id":"3f9ad00efae1c7e9aab7cf03afa538b7","text":"The Lambda function no longer has any route out to the Internet. A NAT Gateway, and associated route, should be added.","correct":true}]},{"id":"3fc48fe5-2386-4e89-ba3e-045136c37de4","domain":"development","question":"You have a legacy application located in your production data centre, which frequently accesses files stored in S3. Due to a significant increase in workload, your application servers are now generating a huge number of requests to your S3 bucket, with many requests now failing. What can you do to improve the situation?","explanation":"Numerous components on a network, such as DNS servers, switches, load balancers, and others can generate errors anywhere in the life of a given request. The usual technique for dealing with these error responses in a networked environment is to implement retries in the client application. Retrying the request with Exponential Backoff technique increases the reliability of the application and reduces operational costs for the developer.","links":[{"url":"https://docs.aws.amazon.com/general/latest/gr/api-retries.html","title":"Error Retries And Exponential Backoff In AWS"}],"answers":[{"id":"895f33b72cf7d11e0e8f0d9fa8ee793c","text":"Configure your application to use Exponential Backoff","correct":true},{"id":"dc46f742e30fa33e5e1c551a3057eecf","text":"Migrate the data to DynamoDB","correct":false},{"id":"b84f4c83038ab1b006eadbd1c591ccaa","text":"Configure your application to read and write to multiple S3 buckets","correct":false},{"id":"40d21c12e8e9a47ba266648c0cd43681","text":"Install a faster network interface in your application servers","correct":false}]},{"id":"075f4eef-bd5d-49b2-ba18-f94cc15c377f","domain":"deployment","question":"Which of the following statements is correct?","explanation":"A primary key can either be a single-attribute partition key or a composite partition-sort key.","links":[{"url":"https://aws.amazon.com/dynamodb/faqs/#Getting_Started","title":"DynamoDB Query Functionality"}],"answers":[{"id":"d7f04a911ddfe5da2f4356ffbd52b25a","text":"In DynamoDB, a primary key can be a single-attribute partition key","correct":true},{"id":"a6bead04f6018113a477c5e6ebb0e82d","text":"In DynamoDB, a primary key can be composite partition/sort key.","correct":true},{"id":"e2240879e0c017447c6a1c59d0abd816","text":"In DynamoDB, a primary key must be a single-attribute","correct":false},{"id":"20cdfac45e66ccd0a06ccea3d171de20","text":"In DynamoDB, a primary key can be a range of values.","correct":false}]},{"id":"25cc389e-1ceb-4b17-b717-9458a1b8e133","domain":"security","question":"One of your junior developers has never had AWS Access before and needs access to an Elastic Load Balancer in your custom VPC. This is the first and only time she will need access. Which of the following choices is the most secure way to grant this access?","explanation":"It's always best practice to grant users access via IAM roles and groups. In this case, we would *not* assign the junior Dev to an existing group, as most Dev groups will have *more* access than is required for this Dev to perform the single task she has been asked to accomplish. Remember - always grant the *fewest* privileges possible.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege","title":"Least Privilege"}],"answers":[{"id":"89c57f09d4cfd1c434b46868a4f3e488","text":"Add that developer to a Group with the requisite access (although that group may have *more* permissions than are needed for the Dev to do her job).","correct":false},{"id":"965dceca3b86aea95ef5de037128780c","text":"None of these.","correct":false},{"id":"f204900100e4e11c7582843062170feb","text":"Create a new IAM user with *only* the required credentials and delete that IAM user after the developer has finished her work.","correct":true},{"id":"2a067a28bf5990259e24ae36fff49ad3","text":"Let her log in with Admin credentials and change the Admin password when she is finished.","correct":false}]},{"id":"f83e6c6c-f206-4918-8457-d6bef8d4a555","domain":"deployment","question":"You have deployed a new version of your Lambda function, however the Application Support team have reported a number of issues with the new code. What is the easiest way to fix this?","explanation":"You can map an alias to different versions of the same function, allowing for easy roll back to a previous version","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda Versions"}],"answers":[{"id":"1275e2de1d5df0a79ed92417584f94e9","text":"Delete the CloudFormation stack and redeploy using the previous version","correct":false},{"id":"8ab78c3f557d33ed014a6819ff45cf93","text":"Troubleshoot the issue using X-Ray, then redeploy an updated version of the function","correct":false},{"id":"80718f268b030c98e8cb509f28eebee4","text":"Roll back by restoring the original function from an EBS snapshot","correct":false},{"id":"bb979a7d188850e77ff0bc3293db2877","text":"Roll back to a previous version by updating your PROD alias to point to the previous version of the function","correct":true}]},{"id":"4a098f64-6cd3-4da4-9c03-e7877d28fb80","domain":"security","question":"Your EC2 instance needs to access a number of files which have been encrypted using KMS. Which of the following must be configured in order for the EC2 instance to successfully read the files?","explanation":"Manage access to KMS keys using a key policy. In the key policy, you must specify the principal (the identity) that the permissions apply to. You can specify AWS accounts (root), IAM users, IAM roles, and some AWS services as principals in a key policy. You can use IAM policies in combination with key policies to control access to your customer master keys (CMKs) in AWS KMS.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/control-access.html","title":"KMS Developer Guide - Access Control"},{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/control-access-overview.html","title":"Managing Access to Your AWS KMS Resources"},{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/iam-policies.html","title":"Using IAM Policies with AWS KMS"}],"answers":[{"id":"070a7d0f2bcb70c9dc90f21deecf341a","text":"The Key Policy must allow the instance role to use the CMK","correct":true},{"id":"3cb9094c5f7738a8b51960d160e7e9fe","text":"The IAM user associated with the application must have a role which has permission run the decrypt operation","correct":false},{"id":"d91e2a356c12a95b1768867410657bd4","text":"The Key Policy must allow the IAM user to use the CMK","correct":false},{"id":"17f84c2da6e5c820c31bc9bc9b8ba783","text":"The EC2 instance must have an instance role which has permission run the decrypt operation","correct":true}]},{"id":"3efc0266-4c2e-11ea-b77f-2e728ce88125","domain":"deployment","question":"Which of the following statements are true about the concept of blue/green deployment regarding development and deployment of your application?","explanation":"With blue/green deployment, you can shift traffic between two identical environments that are running different versions of your application. It allows you to easily deploy changes to your application and roll-back on changes very quickly.","links":[{"url":" https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf","title":"Blue/Green Deployments on AWS"}],"answers":[{"id":"69da7fc6d495def048f27d73e7e55e82","text":"It allows you to shift traffic between two identical environments that are running different versions of your application.","correct":true},{"id":"72f75a1c7558a3b8e4c011917ad6105d","text":"The green environment represents the production environment.","correct":false},{"id":"545fd2a37ac85b63dad4f92c91401436","text":"The green environment represents the staging environment.","correct":false},{"id":"3dae7ad4a3084f0fa3d2598654de9066","text":"The blue environment represents the current version of your application serving production traffic.","correct":false}]},{"id":"44e023dc-183c-4d0a-8c35-a1779dcd7437","domain":"refactoring","question":"Your organization wants you to lead a development project that will perform real-time processing. The application requires the analytics and field teams to respond promptly to emerging situations based on server activity, website clicks, geo-location of devices, people, and service usage. As the development lead, what combination of services would you recommend to build out this project most efficiently and cost-effectively?","explanation":"Streaming data capture and processing is called real-time processing. The best AWS solution in this case is Amazon Kinesis. You can process data captured and stored with Kinesis sequentially and incrementally on a record-by-record basis or over sliding time windows, and use the processed data for a wide variety of analytics including correlations, aggregations, filtering, and sampling. Use AWS Lambda to process streaming data in real time. Lambda can process the data directly from Kinesis Streams, and lets you run code without provisioning or managing servers with help reduce costs. Running regular queries on Redshift is inefficient and expensive. S3 buckets will be able to store data, but won't be able to process data in real-time as efficiently as Kinesis. Amazon Aurora is a relational database solution and does not fulfill the real-time processing requirement.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html","title":"Amazon Kinesis Data Streams Terminology and Concepts"}],"answers":[{"id":"24f2f053a48d8b02ceaae4dc44a376ca","text":"Store the data on Amazon Redshift. Run queries on the Redshift cluster regularly to refresh a dashboard built on Amazon QuickSight.","correct":false},{"id":"f255642584a78d5f043cde30cd580abc","text":"Use Amazon Aurora to store data in real time. Aurora will automatically replicate the data in multiple Availability Zones. Build an application on EC2 that users can call using APIs to retrieve the relevant information they need.","correct":false},{"id":"5a50b0d5b285340be18f4a4c59ba1fd2","text":"Store all data in an S3 bucket with the correct prefixes. Develop Lambda functions for each prefix that will routinely scan and extract necessary information to another S3 bucket that will be the source for an Amazon QuickSight dashboard.","correct":false},{"id":"5d7d039072621b361ce8bde466f0efc4","text":"Use Amazon Kinesis to capture and store streaming data. Process streaming data with Lambda.","correct":true}]},{"id":"ac8156df-b2ea-447a-a070-346b3a5f7234","domain":"security","question":"You are working on an application which handles online credit card applications. It consists of a number of web and application servers running on EC2, customer reference data stored in S3 and transactional data stored in RDS. The security team have noticed that you have a lot of sensitive customer information stored in S3 and you have been asked to configure encryption at rest to protect the data. How can you do this?","explanation":"You can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket. When you use server-side encryption, Amazon S3 encrypts an object before saving it to disk in its data centers and decrypts it when you download the objects.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html","title":"S3 Default Encryption for S3 Buckets"}],"answers":[{"id":"349aac318f19b866d43ee7ad35140312","text":"Use SSL to upload the files","correct":false},{"id":"f7d3a4d76ebd5e4e87874e392c6ee6da","text":"Encrypt the files locally using the AWS Encryption SDK","correct":false},{"id":"46cf5e7bd245efaa706dc8411b60bf17","text":"Encrypt your local root disk before uploading the files","correct":false},{"id":"19fb0f18150f5096d8b66cea58825925","text":"Select default encryption on your S3 bucket","correct":true}]},{"id":"b668531f-edd2-43f5-bf40-b7e68dad0d08","domain":"development","question":"A developer is working on a new green field project within an organization. The developer has been asked to recommend what technology could be used if the project is to be deployed with Elastic Beanstalk.\n\nWhich of the following platforms could the developer recommend for the project to meet its requirements?","explanation":"Elastic Beanstalk currently supports Docker, Ruby, and Go (amongst others). It does not support Perl or Swift.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html","title":"Elastic Beanstalk Supported Platforms"}],"answers":[{"id":"9916d1fc59fe22cc046a2fe1615bc764","text":"Ruby","correct":true},{"id":"5f075ae3e1f9d0382bb8c4632991f96f","text":"Go","correct":true},{"id":"ae832e9b5bda2699db45f3fa6aa8c556","text":"Swift","correct":false},{"id":"0114ad06d728f1834e36fe1a39574ef4","text":"Perl","correct":false},{"id":"c5fd214cdd0d2b3b4272e73b022ba5c2","text":"Docker","correct":true}]},{"id":"c90384c4-a4e3-44cd-909d-9c75fd296455","domain":"security","question":"You are running an application on an EC2 instance. The application needs to be able to access an S3 bucket to read and write data. Which of the following is the best approach to enabling the EC2 instance to access your bucket?","explanation":"Storing credentials in EC2, in the code or in databases is not recommended. Using an IAM role with the requisite permissions and associating that with your EC2 instance is the recommended approach.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html","title":"Using an IAM Role To Allow EC2 Access To S3"}],"answers":[{"id":"cae4292072ddb87ec39678934d1edc5d","text":"Use an IAM role with permissions to read and write to the bucket","correct":true},{"id":"9a58eab1d5aec409abbb6212545b5d45","text":"Store AWS credentials within the application code","correct":false},{"id":"53f6cd0aebbfa4fcbc2c8102ece8c9d8","text":"Store an access key and secret access key in a DynamoDB table","correct":false},{"id":"2e9ca3aed8c9e4854687f9556f8e5b1a","text":"Store AWS credentials locally on the EC2 instance","correct":false}]},{"id":"d5adb3f2-4ddd-4a5e-be37-47d74da0f6d6","domain":"development","question":"You are working on a Serverless application written in Node.js. You updated the Node.js code and uploaded a new zip file containing your code to Lambda. Your application references the function using the alias \"Prod\", however it not seem to be using the new code. Which of the following is likely to fix this?","explanation":"The problem is that the application is referencing the function using an alias pointing to a previous version of the code. When you use versioning in AWS Lambda, you can publish one or more versions of your function. So that you can use different variations of your Lambda function in your development workflow such as development, beta, and production. Lambda also supports creating aliases for each of your Lambda function versions. Conceptually, an AWS Lambda alias is a pointer to a specific Lambda function version. You can update aliases to point to different versions of functions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda Function Versioning and Aliases"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/how-to-manage-versioning.html","title":"Lambda Versioning"}],"answers":[{"id":"6a4ac8c4aa2076312300dc09fa5c0f21","text":"You need to call the function using $LATEST","correct":false},{"id":"1ab8bb5a6ed0944df7316a0ae848727f","text":"You need to update the alias to reference the new version of your function","correct":true},{"id":"68796527ce97010c4a40777a6462b3fc","text":"You need to call version 2 of the function","correct":false},{"id":"33064ca2d30b93ce4d46769270b3e34e","text":"You need to update your application to use an unqualified ARN","correct":false}]},{"id":"f5072793-928c-4fc3-8ce0-bd18571b6765","domain":"deployment","question":"You are developing a gaming website which scores all players scores in a DynamoDB table. You are using a Partition key of user_ID and a Sort Key of game_ID as well as storing the user_score which is the user's highest score for the game and also a timestamp. You need to find a way get the top scorers for each game, who have scored over 50,000 points. Which of the following will allow to to find this information in the most efficient way?","explanation":"A scan operation would be less efficient than a query, so that is definitely not the most efficient way. The Query operation described won't help you find the top scorers for each game. A local secondary index is an index that has the same partition key as the base table, but a different sort key. A global secondary index is an index with a partition key and a sort key that can be different from those on the base table.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html","title":"Queries and Scans"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html","title":"DynamoDB Indexes"}],"answers":[{"id":"eae4d20326b1ecd862aed8669ad068ca","text":"Use a global secondary index with a partition key of game_ID and a sort key of user_ID","correct":false},{"id":"1e267d3212493fc534c9b2e9de826341","text":"Use a local secondary index with a partition key of user_ID and a sort key of user_score","correct":true},{"id":"efcb11f92c2fff3dca6935cb74653ad7","text":"Scan the table and order by score","correct":false},{"id":"7b9684c68079e7ca02fe44efb81fa0eb","text":"Query the table using a partition key of user_ID and sort by game_ID","correct":false}]},{"id":"6d133985-53e5-48d7-a1e9-6db8bb940208","domain":"security","question":"You are working as a Developer for an online retailer. Your Security Architect has requested that any files stored in S3 must be encrypted. However some teams are continuing to upload their files without encrypting them. Which of the following will ensure that only encrypted data is uploaded?","explanation":"There are a few different ways to enforce encryption, however from the provided options, the use of a bucket policy to reject requests that do not include encryption in their header is the best answer","links":[{"url":"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/","title":"How to Prevent Uploads of Un-encrypted Objects to Amazon S3"}],"answers":[{"id":"da402b6935d7bb0b6868419dc1fcf54f","text":"Tell all team members to include the x-amz-encryption parameter in request header","correct":false},{"id":"0f6a3c5c345549f63a0da92ac93e45cf","text":"Use a bucket policy that only allows PUT operations which include the x-amz-server-side-encryption parameter in the request header","correct":true},{"id":"d61960e2fc2a82679514fe6f3401150f","text":"Select the Encrypted Files Only checkbox in the S3 Permissions tab in the AWS console","correct":false},{"id":"e7d6b8da11c238d437edeadb18bf5493","text":"Create a bucket ACL that only allows PUT operations which include the x-amz-encryption parameter in request header","correct":false}]},{"id":"4102ce86-6e8c-47b2-86d4-86e3b8b08558","domain":"mon-trb","question":"Your application runs in an Auto Scaling group to scale based on user demand. The Auto Scaling group runs behind an Elastic Load Balancer (ELB). When you check the ELB logs, you notice that a number of instances are failing the health check during periods of high demand. New instances are launching but they periodically fail health checks and subsequent instances are being launched which is increasing costs. What would you do to troubleshoot this issue?","explanation":"Amazon EC2 Auto Scaling waits until the health check grace period ends before checking the health status of the instance. Amazon EC2 status checks and Elastic Load Balancing health checks can complete before the health check grace period expires. However, Amazon EC2 Auto Scaling does not act on them until the health check grace period expires. To provide ample warm-up time for your instances, ensure that the health check grace period covers the expected startup time for your application. In this scenario, the health checks are most likely occurring before the EC2 instance and its applications have fully loaded, so increasing the health check grace period would likely resolve the issue. The cooldown period helps to ensure that your Auto Scaling group doesn't launch or terminate additional instances before the previous scaling activity takes effect, and are not used for health checks. Creating a new ELB or Auto Scaling group would have no impact but the problem would persist. AWS Config is a governance and management tool and is incapable of itself executing automated actions.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Checks for Auto Scaling Instances"}],"answers":[{"id":"1fec64ae8a7c39cb26d12b3f50438dde","text":"Increase the cooldown period of the Auto Scaling group.","correct":false},{"id":"fe8f15e29da594ce0b07d192b5e8f4f3","text":"Use AWS Config to monitor instances with failed health checks to terminate them.","correct":false},{"id":"47d484cdd97dff7b054f3cc746f8bbc6","text":"Create a new Auto Scaling group behind a new ELB. The current ELB is malfunctioning.","correct":false},{"id":"828102b2cd5f2f5f2873afea3714c7eb","text":"Increase the health check grace period.","correct":true}]},{"id":"55fbda31-57ba-4009-a6da-09f690b40351","domain":"security","question":"You are developing an application which will use Cognito to allow authenticated Facebook users to sign-in and use your application. You would like to use Cognito to handle temporary access allowing authenticated users to access product and transaction data that your application stores in S3 and DynamoDB. Which is the best approach?","explanation":"Cognito is the recommended approach for user sign-up and sign-in for mobile applications which allow access to users with Facebook, Google or Amazon.com credentials. Identity pools enable you to grant your users temporary access to AWS services. User pools are user directories that provide sign-up and sign-in options for your app users.","links":[{"url":"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html","title":"What Is Cognito?"}],"answers":[{"id":"67a47bfe3685e0b65597b1bb14c5426f","text":"Configure an IAM User Group to provide temporary AWS credentials to your users to allow temporary access to AWS resources","correct":false},{"id":"6e4d8be48c9bbd213381e12f94eaa9a7","text":"Configure a SAML 2 Federation to provide temporary AWS credentials to your users to allow temporary access to AWS resources","correct":false},{"id":"e83779413869ed1569c2eb035ca21ef9","text":"Configure an Identity Pool to provide temporary AWS credentials to your users to allow temporary access to AWS resources","correct":true},{"id":"edac71e5f6a783c051f80c7369a90c0d","text":"Configure a User Pool to provide temporary AWS credentials to your users to allow temporary access to AWS resources","correct":false}]},{"id":"f956f10b-d615-4d75-a2a4-dd978268221b","domain":"deployment","question":"You are a developer running an application on AWS Elastic Beanstalk. You are implementing an application update and need to use a deployment policy. The requirements are to maintain full capacity, deploy five instances at once for the new version, and to terminate instances running the old version once the new instances are running successfully. How would you implement this deployment policy?","explanation":"To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances. All at Once deployment takes the instances in your environment out of service for a short time. Rolling deployment also takes a batch of servers out of service while deploying the new version in batches. Blue/Green deployments are for cases when you want to have two versions live simultaneously and be able to swap between the two versions.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html","title":"Deployment Policies and Settings"}],"answers":[{"id":"da598c0ec61581331132eb0291fa30cd","text":"Set the deployment policy as Blue/Green. Set the timeout as 900, and the batch size as 5.","correct":false},{"id":"e693d0e841ddc0314ab81115baff0563","text":"Set the deployment policy as Rolling with Additional Batch. Set the batch size type as Fixed, and the batch size as 5.","correct":true},{"id":"ee833ae676b93a46ca5aafc9ce391102","text":"Set the deployment policy as Rolling. Set the batch size as 5.","correct":false},{"id":"4a909b7335924524609c65e522fe581a","text":"Set the deployment policy as All at Once. Set the batch size type as Fixed, and the batch size as 5.","correct":false}]},{"id":"6f11e8ea-c824-427c-9cf9-7bebec4fe4b6","domain":"development","question":"Where should the appspec.yml be stored?","explanation":"The AppSpec file (appspec.yml) must always be in the root or your application source directory otherwise the deployment will not work. The .ebextensions folder is used to set custom environment variables in Elastic Beanstalk, not CodeDeploy.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html","title":"CodeDeploy AppSpec File"}],"answers":[{"id":"57f3232fcb44408176cbdb5c8b7e4b06","text":"In /opt","correct":false},{"id":"b007c30133faeba98cd0fbcedbf4fa6c","text":"In the root of your application source directory","correct":true},{"id":"0a2d3304fa88233c14e8c9ebffba0882","text":"In the .ebextentions folder","correct":false},{"id":"8ee84dac450c4097f3cce035b8ede57d","text":"In the config directory in your application source directory","correct":false}]},{"id":"fef4400a-946f-4f47-8a06-1d8cac177396","domain":"mon-trb","question":"You are attempting to list the objects contained in an S3 bucket. The bucket contains over 3000 objects and the list-objects command times out and does not complete successfully, however when you run the same command on a different bucket, it works without errors. What could be the reason for this?","explanation":"If you see issues when running list commands on a large number of resources, the default page size of 1000 might be too high. This can cause calls to AWS services to exceed the maximum allowed time and generate a \"timed out\" error. You can use the --page-size option to specify that the AWS CLI request a smaller number of items from each call to the AWS service. The CLI still retrieves the full list, but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call. This gives the individual calls a better chance of succeeding without a timeout. Changing the page size doesn't affect the output; it affects only the number of API calls that need to be made to generate the output.","links":[{"url":"https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html","title":"AWS CLI Pagination"}],"answers":[{"id":"abf2e493ee10caa92d340b7832cc908f","text":"You do not have the required permission to run the list-objects command on the bucket.","correct":false},{"id":"12b3ee65ff4239dcb2e54ada9d0307d5","text":"You are running the command on a bucket which contains a large number of resources, and the default page size might be need to be increased.","correct":false},{"id":"c105a5c0f66ee44cff21cb53bd7ec1e5","text":"You are running the command on a bucket which contains a large number of resources, and the default page size might be too high.","correct":true},{"id":"fe6ecb35fc38b38806053192af45aadd","text":"The command is generating too many API calls due to the large number of objects in the bucket.","correct":false}]},{"id":"f6676496-9451-49c5-b9a0-d9f6db7a2b42","domain":"development","question":"Which of the following Elastic Beanstalk deployment approaches allow you to maintain full capacity while performing an update?","explanation":"Rolling with Additional Batch and Immutable both involve provisioning new servers to ensure capacity is not reduced. All At Once means the application will be offline for the duration of the update. Performing a Rolling Update without an additional batch of servers means a reduction in capacity.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html","title":"Elastic Beanstalk Deployments"}],"answers":[{"id":"f4920797afb92022a9c6608efcd86317","text":"Rolling","correct":false},{"id":"11efd9ae6f76e706e3f1b34d97584ebc","text":"Immutable","correct":true},{"id":"caf75ce833223dcb2c5cfbbcad2ec02a","text":"All At Once","correct":false},{"id":"431fffe43dba24b87f7ce578ca6f418c","text":"Rolling With Additional Batch","correct":true}]},{"id":"00544151-fdaf-446d-ade3-7d6988c9133d","domain":"deployment","question":"You are building a distributed application, which is made up of a number of Docker instances running on an ECS cluster. You would like to configure your application to send data to X-Ray. Where should you install the X-Ray daemon?","explanation":"In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. X-Ray provides an official Docker container image that you can deploy alongside your application.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html","title":"Running the X-Ray daemon on Amazon ECS"}],"answers":[{"id":"5844e2a52b5fd5bfeeab3e714e68ba83","text":"Update the Docker image to include the X-Ray daemon and provision the new version of the application.","correct":false},{"id":"981d352ae2b571b99295e2880457f235","text":"Create a separate Docker image to run the X-Ray daemon.","correct":true},{"id":"63a693207215523784e22cf504e4579b","text":"Install the X-Ray daemon on the same Docker container where the application code is running.","correct":false},{"id":"bfdfc3a1dc6179ede8ebcc926faacd94","text":"Install the X-Ray daemon on the EC2 instance where your Docker containers are running.","correct":false}]},{"id":"91e59ee8-028b-44f1-9d03-1081d09738d3","domain":"development","question":"A clothing company needs to build a REST service to allow salespeople quick access to stock levels. The service must be accessible from an HTTP request. Which of the following solutions addresses the company's requirements?","explanation":"In an AWS Lambda integration in Amazon API Gateway, the HTTP method request from the client is mapped to a backend Lambda function invocation. Depending on your use case, you may choose to use Lambda proxy integration, Lambda non-proxy integration, or both in your API Gateway API. In a Lambda proxy integration, the entire client request is sent to the backend Lambda function as is, except that the order of the request parameters isn't preserved. In a Lambda non-proxy integration (also called a custom integration), you configure the way the parameters, headers, and body of the client's request are translated into the format that your backend Lambda function requires. ","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-getting-started-with-rest-apis.html","title":"Create a REST API with Lambda Integrations in Amazon API Gateway"}],"answers":[{"id":"01f2c91643488c78436cc962fca2f2d7","text":"Amazon EC2 and AWS Auto Scaling","correct":false},{"id":"d87496b040e2581c8e89e43a7e5ed235","text":"Amazon SQS and DynamoDB","correct":false},{"id":"e8445389785666bd90da881eada7e373","text":"Amazon CloudFront and Amazon S3","correct":false},{"id":"e9f6666f3057d0c266ac855cbae770b8","text":"Amazon API Gateway and AWS Lambda","correct":true}]},{"id":"8ee26b88-194a-4069-85a1-e28a48bcca27","domain":"security","question":"An organization has mandated that all data within its DynamoDB tables must be encrypted at rest using an AWS owned key. What must a developer do to ensure this?","explanation":"All DynamoDB tables are encrypted at rest using an AWS owned CMK by default. Non-encrypted DynamoDB tables are no longer supported in AWS. You have the option to pick an alternative AWS or Customer Managed KMS key if required.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html","title":"DynamoDB Encryption at Rest"}],"answers":[{"id":"2a13575a2661e32db0bdf9ad7b95ffee","text":"This cannot be done; DynamoDB does not support encryption at rest.","correct":false},{"id":"852abdb4878eba09a03ca6e37eb1dd16","text":"Enable DynamoDB encryption and select AWS managed CMK.","correct":false},{"id":"92c03c315e7b990388532214f2a73c62","text":"Enable DynamoDB encryption and select AWS owned CMK.","correct":false},{"id":"5744856c94411674b105aff56f20a6a3","text":"There's no need to do anything; all DynamoDB tables are encrypted at rest with an AWS owned key by default.","correct":true}]},{"id":"56a3a6cc-4c72-4b6c-a06c-4c310d107296","domain":"development","question":"An application successfully updates an existing object in S3. When checking the file contents, the developer does not see the updated file contents. What is the cause of this issue?","explanation":"Amazon S3 offers eventual consistency for overwrite PUTS and DELETES in all Regions. Amazon S3 provides high availability and high durability by replicating bucket objects across multiple availability zones and servers. This means that any updates to objects must replicate across all servers storing the data. This can take some time. Therefore, any updates to existing objects (using POST or DELETE), will take some time to be propagated across all of S3, and hence are eventually consistent.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html","title":"Introduction to Amazon S3"}],"answers":[{"id":"a693ecd0f4b5da724ee692d2059fc4c3","text":"S3 bucket policy permissions were not correct.","correct":false},{"id":"4756256f32016be9d23caa16a75bb9c5","text":"HTTP 200 response code was not received.","correct":false},{"id":"47ad6c66cb5b231f66170a797fbb7782","text":"Overwrite PUTS in S3 have eventual consistency.","correct":true},{"id":"57d934f14c5f166d5f0604507d795cc0","text":"S3 Bucket Versioning was not enabled.","correct":false}]},{"id":"322048b9-4d49-4c64-8aca-c7479619982a","domain":"mon-trb","question":"Your company is reaching the end of the financial year and the Finance team are running a lot of large database queries and scans against your DynamoDB tables. The database queries and scans are taking much longer to complete than expected, how can you make them more efficient?","explanation":"Reducing page size for queries and running scans in parallel are both recommended approaches for making DynamoDB operations more efficient. DynamoDB uses eventually consistent reads by default and filtering the results will not improve efficiency","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html","title":"Best Practices for Querying and Scanning Data"}],"answers":[{"id":"867b7b3ff3ddfb076ff319ce80e543c5","text":"Run parallel scans","correct":true},{"id":"e131078f81b76b76be2718610cc739a9","text":"Set your queries to be eventually consistent","correct":false},{"id":"60e26c5aacc3bba5083560f51b354c87","text":"Reduce the page size to return fewer items per results page","correct":true},{"id":"c6c36956db297a402f3fef1bf83c4c7e","text":"Filter your results based on the Primary Key and Sort Key","correct":false}]},{"id":"b9987e32-c46c-4c78-b9db-e9a5283d7b16","domain":"development","question":"You are planning to use CodeDeploy to deploy an application for the first time to a brand new fleet of EC2 instances. Which deployment approach would you recommend?","explanation":"In-Place is the one to use as you are installing to a new fleet of instances, therefore Blue/Green is not possible. Canary and Rolling updates are not an option for CodeDeploy","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html","title":"Working with Deployments in AWS CodeDeploy"}],"answers":[{"id":"53b8ba497ea2cdea89f60da12d94b46d","text":"In-Place","correct":true},{"id":"3a27747f75c4e73e94223a9e4065cd9c","text":"Blue / Green","correct":false},{"id":"ecf715d6d79a2698b7fec0357f9d721f","text":"Canary","correct":false},{"id":"ff2713a6181db42fded101c670bbd0dd","text":"Rolling with additional batch","correct":false}]},{"id":"4750f3bd-de92-4efb-ad08-06c9ea71eccb","domain":"refactoring","question":"Kinesis allows consumer applications to consume records in which order?","explanation":"Kinesis gives you the ability to consume records according to a sequence number applied when data is written to the Kinesis shard","links":[{"url":"https://aws.amazon.com/kinesis/data-streams/faqs/","title":"Kinesis FAQ"}],"answers":[{"id":"b1e05f0db70da1b8dee8592d942ed2e1","text":"According to the timestamp assigned when the record is written to the stream","correct":false},{"id":"689f6f887e0c13fb07b437de23303cac","text":"Last In First Out","correct":false},{"id":"cb1ab21304b72197113dbe18c491e9c2","text":"According a sequence number assigned when the record is written to the stream","correct":true},{"id":"e71e8f5a50819b4773c68991d0c9f602","text":"Records are processed in no particular order","correct":false}]},{"id":"8f888c41-8c19-4f1a-8682-7a8f8235182a","domain":"security","question":"A developer is working on a new HR application that must be able to encrypt sensitive documents, each of which is approximately 100 MB in size. The encryption needs to take place within the HR application, and each document must be encrypted using a unique key. The developer has decided to use envelope encryption, and KMS to manage their keys.\n\nWhat KMS operation should be called for each document, to most efficiently meet the requirements of the HR application?","explanation":"generate-data-key returns a plaintext data key, ready to be used to encrypt a document, and a ciphertext version of the key, encrypted using the Customer Master Key. The command should be called for each document, so a different key is used for each. Once the document is encrypted, the plaintext key is securely discarded, and the encrypted data key is stored along with the encrypted document.\n\ngenerate-data-key-without-plaintext is incorrect because it is the plaintext key that is used to encrypt the document within the application.\n\ngenerate-random is incorrect as while the response could be used as a data key; a second step would also be needed to acquire the ciphertext version of the key. It is, therefore, not the best solution.\n\nencrypt is incorrect because it can only encrypt up to 4 kilobytes of data, and because the encryption process itself would take place within KMS, directly using the Customer Master Key, not within the application. To that end, using 'encrypt' directly does not fall under AWS's definition of envelope encryption.","links":[{"url":"https://docs.aws.amazon.com/cli/latest/reference/kms/generate-data-key.html","title":"KMS CLI: generate-data-key"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/kms/encrypt.html","title":"KMS CLI: encrypt"}],"answers":[{"id":"0b6eb26a9e685b2edba22d0b9f8534a3","text":"generate-data-key","correct":true},{"id":"966a36d051e3b8290953bce53c3513bf","text":"generate-random","correct":false},{"id":"f71001650370495dc4bb6bccf8fc3ac9","text":"generate-data-key-without-plaintext","correct":false},{"id":"53c82eba31f6d416f331de9162ebe997","text":"encrypt","correct":false}]},{"id":"43e8c8e5-d0e3-4502-b7e7-8c236a6625e3","domain":"mon-trb","question":"A company wants to monitor all traffic to a network interface on their bastion host. They wish to be alerted if there are more than 10 attempts to connect to the host via SSH within a one-hour time interval. What solution can the company employ to meet this requirement?","explanation":"VPC flow logs can be sent to CloudWatch Logs. A CloudWatch metric filter and alarm can be configured to send notifications when the specified criteria are satisfied. CloudTrail is not a supported destination for VPC flow logs. Amazon Inspector cannot be used to inspect network traffic in the way specified by the requirements. It performs vulnerability assessments on the host VM. Lambda functions cannot mount EBS volumes.","links":[{"url":"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html","title":"Amazon S3 Glacier Vault Lock"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-cwl.html#flow-logs-cwl-create-flow-log","title":"Creating a Flow Log That Publishes to CloudWatch Logs"}],"answers":[{"id":"6e51df099a849474d4791bce4aa25bb5","text":"Configure a VPC flow log with CloudWatch Logs as the destination. Create a CloudWatch metric filter for destination port 22. Create a CloudWatch Alarm trigger.","correct":true},{"id":"343b93587a12ff88f4e59413fbbd5d96","text":"Install the Amazon Inspector agent on the bastion host. Configure CloudWatch alerts based on Amazon Inspector findings.","correct":false},{"id":"cdab2e49cfa676ebe99e79d8f77f49b3","text":"Create a Lambda function that mounts the bastion host EBS volume and sends logs to CloudWatch logs. Create a CloudWatch metric filter for destination port 22. Create a CloudWatch Alarm trigger.","correct":false},{"id":"ecb0bcaac946feb8b9a9c2abcaace499","text":"Create a VPC flow log for the network interface with CloudTrail as the destination. Create a Lambda function that queries the CloudTrail logs for SSH login attempts. Trigger the Lambda function every 5 minutes with a scheduled CloudWatch event.","correct":false}]},{"id":"616a7b1c-bc17-42a4-b361-74af9a86607f","domain":"development","question":"A financial services company is implementing a payments processing application utilizing DynamoDB tables for its data store. To process payments, the application needs to perform a write operation on a sequence of items, and roll back and reverse all operations in case of any one faulty operation.  What is the best method to accomplish this requirement?","explanation":"DynamoDB transactions feature provides ability to group multiple items into a single atomic transaction and perform all-or-nothing coordinated operations.  This can be done programmatically using the TransactWriteItems operation. The BatchWriteItem operation does not meet the question requirements as it does not guarantee that the actions will be performed on all items as a single atomic coordinated operation. It is possible that only some of the actions in the batch succeed while the others do not. Updating the payments application is not the ideal solution.  It requires application code change and tracking all connected operations and reversing them as required is not trivial to implement.  Using the native transaction ability provided by DynamoDB is a better option.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html","title":"Amazon DynamoDB Transactions: How It Works"}],"answers":[{"id":"aa5647f8e2c8e7147097b79d2b2a5555","text":"Use the BatchWriteItem operation.","correct":false},{"id":"758e6d47da6512d38526cfcfa39bb5c1","text":"Update the application to manage and perform roll-back operations.","correct":false},{"id":"3b08e25699a7082805a9be123c9acbbb","text":"DynamoDB does not support atomic transactions.  Use relational database (such as RDS) that supports atomic transactions.","correct":false},{"id":"cb748237c6e223d0f4506cff55c6b259","text":"Use the TransactWriteItems operation.","correct":true}]},{"id":"b170b108-ed74-40ba-a811-383fa049982d","domain":"mon-trb","question":"You receive a \"timed out\" error message when running a command using the AWS CLI. What could be a possible reason for this?","explanation":"If you see issues when running list commands on a large number of resources, the default page size of 1000 might be too high. This can cause calls to AWS services to exceed the maximum allowed time and generate a \"timed out\" error. You can use the --page-size option to specify that the AWS CLI request a smaller number of items from each call to the AWS service. The CLI still retrieves the full list, but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call. This gives the individual calls a better chance of succeeding without a timeout. Changing the page size doesn't affect the output; it affects only the number of API calls that need to be made to generate the output.","links":[{"url":"https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html","title":"AWS CLI Pagination"}],"answers":[{"id":"afdc0b8222f758f2d5101f7516eac7b5","text":"Your network connection is too slow","correct":false},{"id":"6671b1dd34271b2e7f713a895d4dc800","text":"The AWS service that you are trying to call is unreachable","correct":false},{"id":"9a732fb1da270d734f8c6fd13ec818f7","text":"The AWS service that you are trying to call is taking too long to respond","correct":false},{"id":"693a2ee98a96c9712d1720d141c29c5b","text":"You have run a command which is trying to return a large number of items and has exceeded the maximum allowed time to return results","correct":true}]}]}}}}
