{"data":{"createNewExamAttempt":{"attempt":{"id":"bb693afa-edd8-471e-bea3-f35a892359dc"},"exam":{"id":"1d285e33-ee09-4032-a77d-1c1db81cb5d8","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"c2e1b0c2-2dd7-479b-a78e-8ddd1c6d2448","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a problem.  The client has several Ubuntu Linux servers in a private subnet within a VPC.  The servers are configured to use IPv6 only and must periodically communicate to the Internet to get security patches for applications installed on them.  Unfortunately, the servers are unable to reach the internet.  An internet gateway has been deployed in the public subnet in the VPC and default routes are configured.  Which of the following could fix the issue?","explanation":"With IPv6 you only requires an Egress-Only Internet Gateway and an IPv6 route to reach the internet from within a VPC.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/configure-private-ipv6-subnet/","title":"Configure Private IPv6 Subnets"}],"answers":[{"id":"3528a1941badb3d7623a6df18c766d0a","text":"Create an Internet Gateway in the private subnet and configure the default route for the private subnet to the gateway.","correct":false},{"id":"3bd793f68f596bb23004279fc494f9a6","text":"Implement an Egress-only Gateway in the public subnet and configure an IPv6 default route for the private subnet to the gateway.","correct":true},{"id":"024977f9219f0562163fc06724bc8c99","text":"Implement a Classic Load Balancer in front of the servers and register the servers to the load balancer.","correct":false},{"id":"5250cb1aeb48afd18fee3003d4798203","text":"Assign IPv6 EIP's to the servers and configure a default route to the existing internet gateway","correct":false},{"id":"8afd1250611c55c8d3301faba9716c97","text":"Implement a NAT Instance in the public subnet and configure the instance as the default route for the private subnet.","correct":false}]},{"id":"3440b6ff-6fe9-495d-a765-f69f6b82a628","domain":"awscsapro-domain3","question":"You work for a health record management company which operates a view-only portal for end-users to check their health records online. Users can also raise disputes if anything is incorrect. This 2-tier website that supports only HTTPS will be moved to the AWS Cloud. There are 5 web servers on-premises and an F5 Load Balancer that controls traffic to these web servers. SSL is terminated at the web servers. An Oracle Real Application Cluster (RAC) serves as the database tier. Due to the sensitive nature of personal health information, the web site uses mutual authentication - the server requests browsers for a valid client certificate before establishing a trusted session.\nSelect two alternate working architectures for this application to be migrated to AWS so that code changes are minimized. Choose two responses each of which can act as an independent and functional solution, and also result in minimum application code changes.","explanation":"The areas tested by this question are as follows.\nFirst, if an ELB is not terminating SSL, its listener protocol cannot be HTTPS - it has to be TCP. Additionally, AWS ELB does not support terminating client-side certificates. Therefore, if a website requires client SSL certificates, and if it also uses AWS ELB, the ELB must let the target EC2 instances to terminate SSL and validate the client certificate. This requires the protocol to be TCP/443. Both these facts (one, SSL is not terminated at the Load Balancer level, and two, mutual authentication is used) are stated explicitly in the question so that the candidate identifies at least one of them and is thus able to conclude that HTTPS is not the correct protocol choice for ELB. Hence, amongst the two choices that use ELB, the one that says TCP is the correct one.\nSecondly, RDS does not support Oracle RAC. Hence, the database tier must use EC2 instances. Thus, for the second alternate solution that uses Route 53 multi-value answer records instead of ELB, we should select the option that deploys EC2 instances for the database tier.","links":[{"url":"https://forums.aws.amazon.com/thread.jspa?threadID=109180","title":"Discussion Forums - HTTPS Client certificate validation while using client ELB"},{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"RDS FAQ-s, search for phrase - Is Oracle RAC supported on Amazon RDS"}],"answers":[{"id":"e7a1d88af1880a82062437a4c1005adf","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":true},{"id":"11a4ad1f948d7b2fd3631cb763fa8a00","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring TCP/443 as listener","correct":true},{"id":"307f49d40547367c203a0fcfa1a46be8","text":"Migrate the database to RDS Oracle and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":false},{"id":"7250b61db9c9e69abf4f9e7bd2bfb268","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring HTTPS/443 as listener","correct":false}]},{"id":"ffae5615-188b-4023-aae2-71270158730a","domain":"awscsapro-domain5","question":"Several teams are using an AWS VPC at the same time. The VPC has three subnets (subnet-1a, subnet-1b, subnet-1c) in three availability zones (eu-west-1a, eu-west-1b, eu-west-1c) respectively. As there are more and more AWS resources created, there is a shortage of available IP addresses in subnet-1a and subnet-1b. The subnet subnet-1c in availability zone eu-west-1c still has plenty of IP addresses. You use a CloudFormation template to create an Auto Scaling group (ASG) and an application load balancer for the ASG. You enable three availability zones for the load balancer and the Auto Scaling group also spans all the three subnets. The CloudFormation stack usually fails to launch because there are not enough IP addresses. High availability is not required for your project since it is a proof of concept. Which of the following methods is the easiest one to resolve your problem?","explanation":"The easiest way is to enable only the eu-west-1c availability zone for ELB and launch ASG instances in subnet-1c. Only the IP addresses from eu-west-1c are required to create the resources. For a VPC, the existing CIDR cannot be modified and you also cannot add another CIDR IP range to an existing subnet. For an application load balancer, there is no IP balancing feature and IP addresses from a subnet cannot be reserved by other subnets.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-subnets.html","title":"Availability Zones for Your Application Load Balancer"}],"answers":[{"id":"1f29e88227ce0771f9e43093bd53583a","text":"Add more IP addresses by extending the CIDR range for the VPC. Create a new subnet in each availability zone and reserve at least 128 IP addresses in a subnet. Modify the CloudFormation template to use the new subnets for the Auto Scaling group and ELB.","correct":false},{"id":"94887cfd8a06ae986d5721a940c3c52b","text":"Modify your CloudFormation template to enable only the availability zone eu-west-1c for the application load balancer and launch the Auto Scaling group in subnet-1c which belongs to eu-west-1c.","correct":true},{"id":"b8c040b89e07e5150d4a8a3899d41659","text":"Add another IPv4 CIDR to the VPC which should have at least 256 IP addresses. Add another IP CIDR block to subnet-1a and subnet-1b to increase the available IP addresses.","correct":false},{"id":"9cb12513be8578fb017ebb7d1b302f9a","text":"Enable the IP balancing feature in the application load balancer so that the IP addresses are equally distributed among subnets. When elastic load balancer is created, available IP addresses in one subnet can be reserved by other subnets.","correct":false}]},{"id":"edb30172-3f76-4423-a6bb-78a3d2fdeb42","domain":"awscsapro-domain2","question":"Your team is managing hundreds of Linux and Windows EC2 instances in different environments such as development, QA, staging and production. You need a tool to help you automate the process of patching instances so that the operating systems have latest patches and meet the compliance policies. You want to manage the patching in different groups depending on the environment. For example, patches should be deployed and tested in the QA environment first before the production environment. How would you achieve this requirement through an AWS service?","explanation":"AWS Systems Manager Patch Manager is the most appropriate tool to manage the patching for large groups of EC2 or on-premises instances. For different environments, users can configure patch groups using the \"Patch Group\" tag and then establish a patch baseline for each patch group. It is better to manage instances with the \"Patch Group\" tag rather than other customized tags. AWS SSM Session Manager and AWS SSM Run Command are not suitable to deploy patches across a large number of instances. The AWS-RunRemoteScript command is also incorrect as it is used to execute scripts stored in a remote location.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-patchgroups.html","title":"Patch Manager Patch Groups"}],"answers":[{"id":"32ac8d27221ec8e699eb4e872d3f6ed0","text":"Centrally manage the instances in AWS SSM Managed Instances and divide them into different categories. Perform the patching activities from AWS SSM Session Manager in a maintenance window.","correct":false},{"id":"6c4763d45c6cb24622b0db9533f95e0c","text":"Create environmental tags in EC2 instances such as a tag key named \"env\". In AWS SSM Patch Manager, configure patching activities by selecting the instances using the tag. Patch on the QA environment first and perform the necessary testing.","correct":false},{"id":"08b9feba2a12ba67141a0ffe02938798","text":"Add patch group tags in the EC2 instances. Perform the patching using the command AWS-RunRemoteScript in AWS SSM Run Command. Patch on the QA environment first by selecting the QA patch group tag.","correct":false},{"id":"b99f4271e073e1e030f3c26c383c5959","text":"In AWS Systems Manager Patch Manager, create different patch groups using the tag key \"Patch Group\" and configure a patch baseline for each patch group. Schedule the patching in a maintenance window by selecting a patch group.","correct":true}]},{"id":"12556935-2b08-4f73-b7f6-6b82bd7fa1a0","domain":"awscsapro-domain1","question":"Your company has recently acquired another business unit and is in the process of integrating it into the corporate structure.  Like your company, the acquisition's IT assets are fully hosted on AWS.  They have a mix of EC2 instances, RDS instances and Lambda-based applications but these will become end-of-life as the new business unit transitions to your company's standard applications over the next year.  Fortunately, the CIDR blocks of the respective VPCs do not overlap.  If the goal is to integrate the new network into your current hub-and-spoke network architecture to provide full access to each other's resource, what can you do that will require the least amount of disruption and management?","explanation":"VPC Peering provides a way to connect VPCs so they can communicate with each other.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/peering/invalid-peering-configurations.html","title":"Unsupported VPC Peering Configurations - Amazon Virtual Private Cloud"}],"answers":[{"id":"b4cd9cd07c83868a626224de0d92c70a","text":"Initiate a Transitive Peering request from each new VPC to your hub VPC.  Configure routes in the hub to direct traffic to and from the new VPCs.","correct":false},{"id":"d8d7beaa505f84c757a93d64f70a0ae7","text":"Initiate a VPC peering request from each of your spoke VPCs to each new VPC.  Configure route tables in each spoke VPC and new VPC to route traffic to the respective VPC.","correct":false},{"id":"6088d94d774b3a3e9be7fc71331181d1","text":"setup a VPN connection via Internet Gateway between each new and existing VPC to create a mesh network.  Update route tables to direct traffic to the appropriate VPC.","correct":false},{"id":"0d3332dc947d3085b33697562a70c9f5","text":"Initiate a VPC peering request between your hub VPC and all VPCs from the new business.  Setup routes in the hub to direct traffic to and from the new VPCs.","correct":true},{"id":"8e438cf633d07a0d70f2d21978e083bb","text":"Configure a VPC Gateway Endpoint for EC2, RDS and Lambda services.  Configure route tables in your existing VPCs to use the endpoints to communicate with the new VPCs.","correct":false}]},{"id":"374fde7d-232a-4cfe-b5d6-7755d564c6ca","domain":"awscsapro-domain1","question":"You are consulting for a company that has decided to partially migrate some resources to AWS from their two data centers (DC1 and DC2).  Their first order of business is to design a robust, redundant and cost-effective network connection between their data centers and AWS.  They already have redundant links between DC1 and DC2.  Which of the following architectures provides the highest availability at the least cost?","explanation":"A common and cost effective way to provide a redundant link to AWS with Direct Connect is a VPN connection.  In the event that the Direct Connect path fails at DC1, your on-prem router can redirect traffic over the VPN at DC2 via the DC1-DC2 link.  Having dual Direct Connect links is definitely redundant but more expensive than a VPN.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/","title":"Multiple Data Center HA Network Connectivity – AWS Answers"}],"answers":[{"id":"77c9fb7459e1b19f6f655d63556016e8","text":"Configure a Direct Connect connection from DC1 to a Virtual Private Gateway on AWS.  Setup a VPN connection from DC2 to a Virtual Private Gateway on AWS.  Configure a dynamic route across DC1 and DC2 for both paths with a route priority favoring the Direct Connect path to AWS.","correct":true},{"id":"abd8b38f393b6a0d0b42f68dca5ec24d","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure BGP to dynamically route traffic across the nearest Direct Connect link.","correct":false},{"id":"bcac2f1d84c5f367ede3342f0adda492","text":"Ensure that DC1 and DC2 have separate ISPs.  Setup VPN connections from DC1 and DC2 to a Virtual Private Gateway on AWS.  Create static routes at each DC to use the local VPN to AWS.  Use CloudTrail to monitor traffic on the Virtual Private Gateway and trigger a script to update the static route if one of the VPN connections goes down.","correct":false},{"id":"425adb8435a7c170eef3698fa729f5ea","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure a default route in both DC1 and DC2 to route traffic to the local Direct Connect link.","correct":false}]},{"id":"1520156f-0918-4ab4-a759-ce33a931c744","domain":"awscsapro-domain5","question":"Your company has an online shopping web application. It has adopted a microservices architecture approach and a standard SQS queue is used to receive the orders placed by the customers. A Lambda function sends orders to the queue and another Lambda function fetches messages from the queue and processes them. On some occasions the message in the queue cannot be handled properly. For example, when an order has a deleted production ID, the message cannot be consumed successfully and is returned to the queue. The problematic messages in the queue keep growing and the ability to process normal messages is affected. You need a mechanism to handle the message failure and isolate error messages for further analysis. Which method would you choose?","explanation":"It is not a good idea to adjust the retention period or simply delete the messages that fail to be processed as the question asks for a mechanism to isolate the messages for further troubleshooting. A redrive policy should be used to auto-forward error message to a dead letter queue. Then you can analyze the contents of messages to diagnose the producer’s or consumer’s issues. One thing to note is that a standard queue can only have another standard queue as the dead letter queue. Therefore a FIFO dead letter queue is incorrect as this scenario uses a standard SQS queue and requires a standard dead letter queue.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","title":"Amazon SQS dead-letter queues"}],"answers":[{"id":"3c8453a6c57faf61e761f771bab6f1af","text":"Create a standard queue as the dead letter queue and configure a redrive policy to put error messages to the dead letter queue. Analyze the contents of messages in the dead letter queue to diagnose the issues.","correct":true},{"id":"f7b3898cfcb4851b120c9b14d044ab90","text":"Decrease the message retention period of the queue to 1 day. When the messages are not processed properly and put back in the queue, they can be quickly deleted when the retention period expires.","correct":false},{"id":"64fd54fe78b534f0eac9222a6e32747d","text":"Create a FIFO (First-In-First-Out) queue as the dead letter queue and use a redrive policy to forward problematic messages to this new queue. Create a Lambda function to read the message contents in the FIFO queue for further analysis.","correct":false},{"id":"a7eb53a7df09334677590165f666c58f","text":"Modify the error handling logic of the Lambda function to delete the messages whenever the processing is unsuccessful with an error or exception. The error messages do not return to the queue and the normal message handling is not blocked.","correct":false}]},{"id":"93badb2c-68ab-4715-b29e-1209af0c7b27","domain":"awscsapro-domain2","question":"You are a developer for a Aerospace company.  As part of an outreach and education program, the company has financed the construction of a free public service that provides weather forecasts for the sun.  Anyone can make a call to this REST service and receive up-to-date information on forecasted sun flare or sun spots that might have an electromagnetic impact here on Earth.  You are in the final stages of developing this new serverless application based on DynamoDB, Lambda and API Gateway.  During performance testing, you notice inconsistent response times for the service.  You had expected the API to be relatively consistent since its just retrieving data from DynamoDB and returning it as JSON via the API Gateway.  What might account for this variation in response time?","explanation":"Inconsistent response times can have a few different causes.  The exact nature of the testing is not explained but we can anticipate a few causes.  If you have enabled API Gateway caching, the gateway can return a result from its cache without having to go back to a supplying service or database.  This can result in various response rates depending on if an item is in the cache or not.  (The question did not specify we had slow response...just inconsistent response which could be a response faster than we expected.)  When a Lambda function is run for the first time or after an update, AWS must provision the Lambda environment and pull in any external dependencies.  This can result in a slower response time at first but faster later.  Also, if we do not have sufficient RCU for our DynamoDB table, we could run into throttling of the reads which could appear as inconsistent response times.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html","title":"AWS Lambda Execution Context - AWS Lambda"},{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"Enable API Caching to Enhance Responsiveness - Amazon API Gateway"}],"answers":[{"id":"f13e59f5ef22ace9340e24f434eb09cb","text":"You are experiencing a cold start.","correct":true},{"id":"158838df2d14636ed1429fffbcc6825e","text":"Your DynamoDB RCUs are underprovisioned.","correct":true},{"id":"4c3254ba1ddb0fb7a3ce3824707a7105","text":"You have enabled caching on the API Gateway.","correct":true},{"id":"3bd10d778b04bfa32a7338a04e832d38","text":"The CloudFront distribution used by API Gateway is not deployed fully yet.","correct":false},{"id":"43fb818a88f47b6b3b136abdd40a15a6","text":"There are not enough open inbound ports in your VPC.","correct":false},{"id":"4922e0fbbf140cd0dbb6458bbadbf268","text":"The data is being updated on DynamoDB at the exact same time you are trying to read it.","correct":false},{"id":"b03fe2e16e6904dc4b5fa34c91c1f855","text":"You are using HTTP rather than HTTPS.","correct":false}]},{"id":"a21ff5b0-e658-4466-9b30-ad1292dde65d","domain":"awscsapro-domain2","question":"A composite materials company is implementing a new monitoring solution on their manufacturing floor. Wi-Fi enabled IoT devices will be registered with AWS IoT Core to read data from numerous control systems. Dashboards will be created in Amazon QuickSight to present aggregate metrics to users (average, min, max, standard deviation, variance, and percentile). Drill down capabilities will also be needed for deeper analyses of exception scenarios. Which architecture will provide the most reliable and performance efficient solution for the company's monitoring needs?","explanation":"The MQTT protocol is a publish/subscribe protocol that provides clients with independent existence from one another, enhancing the reliability of the solution. HTTP is a document-centric ,request-response protocol, requiring more processing and storage overhead for IoT devices. There is no need to use Kinesis Data Analytics in this case because QuickSight can perform all of the aggregate functions required for this use case. Answer number four won't allow for data drill down because the device messages are not written to any persistent storage service.","links":[{"url":"https://aws.amazon.com/iot-core/","title":"AWS IoT Core"},{"url":"https://aws.amazon.com/blogs/compute/visualizing-sensor-data-in-amazon-quicksight/","title":"Visualizing Sensor Data in Amazon QuickSight"},{"url":"https://aws.amazon.com/quicksight/","title":"Amazon QuickSight"},{"url":"https://aws.amazon.com/athena/","title":"Amazon Athena"}],"answers":[{"id":"48b8bb005b2b6181fa6fc7161df04e9d","text":"Install HTTP libraries on the IoT devices. Create an IoT Core rule that forwards the HTTP messages to an AWS Lambda function. Have the Lambda function write the messages to S3, and to an Amazon Kinesis Data Analytics stream to aggregate the data. Have an AWS Lambda function trigger to read the aggregate data and deposit it into Amazon DynamoDb tables","correct":false},{"id":"35c4c115504645bedabfaccb200fee7f","text":"Install MQTT libraries on the IoT devices. Create an IoT Core rule that forwards the MQTT messages to an Amazon Kineses Data Analytics stream, which writes aggregate data to an Amazon Kinesis Data Streams stream. Have an AWS Lambda function trigger to read the aggregate data and deposit it into Amazon DynamoDB tables","correct":false},{"id":"2287d42764ee3a22b9bdcb617461dbc6","text":"Install MQTT libraries on the IoT devices. Create an IoT Core rule that forwards the MQTT messages to an AWS Lambda function. Have the Lambda function write the messages to an Amazon Kinesis Data Firehose stream, which deposits them into S3","correct":true},{"id":"d3578dd1f9b95b2daf716b5298605a4d","text":"Install HTTP libraries on the IoT devices. Create an IoT Core rule that forwards the HTTP messages to an Amazon Kineses Data Firehose stream, which deposits the data into S3, and writes the data to an Amazon Kinesis Data Analytics stream to aggregate the data. Have an AWS Lambda function trigger to read the aggregate data and deposit it into S3.","correct":false}]},{"id":"e50e5c98-f9f8-48fd-80fc-6a8741d17482","domain":"awscsapro-domain1","question":"You have just completed setup of Direct Connect from your data center to VPCs in us-east-1.  You would also like to leverage that Direct Connect for communication between your data center and other VPCs in other regions.  What is the simplest way to do this?","explanation":"You can use an AWS Direct Connect gateway to connect your AWS Direct Connect connection over a private virtual interface to one or more VPCs in your account that are located in the same or different regions. You associate a Direct Connect gateway with the virtual private gateway for the VPC, and then create a private virtual interface for your AWS Direct Connect connection to the Direct Connect gateway. You can attach multiple private virtual interfaces to your Direct Connect gateway.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html","title":"Direct Connect Gateways - AWS Direct Connect"}],"answers":[{"id":"d9271b66eb35f235c0b61fd11f3af41d","text":"Associate the Direct Connect gateway with the virtual private gateway for the VPC, and then create a private virtual interface for your Direct Connect connection to the Direct Connect gateway.","correct":true},{"id":"38573789b2722bdd38529167996196d1","text":"You cannot use a Direct Connect gateway in one region to reach another region.  You must order a second Direct Connect link from a Partner in the desired regions.","correct":false},{"id":"15955693a61b06cc43a38a2ac5a2ede9","text":"Setup a Transitive VPC and configure virtual private gateways between the us-east-1 VPC and other VPCs in other regions.","correct":false},{"id":"17077e5e04411e1e4e4b8ab017ae6c18","text":"Add a new route to the VPC peer in the desired region in the Customer Gateway.  Ensure that BGP routing is enable to propagate the route.","correct":false}]},{"id":"629b963f-dfaf-4c3e-95b1-71834591846a","domain":"awscsapro-domain4","question":"Your company has been running its core application on a fleet of r4.xlarge EC2 instances for a year.  You are confident that the application has a steady-state performance and now you have been asked to purchase Reserved Instances (RIs) for a further 2 years to cover the existing EC2 instances, with the option of moving to other Memory or Compute optimised instance families when they are introduced.  You also need to have the option of moving Regions in the future. Which of the following options meet the above criteria whilst offering the greatest flexibility and maintaining the best value for money.","explanation":"When answering this question, it's important to exclude those options which are not relevant, first.  The question states that the RI should allow for moving between instance families and this immediately rules out Standard and Scheduled RIs as only Convertible RIs can do this.  Of the 2 Convertible RI options, one can be ruled out as it suggests selling unused RI capacity on the Reserved Instance Marketplace, but this is not available for Convertible RIs and therefore that only leaves one answer as being correct.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html","title":"Types of Reserved Instances (Offering Classes)"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html","title":"Reserved Instances"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html","title":"Scheduled Reserved Instances"}],"answers":[{"id":"188dc576a2e69c4182330092ab7f3786","text":"Purchase a Scheduled RI for 3 years, then sell the unused RI on the Reserved Instance Marketplace","correct":false},{"id":"89987ea3ddc88d352336561f25d83387","text":"Purchase a 1 year Standard Zonal RI for 3 years, then sell the unused RI on the Reserved Instance Marketplace","correct":false},{"id":"b46dcc09115bb8c1683455addbc4fa46","text":"Purchase a 1 year Convertible RI for each EC2 instance, for 2 consecutive years running","correct":true},{"id":"41620e7974f0d2cca7f57d1972e30387","text":"Purchase a Convertible RI for 3 years, then sell the unused RI on the Reserved Instance Marketplace","correct":false}]},{"id":"7c5f884f-c0f9-4028-a725-50819d704324","domain":"awscsapro-domain5","question":"You deploy an application load balancer and an Auto Scaling group (ASG) in production for a new project. When instances in the ASG have a high CPU utilization, a new instance is launched. However, the new instance fails the health check from the ASG and has been terminated after some time. You check the logs in the instance and find that the startup script does not finish yet before the instance is terminated. How would you resolve the problem?","explanation":"Amazon EC2 Auto Scaling waits until the health check grace period ends before checking the health status of the instance. The grace period timer should be increased to give the instance more time to finish the startup script. Increasing the healthy threshold makes the instance more difficult to become healthy. Decreasing the timeout value also does not help as the instance may become unhealthy very quickly. Modifying the health check type from ELB to EC2 is unsuitable as the ASG cannot get the instance status from the application level. Even if the instance shows as healthy in ASG, the application may not be ready yet.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Check Grace Period"}],"answers":[{"id":"b5ca84726bf6fcfd6522ab188c8224ea","text":"Increase the default healthy threshold in the health check of elastic load balancer from 5 to 10 so that the instance will become healthy more quickly once the startup script finishes in the new instance.","correct":false},{"id":"1d8964ced10da94d49d1a1efe02bbbca","text":"Modify the health check type from ELB to EC2 in the Auto Scaling group. Configure ASG to check the EC2 instance status. As long as the instance does not have a system level issue, it will not fail the health check in the ASG even when the startup script is still running.","correct":false},{"id":"e7ad9ea949ae87c6a6001a94f5a9bf48","text":"Increase the health check grace period in the Auto Scaling group configurations. When a new instance boots up, it is given more time to execute the startup scripts and run applications before the health check from ASG.","correct":true},{"id":"e322fd6a55a044826665dfce4ae7020b","text":"Decrease the timeout value in the ELB health check from 5 seconds to 1 second so that when the ELB performs the health check on the backend instances, the instances are able to respond in time before a timeout occurs.","correct":false}]},{"id":"ebcf9ff3-82a1-48f8-b2fc-5d2aeb1d018c","domain":"awscsapro-domain3","question":"You are consulting with a company who is at the very early stages of their cloud journey.  As a framework to help work through the process, you introduce them to the Cloud Adoption Framework.  They read over the CAF and come back with a list of activities as next steps.  They are asking you to validate these activities to keep them focused.  Of these activities, which would you recommend delaying until later in the project?","explanation":"External communication usually comes much later in the process once project plans are defined and specific customer impact is better understood.","links":[{"url":"https://aws.amazon.com/professional-services/CAF/","title":"The AWS Cloud Adoption Framework"}],"answers":[{"id":"937d0c376f475ce2eac7a3356601b8fb","text":"Investigate the need for training for Program and Project Management staff around agile project management.","correct":false},{"id":"2f063bd85b14ae1bbeec72bf0f6c06f5","text":"Work with the Human Resources business partners to create new job roles, titles and compensation/remuneration scales.","correct":false},{"id":"153eadc71676701cd67fdf00dc6c4723","text":"Work with Marketing business partners to design an external communications strategy to be used during potential outages during the migration.","correct":true},{"id":"199907bba5306a10dbadf5a330d5f1f6","text":"Hold a workshop with IT business partners about the creation of an IT Service Catalog concept.","correct":false},{"id":"525fde29088ab22597d7d8063c7dadf6","text":"Work with internal Finance business partners to design a transparent chargeback model.","correct":false}]},{"id":"9cab3dd3-61ad-44c6-b27a-2999e7aedf61","domain":"awscsapro-domain1","question":"For large organizationally complex AWS landscapes, it is considered a best practice to combine a tagging strategy with lifecycle tracking of various projects to identify orphaned resources that are no longer generating value for the organization and should be decommissioned.  With which AWS Well-Architected Framework Pillar is this best practice most aligned?","explanation":"Tagging has many uses but one strong use-case is in being able to tie resources that incur costs with cost centers or projects to create a direct line of sight to actual AWS expenses.  If this visibility does not exist, costs tend to increase because \"someone else is paying.\"  A Best Practice of the Cost Optimization Pillar is to maintain expenditure awareness.","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"2fa2e9f495df0cdcfa992171784f89b7","text":"Operational Excellence","correct":false},{"id":"bc368b371822f4e7df28c5768582dde8","text":"Performance Efficiency","correct":false},{"id":"2fae32629d4ef4fc6341f1751b405e45","text":"Security","correct":false},{"id":"d0bf6ab153eaa104c878925472ff129a","text":"Cost Optimization","correct":true},{"id":"820b8f74ad843d1574106ec4cadfc07e","text":"Reliability","correct":false}]},{"id":"49f16801-2cc1-48c8-a517-f9192f516318","domain":"awscsapro-domain3","question":"A tire manufacturing company needs to migrate a .NET simulation application to the AWS cloud. The application runs on a single Windows Application Server in their datacentre. It reads large quantities of data from local disks that are attached to the on-premises Application Server. The output from the application is small in size and posted in a queue for downstream processing. On the upstream side, the data acting as the input for the .NET simulation app is generated in the tire-testing Lab during the daytime by processes running on a Linux Lab Server. This data is then copied from the Linux Lab Server to the Windows Application Servers by a nightly process that also runs on the Linux Lab Server. This nightly process mounts the Application Server disks using a Samba client, this is made possible by the Application Server also acting as a Windows File Share Server. When the nightly process runs, it overwrites the input data from last night because of disk space constraint on the Application Server. This is undesirable as the data is permanently lost on a daily basis.\nThe migration is being undertaken because the .NET simulation application needs more CPU and RAM. The company does not want to spend on expensive hardware any more. However, the nightly process is not migrating, nor is the Linux Lab Server. The code of the simulation applications, as well as the nightly process, may change a little as a result of the migration, but leadership wants to keep these changes to a minimum. They also want to stop losing the daily test data and keep it somewhere for possible analytical processing later on.\nAs the AWS architect hired to shepherd this migration and many more possible migrations in the future, which of the following architectures would you choose as the best one, considering the minimization of code changes as the topmost goal, followed by cost-effectiveness as the second but important priority? The data has no security requirement.","explanation":"The two parts of this question are - (a) Do I use EFS or EBS for storing the data from the Lab Servers? (b) Do I copy data from each night to S3 using a NAT Gateway (thereby using the public internet) or a VPC Endpoint (thereby using the private network to copy)?\nThe answer to the first question is EBS because Windows EC2 instances cannot mount EFS, as EFS only supports Linux EC2 instances.\nThe answer to the second question is VPC Endpoint because NAT Gateways are very costly - they are charged 24-7 for just running, in addition to having data transfer rates. S3 VPC Endpoints are a cost-effective mechanism to copy data to S3. Note that the S3 put-object cost will be the same for both cases. The question tries to distract the candidate by stating that there is no security requirement, trying to confuse the candidate into selecting NAT Gateway in case they perceive the only distinction between NAT Gateway and S3 VPC Endpoint to be the usage of public network versus private.\nThis is an example of highly verbose question describing a complex scenario. There will definitely be quite a few such questions in the AWS SA-P exam that are challenging in terms of time management. You may use vertical scanning of the answer choices to spot the differences first. That way, you can focus on determining which of the variations is correct because you would know what is different between them.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html","title":"EFS Support for Windows EC2 Instances"},{"url":"https://aws.amazon.com/vpc/pricing/","title":"Search for NAT Gateway Pricing here"}],"answers":[{"id":"220d73c7cceb53661a402d59038118ee","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"c7311a2612bfc63a966317d468a5b4dd","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"fabdb7e05e5bb1c78dd6e166134202ca","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"3ae5ae41663ec801882e10e7fa394613","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":true}]},{"id":"768271e9-9fd0-4921-a473-49ec465a0b34","domain":"awscsapro-domain4","question":"Your company is preparing for a large sales promotion coming up in a few weeks.  This promotion is going to increase the load on your web server landscape substantially.  In past promotions, you've run into scaling issues because the region and AZ of your web landscape is very heavily used.  Being unable to scale due to lack of resources is a very real possibility.  You need some way to absolutely guarantee that resources will be available for this one-time event.  Which of the following would be the most cost-effective in this scenario.","explanation":"If we only need a short-term resource availability guarantee, it does not make sense to contract for a whole year worth of Reserved Instance.  We can instead use On-Demand Capacity Reservations.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html","title":"On-Demand Capacity Reservations - Amazon Elastic Compute Cloud"}],"answers":[{"id":"08d50c618757e489c5ac563be91f25f7","text":"Purchase a regional Reserved Instance.","correct":false},{"id":"9dc64814f005254de6e1a269849ae0b1","text":"Purchase zonal Reserved Instance.","correct":false},{"id":"f10307b8e644773b68b43dade41382f1","text":"Purchase Dedicated Instances.","correct":false},{"id":"0862f1bd2009c6ca8b52898831991642","text":"Use an On-Demand Capacity Reservation.","correct":true},{"id":"0b48767c7357e407fa6f1e958fe0f051","text":"Purchase a Dedicated Host.","correct":false}]},{"id":"c9d5dfbb-8dda-4a84-bf04-49aa7f88d4db","domain":"awscsapro-domain2","question":"A healthcare provider has a recent history of failing to match patient clinical records from other providers with their own records. This has resulted in missed diagnoses and delayed treatments. To address the issue, they've begun an initiative to store patient records, both those generated in-house and those coming from other providers, in CSV format on an Amazon S3 data lake. They'll use other AWS services to aggregate and deduplicate patient information. Which HIPAA compliant solution will provide them with the highest level of accuracy to improve their level of patient care?","explanation":"An AWS Glue crawler can scan the raw patient CSV data and store the schemas in the Glue data catalog. A Glue ML Transform can be written to find patient information matches in tables defined in the Glue data catalog. If you don't want to write an ML Transform to find the matches, AWS provides a custom one called FindMatches. After training the ML Transform, you can use it as part of a scheduled Glue ETL job, which can write results back to the S3 data lake. Kinesis Data Analytics isn't capable of querying data in S3. It only provides SQL manipulation of the incoming stream. An EMR Hive job will be able to perform some basic record matching, but machine learning will provide greater matching accuracy over time. While SageMaker could provide a viable solution, the Caffe framework is most commonly used for computer vision use cases, not record matching.","links":[{"url":"https://aws.amazon.com/glue/","title":"AWS Glue"},{"url":"https://aws.amazon.com/about-aws/whats-new/2019/08/aws-glue-provides-findmatches-ml-transform-to-deduplicate/","title":"AWS Glue now provides FindMatches ML transform to deduplicate and find matching records in your dataset"},{"url":"https://aws.amazon.com/blogs/big-data/matching-patient-records-with-the-aws-lake-formation-findmatches-transform/","title":"Matching patient records with the AWS Lake Formation FindMatches transform"}],"answers":[{"id":"7a5ba1c8cc4da315c99a4458891c790d","text":"Prior to depositing patient records in the S3 data lake, ingest them with Amazon Kinesis Data Streams and configure Amazon Kinesis Data Analytics as the consumer of the stream. Have Kinesis Data Analytics correlate records with those already ingested to S3 and write them to the corresponding patient folder","correct":false},{"id":"734eda6752df4074b7d1de2dc63055d6","text":"Create and train an Amazon SageMaker Caffe model to match patient records. Schedule an AWS Lambda function to periodically run the SageMaker model and deposit the results back into S3","correct":false},{"id":"50bb504039515eef7c1372c744aa4d79","text":"As new patient records are ingested to S3, trigger an AWS Lambda function to start an AWS Glue crawler to update the Glue data catalog. Have the Lambda function then run an Amazon EMR Hive job to perform patient record matching and write the results back to the data lake","correct":false},{"id":"4745b6f6f2c711702b03c99faaab04b1","text":"Implement an AWS Glue crawler to determine patient record formats. Create and train a Glue ML Transform to match patient records. Execute a Glue ETL job using the ML Transform, and store results back in the data lake. Configure the Glue scheduler to run the crawler and the ETL jobs periodically","correct":true}]},{"id":"9038ce91-1730-47e6-b804-571614ac4752","domain":"awscsapro-domain5","question":"For your production web farm, you have configured an auto scaling group behind a Network Load Balancer.  Your auto-scaling group is defined to have a core number of reserved instances and to scale with spot instances.  Because of differences in spot pricing across AZs, sometimes you end up with many more instances in one AZ over another.  During times of peak load, you notice that AZs with fewer instances are averaging 70% CPU utilization while the AZ with more instances average barely above 10% CPU utilization.  What is the most likely cause of this behavior? ","explanation":"Cross-zone load balancing ensures that requests are equally spread across all available instances, regardless of AZ.  When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. When cross-zone load balancing is disabled, each load balancer node distributes traffic across the registered targets in its Availability Zone only. ","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html","title":"How Elastic Load Balancing Works - Elastic Load Balancing"}],"answers":[{"id":"dbec1ca528c5fed82acd6704fa187842","text":"The cooldown time is too short in the launch configuration.","correct":false},{"id":"61e42ddbbd5b30ec8e73f350f7782986","text":"At the time a scale event is triggered, there are no more available resources in the AZ or region of the instance type you have configured in your auto scaling group.","correct":false},{"id":"1d1ef2b9e2525e667cc97a8dd5e91429","text":"The TTL for sticky sessions is set too high and therefore are blocking a scale-out event until some connections are dropped.","correct":false},{"id":"19fc4224d11c7dbb028fcb29fc93ce38","text":"Cross-zone load balancing is disabled on the Network Load Balancer.","correct":true},{"id":"733135f9c29b6aef570b5c4f6748df2f","text":"CloudWatch is not accurately reflecting the true CPU load due to mesh processing on Nginx.  The real CPU load might in fact not reach the threshold which explains why the group is not scaling.","correct":false}]},{"id":"760cbc05-d8ba-4d58-a555-250f26815963","domain":"awscsapro-domain4","question":"Your team is working on a long-term government project. You have purchased several reserved instances (c5.18xlarge) to host the applications to reduce costs. There is a requirement to track the utilization status of these instances and the actual utilization should be always over 80% of the reservation. When the utilization falls below 80%, the team members need to receive email and SMS alerts immediately. Which AWS services would you use to achieve this requirement?","explanation":"The AWS Budgets service is able to track the reserved instances utilization. Several optional budget parameters can be configured such as linked account, region and instance type. You can use an SNS notification to receive alerts when the actual usage is less than a defined threshold. You do not need to maintain a new Lambda function to get the usage data and send alerts. Besides, either CloudWatch alarm or Trusted Advisor cannot provide alerts based on the reserved instances utilization.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-create.html#create-reservation-budget","title":"Create a reservation budget"}],"answers":[{"id":"a4024e0dbac3a9e9e371ebfe3a511ba7","text":"In the Cost Optimization category of AWS Trusted Advisor, monitor the usage of the EC2 reserved instances. If the usage is less than 80% of the reservation, Trusted Advisor raises an action recommended alarm and sends a message to an SNS topic which notifies the team via email and SMS.","correct":false},{"id":"ba6dc1f65ab53b3ac3bb10944df422b4","text":"Create a Lambda function using Python boto3 to get the utilization status of the reserved instances. Execute the Lambda function at a regular basis such as every day and notify the team by an SNS topic if the utilization drops below 80%.","correct":false},{"id":"9cb24caa236c81ae714987bb14b0faee","text":"Create a new reservation budget in AWS Budgets service. Set the reservation budget type to be RI Utilization and configure the utilization threshold to be 80%. Use an SNS topic to notify the team when the utilization drops below 80%.","correct":true},{"id":"c13e13d0cd86acac35a5358fff11066a","text":"Configure the reserved EC2 instances to send the utilization metrics to CloudWatch. Create a CloudWatch dashboard based on the metrics and set up a CloudWatch alarm. Use an SNS topic to receive the alert when the alarm is on.","correct":false}]},{"id":"b5f80e7e-7b92-402f-b74d-7397f0778eaf","domain":"awscsapro-domain4","question":"You are an AWS administrator and you want to enforce a company policy to reduce the cost of AWS usage. For the AWS accounts of testing environments, all instances in Auto Scaling groups should be terminated at 10:00PM every night and one instance in ASG should be launched at 6:00AM every morning so that the team can resume their work. At 10:00PM and 6:00AM, the team should get an email alert for the scaling activities. Which of the following methods would you use to implement this?","explanation":"A Lambda function can be used to modify the desired capacity at 10:00PM and 6:00AM. Only the desired capacity should be changed and the maximum capacity should be kept. If the maximum capacity is 1 during the day, the number of instances in ASG cannot be over 1. AWS CLI \"terminate-instances\" is inappropriate to terminate all instances in an ASG because ASG will automatically create new ones to maintain the desired capacity.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-manual-scaling.html","title":"Manual Scaling for Amazon EC2 Auto Scaling"}],"answers":[{"id":"32fa65e299613094d47b0e1a67a22636","text":"Create a cron job running in an EC2 instance. The cron job modifies the minimum, desired and maximum capacities to 0 at 10:00PM and changes the minimum, desired and maximum capacities back to 1 at 6:00AM. Configure a CloudWatch Event rule to notify the team.","correct":false},{"id":"0344a7b1df36d0f6e904c191acd31686","text":"Create a Lambda function that runs at 10:00PM and 6:00AM every day. The function terminates all EC2 instances using AWS CLI \"terminate-instances\" at 10:00PM and launches a new instance using AWS CLI \"run-instances\" at 6:00AM. Notify the team by publishing a message to an SNS topic.","correct":false},{"id":"5e1b982a6d6e29c160572d2430d8e3bb","text":"Use AWS CLI \"aws autoscaling put-scheduled-update-group-action\" to modify the desired capacity and maximum capacity to be 0 at 10:00PM and 1 at 6:00AM. Notify the team by configuring a CloudWatch Event rule.","correct":false},{"id":"8bbb4a7f1fc80cbb616df074b2984bd7","text":"Create a Lambda function that loops through all Auto Scaling groups, modifies the desired capacity to be 0 every 10:00PM and increases the desired capacity to be 1 every 6:00AM. Notify the team through an SNS notification in the Lambda function.","correct":true}]},{"id":"e70b0410-8628-4150-845e-a4bba1f66ec6","domain":"awscsapro-domain2","question":"You work for a credit union which has two VPC-s in us-west-1 - VPC A and VPC B. The CIDR of VPC A is 172.16.0.128/25, and 50% of its available private IP addresses are already used up. The CIDR of VPC B is 172.16.0.0/25, and 5% of its available private IP addresses are already used up.\nThe development team is creating a new service that will deploy an API Gateway and use a Lambda function as its back-end. The Lambda function must read an S3 bucket using an S3 VPC Endpoint deployed in VPC A. Then, it must write to a Cassandra database hosted on an EC2 instance in VPC B.\nThe Lambda function must be deployed in either VPC A or VPC B. Also, the expected number of peak concurrent Lambda function instances is 300, with each instance needing 1 GB of memory. The development team expects that at peak, 100 free IP addresses will be needed to accommodate all the Lambda function instances.\nAs the AWS Architect, you need to advise the development team of the right AWS Architecture to make this possible. What should you suggest?","explanation":"This question requires the knowledge of the following:\n(a)Counting the number of the available private IP address in a CIDR. A CIDR ending with /25 has 128 IP addresses available, minus whatever number AWS reserves for its own (which is 5 per subnet, so we cannot really tell that number here as we do not know how many subnets are there in VPC A or B). For VPC A, half of these are already used, so around 64 are remaining. For VPC B, 5% is used, so 6 or 7 are used, leaving more than 120 available. Thus, VPC A clearly does not have enough space left for 100 Lambda functions, while VPC B surely does. This eliminates the choice that wants to deploy the Lambda function in VPC A without adding a secondary CIDR to it. Though we can eliminate the choice because of this reason, it can be worthwhile to note that it does state something important and true - S3 VPC Endpoint cannot be accessed from a different VPC using a peering connection.\n(b)Determining if CIDR-s are overlapping. 172.16.0.0/25 and 172.16.0.128/25 are not overlapping. Hence, the choice that uses the overlapping argument against peering is eliminated.\n(c)Whether an S3 VPC Endpoint in one VPC can be accessed via a VPC-peered network from a second VPC. The answer is no. Though most VPC Endpoints (that are of the type Interface Endpoint, also called PrivateLink) do not have this limitation, S3 and DynamoDB VPC Endpoints are not of this type. They are of type Gateway Endpoint. Gateway Endpoints cannot be accessed from another VPC if the VPC-s are peered. This eliminates VPC B as the hosting choice for the Lambda function. Remember that the Cassandra Database Instance can be accessed easily over a VPC Peering Connection, so VPC A and B needs to be peered, and VPC A must house the Lambda function, provided we make enough space in VPC A.\nThus, the only correct answer is to host the Lambda function in VPC A after adding a secondary CIDR to make space for the 100 ENI-s needed by the Lambda function at its peak","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html","title":"Gateway VPC Endpoints"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html","title":"Interface VPC Endpoints (AWS PrivateLink)"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html","title":"Search for VPC and Subnet Sizing"}],"answers":[{"id":"9d99afaedfe56a6a5a3e43972f59eb43","text":"VPC A and VPC B cannot be peered owing to their overlapping IP Address ranges. Therefore, it is impossible to deploy the Lambda Function in either VPC because it needs to access some resources in both the VPC-s, unless a new S3 VPC Endpoint is deployed in VPC B. In that case, VPC B will house both the new S3 Endpoint and the Cassandra Database Instance, and therefore, the Lambda function can be deployed in VPC B","correct":false},{"id":"d49117fbb6670c3f47211d6c4410e9e8","text":"Peer VPC A and VPC B. Deploy the Lambda function in VPC B because only VPC B has enough private IP addresses left to accommodate the Lambda function's peak usage demands. The Lambda function can then access the Cassandra Database Instance in the same VPC, and also access the S3 VPC Endpoint over the Peering connection using appropriate routes added to the Routing Table","correct":false},{"id":"b281cc03c30d338041be0e55efeba980","text":"Peer VPC A and VPC B. Deploy the Lambda function in VPC A because S3 VPC Endpoint cannot be accessed from a different VPC using a peering connection. The Lambda function can then access the S3 VPC Endpoint in the same VPC, and also access the Cassandra Database Instance over the Peering connection using appropriate routes added to the Routing Table","correct":false},{"id":"45c82d2dc99061d7e4f27c276f170b4b","text":"Peer VPC A and VPC B. Deploy the Lambda function in VPC A after adding a secondary CIDR Range to VPC A so that the available number of free IP addresses in VPC A expands to accommodate the Lambda function's peak usage demands. The Lambda function can then access the S3 VPC Endpoint in the same VPC, and also access the Cassandra Database Instance over the Peering connection using appropriate routes added to the Routing Table","correct":true}]},{"id":"6d93e859-e1a9-468f-9a05-61a2dbc2be9c","domain":"awscsapro-domain5","question":"You manage a group of EC2 instances that host a critical business application.  You are concerned about the stability of the underlying hardware and want to reduce the risk of a single hardware failure impacting multiple nodes.  Regarding Placement Groups, which of the following would be the best course of action in this case?","explanation":"Spread Placement Groups ensure your instances are each placed on separate underlying hardware so this reduces the risk of a single hardware failure taking down multiple instances.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-spread","title":"Placement Groups - Amazon Elastic Compute Cloud"}],"answers":[{"id":"c8b809a2fef3f21146774b82e8f03f12","text":"You would move the instances onto a Dedicated Host.","correct":false},{"id":"e346d1667489b5726c0367eff4ea4a34","text":"You would use the AWS CLI to move the existing instances into a spread placement group.","correct":true},{"id":"3bde2d7fff7ead39c4035ab0600275f1","text":"You would use the AWS CLI to move the existing instances into a diversified placement group.","correct":false},{"id":"b6153af57a0b8805fc28d93a2859fb9e","text":"You cannot move existing instances into a new placement group.  You would create AMIs from the existing instances and redeploy them into a clustered placement group.","correct":false},{"id":"e6d129a06320300aaf95634a6691b7cb","text":"You would the AWS Console to move the existing instances into a clustered placement group.","correct":false}]},{"id":"4eb19466-6d1a-4ccd-987c-3f8f0cc71479","domain":"awscsapro-domain1","question":"A client wants help setting up a way to manage access to the AWS Console and various services on AWS for their employees.  They are starting out small but expect to provide AWS-hosted services to their 20,000 employees within the year.  They currently have Active Directory on-premises, use VMware to host their VMs.  They want something that will allow for minimal administrative overhead and something that could scale out to work for their 20,000 employees when they have more services on AWS.  Due to audit requirements, they need to ensure that the solution can centrally log sign-in activity.  Which option is best for them?","explanation":"For userbases more than 5,000 and if they want to establish a trust relationship with on-prem directories, AWS recommends using AWS Directory Service for Microsoft Active Directory.  This is also compatible with AWS Single Sign-On which provides a simple way to provide SSO for your users across AWS Organizations.  Additionally, you can monitor and audit sign-in activity centrally using CloudTrail. ","links":[{"url":"https://aws.amazon.com/single-sign-on/faqs/","title":"AWS Single Sign-On FAQs – Amazon Web Services (AWS)"},{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ad_connector_best_practices.html","title":"Best Practices for AD Connector - AWS Directory Service"}],"answers":[{"id":"a3c1cc18aa02ff66c06476aa7b8e78ed","text":"Download and install the AWS ActiveDirectory Sync appliance and install it in vCenter.  Configure the Sync appliance to connect to the local AD and replicate to an instance of Simple AD on AWS.  In IAM, create corresponding roles and policies for the permissions you want to allow on AWS.  Assign these roles to the synchronized Simple AD users in IAM.","correct":false},{"id":"b1b70c7c05b814cf9685b3a64175cd80","text":"Connect the multiple accounts with AWS Organizations.  Deploy AWS Directory Service for Microsoft Active Directory on AWS and configure a trust with your on-premises AD.  Configure AWS Single Sign-On with the users and groups who are permitted to log into AWS.  Give the users the URL to the AWS SSO sign-in web page.","correct":true},{"id":"878c950f9d14c2d81185f1950edda98c","text":"Configure Cognito with web federation against the on-prem Active Directory.  In IAM, create corresponding users corresponding to the Cognito accounts you want to allow on AWS.  Assign these roles to the user pools within Cognito.  Distribute the Cognito SSO client to your users.","correct":false},{"id":"1eb0420847890f99ddd38092ac88c5f0","text":"Create a OAuth Identity Provider in IAM and create roles and policies with the appropriate level of permissions.  In AD, create groups which correspond to the roles you have created in IAM and populate the AD groups with the desired users.  Download and install the OAuth Identity Connector for AD.  Configure the connector for the OAuth Identity Provider on AWS.  ","correct":false},{"id":"32479d3559d6ead0328c6419249c7859","text":"Connect the multiple accounts together using AWS Organizations.  Deploy AD Connector on AWS and configure their on-prem AD.  Create corresponding roles and groups in IAM and map those to their local AD groups.  Use STS to allow users to authenticate into AWS.","correct":false}]},{"id":"0906c4cf-83a1-4cec-b2ab-c010dcdee73f","domain":"awscsapro-domain1","question":"The alternative energy company you work for has four different business units, each of which would like to run workloads on AWS. Each business unit has it's own AWS account, and a shared services AWS account has been created. An established process for tracking software license usage exists for on-premises applications, but the finance department has concerns that the self-serve nature of the cloud may result in license overages for applications deployed on AWS. You've been tasked with setting up a governance model whereby users are only given access to a standard list of products. Which architecture will provide an effective way to implement the governance requirements and manage software license usage on AWS?","explanation":"AWS Service Catalog allows organizations to create and manage catalogs of approved products for use on AWS. Products are defined as CloudFormation Templates. Software license information can be associated with Service Catalog products through tags. AWS Step Functions can orchestrate the process of incrementing usage counts and notifying of over-usage situations when products are launched by users. AWS License Manager is a robust solution for managing software licenses, but it needs to be coupled with Service Catalog to meet the requirement for limiting access to a standard set of products. A Lambda trigger is not currently available for Service Catalog product deployments. Elastic Container Registry provides tagging at the repository level, not at the individual container image level.","links":[{"url":"https://aws.amazon.com/servicecatalog/","title":"AWS Service Catalog"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://aws.amazon.com/blogs/mt/tracking-software-licenses-with-aws-service-catalog-and-aws-step-functions/","title":"Tracking software licenses with AWS Service Catalog and AWS Step Functions"}],"answers":[{"id":"e2441352bdc2d4db956149f0a10b6738","text":"Create Docker images for each of the standardized applications that will be deployed and register them with Amazon Elastic Container Registry (ECR). Populate ECR tags with software license metadata. Create an Amazon DynamoDB table to store software license usage counts. Whenever a container is launched in Amazon Elastic Container Service, trigger an AWS Lambda function to increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"33db838110cfb8b002590cbff630b825","text":"Create Amazon Machine Images for all of the instance configurations that will be deployed. Implement AWS License Manager license configurations and attach them to the AMIs. Create AWS CloudFormation StackSets for the AMIs in the shared AWS account and make them available to users in each business unit","correct":false},{"id":"62788b95bb6b249e6511d14023a20364","text":"Implement AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Create an Amazon DynamoDB table to store software license usage counts. Trigger an AWS Lambda function to run each time a Service Catalog product is launched. Have the Lambda function increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"41d11d1142468e8b0d2a29674d1eaa2c","text":"Deploy AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Populate Service Catalog product tags with software license information. Create an Amazon DynamoDB table to store software license usage counts. Have Amazon CloudWatch detect when a user deploys a Service Catalog product. Launch an AWS Step Functions process to increment license counts in the DynamoDB table, and send notifications when overage thresholds are met","correct":true}]},{"id":"d8b88385-e15f-4313-bc53-e9fb82f89cc3","domain":"awscsapro-domain2","question":"You are developing an application to be hosted on EC2. The application uses some environmental configuration data and other necessary parameters when running. For example, the application needs to get the correct username and password in order to communicate with a RDS database. You want to find a free AWS service to store these parameters. To meet security requirements, these stored parameters must be encrypted by the AWS Key Management Service. Which of the following methods is the best?","explanation":"You can manage parameters in Systems Manager Parameter Store. The type of the parameters must be SecureString so that they are encrypted by KMS. Parameter Store has standard tier and advanced tier. In this scenario, standard tier is enough and advanced tier is not a free service. AWS Secrets Manager does not have the concept of standard or secure parameter. It also charges you per secret per month.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html","title":"How AWS Systems Manager Parameter Store uses AWS KMS"}],"answers":[{"id":"c08527f2eeacd9f56133d1dfed701f1a","text":"Create secure parameters in AWS Secrets Manager. Secrets Manager protects secrets by integrating with KMS and all stored parameters are automatically encrypted by the AWS managed key \"aws/secretsmanager\". You can also configure Secrets Manager to rotate the secrets.","correct":false},{"id":"41e6f55938cd5c32278c6f4b6708641e","text":"Create standard string parameters in AWS Systems Manager Parameter Store as it is a free service. The parameters are automatically encrypted with envelope encryption by the default AWS managed key (aws/ssm) in KMS. Use AWS Encryption SDK in the application to fetch the parameters.","correct":false},{"id":"3295f034025fe6d21155d228ee3dd0a2","text":"Store secure string parameters in AWS Systems Manager Parameter Store so that the parameters are encrypted by KMS. Use the standard tier as there is no additional charge for it. Use AWS Encryption SDK in the application to get the parameters.","correct":true},{"id":"4f357d94d3f20ee0d0c3c7c892ef0d70","text":"Create standard parameters in AWS Secrets Manager. Use the advanced tier as it uses envelope encryption to encrypt the stored parameters with KMS. You can also configure Secrets Manager to rotate the stored secrets or API keys automatically.","correct":false}]},{"id":"a7c939f1-277e-469f-a209-9b290e8136c9","domain":"awscsapro-domain5","question":"Your company has contracted with a third-party Security Consulting company to perform some risk assessments on existing AWS resources.  As part of a routine list of activities, they inform you that they will be launching a simulated attack on one of your EC2 instances.  After the Security Group performed all their activities, they issue their report.  In their report, they claim that they were successful at taking the EC2 instance offline because it stopped responding soon after the simulated attack began.  However, you're quite certain that machine did not go offline and have the logs prove it.  What might explain the Security company's experience?","explanation":"AWS Shield and other counter-measure technologies work to protect all AWS customers from DDoS attacks.  Unless AWS was aware of the test time and expected duration, its likely the traffic was blocked as suspicious.  AWS Firewall Manager is used to manage WAF ACLs and not dynamically blacklist IPs.  Similarly, VPC Flow Logs cannot automatically implement NACL changes as described here. Despite being a permitted service, traffic suspected of being malicious will still be blocked","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/penetration-testing/","title":"Submit a Penetration Testing Request"}],"answers":[{"id":"82a05cb45adab6d248655e827de16c6f","text":"AWS Firewall Manager is dynamically adding a blacklist entry for the Security Company's testing machine because it sees the traffic as a threat.","correct":false},{"id":"f3f963b71e307f8c28109631df115418","text":"The EC2 instance is using an ENI and the Security Company temporarily exceeded the throughput limit resulting in a throttling of their connection.","correct":false},{"id":"7d4826f179b8dc854c9cfb6e43678373","text":"The VPC Flow Logs record the spike in suspicious traffic and implement an update to the inbound NACL to block the remote IP address.","correct":false},{"id":"e3facacbe52b6423f9cf2e700d8e0b81","text":"The Security Company's traffic was seen as a threat and blocked dynamically by AWS.  AWS must grant permission before any penetration testing is done.","correct":true}]},{"id":"0bfd631e-c08c-406a-acf5-a07416aab129","domain":"awscsapro-domain5","question":"Your team uses a CloudFormation stack to manage AWS infrastructure resources in production. As the AWS resources are used by a large number of customers, the update to the CloudFormation stack should be very cautious. Your manager asks for additional insight into the changes that CloudFormation is planning to perform when it updates the stack with a new template. The change needs to be reviewed before being applied by a DevOps engineer. What is the best method to achieve this requirement?","explanation":"CloudFormation Change Sets are able to provide the information on how the running resources are affected by a stack update. The outputs can be reviewed before being executed. Users can view the Change Set through AWS Console or CLI. The Retain option in the DeletionPolicy, CloudFormation stack policy or termination protection helps on protecting the stack resources. However, they cannot provide a summary of  changes in a stack update.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"4657e544cc1daf4315865e230d92dd00","text":"For key AWS resources in the CloudFormation stack, add a Retain option in the DeletionPolicy attribute, which prevents the resources from being accidentally deleted by a stack update. Add a Delete option for the resources that you want to delete along with the stack.","correct":false},{"id":"935df51ce2c7f07994c2b8a257489e00","text":"Create a CloudFormation Change Set using AWS Management Console or CLI, review the changes to see if the modifications are as expected and execute the changes to update the stack.","correct":true},{"id":"700aa7cb0f0e8cfb417b67ae5d49e962","text":"Add a CloudFormation stack policy to prevent updates to stack resources. Only after the changes are reviewed and approved, change the stack policy to allow the stack update. Revert the stack policy after the change.","correct":false},{"id":"550c80eaf15eeb89d49aa2a86eb747a6","text":"Enable termination protection in the CloudFormation stack so that the AWS resources cannot be accidentally deleted or modified. Disable the protection only if the changes are approved. Execute the changes in a maintenance window.","correct":false}]},{"id":"6c770228-3fa1-4020-94a8-119c67a7e2d1","domain":"awscsapro-domain2","question":"You are volunteering with a local STEM (Science, Technology, Engineering and Math) program for youth.  You have decided that you'd like to help them learn about AWS by spinning up their very own WordPress site.  Given that the youth have no experience with AWS and the program, you want to choose the easiest way for students to spin up a simple webserver.  Which AWS technologies would you choose?","explanation":"AWS Lightsail is designed to be a very easy entry-level experience for those just starting out with virtual private servers.  A WordPress site can be deployed with literally a single click and does not require AWS Console access or knowledge of EC2 or VPCs.","links":[{"url":"https://aws.amazon.com/lightsail/","title":"Lightsail"}],"answers":[{"id":"d85c80578ad0849a611c1056b63e385c","text":"VPC","correct":false},{"id":"81b22456f78954c460ce2f531b5e048f","text":"EC2","correct":false},{"id":"cab27dc53edb571cac663ce2e16450dc","text":"AWS Marketplace","correct":false},{"id":"c8f63ecaff5e983a2441126a241c4cfa","text":"ECS","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":false},{"id":"b21eea42e76007ac061cf37a5a41037d","text":"Lightsail","correct":true}]},{"id":"b303f8e0-2c68-44aa-93bb-45b987b17d95","domain":"awscsapro-domain3","question":"You are helping a client build some internal training documentation to serve as architectural guidelines for their in-house Solutions Architects.  You suggest creating something inspired by the AWS Well-Architected Framework.  The client agrees and wants you to come up with some examples of each pillar.  Which of the following are examples of the Reliability pillar?","explanation":"The Reliability pillar includes five design principles:  Test recovery procedures, Automatically recovering from failure, Scaling horizontally to increase aggregate system availability, Manage change in automation and Stop guessing capacity.  By being able to closely monitor resource utilization, we can increase the Reliability and efficiency to right-size capacity.","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"e82a19c63c4c9fedccc997eece7eccdc","text":"With virtual and automatable resources, we can quickly carry out comparative testing using different types of instances, storage, or configurations.  This will allow us to experiment more often.","correct":false},{"id":"c57c8249b3885f2078d8dac40d695dec","text":"We can design workloads to allow components to be updated regularly, making changes in small increments that can be reversed if they fail.","correct":false},{"id":"c257f1eec7a5a7f1eae9338f4de45cb0","text":"On AWS, we'll be able to monitor demand and system utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over or under-provisioning.  We can stop guessing on capacity needs.","correct":true},{"id":"3eb456756ecb767ab18179d87ec49a6b","text":"We can drive improvement through lessons learned from all operational events and failures. Share what is learned across teams and through the entire organization.","correct":false}]},{"id":"3b08a75a-01b7-4083-bbd1-af1acd7e5314","domain":"awscsapro-domain2","question":"A clothing retailer has decided to run all of their online applications on AWS. These applications are written in Java and currently run on Tomcat application servers hosted on VMware ESXi Linux virtual machines on-premises. Because many of the applications require extremely high availability, they've deployed Oracle RAC as their database layer. Some business logic resides in stored procedures in the database. Due to the timing of other business initiatives, the migration needs to take place in a span of four months. Which architecture will provide the most reliable and operationally efficient solution?","explanation":"Elastic Beanstalk provides the most operationally efficient solution for the application server layer. With Elastic Beanstalk, you can quickly deploy and manage applications without worrying about the infrastructure that runs those applications. VMware Cloud on AWS delivers a robust environment for running Oracle RAC environments. Oracle RAC doesn't run natively on EC2. Due to the time constraints for the project, a migration to Aurora Multi-Master is probably not feasible in four months, especially when the migration of stored procedure code is involved. RDS Oracle Multi-AZ provides active/passive failover, whereas Oracle RAC is active/active, providing no-downtime failovers. Oracle Recovery Manager can run on EC2 and access the database via the VMware Cloud ENIs to perform backups to S3 over a VPC Endpoint.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_Java.html","title":"Creating and Deploying Java Applications on AWS Elastic Beanstalk"},{"url":"https://aws.amazon.com/vmware/","title":"VMware Cloud on AWS"},{"url":"https://d1.awsstatic.com/VMwareCloudonAWS/aws_reference_architecture_oracle_rac_on_vmware_cloud.pdf?did=wp_card&trk=wp_card","title":"Oracle RAC on VMware Cloud on AWS"}],"answers":[{"id":"3cac9f7ec8cba84a36f70ab25c667485","text":"Use a certified Tomcat AMI to deploy the application servers on VMware Cloud on AWS EC2 instances with Auto Scaling across multiple Availability Zones. Configure Oracle on Amazon RDS with Multi-AZ. Connect the Tomcat servers to the database instances with VMware Cloud ENI route table entries. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":false},{"id":"b555cc3afefbe63e6b58594cd86b4fa5","text":"Deploy AWS Elastic Beanstalk to run the Tomcat servers in multiple Availability Zones. Migrate the database to Amazon Aurora Multi-Master. Connect the Tomcat servers to the database via JDBC. Leverage Aurora's Multi-AZ and automated backup capabilities to achieve high availability.","correct":false},{"id":"2a2d87b2ff0208d44d780f2cf354f3f2","text":"Use a certified Tomcat AMI to deploy the application servers on EC2 instances with Auto Scaling across multiple Availability Zones. Install Oracle RAC on EC2 across multiple Availability Zones. Connect the Tomcat servers to the database via JDBC. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":false},{"id":"69173b8054edfd0b9be84428b694bc53","text":"Implement AWS Elastic Beanstalk to run the Tomcat servers in multiple Availability Zones. Run Oracle RAC on VMware Cloud on AWS in multiple Availability Zones. Connect the Tomcat servers to the database instances with VMware Cloud ENI route table entries. Use Oracle Recovery Manager to backup the database to Amazon S3.","correct":true}]},{"id":"1db0c4e1-44ef-4a3a-8e71-6dce38e4a0bb","domain":"awscsapro-domain3","question":"You are considering a migration of your on-prem containerized web application and CouchBase database to AWS.  Which migration approach has the lowest risk and lowest ongoing administration requirements after migration?","explanation":"A lift-and-shift approach when containers are involved is often a very easy and low-risk way to migrate to the cloud.  ECS is a good option of you already have a container landscape.  Fargate provides more automated scale and management, but AWS wants users to treat Fargate as an ephemeral platform, so an application like CouchBase that requires persistent storage would not work well.  Our best option for least management is ECS on an EC2 cluster.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud | AWS Cloud Enterprise  Strategy Blog"},{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_GetStarted.html","title":"Getting Started with Amazon ECS using Fargate - Amazon Elastic Container  Service"}],"answers":[{"id":"77591c2773a9058fb91669c7bb4104ac","text":"Provision sufficient sized EC2 instances to host the web application and Couchbase.  Manually install the web application and Couchbase on the EC2 instances and configure rsync and DMS to synchronize the web server and database respectively.  Once the AWS environment is proven, change the DNS entries to point to the new AWS landscape.  Mange the instances going forward with AWS Config.","correct":false},{"id":"ecd3b08ae8761671f56d59cca00dba2e","text":"Use Server Migration Service to migrate the on-prem servers into AWS as AMIs.  Configure data volume replication to synchronize both the web server and database AMIs.  Run in parallel for no longer than 90 days.  When the new environment is proven, change over the DNS entry to point to the new AWS landscape.","correct":false},{"id":"e2ca91149ec9ad6a9ca978f52b63f9a7","text":"Use SCT to read the existing Couchbase schema and recreate it in DynamoDB.  Use DMS to initially migrate the data from Couchbase and keep it in sync.  Import the web application into ECS using a Fargate cluster.  Update the ECS web application to use DynamoDB.  Once the AWS landscape is proven, do a final commit from the web application container state to the latest version in the registry.  Wait until ECS completes the update of the new container and change DNS entries to point to the new AWS landscape.","correct":false},{"id":"e1b011698db39a5bd75f9561625d28a0","text":"Import the containers into Elastic Container Registry.  Deploy the web application and database on ECS using an EC2 cluster.  Once the AWS version is proven, do a final commit of the container state to the latest version in the registry and use Force New Deployment on the ECS console for the service.  Change over DNS entries to point to the new AWS landscape.","correct":true}]},{"id":"d58acd29-561d-4017-a55d-bcff8eb40f90","domain":"awscsapro-domain4","question":"Last month's AWS service cost is much higher than the previous months. You check the billing information and find that the used hours of Elastic Load Balancer (ELB) increases dramatically. You manager asks you to plan and control the ELB usage. When the ELB service has been used for over 5000 hours in a month, the team should get an email notification immediately and further actions will be taken accordingly. Which of the following options is the easiest one for you to choose?","explanation":"AWS Budgets include the types of cost budget, usage budget, reservation budget and savings plan budget. The usage budget enables you to plan the usage of ELB service and receive budget alerts when the actual usage becomes more than a threshold (5000 hours in this scenario). The cost budget type is incorrect as it evaluates the cost instead of usage and you cannot receive budget alerts from either Cost Explorer or AWS Config.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing your costs with Budgets"}],"answers":[{"id":"15fe48e7010a90e06774d9aa9668704a","text":"Create a usage budget in AWS Budgets. Check the ELB running hours for every month and set the budgeted amount to be 5000 hours. Configure an email alert or an Amazon Simple Notification Service (SNS) notification when the actual usage is more than the threshold.","correct":true},{"id":"baf09736ecf556311eb6e77579877ccd","text":"In AWS Config, monitor the usage of all ELB resources within the AWS account. Create a custom Config rule via a Lambda function that calculates the ELB usage and sends an alert message to an SNS topic when the usage is over 5000 hours.","correct":false},{"id":"0f88bd3ee9fd3724922aa89baa9f656d","text":"Launch the Cost Explorer in AWS billing dashboard, filter the EC2 ELB service and configure a CloudWatch alert to track its actual monthly usage. When the monthly ELB usage grows more than 5000 hours, raise the CloudWatch alert and notify an SNS topic.","correct":false},{"id":"39f1073c8a5f51031cb743e7cf45dce2","text":"Calculate the estimated ELB cost when the total ELB usage is 5000 hours in a month. Configure a cost budget in AWS Budgets for the EC2 ELB service and set the number as the threshold. When the cost is greater than the user-defined threshold, send an email alert to the team.","correct":false}]},{"id":"edc23d5d-e9ea-4713-b8c2-e35a1aa13626","domain":"awscsapro-domain2","question":"You are helping a company design a fully cloud-based Customer Service application.  Over 50% of their Customer Service Representatives are remote and that number increases and decreases seasonally.  They need the ability to handle inbound and outbound calls as well as chatbot capabilities.  Additionally, they want to provide a self-service option using interactive voice response to customers who do not need to speak to a person.  Which design is feasible and makes most efficient use of AWS services?","explanation":"AWS Connect is Amazon's \"call center in a box\" solution that enabled interactive voice response with Lex and inbound and outbound calling.  Additionally, you can use Lex to build a chatbot.  AWS Workspaces is a managed DaaS that is we suited for deploying to remote workers.","links":[{"url":"https://aws.amazon.com/connect/","title":"Amazon Connect Overview"},{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html","title":"What Is Amazon WorkSpaces? - Amazon WorkSpaces"}],"answers":[{"id":"29d3b6d296574b52ab6aa78419ca5aad","text":"Create a standardized Customer Service Rep desktop and deploy via CloudFront.  Use Translate and AWS Connect to create a chatbot component.  Leverage Polly to create an interactive voice response component.  Use Alexa for Business for the inbound and outbound calling.","correct":false},{"id":"22128c4eb6db1f8d7e92b3fbb7155565","text":"Use AWS Comprehend to create the chatbot and interactive voice response components.  Use Asterisk PBX from AWS Marketplace to handle the inbound and outbound calling.  Create a standardized Customer Service Rep desktop and deploy using Service Catalog.","correct":false},{"id":"8ed07bd636bec143bc1f7e2ce888bb03","text":"Create a standard Customer Service Rep desktop and deploy using AWS Workspaces. Setup AWS Connect for inbound and outbound calling.  Leverage Alexa for Business to create chatbot and interactive voice response components.  Store call logs in Redshift and analyze using Quicksight.","correct":false},{"id":"da552d1c23cba5e3e05a5dca7bdc0ca5","text":"Setup Twilio with Lambda to manage inbound and outbound calling.  Create a standard Customer Service Rep desktop Windows AMI and deploy via Service Catalog.  Leverage Polly for creating a chatbot and Translate for an interactive voice response system.","correct":false},{"id":"4086f40e467d8d2fadaac8b5b9a2d49b","text":"Setup AWS Connect for inbound and outbound calling.  Make use of Polly and Lex for interactive voice response components.  Create a standard Customer Service Rep desktop and deploy using AWS Workspaces.  Leverage Lex to create a chatbot component.","correct":true}]},{"id":"e720cd54-de67-42de-ba10-593dee0582e6","domain":"awscsapro-domain3","question":"You are the Enterprise Architect in a Risk Quantification firm. The firm has a website which end-users can use to apply for loans and also track the status of their loan application if they log in. When a loan application comes in, several downstream systems need to independently process the application. Right now, the website server-side code invokes these systems one after the other, synchronously, in a tight loop. If one of these downstream systems times out or throws an exception, the entire loan application processing errors out. Even if none of these downstream systems fail, the time it takes to process a loan application is very high due to the serial nature of these systems being invoked. Your CTO wants only the loan-processing application moved to the AWS cloud and re-architected at the same time.\nThe downstream systems are all hosted on-premises and will continue to remain on-premises. They expose REST endpoints that accept POST HTTPS requests, use self-signed certificates and respond synchronously only when they are done processing an application. After re-architecture, all downstream systems must independently start processing an incoming loan application simultaneously.\nYour CTO wants to know how the loan-processing website application can be architected in the AWS Cloud, and what supporting changes will be needed in the downstream systems on-premises. He wants to minimize code changes to the downstream on-premises systems. Choose the best option","explanation":"This is an example of a verbose question with verbose answer choices. You can expect a few such questions in the exam, testing your time management skills. Try to vertically scan the answers to see which parts differ between them. Sometimes, though the answers seem big, a large part of each is identical. You can ignore those parts, as there is nothing to choose between the.\nAmong the four choices, two use SQS and two use SNS to feed the incoming loan applications to the downstream systems. You cannot automatically eliminate either SQS or SNS, as a working solution can be designed with either.\nLet us see how we can achieve this using SNS first. The basic requirement here is fan-out - a single loan application must be processed by several downstream systems, so there are multiple consumers. Hence, SNS is a natural fit. SNS supports multiple subscribers for a topic. SNS also supports HTTP/HTTPS subscribers. SNS makes POST REST API call to as many HTTP/HTTPS subscribers exist on the topic, so it fits the bill. However, there is a small problem - the requirement states that the downstream systems must be changed as little as possible. If we follow this design, we must change the HTTP Listening part of the downstream systems significantly. Because SNS is directly calling them now, SNS will use its own headers and body format. In fact, SNS POST-s two kinds of messages - one is Subscription Confirmation and one is Notification. A special HTTP header (x-amz-sns-message-type) has the right type in its value. The server side now must parse this header out and look for only the Notification type of message. The body itself will then be JSON formatted with the payload. While the server is probably used to process just the core payload (loan application data) as the HTTP body, the same will now be hidden inside a JSON field called Message inside the request body. Additionally, the downstream systems will have to deal with SNS retries, thus the loan application part must be made idempotent (if the same loan application lands twice, it will ignore the duplicates). Thus, though it is technically possible to design the solution using SNS, it will result in a lot of changes in the downstream systems. Hence, though the SNS option will work, it is not the correct answer because of this reason.\nNow, let us see how we can design this using SQS. While SQS does not support fan-out (multiple consumers for the same message), the proposed solution uses a Lambda function to achieve fan-out. The Lambda function will pick up the message, and then call the downstream systems one by one. The key to making this work is, of course, to modify the downstream systems from synchronous monolithic beasts to asynchronous servers so that they can instantly respond to the Lambda function and then continue to process the application. We will then have to provide a callback for when it is done. The solution uses an API Gateway for that purpose. Overall, the solution is elegant, and changes to the downstream systems are less than what SNS requires. Hence, SQS is the correct answer.\nNote that one version of the SNS design proposes to retain the synchronous nature of the downstream systems. That will not work as SNS will not wait more than 15 seconds for a response. The response will then be lost and the main website app will never know the results from the downstream systems.\nAlso, note that though SNS requires the HTTPS subscriber to present a trusted CA-signed certificate, there is no such requirement for Lambda because Lambda is basically your code, you can decide to trust anyone.","links":[{"url":"https://docs.aws.amazon.com/sns/latest/dg/sns-http-https-endpoint-as-subscriber.html","title":"Using Amazon SNS for System-to-System Messaging with an HTTP/S Endpoint as a Subscriber"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","title":"Using AWS Lambda with Amazon SQS"}],"answers":[{"id":"48d42d3290142cbfc8207e042690b35f","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting so that SNS does not retry, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (c) Make them idempotent for the same loan application as SNS may retry in case of lost messages or timeouts (d) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as SNS will not be able to POST to a server with self-signed certificate","correct":false},{"id":"eec2740374df8038093d636a17252168","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Override the default delivery policy on the subscriber endpoint to remove retries so that downstream systems do not have to worry about synchronous responses taking time or idempotency of retries. Make the following changes in the downstream systems - (a) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (b) Procure server certificates from a trusted Certificate Authority (CA) instead of using the self-signed certificate as SNS will not be able to POST to a server with a self-signed certificate","correct":false},{"id":"3fb725a8e71fa96168f18e50a146b4f0","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed and (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems","correct":true},{"id":"3d032e65493c0733ebe65683fb66a562","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems and (c) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as your Lambda function will not be able to POST to a server with self-signed certificate","correct":false}]},{"id":"3ca7c5e0-432a-4d40-afee-ab996819b429","domain":"awscsapro-domain3","question":"You are consulting with a client to guide them on migration of an in-house data center to AWS.  The client has stipulated in the contract that the migration cannot require any more than 1 hour downtime at a time and that there is always a fallback path.  Additionally, they want an overall increase in business continuity capabilities when the migration is done.  Their landscape is as follows:  (1) Several databases with about 1TB of data combined which are heavily used 24x7 and considered mission critical; (2) About 40TB of historic files which are read sometimes but almost never updated; (3) About 150 web servers on VMware in various states of customization of which there is a current project underway to standardize them.  The client's team has suggested some next steps but because they aren't yet familiar with AWS, they are not using equivalent AWS terms.  Translating their suggestions, which of the following activities would you choose to meet the requirements, reducing costs and management where possible?","explanation":"The database migration suggestion aligns well with DMS as it can keep the databases in sync until cutover.  SAN replication sounds a lot like Storage Gateway which is a reasonable way to migrate data to AWS.  However, simply using K8s does not convert your VMs into containers or make them serverless.  We can't restore tapes to AWS.  Creating the same VM landscape on AWS just adds an additional layer of complexity that's not needed.","links":[{"url":"https://aws.amazon.com/dms/faqs/","title":"AWS Database Migration Service FAQs - Amazon Web Services"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"}],"answers":[{"id":"31ea0eccdcea45ea4fce3b9459de52d4","text":"Use some block-level SAN replication tool to gradually migrate the on-prem historic files to AWS.","correct":true},{"id":"75d528d2ec243c60d1478ae605c89f40","text":"Build a matching VMware environment on AWS and use third-party tools to backup and restore the VMs there.","correct":false},{"id":"3a02ebcd33fe18255e4ce43e8babb730","text":"Over several months, at end of business on Friday, backup all the servers and data to tape and restore to new instances in AWS to prove out AWS capabilities and reliability.","correct":false},{"id":"801ce55cfc2a125e7d17c729ca3e2e93","text":"Create new high powered stand-alone database instances in AWS and migrate data from on-prem database.  Use log shipping to keep the databases in sync.  Once we better understand AWS, we'll rebuild the servers and repartition the tables. ","correct":true},{"id":"d2dde578790a34d9e740015474ea23e4","text":"Migrate the majority of the 150 web servers to a serverless concept by moving the VMs to a Kubernetes cluster.","correct":false}]},{"id":"083b20e3-95ff-4b8a-b655-aedf1de67c6c","domain":"awscsapro-domain4","question":"The security monitor team informs you that two EC2 instances are not compliant reported by an AWS Config rule and the team receives SNS notifications. They require you to fix the issues as soon as possible for security concerns. You check that the Config rule uses a custom Lambda function to inspect if EBS volumes are encrypted using a key with imported key material. However, at the moment the EBS volumes in the EC2 instances are not encrypted at all. You know that the EC2 instances are owned by developers but you do not know the details about how the instances are created. What is the best way for you to address the issue?","explanation":"The key must have imported key material according to the AWS Config rule. It should be a new key created in KMS. Existing KMS cannot import a new key material and AWS Managed Key such as aws/ebs cannot be modified either. CloudHSM is more expensive than KMS and is not required in this scenario. Besides, when the new encrypted EBS volume is attached, it should be attached to the same device name such as /dev/xvda1.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html","title":"How to import key material in AWS Key Management Service (AWS KMS)?"}],"answers":[{"id":"5a6b8e38c5d149a42e259d93323f12aa","text":"Modify the AWS Managed Key (AWS/EBS) in KMS to include an imported key material. Create a snapshot of the EBS volume. Then create a new volume from the snapshot with the volume encrypted. Detach the original volume and attach the new encrypted EBS to another device name of the instance.","correct":false},{"id":"03c530623f9019c80c05daa34ad8fab1","text":"Create a new EBS key from CloudHSM with imported key material. Create a new EBS volume encrypted with the new key. Attach the volume to the EC2 instance. Use Linux dd command to copy data from non-encrypted volume to encrypted volume. Unmount the old volume after the sync is complete.","correct":false},{"id":"62f46fb2afeb695bf73a050f1662cd44","text":"Create a Customer Managed Key (CMK) in KMS with imported key material. Create a snapshot of the EBS volume. Copy the snapshot and encrypt the new one with the new CMK. Then create a volume from the snapshot. Detach the original volume and attach the new encrypted EBS to the same device name of the instance.","correct":true},{"id":"94975e581f509af38c350ecdd5b951f5","text":"Import a new key material to an existing Customer Managed Key (CMK) in KMS. Create an AMI from the EC2 instance. Then launch a new EC2 instance from the AMI. Encrypt the EBS volume in the new instance. Terminate the old instance after the new one is in service.","correct":false}]},{"id":"0ee4566a-508e-472d-9789-3318e3284aca","domain":"awscsapro-domain5","question":"You are an AWS Solutions Architect and you maintain a CloudFormation stack that includes the resources of a network load balancer and an Auto Scaling group. The ASG has one running instance. A developer uses the instance for feature development and testing. However, after he adds some configurations and restarts an application process, the instance is terminated by the Auto Scaling group and a new instance is created. The new configurations are lost in the new instance. You need to modify the resource settings to make sure that the instance is not terminated by the ASG when application processes are restarted. Which of the following methods would best achieve this?","explanation":"The instance fails the health check in the ELB target group and is then terminated by ASG whenever the application processes are restarted. The prevent the ASG from terminating the EC2 instance you need to modify the health check type from ELB to EC2. As a result, even if the instance fails the health check in the ELB target group, it will not be terminated by the Auto Scaling group. You do not need to create an AMI or a new launch configuration to address the issue. And the custom health check script that runs every minute cannot prevent the instance from being terminated.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health checks for Auto Scaling instances"}],"answers":[{"id":"1584209c598c68a956227b3770a97fb2","text":"Create an AMI and configure a new launch configuration with the AMI. Then modify the Auto Scaling group to use the new launch configuration and launch a new instance.","correct":false},{"id":"745390da2b4433786bf4cdf17df3d2d3","text":"Edit a health check shell script that performs some sanity checks in the EC2 instance. If the sanity checks pass, the shell script uses AWS CLI “aws autoscaling set-instance-health” to set its status to be healthy. Run the script every minute.","correct":false},{"id":"c66bf36d77a2aa4cf5ef75a4ac494df8","text":"Store all custom configuration scripts in an S3 bucket and create a new launch configuration. In its user data section, download the scripts from the S3 bucket and execute them. Whenever a new instance is launched, the configurations can be installed automatically. ","correct":false},{"id":"8fad414c494de200ee3990b334d22b13","text":"Update the CloudFormation script and modify the health check type from ELB to EC2.","correct":true}]},{"id":"338ce579-a236-4282-9670-8da3b0baf2e9","domain":"awscsapro-domain3","question":"You are helping a Retail client migrate some of their assets over to AWS.  Presently, they are in the process of moving their Enterprise Data Warehouse.  They are planning to re-host their very large Oracle data warehouse on EC2 in a high availability configuration across AZs.  They presently have several Scala scripts that process some detailed Point of Sale data that is collected each day.  The scripts perform some aggregation on the data and import the aggregate into their Oracle database.  They want to move this process to AWS as well.  Which option would be the most cost-effective way for them to do this?","explanation":"AWS Glue is a fully managed extract, translate and loading service and is compatible with Scala.  EMR could do this but represents more overhead than necessary.  Lambda is not compatible with Scala and migrating to Redshift does not bring anything in this case if the customer wants to retain their Oracle database.","links":[{"url":"https://aws.amazon.com/glue/faqs/","title":"AWS Glue Features - Amazon Web Services"}],"answers":[{"id":"4f1ff8b853c3ba363bdd2bda53538ab4","text":"Migrate from Oracle to Redshift and use Kinesis Firehose.","correct":false},{"id":"1a4de6676c8c078310e08aad71d9dce6","text":"Migrate the processing to AWS EMR.","correct":false},{"id":"8b01d948d5ad2f4b1c8e817c2d98e7c2","text":"Migrate the processing to AWS Glue.","correct":true},{"id":"04578ae8419780f9dc441d01fe11582d","text":"Create Lambda functions using your Scala scripts.","correct":false},{"id":"a445a1a877009cd7c31858687a818116","text":"Import your Scala scripts into AWS SCT for processing.","correct":false}]},{"id":"33803c8a-b588-4dca-8067-e500383254f3","domain":"awscsapro-domain4","question":"You work for a retail services company that has 8 S3 buckets in us-east-1 region. Some of the buckets have a lot of objects in them. There are Lambda functions and EC2-hosted custom application code where the names of these buckets are hardcoded. Your manager is worried about disaster recovery. As part of her business continuity plan, she has requested you to set up Cross-Region Replication of these S3 buckets to us-west-1, ensuring that the replicated objects are using a less expensive Storage Class because they would not be accessed unless disaster strikes. You are worried that in the event of failover due to the entire us-east-1 region being unavailable, the application code, once deployed in us-west-1, must continue to work while trying to access the S3 buckets in the new region. She has also requested you to start taking periodic snapshots of EBS Volumes and make these snapshots available in the us-west-1 region so that EC2 instances can be launched in us-west-1 using these snapshots if needed. How would you ensure that (a) the launching of EC2 instances works in us-west-1 and (b) your application code works with the us-west-1 S3 buckets?","explanation":"This question presents two problems - (1) how to ensure that EBS snapshots are created periodically and are also made available in a different region for launching required EC2 instances in case of failure of the primary region (2) how to deal with application code where S3 bucket names are hardcoded and whether this hardcoding will impact disaster recovery while trying to run in a different region. Both of these problems are real-life issues AWS customers face when designing and planning their disaster recovery solutions.\n(1)Remember that Data Lifecycle Manager can only schedule snapshot creation in the same Region. If we want to copy that snapshot into a different region, we must write our own scripts or Lambda functions for doing that. Hence, the choices that state that DLM can be used to directly create the snapshot into different regions are eliminated. Additionally, only root volume snapshots can be used to create an AMI. Non-root EBS Volume snapshots cannot be used to generate an AMI. Hence, the choices that specify using non-root volume snapshots are eliminated.\n(2)Remember that S3 bucket names are globally unique. Hence, one cannot create a second S3 bucket in the DR Region with the same name as the bucket in the primary region. Hence, the options that hint the creation of S3 buckets by the same name are eliminated. This results in a problem if S3 names are hardcoded in the application - that application will simply not run in a new region, it will fail. Hence, it is best to avoid hardcoding, and fetch the S3 bucket name from a key-value storage service like AWS Systems Manager Parameter Store at runtime. Creating this Parameter Store in each region and storing the correct bucket names in them can help in designing this non-hardcoded solution. Additionally, enabling Cross-Region Replication does not copy pre-existing content. Hence, the choices that suggest that pre-existing content will be automatically copied are eliminated.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-launch-snapshot.html","title":"Launching an Instance from a Backup"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/ec2/copy-snapshot.html","title":"Copy-snapshot documentation"}],"answers":[{"id":"fae496411f2d461e0926abfdf8ad8b64","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":false},{"id":"fc1a2efe00ef610249a55dadb0dd64fe","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false},{"id":"02e8aed40d51e4bf9256f1ed436aa069","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false},{"id":"824dc187ab89444593b50521f60b8ff3","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":true}]},{"id":"05e085a9-4de3-46fe-9470-10c7f2faba57","domain":"awscsapro-domain5","question":"You are consulting with a client who is in the process of migrating over to AWS.  Their current on-prem Linux servers use RAID1 to provide redundancy.  One of the big benefits they are looking forward to with moving to AWS is the ability to create snapshots of EBS volumes without downtime.  Right now, they intend on migrating the servers over to AWS and retaining the same disk configuration.  What is your advice for them?","explanation":"Because RAID is based upon multiple volumes being in sync, taking snapshots of an individual volume that's part of a active and mounted RAID array would not create a proper backup.  You must first unmount the RAID volume and then create the snapshots of the component volumes.  This of course means any data on the RAID volume would be unavailable.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html","title":"RAID Configuration on Linux - Amazon Elastic Compute Cloud"}],"answers":[{"id":"bee3a756d6bfddce4e9917e171a4b0e2","text":"Consider using RAID0 when on AWS for performance reasons.","correct":false},{"id":"ead9938f5d4fc4d2df30763406b6a8e5","text":"Consider using RAID10 when on AWS because it offers the best of both RAID0 and RAID1.","correct":false},{"id":"73b207bc7a947de1eed26bc058b4b67b","text":"If snapshots without downtime are the priority, do not use RAID.","correct":true},{"id":"21a6a5218cf9778e0184eed7897c54ce","text":"Consider using RAID6 rather than RAID1 on AWS for performance reasons.","correct":false},{"id":"99a9d29ef15a0ced996c1510ff6d8f6a","text":"EC2 does not support RAID configurations.","correct":false}]},{"id":"4eb7884e-f6fd-4803-83bf-8ab6faca4dd5","domain":"awscsapro-domain1","question":"You have been asked to give employees the simplest way of accessing the corporate intranet and other internal resources, from their iPhone or iPad.  The solution should allow access via a Web browser, authentication via SAML integration and you need to ensure that no corporate data is cached on their device. Which option would meet all of these requirements?","explanation":"Amazon WorkLink is a fully managed, cloud-based service that enables secure access to internal websites and apps from mobile devices. It provides single URL access to the applications and also links to existing SAML-based identity providers.  Amazon WorkLink does not store or cache data on user devices as the web content is rendered in AWS and sent to user devices as encrypted Scalable Vector Graphics (SVG).  WorkLink meets all of the requirements in the question and is therefore the only correct answer.","links":[{"url":"https://aws.amazon.com/worklink/faqs/","title":"Amazon WorkLink FAQs"}],"answers":[{"id":"8b3531ac066ba672af41cfd6c438fdb9","text":"Place all internal servers in a public subnet and lock down access via Security Groups to the IP address of each mobile user","correct":false},{"id":"0d0cb2140013a20863f643412ebd4698","text":"Tunnel through a Bastion Host into your VPC and view all internal servers via a Web Browser","correct":false},{"id":"2ca6fccac916b71e240a465c8caf457e","text":"Connect into the VPC where the internal servers are located using Amazon Client VPN and view the sites using a Web Browser","correct":false},{"id":"668cfaeb2878db8e709660735f0ff009","text":"Configure Amazon WorkLink and connect to the servers using a Web Browser with the link provided","correct":true}]},{"id":"11c28d09-1ccf-46ac-a56e-3998bff9c4e4","domain":"awscsapro-domain2","question":"You bought a domain name \"example.com\" from GoDaddy which is a domain registrar and the domain name will expire in several months. You plan to start using AWS Route 53 to manage the domain and resolve its DNS queries. The transferred domain in Route 53 should be automatically renewed every year so that the domain name will never expire and you do not need to renew it manually. Which method would you use to transfer the domain name properly?","explanation":"Users can register their new domain names in Route 53 or transfer existing domain names from other registrars such as GoDaddy to Route 53. After the transfer, the domain can automatically renew every year if the Auto Renew feature is enabled. To transfer the domain name, you do not need to wait until the domain name expires. And you cannot register the same domain name in both GoDaddy and Route 53 at the same time.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-transfer-to-route-53.html","title":"Transferring Registration for a Domain to Amazon Route 53"}],"answers":[{"id":"e27ac2c31c859905f6a51f22ab3a34bf","text":"Confirm that the domain is transferable in GoDaddy. In the Route 53 AWS Management Console, click \"Transfer Domain\" to transfer registration for the domain name from GoDaddy to Route 53. Enable the automatic renewal for this domain name.","correct":true},{"id":"32c733a4bf01f533d38704f340cb7eb5","text":"Login in the GoDaddy admin account, unlock the domain transfer and request the domain transfer to Route 53. Accept the domain transfer in Route 53 and extend the expiration date to 10 years as transferred domains cannot automatically renew.","correct":false},{"id":"d8a7c0a25d22a5534a30c668de1be1d1","text":"Register the same domain name \"example.com\" in Route 53 three months before it expires in GoDaddy. Enable the feature of Transfer Lock in Route 53 to prevent it from being transferred to another registrar. Do not renew the original domain name in GoDaddy.","correct":false},{"id":"bd0fd22629d8016f6941db326aeb4ba4","text":"Wait until the domain name expires in GoDaddy and then register the domain in AWS Route 53 by clicking the \"Register Domain\" button in AWS management console. Turn on the features of Auto Renew and Transfer Lock for the new domain.","correct":false}]},{"id":"bc9f1b35-bf56-4bf1-b563-9bd2d864e4bc","domain":"awscsapro-domain2","question":"A global digital automotive marketplace is using Lambda@Edge function with CloudFront to redirect incoming HTTP traffic to custom origins based on matching custom headers or client IP addresses with a list of redirection rules. The Lambda@Edge function reads these rules from a file, rules.json, which it fetches from an S3 bucket. The file changes every day because several teams in the company uses the file for different purposes, including but not limited to, (a) the security team uses the file to honeypot potential malicious traffic (b) the engineering team uses the file to do A-B testing on new features, (c) the product team experiments with new mobile platforms by redirecting traffic from a specific kind of mobile device to a specific set of server farms, etc.. As a result, the file can be as big as 200 KB. Recently, the response time of the website has degraded. On investigation, you have found that this Lambda@Edge function is taking too long to fetch the rules.json file from the S3 bucket. The existing CI-CD pipeline deploys the file to a versioning-enabled S3 bucket when any change is committed to source control. Any change in rules.json must reflect within 1 hour at all Cloudfront Edge locations. Select two options from the ones below that will not work in improving the latency of fetching this file?","explanation":"A key to answering this question is to not miss the fact that it asks which two of the answers will not help. AWS SA-P exam can occasionally frame the question with a not. Also, knowledge of how Lambda@Edge functions work with CloudFront is important for the exam.\nThere will be no improvement in the fetching time if we reconfigure the S3 bucket as a static website. In fact, doing so might add a layer of redirection during routing.\nLambda@Edge does not guarantee the persistence of global variables in memory between invocations. While it might be possible to use global variables for a short time as cache, provided the code does not make any assumptions about the guarantee of persistence, it is a bad idea to solely depend on Lambda@Edge memory between invocations. AWS does not guarantee using the same container instance for any number of requests, though it will try to re-use a warmed up instance for the same function invocation landing on the same edge node. If it is re-using the same container instance from the one used by the last Lambda@Edge function, the global variable trick will work. However, as the option clearly says that such usage is guaranteed (which is false and will not work), it is one of the answer choices to select in this case.","links":[{"url":"https://aws.amazon.com/blogs/networking-and-content-delivery/leveraging-external-data-in-lambdaedge/","title":"Leveraging external data in Lambda@Edge"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-limits.html#limits-lambda-at-edge","title":"Limits on Lambda@Edge"}],"answers":[{"id":"058638ecef0dd62ee83782f5cbbba63b","text":"Reconfigure the S3 bucket as a static website. Use the website endpoint to download the file instead of directly accessing the bucket from the Lambda@Edge function. This will cause HTTP GET requests to be cached by S3, thus improving the latency of fetching the file","correct":true},{"id":"5a14a6d0367c2a6daf2d4faccf1fbdb3","text":"Change the Lambda@Edge code to save the contents of the rules.json file in a global variable so that it is cached in Lambda@Edge memory, with a TTL of 55 minutes, persisted between invocations. Lambda@Edge guarantees persistence of variables in memory between invocations.","correct":true},{"id":"fd6d6c32ad84f6d5856477cd8f27d361","text":"Define a separate cache behaviour for *.json in your Cloudfront web distribution, setting the origin as the S3 bucket. Change the Lambda@Edge function code to use the Cloudfront download URL instead of downloading the file directly from S3. This way, the file will be cached by Cloudfront avoiding expensive round trip time to S3 each time. Set the Cloudfront TTL to 45 minutes.","correct":false},{"id":"ffa2541e9e07a612a02728b1135014f9","text":"Include the rules.json file in the Lambda@Edge deployment package. Change the CI-CD pipeline to deploy a new Lambda@Edge version every time the file changes. Change the Lambda@Edge function code to read the file locally instead of reading it from S3. This will improve the latency of fetching the file.","correct":false}]},{"id":"8aa313fe-cd0f-4899-a2f4-e8f2fd64c245","domain":"awscsapro-domain4","question":"Your business depends on AWS S3 to host three kinds of files - images, documents and compressed installation packages. These files are accessed and downloaded by end-users from all US regions and west EU, though the compressed installation packages are downloaded rarely as users tend to access the service from their browsers instead of installing anything on their machines. Each installation package bundles several images and documents, and also includes binaries that are downloaded from a 3rd party service while creating the package files.\nThe images and documents range from a few KBs to a few hundred KBs in size and they are mostly static in nature. However, the compressed installation package files are generated every few hours because of changes done by the 3rd party service to their binaries, and some of them are as large as a few hundred GB-s. The installation package files can be regenerated from the images and documents fairly quickly if required. It is important to be able to retrieve older versions of the images and documents.\nWhich of the following storage solutions is the most cost-effective approach to design the storage for these files?","explanation":"The areas tested by this question are:\\n1. Versioning cannot be enabled at the object level. It is a bucket-level feature. This rules out the choice where we have a single bucket and selectively turn on versioning on for some objects only.\\n2. If you enable Versioning for a bucket containing large objects that are frequently created/uploaded, it will result in higher storage cost as all the previous versions will result in storage volume growing quickly because of frequent writes. In the given scenario, the compressed installation package files are large and also frequently generated (every few hours). There is no requirement to version them, as they can be quickly generated on-demand. Hence, putting them in a bucket that has Versioning enabled is not a good cost-effective solution. This rules out two choices - one where we have a single versioned bucket, the other where we enable versioning for both buckets.\\n3. Note that all options except one correctly identify the storage class requirements - the compressed installation package files should be stored as One-Zone IA because durability is not a prime requirement for these files (simply because they can be regenerated on-demand easily). They are rarely downloaded, hence IA is the correct class. Combined with low durability, One Zone IA is the most cost-effective solution. Only one option uses the incorrect storage tier for these files - note that IA is more expensive than One-Zone IA, and the question is about cost-effectiveness.\nHence, the only correct answer is the one that addresses both Versioning and Storage Class requirements correctly.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html","title":"Documentation on Object Versioning"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-howtoset","title":"Setting the Storage Class of an Object"}],"answers":[{"id":"2e4e12f367286b12f14ae74b1fd4e350","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the image and document objects only, but not for the compressed installation package files. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to Infrequent Access while uploading compressed installation package files","correct":false},{"id":"90f6b37d063a0158682b034d578bf29b","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for both the buckets. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":false},{"id":"9e88398b12bdf0dd2fdc12a19f4962a1","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for Bucket A only. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":true},{"id":"fd1d330daf4d05b4fa4c888a0584130f","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the bucket. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to One-Zone Infrequent Access while uploading compressed installation package files","correct":false}]},{"id":"872cd65b-287b-4fdb-b59f-f07f7bff707f","domain":"awscsapro-domain5","question":"Your client is a small engineering firm which has decided to migrate their engineering CAD files to the cloud.  They currently have an on-prem SAN with 30TB of CAD files and growing at about 1TB a month as they take on new projects.  Their engineering workstations are Windows-based and mount the SAN via SMB shares.  Propose a design solution that will make the best use of AWS services, be easy to manage and reduce costs where possible. ","explanation":"At present, EFS doesn't support Windows-based clients.  Storage Gateway-File Gateway does support SMB mount points.  The other options introduce additional unneeded costs.","links":[{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/limits.html","title":"Amazon EFS Limits - Amazon Elastic File System"}],"answers":[{"id":"9c4821e0d9178e80636a5d4c7d0c6441","text":"Setup Storage Gateway-File Gateway and configure the CAD workstations to mount as iSCSI.  Use a Snowball appliance to sync data daily to S3 buckets at AWS.","correct":false},{"id":"7cad520a624f4c3b174014f339f732df","text":"Use AWS CLI to sync the CAD files to S3.  Use EC2 and EBS to create an SMB file server.  Configure the CAD workstations to mount the EC2 instances.  Setup Direct Connect to ensure performance is acceptable.","correct":false},{"id":"c15496feac5aa7ce58d0c5d4813a5a29","text":"Use AWS CLI to sync the CAD files to S3.  Setup Storage Gateway-File Gateway locally and configure the CAD workstations to mount as SMB.","correct":true},{"id":"340da8cc24330ac5b143b526c880e4f7","text":"Order a Snowball appliance to migrate the bulk of the data.  Setup an EFS share on AWS and configure the CAD workstations to mount via SMB.  ","correct":false},{"id":"f52e177dc6a594ed2c0852c91b6133d3","text":"Order a Snowmobile to migrate the bulk of the data.  Setup S3 buckets on AWS to store the data.  Use AWS WorkDocs to mount the S3 buckets from the engineering workstations.","correct":false}]},{"id":"07f91ae7-094b-48a9-8924-a4d142cbbcb6","domain":"awscsapro-domain5","question":"On your last Security Penetration Test Audit, the auditors noticed that you were not effectively protecting against SQL injection attacks.  Even though you don't have any resources that are vulnerable to that type of attack, your Chief Information Security Officer insists you do something.  Your organization consists of approximately 30 AWS accounts.  Which steps will allow you to most efficiently protect against SQL injection attacks?","explanation":"Firewall Manager is a very effective way of managing WAF rules across many WAF instances and accounts.  It does require that the accounts be linked as an AWS Organization.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"3927b32bfb85cc040d2b7dcb21015fc5","text":"Use AWS WAF to create an ACL that denies requests that include SQL code.  Assign the ACL to Firewall Manager instances in each account using AWS OpsWorks.","correct":false},{"id":"2238e9ae7489214f767fa479d013cd23","text":"Create a custom NACL filter using Lambda@Edge to check requests for SQL code.  Use OpsWorks to apply the NACL across all public subnets across the organization. ","correct":false},{"id":"9a5fa8e9a1a9be9149138c6307abde19","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service.  Use Firewall Manager to create an ACL rule to deny requests that contain SQL code.  Apply the ACL to WAF instances across all organizational accounts.","correct":true},{"id":"ee9c067e3dea2fa4cd2e3968abaf86de","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations.  Use CloudFormation to implement request restrictions for SQL code on the CloudFront distributions across all accounts.  Setup a CloudWatch event to notify administrators if requests with SQL code are seen.","correct":false},{"id":"e072f443d93c569cce94eb4946a912af","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service and use Consolidated Billing. Subscribe to AWS Shield Advanced to automatically enable SQL injection protection across all sub-accounts.","correct":false}]},{"id":"f3efb368-9a43-4e1d-bf0a-1cf55bb918a8","domain":"awscsapro-domain4","question":"An AWS customer makes use of a wide variety of AWS services across multiple AWS regions. As the usage cost keeps increasing, the customer wants to get the detailed billing and cost information of his AWS account. In the meantime, the customer needs to quickly analyze the cost and usage data, visualize it, and share it through data dashboards so that he can get a better understanding of the billing and resource utilization. Which of the following methods would you choose?","explanation":"Users can get the billing and usage information from the AWS Cost and Usage Reports. The reports downloaded in an S3 bucket can be further processed and analyzed by QuickSight. The reports cannot be created in AWS Cost Explorer or billing dashboard. The question also asks for data visualization and dashboards. QuickSight, AWS's Business Intelligence service, is the most appropriate service for this requirement. Athena, Redshift or Elastic MapReduce are not suitable.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-upload-aws-cost-usage-reports-to-redshift-and-quicksight/","title":"Upload AWS Cost & Usage Reports to QuickSight"}],"answers":[{"id":"36fb0d3b243e97c8ed14e32e6a2bc329","text":"Generate the billing reports from the AWS management console and upload the files to an S3 bucket. Create an Amazon Elastic MapReduce (EMR) cluster by specifying data inputs and outputs. Process the data in the cluster and generate reports for the admin IAM user.","correct":false},{"id":"d3ef8ffbe6a0ff9378c5e248f2f7e165","text":"Download the AWS Cost and Usage Reports from the AWS billing dashboard in an S3 bucket which is owned by the administrators. Integrate the reports with an Amazon Redshift cluster. Analyze and view the data in the Redshift data warehouse.","correct":false},{"id":"fc09f5c144565de3b8472bde413e896a","text":"Create a data pipeline that downloads the monthly cost and usage reports from AWS Cost Explorer and uploads the reports to an S3 bucket. Set up a specific billing table with Amazon Athena and analyze the billing and usage data using SQL query commands.","correct":false},{"id":"78f31d3919e8b532d502611700598d75","text":"Create the hourly AWS Cost and Usage Reports and enable the data integration with Amazon QuickSight. Analyze and visualize the data in QuickSight by creating interactive dashboards and generating custom reports.","correct":true}]},{"id":"7b8f1346-92b4-4a87-86d2-c8e65935589b","domain":"awscsapro-domain2","question":"A non-profit organization is putting on a comedy night fundraiser. Tickets are sold online, and attendees are asked to upload a facial photo to check-in at the venue the night of the event. The organization would like to provide the most streamlined entry process possible. A non-profit representative will take a photo at the door to match the photo submitted by the attendee at the time of ticket purchase. Which architecture will provide the highest level of automation to meet their needs?","explanation":"Amazon Rekognition can extract facial features into a feature vector and store that vector in a face collection via the IndexFaces API. Storing the face_id of the vector in DynamoDB, along with the attendee's name, provides the capability to validate the ticket sale at the door. At the event, the picture that's taken can be sent to Rekognition's SearchFaceByImage API to look for a match in the face collection. When a match is found, the face_id can be used to retrieve the attendee's name from DynamoDB to validate the event check-in. AWS DeepLens doesn't provide a SearchFaceByImage API to look for matching images in S3. Having Lambda call the Rekognition CompareFaces API will work, but will require much more coding to compare all the images one-by-one rather than using the automated face collection functionality. Rekognition can not be configured as a target for a Kinesis stream.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition"},{"url":"https://aws.amazon.com/solutions/auto-check-in-app/?did=sl_card&trk=sl_card#","title":"Auto Check-In App"}],"answers":[{"id":"9369a3a40d7028e2ff2c87f02c0339a9","text":"Write the attendee's facial photo to an Amazon Kinesis stream when the ticket is purchased. Configure Amazon Rekognition as the target of the stream to create a facial feature vector and store it in Amazon S3. At the event, have a Python-based UI use a camera to take the attendee's picture and send it to another Amazon Kinesis stream with Rekognition as the target. Have Rekognition compare the attendee's image with those in S3 for a match.","correct":false},{"id":"2caca948e6ce72046a513b4b48b36d19","text":"Send the attendee's facial photo to Amazon S3 when the ticket is purchased. Trigger a Lambda function to store the attendee's name and photo S3 object key in Amazon DynamoDB. At the event, have an AWS DeepLens take the attendee's picture and look to match it with an image in S3 via the SearchFaceByImage API. If a match is found, trigger an AWS Lambda function use the S3 object key to retrieve the attendee's name from the DynamoDB.","correct":false},{"id":"eea359cb33a2ae4f87ccbe358a59201d","text":"Store the attendee's facial photo in Amazon S3 when the ticket is purchased. Trigger a Lambda function to write the attendee's name and photo S3 object key to Amazon DynamoDB. At the event, have a Python-based UI use a camera to take the attendee's picture and send it to Amazon API Gateway, which triggers another Lambda function. Have this Lambda function store the image in S3, and call the Rekognition CompareFaces API to search S3 images for a match. Retrieve the matched attendee's name from the DynamoDB.","correct":false},{"id":"8f60e8fc429b98a1dc7a37e6c71ca61b","text":"Upload the attendee's facial photo to Amazon S3 when the ticket is purchased. Trigger a Lambda function, which calls Amazon Rekognition to create a feature vector and store it in a face collection. Store the attendee's name and vector id in Amazon DynamoDB. At the event, have a Python-based UI use a camera to take the attendee's picture and send it to Amazon API Gateway, which triggers another Lambda function. Have this Lambda function call Rekognition to search the face collection and return an id if a match is found. Retrieve the attendee's name from the DynamoDB.","correct":true}]},{"id":"bbcb9a8c-f84d-4424-b199-9047a4625e15","domain":"awscsapro-domain2","question":"Your company's DevOps manager has asked you to implement a CI/CD methodology and tool chain for a new financial analysis application that will run on AWS. Code will be written by multiple teams, each team owning a separate AWS account. Each team will also be responsible for a Docker image for their piece of the application. Each team's Docker image will need to include code from other teams. Which approach will provide the most operationally efficient solution?","explanation":"AWS CodePipeline, AWS CodeCommit, and AWS CodeBuild all allow cross-account access once the appropriate resource-level permissions have been granted. Orchestrating deployments from a single DevOps account will provide the most operationally efficient solution, resulting in less need for coordination of services and configurations across development team accounts.","links":[{"url":"https://aws.amazon.com/products/developer-tools/","title":"Developer Tools on AWS"},{"url":"https://aws.amazon.com/blogs/devops/how-to-use-cross-account-ecr-images-in-aws-codebuild-for-your-build-environment/","title":"How to Use Cross-Account ECR Images in AWS CodeBuild for Your Build Environment"}],"answers":[{"id":"4a4bd59860afb498d97f0d01cff52b7a","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in a single DevOps account","correct":false},{"id":"b9ae951b67f2fed4a145bd7f591c8631","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the DevOps account to AWS CodeCommit in the team accounts to get the latest code. Perform cross-account access from AWS CodeBuild in the DevOps account to AWS CodeBuild in the team accounts to get the Docker images. Perform deployments from AWS CodeDeploy in the DevOps account","correct":true},{"id":"c32df9c97fbc40e07515d4ca41de63e2","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in the team accounts","correct":false},{"id":"83443cdcabe51198b91ed96d84eed4a6","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform all deployments from AWS CodeDeploy in the DevOps account","correct":false}]},{"id":"0a4b2449-9275-4c2f-af02-0f8c51614f3a","domain":"awscsapro-domain2","question":"You are part of a business continuity team at a consumer products manufacturer.  In scope for the current project is the company web server which serves up static content like product manuals and specification sheets which customers can download.  This landscape consists only of a single NGINX web server and 5TB of local attached storage for the static content.  In the case of a failover, RTO has been defined as 15 minutes with RPO as 24 hours as the content is only updated a few times a year.  Staff reductions and budget constraints for the year mean that you need to carefully evaluate and choose the most cost-effective and most automated solution in the case of a failover.  Which of the following would be the most appropriate given the situation?","explanation":"In this case, the most cost-effective and most automated way to ensure the reliability statistics would be to migrate the static content to S3.  This option has built-in robustness and will cost less than any other option presented.","links":[{"url":"http://d36cz9buwru1tt.cloudfront.net/AWS_Disaster_Recovery.pdf","title":"Using Amazon Web Services for Disaster Recovery"}],"answers":[{"id":"46b84866da301243767946743c6024a1","text":"Download and configure the AWS Storage Gateway, creating a volume which can be replicated to AWS S3.  Attach that volume to the web server via iSCSI and migrate the content to that Storage Gateway volume.  Locate an AMI from the AWS Marketplace for NGINX.  If a failover is required, manually launch the AMI and run an RSYNC between the on-prem server and the EC2 server to migrate the content.","correct":false},{"id":"2d14eb477a2f2c9dc3605ea5740297cd","text":"Create a small pilot-light EC2 instance and configure with NGINX. Configure a CRON job to run every 24 hours that syncs the data from the on-prem web server to the pilot-light EC2 EBS volumes.  Configure an Application Load Balancer to direct traffic to the on-prem web server until a health check fails.  Then, the ALB will redirect traffic to the pilot light EC2 instances. ","correct":false},{"id":"69fd92fc5be4948bfc0128d02ed2f392","text":"Install the CloudWatch agent on the web server and configure an alarm based on a health check.  Create an EC2 replica installation of the web server and stop the instances.  Create a Lambda function that is triggered by the health check alarm which starts the dormant EC2 instance and updates a DNS entry in Route 53 pointing to the new server.","correct":false},{"id":"925fb4f48c2c90011e7e1f92d3412dcd","text":"Migrate the static content to an EFS share.  Mount the EFS share via NFS from on-prem to serve up the web content.  Configure another EC2 instances with NGINX to also mount the same share.  Upon fail-over, manually redirect the Route 53 record for the web server to the IP address of the EC2 instance.","correct":false},{"id":"bceaccaea071754d3724eaf31f0f6189","text":"Migrate the website content to an S3 bucket configured for static web hosting.  Create a Route 53 alias record for the web server domain.  End-of-life the on-prem web server.","correct":true}]},{"id":"b533b3c1-222f-4f33-99da-2c828e98ff91","domain":"awscsapro-domain5","question":"You have run out of root disk space on your Windows EC2 instance.  What is the most efficient way to solve this?","explanation":"We can easily increase the size of an EBS from the console or the CLI (using modify-volume) but then we also need to allow the OS to expand the resized volume so we can use it.  For Windows Server, we could use Disk Manager.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/expand-ebs-root-volume-windows/","title":"Expand the EBS Root Volume of Your EC2 Windows Instance"}],"answers":[{"id":"229b013eff2d5f53df7b9c3a60bd2418","text":"From the AWS Console, select Modify Volume for the EBS volume.  Enter the new size and confirm the change.  Connect to your Windows instance and use Disk Manager to extend the newly resized volume.","correct":true},{"id":"b893490015da5047b17b1220d43f4a1c","text":"From the AWS CLI, use the \"modify-instance\" command for EC2 to resize the volume to a larger size.  Using RDP, connect to the Windows instances and use Disk Manager to expand the volume.","correct":false},{"id":"38862a719074689f75df6a20a42f7df7","text":"Use AWS System Manager Run service to remotely execute a PowerShell script using AWS Tools for PowerShell to expand the volume using the ModifyInstance command.","correct":false},{"id":"346ac05b4353d34c630eb6d233f8a35d","text":"Compress all files on the root volume using the built-in zip utility.  Modern versions of Windows will automatically unzip the files when they are accessed.","correct":false}]},{"id":"f7d7767b-9159-4e53-8e37-ff9bf41ace17","domain":"awscsapro-domain5","question":"You are working with a client to help them design a future AWS architecture for their web environment.  They are open with regard to the specific services and tools used but it needs to consist of a presentation layer and a data store layer.  In a brainstorming session, these options were conceived.  As the consulting architect, which of these would you consider feasible?","explanation":"The only two options which contain feasible options are the Beanstalk and S3/Dynamo methods.  One would not create a new K8s deployment for for a new web update.  CodeBuild and AWS Config are not the correct tools for how they are being suggested.","links":[{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild – Fully Managed Build Service"}],"answers":[{"id":"c27a151a7715575fb1ebf0225c6aee09","text":"Use the AngularJS framework to create a single-page application.  Use the API Gateway to provide public access to DynamoDB to serve as the data layer.  Store the web page on S3 and deploy it using CloudFront.  When changes are required, upload the new web page to S3.  Use S3 Events to trigger a Lambda function which expires the cache on CloudFront.","correct":true},{"id":"01a7f4c09f65e41884b0a72843a8d55b","text":"Deploy an auto scaling group of EC2 instances behind an Application Load Balancer.  Provision a Mulit-AZ RDS instance to act as the data store, configuring a caching layer to offload queries from the database.  Use a User Script in the AMI definition to download the latest web assets from S3 upon boot-up.  When changes are required, use AWS Config to automatically fetch a new version of web content from S3 when a new version is created.","correct":false},{"id":"f5c7f9be39386b15747c0fe57d5040ba","text":"Deploy Kubernetes on an auto-scaled group of EC2 instances.  Define pods to represent the multiple tiers of the landscape.  Use ElastiCache for Memcached to offload queries from a Multi-AZ RDS instance.  To deploy changes to the landscape, create a new EKS deployment containing all the updated service containers and deploy them to replace all the previous existing tiers.  Ensure the DevOps team understands the rollback procedures.","correct":false},{"id":"d6a73928290b05c25d87e26ece9e94a6","text":"Create a monolithic architecture using Elastic Beanstalk configured in the console.  Create an RDS instance outside the Beanstalk environment and configure it for multi-AZ availability.  When a new landscape change is required, use a command line script to implement the change.","correct":true},{"id":"ca4cc92538f3b73d221c9b5d4378e1f8","text":"Setup a traditional three tier architecture with a CloudFormation template per tier and one master template to link in the others.  Configure a CodeBuild stack and set this stack to perform automated Blue Green deployments whenever any code change is made.","correct":false}]},{"id":"02a9611c-591c-4280-bb83-6c65c7c4921f","domain":"awscsapro-domain5","question":"A sporting goods retailer runs WordPress on Amazon EC2 Linux instances to host their customer-facing website. An ELB Application Load Balancer sits in front of the EC2 instances in Auto Scaling Groups in two different Availability Zones of a single AWS region. The load balancer serves as an origin for Amazon CloudFront. Amazon Aurora provides the database for WordPress with the master instance in one of the Availability Zones and a read replica in the other. Many custom and downloaded WordPress plugins have been installed. Much of the DevOps teams' time is spent manually updating plugins across the EC2 instances in the two Availability Zones. The website suffers from poor performance between the Thanksgiving and Christmas holidays due to a high occurrence of product catalog lookups. What should be done to increase ongoing operational efficiency and performance during high-volume periods?","explanation":"ElastiCache Memcached will provide in-memory access speeds for the catalog read transactions. A WordPress plugin is required to leverage caching. WordPress can access an EFS Mount Target for file sharing across all instances. Aurora offers a MySQL option, and WordPress requires MySQL, so the solution would have been set up that way already. CodeDeploy could update plugins on all instances, and will work well for the custom in-house code, but triggering the updates of downloaded plugins will need to be orchestrated. Aurora Auto Scaling will distribute catalog reads across multiple replicas for increased performance, but not to the extent of in-memory caching. Elastic File System is a managed service providing operational advantages over NFS file shares. ElastiCache Redis will provide the in-memory read performance desired, but changing the wp-config.php file won't provide access to it, as a plugin is needed for that. WordPress does work with S3, but a shared file system is easier to implement.","links":[{"url":"https://aws.amazon.com/getting-started/projects/build-wordpress-website/","title":"Build a WordPress Website"},{"url":"https://github.com/aws-samples/aws-refarch-wordpress?did=wp_card&trk=wp_card","title":"Hosting WordPress on AWS"}],"answers":[{"id":"f060160b20c4408b2442010d3ea4d387","text":"Use Amazon ElastiCache Redis as a caching layer between the EC2 instances and the database. Change wp-config.php to point to the Redis caching layer, and have Redis point to Aurora. Move the WordPress files to S3 and have WordPress access them there.","correct":false},{"id":"aeb27370afc3b672eb0a1afcb28e9176","text":"Implement Aurora Auto Scaling to increase the number of replicas automatically as demand increases. Create an NFS file share to hold the WordPress files. Access the file share from the EC2 instances in both Availability Zones.","correct":false},{"id":"17b12e57cd85610e888cda82b5a8a145","text":"Migrate the WordPress database to RDS MySQL since MySQL is WordPress's native database and WordPress is performance optimized for MySQL. Implement AWS CodeDeploy to update WordPress plugins on all EC2 instances.","correct":false},{"id":"bc643d3342a5a675e65e5baed00e88b9","text":"Deploy Amazon ElastiCache Memcached as a caching layer between the EC2 instances and the database. Install a WordPress plugin to read from Memcached. Implement Amazon Elastic File System to store the WordPress files and create mount targets in each EC2 subnet.","correct":true}]},{"id":"343601ea-0262-46dc-baab-511550237b8f","domain":"awscsapro-domain2","question":"You work for a distributed large enterprise that uses Splunk as their log aggregation, management and analysis software. The company has recently shown a keen interest in adopting a microservice based architecture and wants to convert some of its applications to Docker containers. They have selected AWS ECS as the orchestration platform for these containers. They are interested only in EC2 Launch Type as the Security Team wants to harden the EC2 instances with policies they want to control. However, the Chief Architect is concerned about hardcoding the Splunk token inside the container code or configuration file. Splunk requires any logging request to include this token. The Chief Architect wants to know if there is a way to pass the production Splunk token to ECS at runtime so that the container tasks can continue logging to Splunk without exposing the production secret to all developers, and if yes, how. Select the best answer.","explanation":"ECS Task Definition file supports two ways of specifying secretOptions in its logConfiguration element - AWS Systems Manager Parameter Store and AWS Secrets Manager. This question only deals with only one of these two ways, as it does not mention AWS Secrets Manager at all. However, the real focus of the question is how KMS is used by AWS Systems Manager Parameter Store. There is actually nothing special about how Parameter Store deals with KMS. Hence, the question actually is just about knowing the ways KMS Keys can be used - they can either be customer-managed, or AWS managed or both AWS and customer-managed.\nAnother aspect tested by the question is whether to use Task Role or EC2 Instance Role while granting an ECS Cluster permission to access resources during container execution. AWS recommends using the Task Role in this scenario, hence we can eliminate the two choices that specify using EC2 Instance Roles.\nThe option that says that the encryption key must only be stored by the customer is incorrect, as we can use KMS to store the key. The options that state the only supported management options for the KMS-stored keys are either only customer-managed or only AWS managed are both incorrect. The correct option identifies that KMS keys can either be customer-managed or AWS managed.\nOne thing to note is that this question does not need any prior knowledge of Splunk. Everything that needs to be known about Splunk is stated as part of the question. The AWS SA-P exam normally does not require the candidate to know about any 3rd-party product or services, but such services can be explicitly named.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html#secrets-logconfig","title":"Injecting Sensitive Data in a Log Configuration"},{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html","title":"How AWS Systems Manager Parameter Store Uses AWS KMS"},{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html","title":"IAM Roles for Tasks"}],"answers":[{"id":"8a8783b1fab2fb72bc61295df2630a46","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can only be stored and managed by the customer - therefore the value must be encrypted by the customer before writing to the Parameter Store and decrypted by the customer after reading from the Parameter Store. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. Each EC2 node must have an associated EC2 Instance Role that allows it to read AWS Systems Manager Parameter Store","correct":false},{"id":"ee244c2ca9783d96974f0c603a4aa548","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS but must be solely AWS-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. The task definition must include a task role that allows the container task to read AWS Systems Manager Parameter Store and decrypt using an AWS-managed KMS key","correct":false},{"id":"c3037df22b608e2533a356125fab5322","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS but must be solely customer-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. Each EC2 node must have an associated EC2 Instance Role that allows it to read AWS Systems Manager Parameter Store and decrypt using a customer-managed KMS key","correct":false},{"id":"a161521ba68b3fbb6a60532e1bf664c0","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS and can either be customer-managed or be AWS-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. The task definition must include a task role that allows the container task to read AWS Systems Manager Parameter Store and decrypt using a KMS key, this latter permission being required only if the key is customer-managed and not AWS managed","correct":true}]},{"id":"95f1d7a8-c3d4-4fec-952a-72385aa8b4c8","domain":"awscsapro-domain5","question":"You are consulting for a company that performs specialized customer data analytics.  Their customers can upload raw customer data to a website and receive back demographic statistics.  Their application consists of a REST API created using PHP and Apache.  The application is self-contained and works in real-time to return results as a JSON response to the REST API call.  Because there is customer data involved, company policy states that data must be encrypted in transit and at rest.  Sometimes, there are data quality issues and the PHP application will throw an error.  The company wants to be notified immediately when this occurs so they can proactively reach out to the customer.  Additionally, many of the company's customers use very old mainframe systems that can only access internet resources using IP address rather than a FQDN.  Which architecture will meet these requirements fully?","explanation":"The requirement of a static IP leads us to a Network Load Balancer with an EIP.","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/","title":"Elastic Load Balancing Features"}],"answers":[{"id":"a9ed133e35b8332aea2bf603521b891a","text":"Provision an Application Load Balancer in front of your EC2 target group and offload SSL to CloudHSM.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch and configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from CloudHSM.","correct":false},{"id":"569eec0061a1a97be77e3bdab43a1756","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Offload SSL termination using AWS KMS.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"434ce04b2c3a4d2e679d37df43de2585","text":"Provision an Application Load Balancer with an EIP in front of your EC2 target group and terminate SSL at the ALB.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"5e4c5230c7e08202a0ea0575d5412d57","text":"Provision a Network Load Balancer with an EIP in front of your EC2 target group.  Install the CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.  Terminate SSL on the EC2 instances.","correct":true},{"id":"e53df806e37b325d7f61be27772875f1","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Enabled SSL termination on the API Gateway using Certificate Manager.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"0c8e5c32f081b0484e86b71651ae3642","text":"Provision a Network Load Balancer in front of your EC2 target group and terminate SSL at the load balancer using Certificate Manager.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false}]},{"id":"f3fef147-7b9d-45b6-8b2b-d943c90e8920","domain":"awscsapro-domain5","question":"You are assisting a company in the migration of their container-based web landscape over to Amazon.  They have a total of 21 containers which comprise their DEV, QA and Production environments.  All environment are identical in design and size.  Each environment consists of 3 web servers, 3 app servers and 1 datastore server.  Given the landscape, which of the provided options would be best for them to minimize maintenance?","explanation":"Deploying containers via ECS is a good option but we would want to use the EC2 hosted path.  Fargate is generally used for transient workloads and our datastore would be something we'd want to persist.  We might be able to deploy the data store with RDS, but the question does not make it clear if the data store is an RDS-supported database.  It could be a NoSQL data store or some other database unsupported by RDS.  Similarly, a MEAN stack under Elastic Beanstalk might not be compatible with our landscape either.","links":[{"url":"https://aws.amazon.com/ecs/resources/","title":"Resources for Amazon ECS - run containers in production"}],"answers":[{"id":"e0ea997f77cb156d35ec716cf772c49c","text":"Deploy the web, app and database containers using ECS.  Make use of Fargate for the underlying ECS infrastructure.","correct":false},{"id":"619957021a43a829fbb6228467323ca1","text":"Deploy the web, app and database servers using ECS on EC2.  Purchase 1-year reserved instance contracts for the required EC2 instances.","correct":true},{"id":"c1354e6d48fedccbf7b4e9c18854d980","text":"Redeploy the web landscape on a MEAN stack under Elastic Beanstalk, making use of auto-scaling groups to right-size the respective environments.  ","correct":false},{"id":"f05469f4c7578263f4271e7514c338ef","text":"Deploy the web and app servers in each environment using ECS.  Provision an RDS instance for each environment.  Use AWS Systems Manager to provide a common management console.","correct":false}]},{"id":"3b286cd3-ec67-4bc4-8f37-e19e6f629198","domain":"awscsapro-domain2","question":"You are the solution architect for a research paper monetization company that makes large PDF Research papers available for download from an S3 bucket. The S3 bucket is configured as a static website. A Route53 CNAME record points the custom website domain to the website endpoint of the S3-hosted static website. As demand for downloads has increased throughout the world, the architecture board has decided to use a Cloudfront web distribution that fetches content from the website endpoint of the static website hosted on S3. The Route 53 CNAME record will be modified to point at the Cloudfront distribution URL.\nFor security, it is required that all request from client browsers use HTTPS. Additionally, the system must block anyone from accessing the S3-hosted static website directly other than the Cloudfront distribution. Which approach meets the above requirements?","explanation":"The key to answering this question correctly is to note the fact that the origin is a website and not just a plain S3 bucket - note the usage of the phrase website endpoint in the question. While setting up such an origin, one cannot just pick the S3 bucket as the origin, or use OAI. Hence, the two choices that rely on picking the S3 bucket as the origin and using OAI to restrict access are incorrect.\nIn the given scenario, the Cloudfront web distribution is being configured to use the website endpoint of the static website as the origin. A big difference between these two scenarios is - if you use an S3 bucket as the origin, Cloudfront uses the REST API interface of S3 to communicate with the origin. If you use the website endpoint as the origin, Cloudfront uses the website URL as the origin. These endpoints have different behaviours - see the link titled Key Differences Between the Amazon Website and the REST API Endpoint. S3 REST API is more versatile, allowing the client to pass richer information like AWS Identity, thereby allowing the exchange of information that makes OAI possible. That is the reason why OAI cannot be used when Cloudfront is using the website endpoint where only GET and HEAD requests are allowed on objects.\nTherefore, in this scenario, OAI cannot be used. Instead, we have to use a custom header that only Cloudfront can inject into the Origin-bound HTTP request. The bucket policy of the S3 bucket hosting the static website can then check for the existence of said header. The assumption here is that if any browser ever directly uses the website URL of the S3-hosted static website (which is of the format examplestaticwebsitebucket.s3-website-us-east-1.amazonaws.com), their request will not contain this header, and hence will be rejected by the bucket policy.\nAlso, S3-hosted static websites do not support HTTPS. Therefore, Origin Protocol Policy, in this case, cannot be set to HTTPS Only. We can only set Viewer Protocol Policy. Only the browser to Cloudfront half will be HTTPS. The Cloudfront to Origin half cannot be HTTPS in this case","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteEndpoints.html#WebsiteRestEndpointDiff","title":"Key Differences Between the Amazon Website and the REST API Endpoint"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html","title":"Values That You Specify When You Create or Update a Distribution"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/","title":"How do I use CloudFront to serve a static website hosted on Amazon S3?"}],"answers":[{"id":"99836ac4dd080c1897cc5b3fc4d05bf7","text":"While setting up the Cloudfront Web Distribution, select the S3 bucket as the origin. Select Restrict Bucket Access to Yes, and create a new Origin Access Identity (OAI) that will prevent anyone else other than the Cloudfront web distribution to access the S3 bucket. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS. Additionally, set the value of Origin Protocol Policy to HTTPS Only.","correct":false},{"id":"2e8883471b2e107d4883062beed92ea6","text":"While setting up the Cloudfront Web Distribution, use the website endpoint of the S3-hosted static website as the Origin Domain Name. Also, set up Origin Custom Header. Then specify a header like Referrer, with its value set to some secret value. Set the bucket policy of the S3 bucket to allow s3 GetObject on the condition that the HTTP request includes the custom Referrer header. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS. Additionally, set the value of Origin Protocol Policy to HTTPS Only.","correct":false},{"id":"bf689f9ae9dcc4c34420f34e012811a4","text":"While setting up the Cloudfront Web Distribution, use the website endpoint of the S3-hosted static website as the Origin Domain Name. Also, set up Origin Custom Header. Then specify a header like Referrer, with its value set to some secret value. Set the bucket policy of the S3 bucket to allow s3 GetObject on the condition that the HTTP request includes the custom Referrer header. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS.","correct":true},{"id":"3a34f270648088ee645d11ba0aa6420d","text":"While setting up the Cloudfront Web Distribution, select the S3 bucket as the origin. Select Restrict Bucket Access to Yes, and create a new Origin Access Identity (OAI) that will prevent anyone else other than the Cloudfront web distribution to access the S3 bucket. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS.","correct":false}]},{"id":"dac73d1f-8c64-48b1-90be-3432e789933d","domain":"awscsapro-domain2","question":"Your company is bringing to market a new Windows-based application for Computer Aided Manufacturing.  As part of the promotion campaign, you want to allow users an opportunity to try the software without having to purchase it.  The software is quite complex and requires specialized drivers so it's not conducive to allowing the public to download and install in their own systems.  Rather you want to control the installation and configuration.  Therefore, you want something such as a VDI concept.  You'll also need to have a landing page as well as a custom subdomain (demo.company.com) and limit users to 1 hour of use at a time to contain costs.  Which of the following would you recommend to minimize cost and complexity?","explanation":"AppStream is a way to deploy an application on a virtual desktop and allow anyone with a browser to use the application.  This is the most efficient and simplest option given the other choices.","links":[{"url":"https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html","title":"What Is Amazon AppStream 2.0? - Amazon AppStream 2.0"}],"answers":[{"id":"9b85d56999fa276cfa4a01df8195700c","text":"Create a landing page in HTML and deploy to an S3 bucket configured as a Static Web Host.  Embed in the HTML a Javascript-based RDP client that is downloaded with the webpage.  Create a CloudFront distribution with the S3 bucket as the origin.  Use an S3 Event to launch a Lambda function which starts up an EC2 instance with your golden AMI.  Once the instance is up and running, use a web socket call from the Lambda function to initiate the RDP client and log the user in.  After 1 hour, have the Lambda function issue a shutdown command to the EC2 instance.","correct":false},{"id":"129d13b81f52a8ab01ce9e8ea7009289","text":"Create a landing page in HTML and deploy to an S3 bucket configured as a Static Web Host.  Use Route 53 to create a DNS record for the \"demo\" subdomain as alias record for the S3 bucket.  Deploy your application using Amazon AppStream.  Set Maximum Session Duration for 1 hour.","correct":true},{"id":"a1c67181ea5765383a7b477e821391b4","text":"Configure an EC2 auto scaling fleet of spot instances with your golden AMI.  Create security groups to allow inbound RDP for the auto scaling group.  Deploy Apache Guacamole on an EC2 instance and place your landing page in its web server directory.  Use Guacamole to provide an RDP session into one of the EC2 instances directly in the users browser.  Use AWS Batch to reboot the EC2 instances after 1 hour of runtime.  ","correct":false},{"id":"bdc9c29dcecb6611cd71c4660fc235ca","text":"Deploy your application as an app in the Workspaces Application Manager.  Spin up several Workspaces and configure them to automatically install your application via WAM. Create the landing page such that it redirects to the web client for Workspaces and deploy the landing page via S3 configured as a web host.  Use Route 53 to create a DNS record for the demo subdomain as an alias record for the S3 bucket.  Configure the Workspaces for a 1 hour timeout.  ","correct":false}]},{"id":"54d12a9a-149b-42a7-8491-300583d5c2b8","domain":"awscsapro-domain5","question":"A client is trying to setup a new VPC from scratch.  They are not able to reach the Amazon Linux web server instance launched in their VPC from their on-prem network using a web browser.  You have verified the internet gateway is attached and the main route table is configured to route 0.0.0.0/0 to the internet gateway properly.  The instance also is being assigned a public IP address.  Which of the following would be another potential cause of the problem?","explanation":"For an HTTP connection to be successful, you need to allow port 80 inbound and allow the ephemeral ports outbound.  Additionally, it is possible that the subnet is not associate with the route table containing the default route to the internet.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html","title":"Troubleshooting Connecting to Your Instance - Amazon Elastic Compute Cloud"}],"answers":[{"id":"d0f5e76f9fc753305b11c4c3a11e97ef","text":"The IAM role assigned to the LAMP instances does not have any policies assigned.","correct":false},{"id":"c7ca1b6a8fe855bda71123163488960b","text":"The customer has disabled the ec2-user account on the Amazon Linux instance.","correct":false},{"id":"01b7463243eb231b840fcd4b737e044b","text":"The default route to the internet gateway is incorrect.","correct":false},{"id":"c9f886542d1dabe99bc64dd39c5e1615","text":"The inbound security group allows port 80 and 22 only.","correct":false},{"id":"2bde109ce87f4a4f513679f31116184d","text":"The instance does not have an elastic IP address assigned. ","correct":false},{"id":"25d3393c550b8dbc2179367832573598","text":"The outbound network ACL allows port 80 and 22 only.","correct":true},{"id":"32cdd854c9d71059eac396dd1249830e","text":"The subnet of the instance is not associated with the main route table.","correct":true}]},{"id":"9882b1ed-c0de-4205-8a00-54731bef3109","domain":"awscsapro-domain2","question":"You work for an organic produce importer and the company is trying to find ways to better engage with its supply chain.  One idea is to create a public ledger that all members of the supply chain could update and query as products changed hands along the journey to the customer.  Then, your company could create an app that would allow consumers to view the chain from producer to end retailer and have confidence in the product sourcing.  Which AWS service or services could most directly help realize this vision?","explanation":"Amazon Quantum Ledger Database (QLDB) is a fully-managed ledger database that provides a transparent, immutable and verifiable transaction log.  While other products could be used to create such a supply chain logging solution, QLDB is the closest to a ready-made solution.","links":[{"url":"https://aws.amazon.com/qldb/","title":"Amazon QLDB"}],"answers":[{"id":"a9d83c7f8f0b0f2a8f67b7097ee73e3a","text":"Amazon QLDB","correct":true},{"id":"a8312fbd65c49606c59b53a8a062ecff","text":"Amazon Managed Blockchain","correct":false},{"id":"680da36fb45c26a1d8e7996c6f5014cd","text":"Amazon CloudTrail and API Gateway","correct":false},{"id":"35b417ac310d92018a8db5515d4fae60","text":"Amazon DynamoDB and Lambda","correct":false},{"id":"ca649f7447f846ed8add8c396187b83a","text":"Amazon P2PShare and API Gateway","correct":false}]},{"id":"3f7fa126-1155-4aa3-802d-e9eeb75f5e5a","domain":"awscsapro-domain3","question":"You work for a Clothing Retailer and have just been informed the company is planning a huge promotional sale in the coming weeks.  You are very concerned about the performance of your eCommerce site because you have reached capacity in your data center.  Just normal day-to-day traffic pushes your web servers to their limit.  Even your on-prem load balancer is maxed out, mostly because that's where you terminate SSL and use sticky sessions.  You have evaluated various options including buying new hardware but there just isn't enough time.  Your company is a current AWS customer with a nice large Direct Connect pipe between your data center and AWS.  You already use Route 53 to manage your public domains.  You currently use VMware to run your on-prem web servers and sadly, the decision was made long ago to move the eCommerce site over to AWS last.  Your eCommerce site can scale easily by just adding VMs, but you just don't have the capacity.  Given this scenario, what is the best choice that would leverage as much of your current infrastructure as possible but also allow the landscape to scale in a cost-effective manner?","explanation":"A Target Group for an ALB can contain instances or IP addresses.  In this case, we can define the private IP addresses of our on-prem web servers along side the private IP addresses of any EC2 instances we spin up.  The caveat is that we can only use private IP addresses when defining a target group in this way.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancing-via-ip-address-to-aws-on-premises-resources/","title":"New – Application Load Balancing via IP Address to AWS & On-Premises  Resources | AWS News Blog"}],"answers":[{"id":"6d3db4c52e96931f925f17fe8e9fd50f","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using public IP addresses of your on-prem web servers and additional EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"4df24111c113846bfe0505ad0c84d9a3","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define two target groups:  one containing the public IP addresses of your on-prem load balancer and one including an auto scaling group of additional EC2 instances created from the imported AMI.  Assign both target groups to the ALB using the same listener port.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"3e47f65e4524f53faba23e6995b592f5","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":true},{"id":"77592781918fa63474b5efbd5cc9555f","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an NLB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the NLB as an alias record.","correct":false}]},{"id":"49107f33-5b31-4d7e-a2cb-95f3ce8a2d75","domain":"awscsapro-domain1","question":"Your customer has setup AWS Organizations to help manage a collection of AWS Accounts.  They are running into a problem though and need your help.  They have created accounts for each business unit and applied SCPs to those OUs. However, they notice that root accounts in in those sub-accounts can still change root access keys and disable MFA.  How do you instruct your customer?","explanation":"Service Control Policies can control many aspects but they cannot restrict root account actions of changing root access keys or disabling MFA.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"77df34553819fdc2e31fb79762948993","text":"You can establish a trust with the top-level account and use the \"organizations:ServicePrinciple\" condition key to restrict root access at the sub-account level.","correct":false},{"id":"94a2f948d3d2f9c317a6ebb1f5a24ea5","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the specific sub-accounts.","correct":false},{"id":"3d722952f024bcec9174a311c17dcc14","text":"You can not use SCPs to restrict root account activities of changing the root password or managing MFA settings.","correct":true},{"id":"194cc2d07b5c378b62b1e090f0aea956","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the entire OU in the root account.","correct":false}]},{"id":"6b6689f4-b150-482a-aa96-eab1674cb232","domain":"awscsapro-domain5","question":"Quality Auto Parts, Inc. has installed IoT sensors across all of their manufacturing lines. The devices send data to both AWS IoT Core and Amazon Kinesis Data Streams. Kinesis Data Streams triggers a Lambda function to format the data, and then forwards it to AWS IoT Analytics to perform monitoring and time-series analyses, and to take actions based on business processes. After an equipment failure on one of the manufacturing lines causes tens of thousands of dollars in revenue losses, it's determined that alarms for a specific piece of equipment where received seventy-five seconds after the issue originated, and that automated corrective action within a few seconds of the problem could have avoided the financial losses altogether. What changes should be made to the architecture to improve the latency of device alerts?","explanation":"AWS IoT Analytics is useful for understanding long-term device performance, performing business reporting, and identifying predictive fleet maintenance needs, but common latencies run from seconds to minutes. If you need to analyze IoT data in real-time for device monitoring, use Kinesis Data Analytics, which provides latencies in the millisecond to seconds range. A Lambda function can be used as the destination for Kinesis Data Analytics to perform corrective actions. IoT Core rules can write messages to a Kinesis stream, but not directly to Kinesis Data Analytics. Having a Lambda function perform anomaly detection will work, but will require more logic to be written for query setup and execution than using a specialized service like Kinesis Data Analytics. With Amazon CloudWatch Alarms, an alarm will watch a single metric over a period time, but will not provide the capabilities of SQL to detect complex anomaly conditions.","links":[{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/aws-reference-architecture-time-series-processing.pdf?did=wp_card&trk=wp_card","title":"Processing IoT Time Series Data on AWS"},{"url":"https://aws.amazon.com/iot-analytics/faq/","title":"AWS IoT Analytics FAQs"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/05/introducing-real-time-iot-device-monitoring-with-kinesis-data-analytics/","title":"Introducing Real-Time IoT Device Monitoring with Kinesis Data Analytics"}],"answers":[{"id":"7e2eb8f3a96a390aab88e66000821c26","text":"Create an AWS IoT Core rule to write the message to Amazon Kinesis Data Analytics to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":false},{"id":"522af3cd7d520d3e94e97c02d19c0672","text":"Create an AWS IoT Core rule to write the message to Amazon CloudWatch Alarms to detect anomalies in the data. Invoke another AWS Lambda function from CloudWatch Alarms to perform device corrective action when needed.","correct":false},{"id":"fe8ff20697982ca413f81ea14472e603","text":"Add Amazon Kinesis Data Analytics as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":true},{"id":"71ade679994c0c1e6e55b3853194e4c5","text":"Add another AWS Lambda function as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Have the Lambda function write the anomalies to Amazon DynamoDB and perform device corrective action when needed.","correct":false}]},{"id":"2afb0db9-a43f-4e97-8272-5ce423ded162","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They notice on their RDS console that one of their mission-critical production databases has an \"Available\" listed under the Maintenance column.  They are extremely concerned that any sort of updates to the database will negatively impact their DB-intensive mission-critical application.  They at least want to review the update before it gets applied, but they are not sure when they will get around to that.  What do you suggest they do?","explanation":"For RDS, certain OS updates are marked as Required. If you defer a required update, you receive a notice from Amazon RDS indicating when the update will be performed. Other updates are marked as Available, and these you can defer indefinitely.  You can also apply the maintenance items immediately or schedule the maintenance for your next maintenance window.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html","title":"Maintaining a DB Instance - Amazon Relational Database Service"}],"answers":[{"id":"66ed278812c2a52913954afa52952b97","text":"The maintenance will be automatically performed during the next maintenance window.  They have no choice in the matter.","correct":false},{"id":"93df9eb71cb48a0c821fe555e35f5b62","text":"Defer the updates indefinitely until they are comfortable.","correct":true},{"id":"6aeada3a9046319bc858239b15031f66","text":"Backup the database immediate because the updates could come at any time.  If possible, create a Read Replica to act as a standby in case problems are introduced with the update.","correct":false},{"id":"35c3b3d1cb8d9866e1abc96dec8bfa3f","text":"Apply the maintenance items immediately.  AWS validates each update with each customer's RDS instances using a shadow image so there is little risk here.","correct":false},{"id":"b9f81cfac73d5c4d2871a7f969ecb9f3","text":"Disable the Maintenance Window so the updates will not be applied.","correct":false}]},{"id":"f43ec458-0ff5-4633-a57b-6bf82f60bd14","domain":"awscsapro-domain5","question":"You have a target group in an elastic load balancer (ELB) and its target type is \"instance\". You attach an Auto Scaling group (ASG) in the target group. All the instances pass the health check and have a healthy state in the target group. Due to a new requirement, the ELB target group needs to forward the incoming traffic to an IP address that belongs to an on-premise server. The ASG is no longer needed. There is already a VPN connection between the on-premise server and AWS VPC. How would you configure the target in the ELB target group?","explanation":"The target type of existing target groups cannot be changed from \"instance\" to \"IP\". Because of this, users have to create a new target group and set the target type to be \"IP\". After that, the on-premise IP address can be registered as a target. A domain name cannot be registered as a target in the target group. You also do not need to create a new elastic load balancer since you only need a new target group to register the IP address.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#target-type","title":"Target type in ELB target group"}],"answers":[{"id":"99f63c1cf5bcfbcb188328714abb8ed3","text":"Register a record set in AWS Route 53 to forward a domain name to the on-premise IP address. Modify the target group to register the domain name as its target. Remove the previous Auto Scaling group from the target group.","correct":false},{"id":"49646fca04901301d145a2a814a7e481","text":"In the elastic load balancer, create a new target group with an \"IP\" target type. Register the on-premise IP address as its target. Monitor if the target becomes healthy after some time. Remove the old target group.","correct":true},{"id":"e0b619a421d68626ffb82b1a6e1d22d5","text":"Remove the Auto Scaling group from the target group and modify the target type to be \"IP\". Attach the IP address to the target and set up the IP address and port in the health check configurations.","correct":false},{"id":"265de6fdcab5323b156774dcb949d309","text":"Create a new network load balancer with a new listener and target group. Configure the target type to be \"IP\" in the target group and attach the on-premise IP address to it. Set up the health check using the HTTP protocol.","correct":false}]},{"id":"4b00251a-a278-4d88-b715-955b4752a79a","domain":"awscsapro-domain2","question":"You'd like to create a more efficient process for your company employees to book a meeting room.  Which of the following is the most efficient path to enabling this improved business experience?","explanation":"With Alexa for Business, you can enlist Alexa-enabled devices to perform tasks for employees like retrieve information, start conference calls and book meeting rooms.","links":[{"url":"https://aws.amazon.com/blogs/business-productivity/announcing-room-booking-for-alexa-for-business/","title":"Announcing Room Booking for Alexa for Business | Business Productivity"}],"answers":[{"id":"5445641c568edaf760839d9f5bb7169c","text":"Configure an Alexa device with a custom skill backed by a Lambda function.  Use Amazon Lex to convert the audio sent to the Lambda function into an actionable skill.  ","correct":false},{"id":"12cb35ea295a9a96bfba94741cb4f0df","text":"Sign-up for Amazon Chime.  Create conference rooms in the console and place speakerphones in each conference room.","correct":false},{"id":"a723a332c2ed052968becb9b6824e2a4","text":"Sign-up for AWS Alexa for Business. Create conference rooms in the console and place an Alexa device in each conference room.","correct":true},{"id":"e43eb2c75536fac2d714b862f4fec490","text":"Invest in a voice-to-text API from the AWS Marketplace.  Create a custom Lambda function that calls the API and books a conference room.  Equip each conference room with Amazon Dash buttons and configure them to invoke the Lambda function.","correct":false}]},{"id":"9624e171-081f-43e5-a74c-b4b792676b63","domain":"awscsapro-domain2","question":"A connected home solutions company is creating a central console to manage smart home environments. Some of the capabilities needed include capturing device telemetry data, making that data available to query from client applications, alerting on undesirable device situations, and providing device analytics. The company has decided to run the console's application backend on AWS. Devices will communicate with the console via MQTT protocol. Which architecture will provide the best scalability and the highest operational efficiency for the smart home console?","explanation":"AWS IoT Core provides all the capability needed to create a smart home console. IoT rules can be configured to send device messages to DynamoDB for client application querying, and to send device messages to IoT Events for alerting and notification. IoT Analytics is fully integrated with IoT Core to receive messages from connected devices as they stream in. These are all managed services which maximize operational efficiency. AWS IoT Greengrass can perform all the required functions, but must run on a local device, which may not be desirable to customers since another piece of hardware would need to be acquired. Amazon Athena cannot query from DynamoDB directly. IoT Events and IoT Analytics are IoT-centric services which will be better solutions than CloudWatch Events and Amazon Redshift.","links":[{"url":"https://aws.amazon.com/iot-core/","title":"AWS IoT Core"},{"url":"https://docs.aws.amazon.com/iotevents/latest/developerguide/what-is-iotevents.html","title":"What is AWS IoT Events?"},{"url":"https://docs.aws.amazon.com/iotanalytics/latest/userguide/welcome.html","title":"What Is AWS IoT Analytics"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/connected_home_telemetry_ra.pdf?did=wp_card&trk=wp_card","title":"Measuring and collecting data from smart home devices"}],"answers":[{"id":"678fa8ab5656d7f8a5fd360c0cbb3586","text":"Implement AWS IoT Core to receive MQTT messages from the devices. Create an IoT rule to store the device messages in Amazon DynamoDB. Have client applications use Amazon Athena to query DynamoDB. Create a second IoT rule in IoT Core to forward telemetry information to AWS IoT Events. Publish to Amazon Simple Notification Service topics when device issues are identified by IoT Events. Have AWS IoT Analytics receive device messages from IoT Core for visualization with Amazon QuickSight.","correct":false},{"id":"1e93269720ca34bcc18e1d67c5ccee18","text":"Implement AWS IoT Core to receive MQTT messages from the devices. Create an IoT rule to store the device messages in Amazon DynamoDB. Make an Amazon API Gateway API available to client applications. Have API Gateway invoke an AWS Lambda function to query DynamoDB. Create a second IoT rule in IoT Core to forward telemetry information to AWS IoT Events. Publish to Amazon Simple Notification Service topics when device issues are identified by IoT Events. Have AWS IoT Analytics receive device messages from IoT Core for visualization with Amazon QuickSight.","correct":true},{"id":"d70e3ad71c36d95078725805fcfaebf8","text":"Implement AWS IoT Greengrass to receive MQTT messages from the devices. Create Lambda functions on IoT Greengrass to send device information to Amazon DynamoDB and Amazon CloudWatch Events. Have client applications use Amazon Athena to query DynamoDB. Publish to Amazon Simple Notification Service topics when device issues are identified by CloudWatch Events. Use an AWS Glue job to aggregate the device information in DynamoDB and write summaries to Amazon Redshift. Create visualizations of the Redshift summaries in Amazon QuickSight.","correct":false},{"id":"518de0ee58995da859fef3f8b0108750","text":"Implement AWS IoT Greengrass to receive MQTT messages from the devices. Create Lambda functions on IoT Greengrass to send device information to Amazon DynamoDB, Amazon CloudWatch Events, and AWS IoT Analytics. Make an Amazon API Gateway API available to client applications. Have API Gateway invoke a Lambda function to query DynamoDB. Publish to Amazon Simple Notification Service topics when device issues are identified by CloudWatch Events. Send IoT Analytics summaries to Amazon QuickSight for visualization.","correct":false}]},{"id":"599dee9a-6ae7-4c85-a7c6-49edc6ae7d6b","domain":"awscsapro-domain5","question":"A development team is comprised of 20 different developers working remotely around the globe all in different timezones.  They are currently practicing Continuous Delivery and desperately want to mature to true Continuous Deployment.  Given a very large codebase and distributed nature of the team, enforcing consistent coding standards has become the top priority.  Which of the following would be the most effective to address this problem and get them closer to Continuous Deployment?","explanation":"Including an automated style check prior to the build can move them closer to a fully automated Continuous Deployment process.  A style check only before UI testing is too far in the SDLC.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"db4ecdbd1c7c8fda5d3e0792a15411ab","text":"Include code style check in the build stage of the deployment pipeline using a linting tool.  ","correct":true},{"id":"e60e97c4fb6cbf6c1dcf3e806624762f","text":"Require all developers to use the Pair Programming feature of Cloud9.  The commits must be signed by both developers before merging.","correct":false},{"id":"628453003287afe2200912bb38d0456b","text":"After integrating and load testing, run a code compliance check against the binary created during the build.","correct":false},{"id":"f4cd7f15eb32d8ddd77234b38d0b35b8","text":"Incorporate a code style check right before user interface testing to ensure standards are being followed.","correct":false},{"id":"ec61b60c7eeb3bf9ca9c4149c09c5f3d","text":"Issue a department directive that standards must be followed and require the developers to sign the document.","correct":false},{"id":"1f40591b9d9dbe7a2371e5e82ec05997","text":"Introduce a peer review step into their deployment pipeline during the daily stand-up, requiring sign off for each commit.","correct":false}]},{"id":"c2d46981-3dac-4e68-81d1-9eedf0cbf264","domain":"awscsapro-domain2","question":"Your company is preparing a special event for its 100th year in business.  As part of that event, the event committee would like to create a kiosk where employees can browse the thousands of photographs captured over the years of the employees and company events.  In a brainstorming session, one event staff member suggests the crazy idea of allowing employees to quickly pull up photographs which they appear in.  What AWS service might be able to make this a reality?","explanation":"AWS Rekognition is a service that can detect and match faces in a photograph.  The kiosk could include a camera that allows event-goers to snap a picture of themselves and then it could scan the photo archive for facial matches.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition – Video and Image - AWS"}],"answers":[{"id":"cbe32e918d6a248e3e8ed74e6f0b72f6","text":"AWS Comprehend","correct":false},{"id":"438e06c02cba48ce4ffbf026d97488b4","text":"Amazon DeepView","correct":false},{"id":"42e6f83b4b8205e6c2e62ddafdd3bbe3","text":"Amazon Chime","correct":false},{"id":"ade161cea9b509c72570ba6ae5238e5f","text":"Kinesis for Video","correct":false},{"id":"07c025c347c7483abdd039cd36be4220","text":"AWS Rekognition","correct":true}]},{"id":"de88bc69-44a8-4a12-b28f-0a5e86db3939","domain":"awscsapro-domain1","question":"You have been entrusted to act as the interim AWS Administrator following the departure of the erstwhile Administrator in your company. You notice that there are several existing roles called role-engineer, role-manager, role-qa, role-dba, role-data-scientist, etc. When a new person joins the company, the new IAM user simply assumes the right role while using AWS - this allows central management of permissions and eliminates the need to manage permissions on a per-user basis.\nA new QA hire joins the company a few days later. You create an IAM User for her. You attach a Policy to the new IAM User that allows Action STS AssumeRole on any Resource. However, when this employee logs in the same day and tries to switch roles to role-qa, she is denied and is unable to assume the role-qa Role.\nWhat could be one reason why this is happening and how can it be best fixed?","explanation":"In order to allow an IAM User to successfully assume an IAM Role, two things must happen. First, the Policy attached to the User must allow the action STS AssumeRole. This is already true according to the question. Second, the Trust Policy of the Role itself must allow the User in question to assume the Role. This second condition can be met if we specify the arn of the User in the Principal element of the Trust Policy. In general, this question can be answered if the candidate is familiar with the concept of Principal in a Role, see link - A Principal within an Amazon IAM Role specifies the user (IAM user, federated user, or assumed-role user), AWS account, AWS service, or other principal entity that is allowed or denied to assume or impersonate that Role. Trust Policy is different than the Policy permissions - think of Policy Permissions as [what can be accessed] and Trust Policy as [who can access].\nTrust Policy cannot belong to an IAM User, hence the choice that claims the problem to be an unmodified User Trust Policy is incorrect. IAM changes are instantly effective, so the choice that points at the need of a time delay is also incorrect. Among the other two choices, the knowledge needed to pick the right one is an awareness of the Principal element.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html","title":"AWS JSON Policy Elements - Principal"},{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html","title":"IAM Roles"}],"answers":[{"id":"765f20e64b35dbc08c2bc319bcbe7e1a","text":"Sufficient time has not passed since you made the changes. It takes up to 12 hours to propagate IAM role changes. To fix this, ask her to try again the next day.","correct":false},{"id":"dbb48c05e238c18bdb9c17ee265e387b","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Condition element of the Trust Policy of the Role","correct":false},{"id":"b3bf261fca3a734ad312d3ac0e5d0589","text":"You have not modified the Trust Policy of the IAM User to trust the Role role-qa. To fix this, add a Condition to the IAM Policy attached to the new user that filters on the role and specify the arn of role-qa","correct":false},{"id":"e6dacaf19a289e1f73855c5e904b21fb","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Principal element of the Trust Policy of the Role","correct":true}]},{"id":"06504582-ce03-4252-b1dc-29654ff427bb","domain":"awscsapro-domain5","question":"You have just set up a Service Catalog portfolio and collection of products for your users.  Unfortunately, the users are having difficulty launching one of the products and are getting \"access denied\" messages.  What could be the cause of this?","explanation":"For Service Catalog products to be successfully launched, either a launch constraint must be assigned and have sufficient permission to deploy the product or the user must have the same required permissions.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-launch.html","title":"AWS Service Catalog Launch Constraints - AWS Service Catalog"}],"answers":[{"id":"43d54acb2dbc9f2dc8a0d793553b965e","text":"The user launching the product does not have required permissions to launch the product.","correct":true},{"id":"a987914548e48bb64be70c11a97ec644","text":"A Service Catalog Policy has not yet been applied to the account.","correct":false},{"id":"7e0083aafd999688d628f67e003d79be","text":"The product does not have a launch constraint assigned.","correct":true},{"id":"d11a80651ecb668bdbe507d4e7398b6a","text":"The launch constraint does not have permissions to CloudFormation.","correct":true},{"id":"779fec840f0f81e772ba3137d7ac28ad","text":"The notification constraint did not have access to the S3 location for the product's CloudFront template.","correct":false},{"id":"8bc518c42ab2dfa122390f1a497349a2","text":"The template constraint assigned to the product does not have the proper permissions.","correct":false}]},{"id":"9fc0785a-d5cb-47e3-bc2f-829b5a36ba26","domain":"awscsapro-domain3","question":"You work for a Genomics company which has decided to migrate its DNA Sequencing application to the AWS Cloud. The application is containerized. Currently, container image A works on genomics data residing on an on-premises file server, validating the data and updating the metadata in a local database. When it is done, engineers manually trigger 100 or more instances of container image B that process this data in parallel by reading the metadata, creating output files. When all these container instances have done their job, engineers manually trigger container image C that validates the results, cleans up and sends notifications.\nThe CTO has decided to use S3 for storing the input and output data files. She has also mandated that the parallel processing phase should run on a fleet of Spot EC2 instances to reduce compute costs. She also wants to automate the workflow, so that engineers do not have to manually trigger the next set of actions. The requirement is to minimize administrative overhead and custom development for the migration.\nAs the AWS Architect, which of the following approaches should you recommend?","explanation":"AWS ECS does not natively provide workflow management. In an ECS service definition file, you cannot specify a sequence of tasks with execution dependencies such that one will be run only after the previous one completes. Hence, the two ECS choices are ruled out.\nDistraction warning - Fargate does not allow you to specify Spot instances as it is serverless in nature (it absolves you from specifying server details). This effectively creates a distraction - when the candidate rules out ECS Fargate due to this reason, they may be relieved to see the ECS EC2 choice and jump to a conclusion because it is relatively easy to remember that EC2 launch type actually lets you select Spot instances. However, this distraction is designed to take focus away from the fact that neither of these two choices is correct. Both of the choices require service definition files to set up execution workflows. Task instances mentioned in an ECS service definition file are executed in parallel - ECS does not control the sequence of tasks.\nAWS SWF does not let you specify Spot instances either. Also, SWF is usually used in cases where human intervention is needed in the workflow.\nThis leaves AWS Batch as the correct answer. AWS Batch is indeed the most suitable AWS service for this scenario as it meets all requirements.","links":[{"url":"https://docs.aws.amazon.com/batch/latest/userguide/create-compute-environment.html","title":"How to create a compute environment for AWS Batch"},{"url":"https://docs.aws.amazon.com/batch/latest/userguide/example_array_job.html","title":"Example AWS Batch Array Job Workflow"},{"url":"https://aws.amazon.com/ec2/spot/containers-for-less/get-started/","title":"How to run ECS clusters in EC2 Spot Instances"}],"answers":[{"id":"49755d6c34da495b8c91964f52946d29","text":"Use AWS SWF workers and deciders to manage the workflow. Configure the workers to use EC2 Spot Instances","correct":false},{"id":"757ddde350053553e44844d066c91386","text":"Use AWS ECS with Fargate Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"e46ada36d33a9e5b23aa37ee94c4c5d6","text":"Use AWS ECS with EC2 Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"ceb4c03a526e8ddb01ada7a40bb60001","text":"Use AWS Batch, setting up an array job with 100 or more copies preceded by pre-requisite and follow-up jobs where the workflow is controlled by dependencies between jobs. Also, use Spot as the Provisioning Model for compute environment","correct":true}]},{"id":"a2fb4f91-4c73-4080-bbf3-6d07a1b2ce03","domain":"awscsapro-domain2","question":"You have been asked to investigate creating a production Oracle server in RDS.  You need to choose the correct options that will allow you to run the latest version of Oracle 12c with High Availability.  You do not currently have any Oracle licenses. Which of the below are valid options?","explanation":"To get to the correct answer, you must first disregard any option with Oracle Data Guard as this is not available in RDS, then remove any answer containing the editions SE or SE1 as they only allow version 11g to be deployed, not 12c.  The remaining two options are correct as they allow High Availability, 12c and either a Bring-You-Own or licence included option, so you can ensure you get the best value.","links":[{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"Amazon RDS for Oracle FAQs"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Oracle.html","title":"Oracle on Amazon RDS"}],"answers":[{"id":"5d1af18d1bfbdb6d990f138d844f75dc","text":"Choose either the SE or SE1 editions, Bring-Your-Own-Licence and Oracle Data Guard","correct":false},{"id":"1ed8c7e073924060dbe648fbdf8a9c17","text":"Choose the SE2 edition with licence included and enable Multi-AZ","correct":true},{"id":"0cb21536be9ea582178c7e54e365bd33","text":"Choose either the SE or SE1 editions, purchase your own licenses from Oracle and enable Multi-AZ","correct":false},{"id":"0d9a63f8cb9a74845f363b450fa5cf11","text":"Purchase your own licenses from Oracle, Choose either the Enterprise or SE2 editions, Bring-Your-Own-Licence and enable Multi-AZ","correct":true}]},{"id":"d2b0a9d5-1875-4a55-968d-3a2858601296","domain":"awscsapro-domain2","question":"You currently are using several CloudFormation templates. They are used to create stacks that include the resources of VPC subnets, Elastic Load Balancers, Auto Scaling groups, etc. You want to deploy all the stacks with a root stack so that all the resources can be configured at one time. Meanwhile, you need to isolate information sharing to within this stack group, which means other stacks outside of the stack group can not import its resources. For example, one stack creates a VPC subnet resource and this subnet can only be referenced by the stack group. What is the best way to implement this?","explanation":"As the stack outputs should be limited within the stack group, nested stacks should be chosen. The export stack outputs cannot prevent other stacks to use them. The AWS::CloudFormation::Stack resource type is used in nested stacks to provide dependencies. The DependsOn attribute is not used for configuring nested stacks.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html","title":"Exporting stack output values"}],"answers":[{"id":"fe3bf9eee211c59e9c64ad17a2d32e27","text":"Export output values for each child stack if needed. Create a parent stack to use the exported values from child stacks to deploy and manage all resources at one time.","correct":false},{"id":"aca679ff150a8a8176faf99cc057e825","text":"Create nested stacks with the \"AWS::CloudFormation::Stack\" resources. Use the outputs from one stack in the nested stack group as inputs to another stack in the group if needed.","correct":true},{"id":"e208251ba3e5646edab96df0da85794c","text":"Upload stack templates to an S3 bucket. Create a root CloudFormation stack to use the uploaded templates with the resource type of \"AWS::CloudFormation::Template\". Configure the \"TemplateURL\" field with the template location in S3.","correct":false},{"id":"e44006dac54e63b93a8804a4e632eeb5","text":"Upload the root and all child stack templates to an S3 bucket under the same directory. Use the \"DependsOn\" attribute in the root template to add dependencies. When the root stack is created, all the child stacks are created first. ","correct":false}]},{"id":"74aec97e-c092-4588-8da4-43dca3ddd0eb","domain":"awscsapro-domain5","question":"You are trying to help a customer figure out a puzzling issue they recently experienced during a Disaster Recovery Drill.  They wanted to test the failover capability of their Multi-AZ RDS instance.  They initiated a reboot with failover for the instance and expected only a short outage while the standby replica was promoted and the DNS path was updated.  Unfortunately after the failover, they could not reach the database from their on-prem network despite the database being in an \"Available\" state.  Only when they initiated a second reboot with failover were they again able to access the database.  What is the most likely cause for this?","explanation":"The routes for all subnets in an RDS subnet group for a Multi-AZ deployment should be the same to ensure all master and stand-by units can be reached in the event of a failover.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/avoid-route-table-issues-rds/","title":"Avoid route table issues RDS Multi-AZ fail over"}],"answers":[{"id":"1dbb40d7bdb88fbe54fead30b1bf5f12","text":"There was a lag in the state update on the AWS console showing \"Available\".  If they would have waited longer, it likely would have changed to \"Degraded\".  A failover can take 30 minutes or more because AWS automatically creates a snapshot before promoting the standby.","correct":false},{"id":"1105b24fe7a3c1eb0a86d0fa9a3cdc62","text":"This was most likely an AWS error.  They should submit a support ticket with the RDS instance identifier and the approximate time of the failover test via the Support Center.","correct":false},{"id":"46b9b5188d94533190c30de816355165","text":"They used the AWS Console to issue the reboot.  You can only force a failover of RDS by using the AWS CLI and adding the --force-failover parameter to the \"aws rds reboot-db-instance\" command.","correct":false},{"id":"f637d9886a1ea7b52d608046e233e504","text":"The subnets in the subnet group did not have the same routing rules.  The standby subnet did not have a valid route back to the on-prem network so the database could not be reached despite being available.","correct":true},{"id":"260ae29233a6047971894eff9d05fa55","text":"They initiated a failover with an IAM account that did not have sufficient rights to perform the reboot.  This resulted in an incomplete failover that was only corrected by executing the failover again to reset the DNS entries.","correct":false}]},{"id":"375e7161-43df-4d2f-adab-75cc6166a453","domain":"awscsapro-domain5","question":"You build a CloudFormation stack for a new project. The CloudFormation template includes an AWS::EC2::Volume resource that specifies an Amazon Elastic Block Store (Amazon EBS) volume. The EBS volume is mounted in an EC2 instance and contains some important customer data and logs. However, when the CloudFormation stack is deleted, the EBS volume is deleted as well and the data is lost. You want to create a snapshot of the volume when the resource is deleted by CloudFormation. What is the easiest method for you to take?","explanation":"The easiest method is using the DeletePolicy attribute in the CloudFormation template. The \"Snapshot\" value ensures that a snapshot is created before the CloudFormation stack is deleted. The \"Retain\" value is incorrect as it keeps the volume rather than creates a snapshot. The EBS lifecycle manager can create daily snapshot however it is not required in the question. When the CloudWatch Event rule is triggered, the EBS volume may already be deleted and no snapshot can be taken. Besides, the CloudFormation deletion cannot be suspended by a Lambda function.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html","title":"DeletionPolicy Attribute"}],"answers":[{"id":"692ea271f5ada1d555a5889be933267e","text":"Modify the CloudFormation template to create an EBS snapshot strategy in EBS lifecycle manager which creates a daily snapshot as backup and also another snapshot when the EBS volume’s CloudFormation stack is being deleted.","correct":false},{"id":"555e7ef93e63402dd6a074edd5b8a16d","text":"Create a CloudWatch Event rule that checks the CloudFormation delete-stack event. Trigger a Lambda function that pauses the CloudFormation stack deletion, creates the EBS snapshot of the volume and resumes the stack deletion after the snapshot is created successfully.","correct":false},{"id":"0719057e0f4aca49eb47cd94c333e599","text":"Add a DeletePolicy attribute in the CloudFormation template and specify \"Snapshot\" to have AWS CloudFormation create a snapshot for the EBS volume before deleting the resource.","correct":true},{"id":"061af6db30e4b65a1d544d4e64546085","text":"Modify the CloudFormation template by adding a DeletePolicy variable for the AWS::EC2::Volume resource. Specify the value of \"Retain\" to automatically create a snapshot of the EBS volume before the stack is deleted.","correct":false}]},{"id":"63a6def8-9b52-4d89-8248-6079ca1393e2","domain":"awscsapro-domain3","question":"You are helping a client prepare a business case for cloud migration.  One of the required parts of the business case is an estimation of AWS costs per month.  The client has about 200 VMs in their landscape under VMware vCenter.  Due to security concerns, they will not allow any external agents to be installed on their VMs for discovery.  How might you most efficiently gather information about their VMs to build a cost estimate with the least amount of effort? ","explanation":"The Application Discover Service uses agent-based or agentless collection methods.  Agentless collection is only available for those customers using VMware.  The AWS Application Discovery Agentless Connector is delivered as an Open Virtual Appliance (OVA) package that can be deployed to a VMware host. Once configured with credentials to connect to vCenter, the Discovery Connector collects VM inventory, configuration, and performance history such as CPU, memory, and disk usage and uploads it to Application Discovery Service data store.  This data can then be used to estimate monthly costs.","links":[{"url":"https://aws.amazon.com/application-discovery/faqs/?nc=sn&loc=6","title":"AWS Application Discovery Service FAQs"}],"answers":[{"id":"30cbd0a822a4905f8a795dcb7cc3d31e","text":"Use a custom script to iteratively log into each VM and pull network, hardware and performance details of the VM.  Write the data out to S3 in CSV format.  Use that data to select corresponding EC2 instance sizes and calculate estimated monthly cost.","correct":false},{"id":"09d153439d976dabcdd13a4a2f8a4a5f","text":"Use Application Discovery Service to gather details on the network connections, hardware and performance of the VMs.  Export this data as CSV and use it to approximate monthly AWS costs by aligning current VMs with similar EC2 instances types.","correct":true},{"id":"0a144eb3993e48693aab4c9744b6acb2","text":"Use AWS OpsWorks to remotely pull hardware, network connection and performance of the VMs.  Export the collected data from OpsWorks in Excel format.  Use the collected data to align current VMs with similar EC2 instance types and calculate an estimated monthly cost.","correct":false},{"id":"b279286ed54b2394c29d2fbd0061b4c4","text":"Provision an S3 bucket for data collection.  Use SCT to scan the existing VMware landscape for VM hardware, network connection and performance parameters.  Retrieve the SCT CSV data from the data collection bucket and use it to align EC2 instance types with existing VM parameters.  Use this cross-reference to calculate estimated monthly costs for AWS.","correct":false}]}]}}}}
