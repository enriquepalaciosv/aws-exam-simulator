{"data":{"createNewExamAttempt":{"attempt":{"id":"f61e3c8c-50f4-443d-971b-2edd2acb98d8"},"exam":{"id":"bbfb5e42-69ac-46b2-9032-e358d06571b6","title":"AWS Certified SysOps Administrator - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"b2d3b949-889b-4bbd-8ec9-c65b764c47c3","domain":"mon-rep","question":"You are a SysOps Administrator monitoring a web app that lets users upload high-quality images and use them online. Each image requires resizing and encoding. The images are placed in an Amazon SQS queue for processing by an EC2 instance. It processes the images and then publishes the processed images where they can be viewed by users. When you monitor the EC2 instance you see that the CPU utilization is consistently at 90% and that image processing time is being delayed. The team is looking for a cost-effective solution. What would you recommend?","explanation":"You can use an Auto Scaling group to manage EC2 instances for the purpose of processing messages from an SQS queue. Set a custom metric to send to Amazon CloudWatch that measures the number of messages in the queue per EC2 instance in the Auto Scaling group, and then set a target tracking policy that configures your Auto Scaling group to scale based on the custom metric and a set target value. CloudWatch alarms invoke the scaling policy. Increasing the size of the instance may work but is not a cost-effective solution since Auto Scaling gives you the option to scale down during low demand. Kinesis Data Streams are best suited for real-time data processing, and they have a size limit of 1MB which would be too low for high-quality images. Migrating the data to DynamoDB would not be a viable, let alone cost-effective, solution.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html","title":"Scaling Based on Amazon SQS"}],"answers":[{"id":"3a781b7e075f07b31a51c98b18d84a2e","text":"Increase the size of the instance and ensure that it is compute-optimized to boost it's capacity to process the images.","correct":false},{"id":"52bcbdff61b39eafa67d9496dc77ee09","text":"Migrate the image data into DynamoDB. Attach a role to the instance to be able to access the data from DynamoDB and process the images.","correct":false},{"id":"c1c44e18b3c0f81b8ec026e9e1ab5b38","text":"Place the instance in an Auto Scaling group. Use CloudWatch metrics to scale out the Auto Scaling group depending on the size of the SQS queue.","correct":true},{"id":"40fe2365fa7bae167e628c9ef29bd6ca","text":"Move the images into Kinesis Data Streams where you'll be able to process the data in real time.","correct":false}]},{"id":"64058244-581f-4354-9a9a-76108a5da03e","domain":"data-man","question":"A developer with the proper IAM permissions on your team is attempting to list objects from Amazon Glacier. The objects in Amazon Glacier are offsite enterprise information that were archived using Amazon S3 lifecycle policies. The developer only has Programmatic access to AWS but when attempting to use the Amazon Glacier API, she can't see the objects as archives in the Amazon Glacier vault. What would you do to remedy the situation?","explanation":"Note that when using Amazon Glacier as a storage class in Amazon S3 you use the Amazon S3 API, and when using “native” Amazon Glacier you use the Amazon Glacier API. For example, objects archived to Amazon Glacier using Amazon S3 lifecycle policies can only be listed and retrieved by using the Amazon S3 API or the Amazon S3 console. You can’t see them as archives in an Amazon Glacier vault. You cannot unlock a vault once the Vault Lock has been activated after the 24 hour validation period. Management Console access would not help. The 3-5 hour retrieval time is expected when retrieving objects from Amazon Glacier but not for listing objects.","links":[{"url":"https://d0.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf & https://docs.aws.amazon.com/amazonglacier/latest/dev/amazon-glacier-accessing.html","title":"Amazon S3 Glacier"}],"answers":[{"id":"0a03188271930e5829e002258dac4ffe","text":"Provide the developer the Vault Lock access keys to first unlock the vault.","correct":false},{"id":"7726f2f0aca541c8ab9a1d55a6e42ad1","text":"Inform the developer to wait 3-5 hours to view the objects.","correct":false},{"id":"be52886c7c2a9deee49a5931fdb158f9","text":"Grant the developer AWS Management Console access.","correct":false},{"id":"3560f5083a7d4cce9b3e658f2190f274","text":"Inform the developer to use the Amazon S3 API.","correct":true}]},{"id":"29e51c05-d93d-4868-9c14-8c7940bf9d4c","domain":"security-comp","question":"The CFO has raised concerns about rising AWS costs and has asked you to look into this potential issue. When you look at CloudTrail logs you notice that certain IAM Users in multiple AWS accounts within your organization are using AWS services without discretion, and these services are not relevant to their business functions. How would you implement security policies to limit which AWS services these users can access in the most effective way?","explanation":"Creating IAM policies for each IAM User is not best practice and can become an administrative burden. AWS GuardDuty does not provide IAM-related services, and thus is irrelevant. It is not possible to view CloudTrail logs at the OU level. The best solution is to use Service Control Policies (SCPs) to govern use of the AWS environment at the OU level.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"AWS Organizations: Service Control Policies (SCPs)"}],"answers":[{"id":"93faedb0826b3c6a97423da318c0b775","text":"Use AWS GuardDuty to detect and monitor each IAM Users' activity to document unauthorized behavior.","correct":false},{"id":"5b2ff64d8adf96a19f7dc89c3b3d76ef","text":"Attach a Service Control Policy at the Organization Unit to blacklist AWS Services that all Accounts under the OU shouldn't use.","correct":true},{"id":"633fa64e9f9d9f6b601b4400d9ca6b30","text":"Identify the IAM Users within each Organizational Unit. Attach IAM policies to each user to restrict access based on each users' job function.","correct":false},{"id":"c6d620763c9f148cf7eaab8ef0467d62","text":"Send CloudTrail logs at the Organizational Units' level to CloudWatch Events. Use Lambda to change IAM permission to DENY any AWS service if users attempt to use non-business critical services.","correct":false}]},{"id":"6b5ed706-afce-4eda-8d2c-dc54e1741a8e","domain":"dep-prov","question":"You are running a WordPress site on an EC2 instance. The site gets visitors from all over the world and they are sometimes complaining about slow page load times. To serve content to your visitors faster, what could you do?","explanation":"S3 static web hosting only supports static websites, not dynamic sites like WordPress. Changing the tenancy won't change the instance specs. Scaling up the instance might help in another scenario, but here the issue is latency and transfer speed. A CloudFront distribution would cache your content close to your visitors and resolve the slow load times. As an added benefit, the caching would also take some load off the instance.","links":[{"url":"https://aws.amazon.com/cloudfront/","title":"CloudFront"}],"answers":[{"id":"fea505586259520090752b2fb572d489","text":"Change the tenancy to a dedicated instance to get more CPU power","correct":false},{"id":"e393aadc6f1e8478bfc3cc962b86e18c","text":"Move the site to S3 static web hosting","correct":false},{"id":"333dd3e9883af7f7cc6ece108e9ffe30","text":"Scale up the instance to a one with more CPU power","correct":false},{"id":"29db6de8b90b19b898fa59f6320deba7","text":"Place a CloudFront distribution in front of the instance","correct":true}]},{"id":"84adda98-8315-454d-b0c1-b6478c5c0d98","domain":"mon-rep","question":"You are performing an update to all of your application servers, however some of your applications are failing following the upgrade and you notice that this seems to only be affecting servers with a specific application profile. How can you easily identify which of your systems are likely to be affected?","explanation":"AWS Config is a service that enables you to assess, audit and evaluate the configurations of your AWS resources.","links":[{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"}],"answers":[{"id":"9deb03cd21d41a691cdc24bfaab2820c","text":"Inspector","correct":false},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":true},{"id":"055f466b265e26667e0bb23ddffc7970","text":"Run Command","correct":false}]},{"id":"c3455562-2ff1-472f-bf2c-1fbeddfdbe17","domain":"data-man","question":"You are a SysOps Administrator tasked with finding a storage solution as your organization moves data from you on-prem data center to AWS. The storage solution requirements are: needs to be able to scale, have a hierarchical directory structure, and have control access with POSIX permissions. What AWS storage solution would you recommend?","explanation":"Amazon EFS fulfills all these requirements. S3 An on-premises storage appliance that integrates with cloud storage. Amazon EBS is a service that provides block storage volumes for EC2 instances. S3 does not provide access control with POSIX permissions.","links":[{"url":"https://aws.amazon.com/efs/","title":"Amazon Elastic File System"}],"answers":[{"id":"270fcb785810d0206945029bb05f4e97","text":"Amazon S3","correct":false},{"id":"f7b96044a16becafecad63df1725e9c8","text":"Amazon EFS","correct":true},{"id":"516729a7c0562425406a22cfe6a2c163","text":"Amazon EBS","correct":false},{"id":"4975185c02158da3ee3dd512c1f3238a","text":"Amazon Storage Gateway","correct":false}]},{"id":"2630cbd0-6719-4caf-ad19-8d6dcec5ed01","domain":"high-avail","question":"You need to update the AMI in your EC2 Auto Scaling launch configuration. Which of the following statements are true? Select three.","explanation":"You cannot modify an existing launch configuration so you must create a new one. Changing the launch configuration doesn't affect existing instances, however, new instances will be launched using the new configuration. Finally, you can only specify one launch configuration for an EC2 Auto Scaling group at a time.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html","title":"Launch Configurations"}],"answers":[{"id":"61af16d343d54d2bdf191bb49d7eba82","text":"You can specify multiple launch configurations for an EC2 Auto Scaling group at a time.","correct":false},{"id":"db998506c02ca5969af030efbf20e618","text":"You can only specify one launch configuration for an EC2 Auto Scaling group at a time.","correct":true},{"id":"34466636e39e2915f7b2261da2a0bdc3","text":"The launch configuration can be modified freely so there is no need to create a new one.","correct":false},{"id":"856e16f94e7f758bb11bf8fe8581c991","text":"Changing the launch configuration of an Auto Scaling group doesn't affect existing instances.","correct":true},{"id":"d29a4d9b005ee918846f982990ec78a2","text":"Changing the launch configuration of an Auto Scaling group triggers existing instances to be relaunched using the new configuration.","correct":false},{"id":"f060652b627c2d9b82bc7d53b685bbdd","text":"The existing launch configuration cannot be modified so a new one must be created.","correct":true}]},{"id":"0ac7251e-689e-4cd6-bfb9-992628fb3e3c","domain":"mon-rep","question":"There has been a major outage of S3 in US-East-1 where many of your company’s AWS assets are. Your boss wants to know what effect this will have on your organization. What dashboard can you use to help diagnose how this will affect your individual organisation?","explanation":"AWS Personal Health Dashboard provides alerts and remediation guidance when AWS is experiencing events that may impact you. Inspector is an automated security assessment service. AWS X-Ray helps developers analyze and debug production, distributed applications.","links":[{"url":"https://aws.amazon.com/premiumsupport/phd/","title":"AWS Personal Health Dashboard"}],"answers":[{"id":"cf4db9f312542c8284a6ccdefcd98544","text":"Personal Health Dashboard","correct":true},{"id":"3473fa31769f9b170662878d3f67fc8c","text":"AWS Inspector Dashboard","correct":false},{"id":"6bd8c280d2d212f5f6338714620001a4","text":"AWS Service Dashboard","correct":false},{"id":"0863526e514d88cd2d00b39f8dda0920","text":"X-Ray","correct":false}]},{"id":"9c6bdf99-3728-43be-9772-59500b1f2276","domain":"dep-prov","question":"A team of developers plans to migrate their GraphQL-powered web application to AWS and the development lead has been instructed to use managed services whenever possible. How can the team accomplish this?","explanation":"API Gateway is used for RESTful applications. AWS AppSync is used for GraphQL powered applications. RDS and DynamoDB are managed database services but Amazon EC2 is not a managed service.","links":[{"url":"https://docs.aws.amazon.com/appsync/latest/devguide/designing-a-graphql-api.html","title":"Designing a GraphQL API"}],"answers":[{"id":"58fa18d14ef959d0d59e88ef26f8c391","text":"Use Amazon EC2 for the GraphQL API. Use RDS for the managed database service.","correct":false},{"id":"6f76fbab4996d8c8cb993a9c4569275b","text":"Use API Gateway for the GraphQL API. Use DynamoDB for the managed database service.","correct":false},{"id":"4481f8f19e1b5f28da2b0f396643167b","text":"Use Lambda for the GraphQL API. Use RDS for the managed database service.","correct":false},{"id":"13017b9feb28c5d8fdda19e3c79df90d","text":"Use AppSync for the GraphQL API. Use DynamoDB for the managed database service.","correct":true}]},{"id":"d584866c-4884-40ad-a263-94eaad98f894","domain":"mon-rep","question":"AWS Config is a managed service which is part of the AWS Management & Governance portfolio of services.  Which of the following options are functions of the AWS Config service?","explanation":"AWS Config is a service that provides access to resource configuration history, an inventory of resources and alerts on any configuration changes, however it doesn't log API calls as this is the function of CloudTrail.","links":[{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"}],"answers":[{"id":"511bc797ffd63fd0702e8632fb5156db","text":"Provides a log of all configuration related API calls","correct":false},{"id":"b3a15d8a30c303c913c4b2976f61f5c7","text":"Provides an inventory of all AWS resources","correct":true},{"id":"46057f48c99511ac9dfb7fe92df1aefa","text":"Provides notification of configuration item changes","correct":true},{"id":"77eef0aefcbae3f15cc5ce086d0bff63","text":"Provides access to resource configuration history","correct":true}]},{"id":"7afdb34a-f5da-4e8f-ab9c-e059038d650e","domain":"networking","question":"You have just launched an EC2 instance in the public subnet of a newly-created VPC, but you forgot to assign a public IP address during creation. How might you make your instance reachable from the outside world?","explanation":"For an instance to be reachable from the internet, your VPC must have an Internet Gateway and your instance must have a Public or Elastic IP. Public IP addresses can be created only at the time of instance creation. You cannot \"go back\" and create one.  In order for a subnet to be described as a \"Public subnet\" it must already have a route to the Internet","links":[{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Internet_Gateway.html","title":"Internet Gateways"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html","title":"About Elastic IP Addresses"},{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html","title":"About Public subnet"}],"answers":[{"id":"0fa79a82264c55eff18e206e030f5c16","text":"Create an Elastic IP address for your instance. Associate the Elastic IP with the EC2 instance.","correct":true},{"id":"93d71e191e2b0ea46c66ef9f7de5f064","text":"Go back and create a Public IP address. Associate it with your Internet Gateway.","correct":false},{"id":"e9ce11f00035e2973045eabe5deb1a36","text":"Create an Internet Gateway and associate it with the private IP address of your instance with it.","correct":false},{"id":"568466971ef09a401e2f77e242156994","text":"Create an Internet gateway and an Elastic IP address. Associate the Elastic IP with the EC2 instance.","correct":false}]},{"id":"a085468f-b362-451e-a53b-2a027f39f4b4","domain":"security-comp","question":"Your team has a new web application in AWS that has customers in different countries. As the service needs to be highly available, it must be well protected from common DDoS attacks such as SYN floods. When the web site is under attack, you can get instant support to assist you in mitigating the issue. Which option is the most suitable to achieve this requirement?","explanation":"When the AWS Shield Advanced feature is activated, you can get support from the DDoS response team. The team helps you to analyze the suspicious activity and fix the issue. AWS Shield Standard or AWS WAF do not have this service. AWS Enterprise Support plan is not cost-efficient. As there is only one web application that needs to be protected from DDoS attacks, AWS Shield Advanced is enough.","links":[{"url":"https://docs.aws.amazon.com/en_pv/waf/latest/developerguide/shield-chapter.html","title":"AWS Shield"}],"answers":[{"id":"3bb0eac221e4844f28e6ad4ea5db5f86","text":"Enable AWS WAF rules to protect the application from DDoS attacks.","correct":false},{"id":"f3573cee21c8731886863e4c267344fe","text":"Activate the AWS Shield Standard service.","correct":false},{"id":"2d4b07f7063adc3f7d26742bd531f447","text":"Enable the AWS Enterprise Support plan.","correct":false},{"id":"2f94e6d78605c4eef8c11c646aab2420","text":"Activate the AWS Shield Advanced feature.","correct":true}]},{"id":"9423b7e6-b070-49a7-a8c4-de6a7c446e79","domain":"networking","question":"You have configured Direct Connect between you data center and your VPC in the US West Region, you then create a new VPC in US East. How can you use your Direct Connect connection to access your VPC in US East?","explanation":"You can use an AWS Direct Connect gateway to connect your AWS Direct Connect connection to one or more VPCs in your account that are located in the same or different regions.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html","title":"Direct Connect Gateways"}],"answers":[{"id":"a7f9ed179820688ffcef92bb3ce621ee","text":"Use a Direct Connect gateway to connect to the VPC using a private virtual interface","correct":true},{"id":"51c17cb2e52441d24159591098c03005","text":"Use Direct Connect ","correct":false},{"id":"d4ef5b9099dae72e2ef10d48e7086223","text":"Use Direct Connect to connect to the VPC using a VPN Gateway ","correct":false},{"id":"ca2a3e2138030d9141c0e8f3af86f76b","text":"Use VPC peering connect to the US East VPC","correct":false}]},{"id":"afb14785-ba2f-4fea-b819-21804b895752","domain":"mon-rep","question":"You use the CloudWatch Agent to collect system-level metrics in an EC2 instance and send them to AWS CloudWatch. Then you create a CloudWatch alarm based on the mem_available metric. The alarm status is OK for some time but it suddenly becomes “Insufficient data”. The EC2 instance is still up and running. Its status checks also pass. How would you resolve the problem?","explanation":"The CloudWatch agent is responsible for transmitting metrics data to CloudWatch. If the agent is not running properly, CloudWatch will not receive the metrics and the alarm will become \"Insufficient data\". You should check the running status of the CloudWatch agent process. Adjusting alarm threshold does not help as the CloudWatch alarm still does not receive data from the agent.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html","title":"Amazon CloudWatch Alarms"}],"answers":[{"id":"f116df32e8b60a8f3d2d7c1d5d4182eb","text":"Check if the CloudWatch agent is running properly. Restart the process if it is not running.","correct":true},{"id":"d6dcf40b4663a8af0717c19f3c8c3c6e","text":"Delete the CloudWatch alarm and recreate a new one as the connection may get hung.","correct":false},{"id":"00e24de8460e88befdad35029ca80303","text":"Adjust the defined threshold of the alarm. The threshold may be too high for the reported data.","correct":false},{"id":"86fa960aa5069a37c3dec9725cba235a","text":"Create an image for the EC2 instance and launch a new EC2 instance using the AMI. Terminate the original instance.","correct":false}]},{"id":"e05ee44b-cd10-4658-9853-ff5cea9c9d32","domain":"automation","question":"The company has started experiencing deployment issues due to the increasing complexity of the application and the lack of a structured testing and release process. The DevOps team of the company plans to set up a continuous integration pipeline in AWS to improve the stability of the releases and through the enforcement of the use of automated tests. The Head of DevOps has been instructed to use managed services as much as possible to reduce the maintenance overhead of the continuous integration pipeline. How can the DevOps team accomplish this?","explanation":"CodeBuild and CodePipeline are managed services that can be used to easily build a continuous integration pipeline. CodeBuild can run the tests and CodePipeline can manage the pipeline steps for the CI testing and deployment pipeline. Given that managed services are preferred, running Jenkins in an EC2 instance is not the priority option. AppSync is for building GraphQL powered APIs and is not used for continuous integration pipeline requirements.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html","title":"Use CodePipeline with CodeBuild to Test Code and Run Builds"}],"answers":[{"id":"4c32bcc37cbaab42d77738b8b6875d19","text":"Use AppSync and CodeBuild for the continuous integration pipeline.","correct":false},{"id":"8adba880488851800b50283a6de55215","text":"Use Jenkins in an EC2 instance and CodePipeline for the continuous integration pipeline.","correct":false},{"id":"ce1176f044958de744d567d7ac7d0534","text":"Use Jenkins in an EC2 instance and AWS Step Functions for the continuous integration pipeline.","correct":false},{"id":"6574dc863e2d0855e66841be7aee54e7","text":"Use CodeBuild and CodePipeline for the continuous integration pipeline.","correct":true}]},{"id":"3eaf9c7f-ec35-42cb-af59-f26b9b358432","domain":"automation","question":"Which of the following sections is required for a CloudFormation template to be valid?","explanation":"The Resources section is the only required section. It specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resources-section-structure.html","title":"CloudFormation - Resources"}],"answers":[{"id":"229eb04083e06f419f9ac494329f957d","text":"Conditions","correct":false},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false},{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":false},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":true}]},{"id":"17885db5-c61d-4edf-b0e3-e9d449d8e618","domain":"mon-rep","question":"Which of the following EC2 instance metrics are sent to Amazon CloudWatch by default? Select three.","explanation":"CPU utilization, disk I/O and network traffic are visible to the hypervisor running the instance and are sent to CloudWatch by default. For the others, you would need to install CloudWatch Agent on the instance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html","title":"Available CloudWatch Metrics for Your Instances"}],"answers":[{"id":"c4903df1e41e0ba0b4636e753d8c7661","text":"Disk read and write operations","correct":true},{"id":"2105454033539f83d3b07265aac88d7a","text":"The amount of swap space currently in use","correct":false},{"id":"fb8326e1edbd06b1bf6ea0332e089055","text":"CPU utilization","correct":true},{"id":"613b1188dd73dfdb768f39cfad3cc9a3","text":"Memory utilization","correct":false},{"id":"ec1e54ae04652319df5c011f228c07ac","text":"Free disk space","correct":false},{"id":"b4e5bb2b6842990e919682b3d6d5726c","text":"Volume of incoming and outgoing network traffic","correct":true}]},{"id":"1982b211-0620-43ed-9a88-f5237a42eae2","domain":"automation","question":"You're using CloudFormation templates to build out staging environments. Which section of the template would you edit in order to allow the user to specify the SSH key-name at start time?","explanation":"The parameters property type in CloudFormation allows you to accept user input when starting the template, allowing you to reference the user input as variable throughout your cloud formation template.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html","title":"About CloudFormation Parameters"}],"answers":[{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false},{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":true},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":false}]},{"id":"b84511ce-eca6-4a78-8b45-e76b4a0f37af","domain":"data-man","question":"An organization is moving its existing data lake from on-premises SAN storage to AWS.  You have 30TB of data to move. Your new cloud footprint includes an AWS Direct Connect link of 200Mbps, which is used for the mission critical link between your offices and the new systems in Production in AWS. Which of the below options would you recommend for the fastest transfer to AWS S3?","explanation":"With such a large amount of data to transfer it makes sense to use the AWS Snowball service.  Additionally flooding the Direct Connect connection with such a lot of traffic for several weeks would be a high risk to Production.  Snowball will provide a reliable and quick way to move the data. AWS DataSync is another possible candidate but it will place a heavy load on the Direct Connect link.","links":[{"url":"https://aws.amazon.com/snowball/faqs/#when-to-use","title":"AWS Snowball FAQs"}],"answers":[{"id":"64bf8ac94115dbfb0847b75ceda67d5a","text":"AWS S3 Accelerated Transfer","correct":false},{"id":"56169776bdeba6f7384a8ad473b0fc72","text":"AWS Data Sync","correct":false},{"id":"c2d838ea2b9043be2161255da328dc60","text":"AWS Snowball","correct":true},{"id":"0d9da8d932c0312c10d819af40de0ba8","text":"AWS Snowball Edge","correct":false}]},{"id":"c7e5ebf3-7eae-45bf-a02b-28bc06a6d575","domain":"mon-rep","question":"You have several CloudWatch Log Groups and Lambda Functions send logs to them. You need to use a tool to quickly search and analyze the log data in the log streams. The tool should automatically discover information in the Lambda logs such as the timestamp, max memory used and execution duration. It should also help you to perform the query using simple and pre-built query languages. Which tool is the best one for you to choose?","explanation":"CloudWatch Logs Insights is the most suitable tool to perform pre-build queries on CloudWatch Logs. Users do not need to transfer or transform the log streams. The log fields contained in the Lambda logs are automatically discovered. AWS Athena only performs queries on S3 objects. For Amazon ElasticSearch or Kinesis stream, extra configuration steps are required.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html","title":"Analyze log data with CloudWatch Logs Insights"}],"answers":[{"id":"01551b4fd4b50ed9ceb2be1d0e338932","text":"Use AWS Athena to run queries on the log streams. The query language of Athena is based on SQL.","correct":false},{"id":"7edc1b3e500b915d821d4db1db01280c","text":"Use CloudWatch Logs Insights to select the log groups and perform queries.","correct":true},{"id":"79930c2280cb2398212ed1465b34b81e","text":"Export the log data to an Amazon ElasticSearch Service. Use ElasticSearch to perform queries or analysis.","correct":false},{"id":"2c4abdd6514dcf68f6bace3f1a769e4f","text":"Stream the log data to an Amazon Kinesis stream and perform real time queries or analysis in the stream.","correct":false}]},{"id":"ab86f355-ca8c-4f03-bc21-bd2b6add379f","domain":"high-avail","question":"You need to deploy a DynamoDB table in the production environment for an application. The read and write traffic is low during weekdays. On weekends, the traffic becomes much higher due to the increasing number of users. The traffic pattern is very predictable and there is no unexpected spike. Which kind of capacity mode would you configure for the DynamoDB table?","explanation":"In this scenario, DynamoDB Auto Scaling should be used as the pattern is predictable. On-demand mode is more suitable for unknown workloads and development environments. Reserved capacity would cause a waste of resources. And users cannot configure a schedule to automatically adjust the capacity in DynamoDB.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html","title":"DynamoDB read/write capacity mode"}],"answers":[{"id":"4fdafc1c54632989128146ba919a0e07","text":"Provisioned capacity with auto scaling.","correct":true},{"id":"35ba3d07134765e8d2b001befc79e653","text":"Reserved capacity that is able to cover the high traffic on weekends.","correct":false},{"id":"b799c024f6af20489d00692976f1bfd1","text":"On demand mode without the need to configure the capacity.","correct":false},{"id":"fe576b38a59cfeaa466623938285521e","text":"Provisioned capacity on weekdays with a schedule to automatically increase the capacity on weekends.","correct":false}]},{"id":"a8d6d4f3-6877-4e89-9e7c-c6f83b83e1de","domain":"automation","question":"A small business has a monolithic application utilizing several EC2 instances. The technology consultant has suggested to the engineering team of the business that the application is containerized to improve the deployment and scaling processes of the team. The engineering team processes both critical and non-critical workloads within the application. Given the size of the application and the number of transactions being processed by the system, the CFO has instructed the engineering team to ensure that the setup must be as cost-effective as possible without introducing the risk of data loss for critical workloads. How can the engineering team accomplish this?","explanation":"On-demand EC2 instances should be used for critical workloads and spot instances can be used for non-critical workloads. ECR (Elastic Container Registry) is not used to run container processes. Instead, it is used to store container images. For containerization requirements, ECS (Elastic Container Service) is used to manage container workloads. Step Functions is used for orchestrating different processes and can not be used by itself to manage the workloads.","links":[{"url":"https://aws.amazon.com/blogs/compute/running-high-scale-web-on-spot-instances/","title":"Running High-Scale Web Applications on Spot Instances"}],"answers":[{"id":"b8864524b96a467a8f1023b5a2c9f003","text":"Use Step Functions and on-demand EC2 instances for the critical workloads. Use Step Functions and spot EC2 instances for non-critical workloads.","correct":false},{"id":"ddf2c2e29f2118061f6a160054593338","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use ECR and spot EC2 instances for non-critical workloads.","correct":false},{"id":"1d3682fdb0beec13b13a37502cfb3108","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use Step Functions and on-demand EC2 instances for non-critical workloads.","correct":false},{"id":"47e42a6e665f0a913236e5580553e3ad","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use ECS and spot EC2 instances for non-critical workloads.","correct":true}]},{"id":"99568617-996c-454f-98ce-c120f3ea2c38","domain":"automation","question":"By default, what status message will you see if your CloudFormation stack encounters an error during creation?","explanation":"You will see the ROLLBACK_IN_PROGRESS message if your CloudFormation stack encounters an error during creation.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-describing-stacks.html","title":"Describing Your Stacks"}],"answers":[{"id":"0ad1f7c4221eb5f69b578b85f24935d0","text":"ROLLBACK_IN_PROGRESS","correct":true},{"id":"d2884fa6c360974bc033ce3ab23c63f6","text":"STACK_ERROR","correct":false},{"id":"31f5c2cdf809b189d85d0dfcc0d7a7b4","text":"DELETE_IN_PROGRESS","correct":false},{"id":"f3337297902478338063007adf324783","text":"ERROR_STACK_DELETE","correct":false}]},{"id":"daea596c-77ce-4763-bc26-c7eabbeb3fed","domain":"automation","question":"The CTO of an e-commerce company has mandated the use of Infrastructure as Code (IaC) services and tools to manage the application resources and processes. The engineering team has divided the resources into two groups: application resources and network (VPC) resources. The engineering team lead has been instructed to transform the configuration of these resources into templates that can easily be configured to prepare different environments. How can the engineering team lead accomplish this?","explanation":"CloudFormation can be used for the IaC requirements in provisioning and managing AWS resources including VPC resources and other managed services. Out of all the options, only OpsWorks can be used as a Configuration Management service for the management application level processes and workloads inside EC2 instances. OpsWorks is not used for provisioning network resources. Amazon EKS is for managing and orchestrating containers using Kubernetes.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/welcome.html","title":"OpsWorks"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-vpc.html","title":"EC2 VPC"}],"answers":[{"id":"450822349ee2cf9d667ee39de6b39f39","text":"Use CloudFormation for the network resources. Use AppSync for the application resources.","correct":false},{"id":"9328d9954617430931d24f7812f5d613","text":"Use Amazon EKS for the network resources. Use OpsWorks for the application resources.","correct":false},{"id":"1148de578466f53ad53ec6613254366b","text":"Use CloudFormation for the network resources. Use OpsWorks for the application resources.","correct":true},{"id":"17ad6545a81ec8297ada8e103fe040f9","text":"Use OpsWorks for the network resources. Use CloudFormation for the application resources.","correct":false}]},{"id":"b63a9012-82a9-4a3f-9c32-2766b7adbdf6","domain":"networking","question":"You are a SysOps Administrator setting up a VPN connection between your on-premises data center and with AWS. You currently have an Amazon VPC setup with a Virtual Private Gateway. You have installed a customer gateway to your on-prem data center and router for your on-premises network is showing status OK. When you try to connect the EC2 instance in your Amazon VPC to a virtual machine in your data center it does not work. How should you set up the route table in the Amazon VPC?","explanation":"To enable instances in your VPC to reach your customer gateway, you must configure your route table to include the routes used by your Site-to-Site VPN connection and point them to your virtual private gateway. You can enable route propagation for your route table to automatically propagate those routes to the table for you.","links":[{"url":"https://docs.aws.amazon.com/vpn/latest/s2svpn/SetUpVPNConnections.html","title":"AWS Site-to-Site VPN"}],"answers":[{"id":"92b30688af3ba53d2f0a13cd249bd865","text":"Configure a route to the virtual private gateway.","correct":true},{"id":"7607506e7811e80615e897ab0057315b","text":"Configure a route to the customer gateway.","correct":false},{"id":"06625c579c97e66a67f1dda1fb750f64","text":"Configure a route to the NAT gateway.","correct":false},{"id":"39e27dda89073759c58b8260fe34ffb8","text":"Configure a route to the internet gateway.","correct":false}]},{"id":"eba4823a-9cb9-440a-99cd-38f38ed3c10d","domain":"mon-rep","question":"An IoT company is producing a large stream of records and events and aims to store the event records in a persistent storage service of AWS. The CTO of the company has instructed the Solutions Architect of the IoT company to provide a solution that has the least amount of work required to set up and manage the data stream setup. How can the Solutions Architect accomplish this?","explanation":"Out of all the options, S3 will have the least storage costs and can easily be integrated with AWS Kinesis Firehose as a target destination of event records. The AppSync real-time support is for web and mobile applications. For an event bridge requirement that processes logs and event records from IoT sources, the Kinesis family of services is the primary option for handling realtime data streaming requirements. Given the amount of records to be stored, EBS is not a reliable solution and is also not a target destination for Kinesis.","links":[{"url":"https://aws.amazon.com/blogs/big-data/persist-streaming-data-to-amazon-s3-using-amazon-kinesis-firehose-and-aws-lambda/","title":"Amazon Kinesis Firehose"}],"answers":[{"id":"5ced4482b5eafc08a6df5d246887a232","text":"Use AWS Kinesis Firehose for the real-time data streaming service and AWS EBS for the storage.","correct":false},{"id":"cd4cace18ffdaa97f0968c443e1ffd16","text":"Use AWS Kinesis Firehose for the real-time data streaming service and S3 for the storage.","correct":true},{"id":"03c8bfac94e1e7b5ad38f4d55bdb2f6b","text":"Use AWS AppSync and Lambda for the real-time data streaming service and DynamoDB for the storage.","correct":false},{"id":"532a348bf03c7431c7f01baea9a4e380","text":"Use AWS Lambda and API Gateway for the real-time data streaming service and S3 for the storage.","correct":false}]},{"id":"9ad0ca0f-bcc1-4a4c-866c-62f4fae5b51c","domain":"networking","question":"You have a customer who wants to connect her on premise data center to AWS. The customer plans to transfer a large amount of data starting in a few months to AWS. The requirement is to have a consistent network connection. It is arbitrary whether the connection itself is public or private. What AWS solution would you recommend to the customer?","explanation":"Using AWS Direct Connect, data that would have previously been transported over the Internet can now be delivered through a private network connection between AWS and your data center or corporate network. In many circumstances, private network connections can reduce costs, increase bandwidth, and provide a more consistent network experience than Internet-based connections. The requirement is to have a consistent network connection, and the transfer can happen in a few months. If you need an immediate connection, then use a VPN as it can be set up within a few hours or less. However a VPN does not provide a consistent network experience as it depends on an Internet connection. Uploading data with S3 is an option but would require administrative overhead, as will uploading EBS snapshots to S3.","links":[{"url":"https://aws.amazon.com/directconnect/faqs/","title":"AWS Direct Connect FAQs"}],"answers":[{"id":"86841d550579b63dca9f9de04558994c","text":"Upload data using multi-part upload to an S3 bucket. Schedule the uploads on weekends to minimize business operation disruption.","correct":false},{"id":"0d2cde735fe999ffbbb7eda1d5814a11","text":"Save data to EBS volumes. Take snapshots of those volumes and upload to S3.","correct":false},{"id":"96b0822a1636db65a54a992ab74c239f","text":"Set up an AWS Direct Connect connection between the customer corporate data center and AWS. Transfer data to AWS via Direct Connect.","correct":true},{"id":"eaa0636a71f37c92d3dd6ec32277a3c1","text":"Set up a private VPN connection between the customer corporate data center and AWS. Attach a customer gateway to the customer data center, and a virtual private gateway in Amazon VPC. Transfer data via the VPN connection.","correct":false}]},{"id":"6da1a2f4-4dc6-48be-8e42-c838e3815602","domain":"data-man","question":"Your CTO wants to store company data in the Cloud. The full migration plan includes moving 500 TB of data to Amazon S3. What would be the fastest, and most cost-effective way to move this amount of data to AWS?","explanation":"Snowball is a strong choice for data transfer if you need to more securely and quickly transfer terabytes to many petabytes of data to AWS. As a rule of thumb, if it takes more than one week to upload your data to AWS using the spare capacity of your existing Internet connection, then you should consider using Snowball. If you have a 100 Mb connection that you can solely dedicate to transferring your data and need to transfer 100 TB of data, it takes more than 100 days to complete data transfer over that connection. You can make the same transfer by using multiple Snowballs in about a week. Direct Connect would take longer and is more expensive, and there would be no need for the connection after moving the 500 TB. VPN is faster to set up than Direct Connect but would take even longer. Storage Gateway would have the same issues and is ideally meant to be used as a hybrid cloud storage service.","links":[{"url":"https://aws.amazon.com/snowball/faqs/","title":"AWS Snowball FAQs"}],"answers":[{"id":"6459fb2dead422bbe8c534a859c17e67","text":"Set up a Direct Connection connection between your on-premise data center and AWS. Transfer the data over the Direct Connect connection.","correct":false},{"id":"4d4f0e9f5d90bedd7d63f07e5d7102a7","text":"Transfer your data with multiple instances of Snowball. Install the Snowball client on multiple workstations and transfer data to the Snowball devices.","correct":true},{"id":"49d1df753d0b6625015b1b5a57b60b6a","text":"Set up a VPN connection between your on-premise data center and AWS. Transfer the data over IPSec Tunnel using encryption.","correct":false},{"id":"37724cf12ad7358208dd202bd2037e64","text":"Use AWS Storage Gateway using file gateway and copy the data to S3 using the file gateway mount point.","correct":false}]},{"id":"316dd185-ef97-4e16-81d7-b5c062f1ec47","domain":"high-avail","question":"You are providing a storage solution for a customer. The customer requires a scalable, secure, and highly available network file system accessed by EC2 instances to support highly parallelized workloads and performance needs of big data and analytics. What AWS storage solution would you build for your customer?","explanation":"To understand Amazon EFS, it is best to examine the different components that allow EC2 instances access to EFS file systems. You can create one or more EFS file systems within an AWS Region. Each file system is accessed by EC2 instances via mount targets, which are created per Availability Zone. You create one mount target per Availability Zone in the VPC you create using Amazon Virtual Private Cloud. Traffic flow between Amazon EFS and EC2 instances is controlled using security groups associated with the EC2 instance and the EFS mount targets. Access to EFS file system objects (files and directories) is controlled using standard Unix-style read/write/execute permissions based on user and group IDs.","links":[{"url":"https://d0.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf & https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html","title":"Amazon EFS: How It Works"}],"answers":[{"id":"1bb9e848b8d17a7885bee17d76e56c27","text":"Create an EFS file accessible by EC2 via mount targets in each Region. Control traffic flow between Amazon EFS and EC2 instances using security groups.","correct":false},{"id":"bb6c11aa159a74b89de0fe51a29de08f","text":"Create an EFS file accessible by EC2 via mount targets in each Region. Control traffic flow between Amazon EFS and EC2 instances using NACLs.","correct":false},{"id":"ed0d300169a44f10c753efc49ccb714c","text":"Create an EFS file accessible by EC2 via mount targets in each Availability Zone. Control traffic flow between Amazon EFS and EC2 instances using NACLs.","correct":false},{"id":"1ec0f5d67390f1b741ec4ec61ce713f1","text":"Create an EFS file accessible by EC2 via mount targets in each Availability Zone. Control traffic flow between Amazon EFS and EC2 instances using security groups.","correct":true}]},{"id":"e2dd07ca-6d80-4c5b-87ce-95bd37a03b28","domain":"dep-prov","question":"You are helping to design a multi-instance storage solution for volume data. Your CTO believes utilizing Amazon EBS volumes is the best solution but has come to you for advice. What would you suggest?","explanation":"Amazon EBS volumes can only be attached to one EC2 instance at a time. If you need multiple EC2 instances accessing volume data at the same time, consider using Amazon EFS as a file system. Taking a snapshot and attaching new volumes from the snapshot to every instance would be an administrative burden. S3 is not used for volume data.","links":[{"url":"https://d0.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf","title":"AWS Storage Services Overview"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html","title":"Amazon EFS: How It Works"}],"answers":[{"id":"43fe6d9ea40b98964da1097e42fb0c54","text":"Use Amazon EBS volumes. Attach the same EBS volume to all instances.","correct":false},{"id":"e417bdbb8845ac556e877cff73411a3a","text":"Use Amazon EFS. Mount EFS file system on instances within a VPC.","correct":true},{"id":"b7e0cfad15004b2f664c81878bfd1091","text":"Use Amazon EBS volumes. Take a snapshot of the volume. Create a new volume from the snapshot and attach to all instances.","correct":false},{"id":"03c2ae52e51575a97a45797cd779317b","text":"Use Amazon S3. Attach an IAM role to all instances granting access to the S3 bucket hosting the data.","correct":false}]},{"id":"c51ec258-f289-41da-9ffe-77c296f5998f","domain":"data-man","question":"A company is reaching capacity with its on-premises SAN storage.  It has started to build-out a cloud presence and already has a Direct Connect from its Data Center to AWS.  A new application server is being deployed on-premises, but the company is unsure what the storage requirements might be for the application - they could quite quickly outgrow the remaining capacity of the data center.  What AWS service could be used in order to present an NFS mount point to the server, but store the files in the Cloud?","explanation":"The file gateway enables you to store and retrieve objects in Amazon S3 using file protocols, such as NFS. Objects written through file gateway can be directly accessed in S3.  Although Volume and Tape Gateway both push your data to S3 as a back-end, they both provide iSCSI or Virtual Tape devices, and do not provide an NFS or SMB mount point in order to work from S3 as if it was a local file store.  S3 Gateway is not a service provided by AWS.","links":[{"url":"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html","title":"What Is AWS Storage Gateway"}],"answers":[{"id":"daf71e62fcf30c944e8792f733eb9324","text":"AWS Storage Gateway - File Gateway","correct":true},{"id":"7ab2d24d1894cccf8593c88513473aa9","text":"AWS Storage Gateway - Volume Gateway","correct":false},{"id":"316d2e201514c4d952cfdd81c270e9c1","text":"AWS Storage Gateway - S3 Gateway","correct":false},{"id":"1461208499456e0ec38b4c464e8c8035","text":"AWS Storage Gateway - Tape Gateway","correct":false}]},{"id":"4c1c93f7-a7c8-4c21-be81-6719e4c149e6","domain":"dep-prov","question":"A company has an existing static blog site hosted on top of Amazon S3 that has been running for weeks. The development team has been instructed to upgrade the blog site to serve dynamic content. The development manager has mandated that managed services and serverless architecture patterns must be used as much as possible. How can the development team accomplish this?","explanation":"For a serverless architecture, the API Gateway, Lambda, and DynamoDB combo would allow a user to prepare serverless API endpoint that allows custom logic to be performed inside a Lambda function. The DynamoDB table(s) would contain the data being processed by the Lambda function.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html","title":"API Gateway - Getting Started with Lambda Integration"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-on-demand-https-example-configure-event-source_1.html","title":"Lambda - Configure Event Source"}],"answers":[{"id":"ab2bbd6f5fb42ed01a72bc28be974191","text":"Use API Gateway, SQS, and RDS for the API serving the dynamic content.","correct":false},{"id":"3de8f0c485401d6141e7f0aca4b8c68e","text":"Use API Gateway, Lambda, and DynamoDB for the API serving the dynamic content.","correct":true},{"id":"12c194830010e504e8e7e72650cb6952","text":"Use API Gateway, CodeBuild, and DynamoDB for the API serving the dynamic content.","correct":false},{"id":"5780728868d4030a2f6f36eedfd3f2d5","text":"Use API Gateway, CodeBuild, and RDS for the API serving the dynamic content.","correct":false}]},{"id":"fcfd9e09-dd0c-45a2-abb0-a6d92297ef92","domain":"security-comp","question":"You have just been hired as a CISO at a space exploration company that makes rockets. The company has contracts with the US airforce and has very strict IT requirements. You discover that on your first day a third party IT auditing company is on site and they are after security and compliance documents such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. Which AWS service can help you meet this need?","explanation":"All AWS accounts can get access to AWS compliance documentation using AWS Artifact","links":[{"url":"https://aws.amazon.com/artifact/","title":"Artifact FAQs"}],"answers":[{"id":"05a080cb7d6b6da90ca133767496ccb0","text":" AWS Artifact","correct":true},{"id":"fa092ee9faf62930336257691a3dbfe8","text":"AWS Config Manager","correct":false},{"id":"7c90c8f2a24f3a1a28525f19fb2c75ab","text":"AWS Inspector","correct":false},{"id":"05f43441d2d29ae2bb38fc8596ca6ff7","text":"AWS Trusted Advisor","correct":false}]},{"id":"1db48074-bc12-4a56-92ce-865f2766c080","domain":"dep-prov","question":"You are running an application where you need to bring your own licenses for your AWS EC2 instances. To maintain the license compliance, the instances should be consistently deployed to the same physical servers over time. And the number of sockets and physical cores used in EC2 need to be well controlled. Which instance purchasing option would you choose?","explanation":"In this scenario, it is required that the instances are always deployed to the same physical hosts. Dedicated hosts should be chozen and this option also supports Bring Your Own License (BYOL). Dedicated instances are dedicated to a single customer, however it is not guaranteed that they are deployed to the same physical servers. Both scheduled reserved and reserved instances are incorrect because they are not dedicated to the same machines.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html","title":"Dedicated hosts"}],"answers":[{"id":"82595be34ecee78fa9be27b7d108b98b","text":"Scheduled reserved instances","correct":false},{"id":"46c8b5c0c6ebe7840383ec6b058d0394","text":"Dedicated instances","correct":false},{"id":"9d6afa7b92408e486a11a7f9b3179f8f","text":"Dedicated hosts","correct":true},{"id":"bcc129e8d9496a05a331c903d4c5bcb0","text":"Reserved instances","correct":false}]},{"id":"aa05a3d6-0a00-44fb-97e7-cf343aa7ca84","domain":"dep-prov","question":"Your Dev team in Ireland needs an AMI that was created in us-east-1. The Irish Dev team have a copy of the AMI and are attempting to use it to launch instances in eu-west-1, however they are unable to make it work. Which of the following is the most likely cause of the problem?","explanation":"Copying a source AMI results in an identical but distinct target AMI with its own unique identifier. AWS does not copy launch permissions, user-defined tags, or Amazon S3 bucket permissions from the source AMI to the new AMI. After the copy operation is complete, you must apply launch permissions manually.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html","title":"Copying an AMI"}],"answers":[{"id":"d81e2ae842a8098be3e06c2c3b5042f5","text":"You cannot use AMIs created by a different account","correct":false},{"id":"b1a80cc35924d3526e2b247428ef3a0b","text":"The US based account which created the AMI has not shared it with the Ireland based account.","correct":false},{"id":"b3118e4c7922300a27c60efc715a92d6","text":"The Dev team does not have launch permissions to use the AMI to launch instances.","correct":false},{"id":"dffd2baaa68b8abf554da0ad66c1ce59","text":"The Dev team has failed to manually re-create the launch permissions.","correct":true}]},{"id":"ff6e1b5e-e383-4b55-bf94-6721270ee9b5","domain":"security-comp","question":"The engineering team of a FinTech company has migrated their on-premise application to AWS and has decided to use AWS DynamoDB to store the records and a combination of EC2 instances and Lambda functions for the data processing requirements. The Chief Security Officer of the company has mandated that the DynamoDB table is accessed without the use of access keys and secrets. How can the engineering team accomplish this?","explanation":"IAM roles allow EC2 instances and similar resources such as Lambda functions to perform operations on other resources without the need for access keys and secrets. There is no such thing as Cognito roles.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg//access-control-identity-based.html","title":"Access Control Identity Based"}],"answers":[{"id":"2d476ce7d034e0bb6e4bbf75b24aec29","text":"Create and associate IAM resource policies to the EC2 Instances. Create and associate IAM roles to the Lambda functions.","correct":false},{"id":"cc59ffeba9257938c9ae467e4e7ea6ee","text":"Create and associate IAM roles to the EC2 Instances. Create and associate IAM resource policies to the Lambda functions.","correct":false},{"id":"c026f0685e3e2bdc391b7f1d5d2036c2","text":"Create and associate IAM roles to the EC2 Instances and Lambda functions.","correct":true},{"id":"e81471d72607716ab24cf65e93a13bc6","text":"Create and associate IAM roles to the EC2 Instances. Create and associate Cognito roles to the Lambda functions.","correct":false}]},{"id":"3acceef5-9df8-49d6-a227-cd414a046293","domain":"networking","question":"As a SysOps Administrator you are tasked with improving the performance of a website that is serving customers in two separate Regions. There are customers in us-west-2, and customers in ap-southeast-1. There is growing demand for the website from customers in ap-southeast-1, and customers have been complaining about poor latency. How would you ensure that users are directed to the Region with the best performance?","explanation":"If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Geolocation may sound like the right answer but it may trick you thinking that the closest Region assumes the lowest latency, but this is not the case. There is no need to separate the website into two Regions as this is an administrative burden. An ALB with HTTP path routing has nothing to do with improving latency.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency","title":"Choosing a Routing Policy"}],"answers":[{"id":"eb8c189707a52253445f78cd5791eb37","text":"Use Amazon Route 53 geolocation routing to direct customers.","correct":false},{"id":"20977b79538e17806537e17b503fb602","text":"Separate your website into two websites -- one in us-west-2, and one in ap-southeast-1. Routinely log into both Regions to maintain both websites.","correct":false},{"id":"57eb7a3ea4c9eabfca928f3ce81be0b7","text":"Use Amazon Route 53 latency-based routing to direct customers.","correct":true},{"id":"4a2ab96fd93984723a369708f83fec95","text":"Configure an Application Load Balancer in front of your website. Use the HTTP path routing to direct the customer to the best performing website.","correct":false}]},{"id":"218d2f45-18d6-4e2a-9d8e-723b053aacfc","domain":"security-comp","question":"A property management company hosts their customer portal on AWS Elastic Beanstalk. They have other applications that run on EC2 behind Application Load Balancers. A single custom Security Group provides firewall protection for all of the EC2 load balancers. The customer portal Application Load Balancer currently uses a Security Group assigned by AWS. They'd like to manage all of their firewall rules in a single place by standardizing on the custom Security Group. How will they be able to assign the custom Security Group to the customer portal load balancer?","explanation":"Populating an elbsg.config file with the ID of a managed Security Group will assign that Security Group to an Elastic Beanstalk load balancer when you redeploy your code. The elbsg.config file needs to reside in the .ebextensions/ directory of your local application code directory. The capability to assign a Security Group to a load balancer is not available in the Elastic Beanstalk management console. The 'eb config' command in the Elastic Beanstalk CLI doesn't provide the option to change the load balancer's Security Group. Changing an Elastic Beanstalk load balancer's Security Group in the EC2 management console will cause an 'ELB health is failing or not available for all instances' severe status in the Elastic Beanstalk management console.","links":[{"url":"https://aws.amazon.com/elasticbeanstalk/","title":"AWS Elastic Beanstalk"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/security-group-elastic-beanstalk/","title":"How do I use my own security group for my load balancer when I deploy an AWS Elastic Beanstalk application?"}],"answers":[{"id":"eab80574f1731b00df7e3f512311830a","text":"Assign the custom Security Group to the load balancer in the Elastic Beanstalk management console","correct":false},{"id":"7c9e9b74dd08f4ec59abefb42eb0fcec","text":"Find the load balancer in the EC2 management console and assign the custom Security Group to it","correct":false},{"id":"774fc838c180260d18cc94b4fb6fcb8a","text":"Run the 'eb config' command from the Elastic Beanstalk CLI to set the load balancer's Security Group to the ID of the custom Security Group","correct":false},{"id":"0dfbe8d3180db91dcc4deb605ad11c1e","text":"Update the elbsg.config file under the local application code directory to point to the ID of the custom Security Group. Redeploy your code","correct":true}]},{"id":"78ad6dfb-14bd-44bf-a5a4-4af2076aae23","domain":"networking","question":"As your company's lead network administrator, you are helping the development team set up a VPC for an application in their AWS account. The application requires a network configuration such that the web servers of the application have connectivity to the Internet, and the database servers have VPN-only connectivity to the corporate network servers. What VPC set up would support this desired configuration? (Select all that apply)","explanation":"The scenario requires a VPC with an internet gateway, a virtual private gateway, a public subnet, and a VPN-only subnet. One route table has a route to the virtual private gateway in a private subnet. Another route table is explicitly associated with the public subnet. The custom route table has a route to the internet (0.0.0.0/0) through the internet gateway. A NAT instance in another private subnet would not allow Internet connectivity. The Direct Connect connection is unnecessary. The requirements does not allow placing database servers in public subnets.","links":[{"url":"https://docs.aws.amazon.com/en_pv/vpc/latest/userguide/VPC_Route_Tables.html","title":"Route Tables"}],"answers":[{"id":"31d58d613e8532ad19cdb0c73912f36d","text":"Place the database servers in a private subnet. Associate a route table to the private subnet that has a route to a virtual private gateway.","correct":true},{"id":"f8f4044343da77cd7ab9710d8b6f5067","text":"Place the web servers in a public subnet. Associate a route table to the public subnet that has a route to the Internet through the Internet Gateway.","correct":true},{"id":"ac748fb5daa7fcaff151922fb2c482a4","text":"Place the database servers in a public subnet. Associate a route table to the public subnet that has a route to the Internet through the Internet Gateway.","correct":false},{"id":"4d167d5d0ddda70960f354da47b72e33","text":"Place the web servers in a private subnet. Associate a route table to the private subnet that has a route to a NAT instance in another private subnet.","correct":false},{"id":"d7235003c798151a119f9b00b31cce7f","text":"Place the database servers in a public subnet with Direct Connect. Set up a Direct Connect connection to the servers in your on-premises environment.","correct":false}]},{"id":"d0b24152-771d-489e-8853-7221454dbcdb","domain":"networking","question":"You are running a Python Flask application on a private subnet in your Amazon VPC. The web application has been running into some issues, and you need to re-code the application to include more robust Python libraries. When you try to install Python libraries you get a network error but you are able to install these packages on your public-facing servers. How would you allow your private servers to download and install these libraries in the most effective manner?","explanation":"To enable instances in a private subnet to connect to the internet, you can create a NAT gateway or launch a NAT instance in a public subnet. Then add a route for the private subnet that routes IPv4 internet traffic (0.0.0.0/0) to the NAT device. To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. Copying files in an EBS volume for installation is administratively not a good idea if Internet connectivity is going to be persistent. A Bastion Host is not the required service to connect to the Internet.","links":[{"url":"https://docs.aws.amazon.com/en_pv/vpc/latest/userguide/route-table-options.html","title":"Routing Options"}],"answers":[{"id":"cf4856324ef6b157b83f98f140d49f7b","text":"Copy the Python libraries on your public servers unto an EBS volume. Detach the EBS volume and attach them to the private servers. Install the packages from the EBS volume.","correct":false},{"id":"24db8e12a44c9ec6662d67dca5687d5b","text":"Launch a NAT instance in a public subnet. Add a route for the private subnet that routes IPv4 0.0.0.0/0 traffic to the NAT device.","correct":true},{"id":"30d834ca94e17c6e2e40caf78391e8e9","text":"Launch a Bastion Host in a public server. Open port 80 on the Bastion Host to the IP range of your private web servers. Access downloadable libraries from your Bastion Host via SSH.","correct":false},{"id":"273a72b9d0bfd2145fb5f6c102cdae13","text":"Launch a NAT Gateway in a private subnet. Add a route for the web servers in the private subnet to 0.0.0.0/0 through the NAT device.","correct":false}]},{"id":"31081113-ca90-46fd-b9de-b0d861af38ab","domain":"automation","question":"A sports company has migrated systems to AWS but has not implemented any patching policy. A SysOps administrator has been hired to understand the current state of patching and help plan remediation. Which service can they use to understand the patch level of the EC2 instances?","explanation":"AWS Systems Manager provides a centralised location to view patching status of all Managed EC2 and on-prem instances. AWS Inspector, AWS Config and Macie are not services that can provide patch status reports.","links":[{"url":"https://docs.aws.amazon.com/en_pv/systems-manager/latest/userguide/systems-manager-patch.html","title":"AWS Systems Manager Patch Manager"}],"answers":[{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":false},{"id":"7c90c8f2a24f3a1a28525f19fb2c75ab","text":"AWS Inspector","correct":false},{"id":"113b1ad9ce6cdc3a37ad8475bc9bb2b2","text":"AWS Systems Manager","correct":true},{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false}]},{"id":"eb2317bb-3bd4-4593-b217-e0610e79e106","domain":"mon-rep","question":"A critical application which runs on an EC2 instance behind an ELB is experiencing occasional outages. Associated with the outages are Windows event log entries. How can you detect these events and alert your team?","explanation":"CloudWatch Logs let you to stream logs from your EC2 instances to the CloudWatch service. A log filter on a Log Group allows you to detect occurrences of a key word or phrase. SNS can then deliver alerts for these alarms. ELB logs would not contain the keyword/phrase, nor would CloudTrail or the Autoscaling activity history.","links":[{"url":"https://docs.aws.amazon.com/en_pv/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html","title":"What Is Amazon CloudWatch Logs?"},{"url":"https://docs.aws.amazon.com/en_pv/AmazonCloudWatch/latest/logs/CountOccurrencesExample.html","title":"Example: Count Occurrences of a Term"}],"answers":[{"id":"33e9387fac100300d7d2d03d2561f1f4","text":"Use ELB logs and a filter to alarm on the event then message the team using SNS","correct":false},{"id":"cff2a2162045687c91304445129e482d","text":"Use CloudWatch logs with a log filter to alarm on an occurrence of the event then message the team using SNS","correct":true},{"id":"a325e8469dc87698c1ba13863e512af5","text":"Use Autoscaling activity logs and message the team using SNS","correct":false},{"id":"d0520b3777e34e62b94d0046d78ec3b3","text":"Use CloudTrail logs and message the team using SNS","correct":false}]},{"id":"7a800486-b78e-43ec-88ea-ffa673a94745","domain":"mon-rep","question":"A SysOps administrator has been asked to implement monitoring of an application using Elasticache to improve database response times.  Recently the application has begun to perform slowly.  They notice that the eviction rate is high for the Cluster.  What could you consider doing to rectify this issue?","explanation":"Scaling up and out is the recommended approach when the eviction rate is high on the cluster.  Elasticache is an in-memory service and therefore cannot use Provisioned IOPs for storage.  AWS does not provide an auto scaling service in Elasticache. Increasing the ConnectionOverhead value would reduce the amount of memory available for storing cache items, so is unlikely to improve the eviction rate.","links":[{"url":"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/CacheMetrics.WhichShouldIMonitor.html#metrics-evictions","title":"Which Metrics Should I Monitor?"}],"answers":[{"id":"ea661bc3f4ee7300ed230572ef878635","text":"Increase the cluster size or add more nodes to the cluster","correct":true},{"id":"0dedcafa7f1671fda48c2d4515da1fcb","text":"Enable Autoscaling on the Elasticache Cluster","correct":false},{"id":"c8e144f5e160266566a48649def7cb74","text":"Redeploy the Elasticache cluster to use Provisioned IOPs storage","correct":false},{"id":"85688e53214b00837ac8ae9bb22e9d81","text":"Increase the ConnectionOverhead value of the cluster","correct":false}]},{"id":"11e35452-b0ac-48fe-842b-faeedd3d58f1","domain":"mon-rep","question":"A SaaS company has 30 running EC2 instances that produces a significant quantity of web application logs on a daily basis. Due to the SLAs and other constraints given to the engineering team, the logs of these instances need to be analyzed in near real-time and the deployed versions of the applications need to be rolled back if there are issues observed in the logs from the latest deployment. How can the DevOps team accomplish this?","explanation":"Services such as Amazon Kinesis streams would solve the real-time processing requirement of this scenario. Amazon Kinesis Firehose has a 1-min lag therefore it cannot be used to solve the real-time requirement. All other options involve running a log processing workload every X minutes which cannot be considered real-time.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/streaming-cloudwatch-logs/","title":"Streaming CloudWatch Logs"}],"answers":[{"id":"5502483a37b221760b333eb62c65239f","text":"Stream the logs to an S3 bucket. Use Amazon Athena to process the logs and generate reports every 30 minutes.","correct":false},{"id":"508417d6030658c25449604daac66ce6","text":"Stream the logs to a DynamoDB table using DynamoDB streams. Use Amazon EMR to process the logs and generate reports every 30 minutes.","correct":false},{"id":"f410ecf126408d1c95ad85902ed7b6ff","text":"Stream the logs to an Amazon Kinesis stream. Have the users and consumers analyze and process the logs accordingly.","correct":true},{"id":"4b7916aca484485ba20548fe4006a647","text":"Stream the logs to an Amazon Kinesis Firehose that stores data in CloudWatch logs. Use Amazon EMR to process the logs accordingly.","correct":false}]},{"id":"05e4d4e8-28e3-4373-bbbd-06c207454310","domain":"data-man","question":"What is the first thing you should do to prevent your users from accidentally deleting objects in an S3 bucket?","explanation":"When a user performs a DELETE operation on an object, subsequent simple (un-versioned) requests will no longer retrieve the object. However, all versions of that object will continue to be preserved in your Amazon S3 bucket and can be retrieved or restored. Only the owner of an Amazon S3 bucket can permanently delete a version. You can set Lifecycle rules to manage the lifetime and the cost of storing multiple versions of your objects.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html","title":"Using Versioning"}],"answers":[{"id":"0b291595e5f35c7ba8fed39ca7464577","text":"Enable Versioning on the bucket.","correct":true},{"id":"4640422acdab41a13ddc95492dc9c60d","text":"Distribute objects via CloudFront.","correct":false},{"id":"7ce4f0314f67bee93968904b2ab51ac3","text":"Allow objects to be accessed only with signed URLs.","correct":false},{"id":"534876320349a72fec1b677bbcc87c53","text":"Change all users' IAM permissions so they can only s3:Get*, s3:List* and s3:Put*.","correct":false}]},{"id":"082f8b00-13da-4a51-918c-f976a16580ca","domain":"high-avail","question":"You have a web application that queries ElastiCache to cache your database queries. You are using Memached with ElastiCache and you use CloudWatch metrics to monitor your memcached performance. You notice that two metrics, Evictions (The number of non-expired items the cache evicted to allow space for new writes) and GetMisses (The number of get requests the cache has received where the key requested was not found), are getting very high. What should you do to scale your environment further?","explanation":"You should increase the number of nodes in your Memcached cluster or increase the size of each node in your cluster.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CacheMetrics.Memcached.html","title":"Metrics for Memcached"}],"answers":[{"id":"e97e7e506a3503958146ba4d62e89c26","text":"Increase the number of nodes in your Memcached cluster or increase the size of each node in your cluster.","correct":true},{"id":"ba0c29a489d86a1d6386e6b65cbd93d3","text":"Use CloudFront as an alternative caching engine.","correct":false},{"id":"3678711f6b0323c2c64b95f85b586d6d","text":"Migrate from Memcached to Redis.","correct":false},{"id":"fdc0f2a5336b08f741567e4b5c2cc2a0","text":"Decrease the number of nodes in your memcached cluster or decrease the size of each node in your cluster.","correct":false}]},{"id":"9f4a3688-de11-407f-99a7-f332ade4066e","domain":"dep-prov","question":"You are running an EC2 instance and have created and attached an EBS volume with default settings. You notice that the volume status check for the volume has failed and the instance can no longer access the volume. How can you access the information on the volume?","explanation":"When Amazon EBS determines that a volume's data is potentially inconsistent, it disables I/O to the volume from any attached EC2 instances by default. This causes the volume status check to fail, and creates a volume status event that indicates the cause of the failure. Switching on Enable Volume IO will allow the instance to access the volume. Switching on Auto-Enabled IO will also achieve the same outcome automatically but this is disabled by default. All other options are incorrect.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html#work_volumes_impaired","title":"Working with an Impaired Volume"}],"answers":[{"id":"bf737cfd17a3b913cb405f896e5e9b7d","text":"Switch on Enable Volume IO","correct":true},{"id":"628b701156eef0c168e27dc6d58a9d15","text":"Switch off Auto-Enabled IO","correct":false},{"id":"7de363a9824728f9b2e5c0a8efd8c6c3","text":"Switch off Enable Volume IO","correct":false},{"id":"8daa3d585fbafd5057d0e3bf2af521e0","text":"The volume can no longer be accessed","correct":false}]},{"id":"aba20312-a023-419f-b495-12e7dd2c6577","domain":"mon-rep","question":"In which of the following are your CloudTrail logs stored?","explanation":"Logs are stored in S3. You must specify a storage  bucket name to enable CloudTrail.","links":[{"url":"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.html","title":"Finding Your CloudTrail Log Files"}],"answers":[{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":false},{"id":"7b1fb630c85b556e31fa54e3d2b6201a","text":"An EBS Volume","correct":false},{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":false}]},{"id":"d585a94b-6bfb-47bf-83b2-d92525fa925b","domain":"security-comp","question":"Your CFO would like to hire a consulting company called CostControl Corp to monitor your AWS account and help optimize costs. In order to track your daily spending, CostControl Corp needs to access your AWS resources. CostControl Corp also monitors many other AWS accounts for other customers. How would you grant CostControl Corp access to your account in a secure and administratively efficient way?","explanation":"Do not give CostControl Corp access to an IAM user and its long-term credentials in your AWS account. Creating an IAM Group would still require long-term credentials. Instead, use an IAM role and its temporary security credentials. An IAM role provides a mechanism to allow a third party to access your AWS resources without needing to share long-term credentials. You can use an IAM role to establish a trusted relationship between your AWS account and a third party. After this relationship is established, a member of the exteranl account can call the AWS STS AssumeRole API to obtain temporary security credentials. Cognito is a web service that delivers scoped temporary credentials to mobile devices and other untrusted environments.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html","title":"How to Use an External ID When Granting Access to Your AWS Resources to a Third Party"}],"answers":[{"id":"ef9e894add84e3a1607217b2cfa0f98d","text":"Use Amazon Cognito User Pools to enable authentication with the external party. Cognito will create a unique identifier for each user in CostControl Corp to access temporary, limited-privilege AWS credentials.","correct":false},{"id":"2df4700c329bc33dc8133ca8de8c37ad","text":"Create an IAM role. Allow CostControl Corp users to assume this role for access to AWS resources in your account.","correct":true},{"id":"0e0f124a17ea65e797f45765d7fac5a1","text":"Create an IAM Group for CostControl Corp users. Give this group limited AWS permissions. Create CloudTrail log for this group to ensure their API calls in your AWS are within the realm of their scope of work.","correct":false},{"id":"511bbeb0af589b7478846f2bc4488bb7","text":"Identify a secure user within CostControl Corp. Create an IAM user in your AWS account with the necessary permissions and grant access to that securely identified user.","correct":false}]},{"id":"e8355e44-b357-48de-b17b-b2373c316edd","domain":"networking","question":"An organization runs a website on an Autoscaling Group behind an Application Load Balancer (ALB). During deployments the application team creates a new ASG and Load Balancer. Which DNS service can you use to flip between the two environments in a Blue/Green manner and direct all users to the new environment?","explanation":"Route53 allows various routing policies to direct users to one or more resources.  In this case all users must be directed from one Load Balancer to another.  A simple routing policy would fulfil this requirement as it will completely replace the DNS pointer from the 'blue' load balancer with the 'green'. All other routing policies perform more complicated routing based on other variables such as healthchecks or country of origin which is not needed here.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-simple","title":"Amazon Route 53 Simple Routing"}],"answers":[{"id":"1844cecdf015b88aef8e88746b321731","text":"Use Route53 latency routing","correct":false},{"id":"816c21d5068061f198f8df28aaf61e0d","text":"Use Route53 multi-answer routing","correct":false},{"id":"3299777a46e7aa18ad281b2a40bf2894","text":"Use Route53 simple routing","correct":true},{"id":"ebeefbe1e321377375eb3065b4a2fc20","text":"Use Route53 failover routing","correct":false}]},{"id":"571b9603-45b6-45b1-aa47-68849ac814bb","domain":"automation","question":"A big-box retailer runs their in-store point-of-sale system on EC2 linux instances. All of the infrastructure is managed as part of a CloudFormation stack. The web servers are part of an Auto Scaling Group. The application only needs to be available during business hours from 9:00am until 6:00pm. What would be the best way to scale the web servers cost efficiently based on demand?","explanation":"Authoring the CloudFormation template to include an AutoScaling:ScheduledAction resource to increase the Auto Scaling Group's MinSize and MaxSize values at 9:00am, and another AutoScaling:ScheduledAction resource to decrease the Auto Scaling Group's MinSize and MaxSize values at 6:00pm will save costs for the retailer during non-business hours. CloudFormation conditions control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update, but don't control the actions of an Auto Scaling Group. Using an Auto Scaling Group scheduled action provides more streamlined automation than using a Lambda function. CloudFormation mappings are key/value pairs that can be used to specify conditional parameter values, but they have no impact on the Auto Scaling Group unless they are used to create a resource.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html","title":"What is AWS CloudFormation?"},{"url":"https://s3-us-west-2.amazonaws.com/cloudformation-templates-us-west-2/AutoScalingScheduledAction.template","title":"Cloud Formation Sample Template for Time-based Auto Scaling"}],"answers":[{"id":"725933a9f7ea7ef281e2decf6cefadae","text":"Configure AutoScaling:ScheduledAction mappings in the CloudFormation template with Maxsize, MinSize, and Recurrence values based on business hours","correct":false},{"id":"765dccb8332e2036eb70a0b6276e7285","text":"Use CloudWatch Events to trigger a Lambda function at business opening and closing that adjusts the Auto Scaling Group's MinSize and MaxSize accordingly","correct":false},{"id":"a0f99978bb65910b10b75a32e1b92a2a","text":"Create AutoScaling:ScheduledAction conditions in the CloudFormation template that change Maxsize and MinSize values based on business hours","correct":false},{"id":"0db6203397832efddcbca893f96ba1b6","text":"Include AutoScaling:ScheduledAction resources in the CloudFormation template that change Maxsize, MinSize, and Recurrence values based on business hours","correct":true}]},{"id":"281dba1d-32b8-4e19-a324-72e004b2f7c6","domain":"mon-rep","question":"The Jet Engine Division of Consolidated Aerospace Corporation would like to centralize monitoring of both their on-premises systems and their AWS servers in the AWS cloud. On-premises operating systems include Red Hat, Debian, AIX, and Windows. EC2 operating systems used include Red Hat and Windows. Which architecture will provide the most robust monitoring and alerting solution?","explanation":"The CloudWatch agent is supported on Red Hat, Debian, and Windows operating systems for writing to CloudWatch Logs for both on-premises and EC2 systems. The agent is not supported for AIX, so writing those log messages to an EC2 instance for forwarding to CloudWatch Logs will work. Configuring CloudWatch metric filters will result in numerical metrics for graphing or alarming. CloudWatch alarms perform actions based on CloudWatch metrics, so setting up the metric filters needs to happen first","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html","title":"What Is Amazon CloudWatch Logs?"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html","title":"Searching and Filtering Log Data"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html","title":"Installing the CloudWatch Agent on On-Premises Servers"}],"answers":[{"id":"b506e93e4197e20db1193673763fa6b9","text":"Install the CloudWatch agent on the EC2 systems. Write on-premises logs to an EC2 instance which sends log messages to CloudWatch Logs. Configure CloudWatch alarms to alert on error messages in the logs.","correct":false},{"id":"4a3ae3aa58ce592572b629601964f7bf","text":"Install the CloudWatch agent on the EC2 systems. Write on-premises logs to an EC2 instance which sends log messages to CloudWatch Logs. Set up CloudWatch metric filters to alarm on potential issues.","correct":false},{"id":"1247cb867a2d6b15f0e4e149aee44e02","text":"Install the CloudWatch agent on the Red Hat, Debian, and Windows systems, both on-premises and EC2. Write AIX logs to an EC2 instance which sends messages to CloudWatch Logs. Set up CloudWatch metric filters to alarm on potential issues.","correct":true},{"id":"150fdd86d9a864feb197592ccedbeee3","text":"Install the CloudWatch agent on the Red Hat and Windows systems, both on-premises and EC2. Write Debian and AIX logs to an EC2 instance which sends messages to CloudWatch Logs. Configure CloudWatch alarms to alert on error messages in the logs.","correct":false}]},{"id":"6eb6dee0-9456-492e-afc7-b7c860b04c92","domain":"mon-rep","question":"You have a fleet of EC2 webservers behind an application load balancer. Your web application had some down time which involved some 5XX errors during a very important time in your business 1 week ago. Although you maintain application logs on individual EC2 instances, you do not store these logs anywhere central and unfortunately the EC2 instances that experienced the downtime have since been terminated. How could you review this log data?","explanation":"Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. These logs are encrypted and stored in S3.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html","title":"Application Load Balancer Access Logs"}],"answers":[{"id":"d53665e5cde1c8f661130ca2882d789a","text":"Open the AWS artifact service. Create a new artifact job and point the AWS artifact agents at the terminated EC2 instances. Download the metrics and review in CloudWatch.","correct":false},{"id":"c13c66278433ebb606e21e9c9f5250fd","text":"Create a new AWS inspector job to pull the snapshots of the EC2 instances from S3 and run a report in conjuction with AWS Athena.","correct":false},{"id":"49774faac3030c4425f2aa345cb89dc0","text":"If access logs is turned on for your application load balancer you could review this data by reviewing the logs in S3.","correct":true},{"id":"ae59c599efa75d438daecb43cc08ad41","text":"Use AWS X-ray to restore the logs from the terminated EC2 instances","correct":false}]},{"id":"cde5021e-5b37-406b-b25a-1bdb489d3b24","domain":"security-comp","question":"Following a recent security event, a SysOps administrator has been asked to provide details of source IP addresses of requests to a website which is hosted on EC2 instances behind an Application Load Balancer. Where can the administrator find these details?","explanation":"Application Load Balancer (ALB) Access Logs record details of client connections which include the client's IP address and port.  CloudTrail logs AWS API calls so will not provide client IP addresses, neither will CloudWatch Custom Metrics which are for logging performance metrics.  EC2 instances will log the ALB's internal IP address as the source unless specifically configured to record the true source IP by using the x-forwarded-for header, which is not enabled by default.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html#access-log-entry-format","title":"ALB Access Log Entries"}],"answers":[{"id":"6a219c87573826721cb51b987140a267","text":"CloudTrail Event Trail","correct":false},{"id":"8480ea5c787566c9a8bd2a608c725a06","text":"ALB Access Logs","correct":true},{"id":"43bc48b76d9918101fb772b4cfc58235","text":"/var/log/httpd/","correct":false},{"id":"9a19403fef543b4899ec570ad1a29ca6","text":"CloudWatch Custom Metrics","correct":false}]},{"id":"466da628-4a01-43da-bb82-90115f555b37","domain":"security-comp","question":"During a security audit your team has been asked by the auditor whether the Cloud allows for packet capture in the same way as a fiber tap can work in your old data center.  What should you tell them?","explanation":"Amazon VPC traffic mirroring makes it easy for customers to replicate network traffic to and from an Amazon EC2 instance and forward it to out-of-band security and monitoring appliances for use-cases such as content inspection, threat monitoring, and troubleshooting. These appliances can be deployed on an individual EC2 instance or a fleet of instances behind a Network Load Balancer (NLB) with User Datagram Protocol (UDP) listener.  The mirror destination can be a network load balancer or ENI, within the same AWS account/VPC as the Mirror Source, or in a different VPC/account.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-vpc-traffic-mirroring/","title":"VPC Traffic Mirroring - Capture & Inspect Network Traffic"}],"answers":[{"id":"b91e58f3583a5c4b482c731edfb82ba4","text":"Yes, VPC Traffic Mirroring supports direct access to the network packets flowing through your VPC by allowing you to mirror and forward them to another network interface in the same or another VPC in the same account","correct":false},{"id":"3c9c8b8c88c4d9a213824e6cf5e415b2","text":"Yes, VPC Traffic Mirroring supports direct access to the network packets flowing through your VPC by allowing you to mirror and forward them to another network interface as long as it is in the same VPC","correct":false},{"id":"d07368ced3edf1127eb01254521ab610","text":"No, AWS does not allow packets to be mirrored to another destination for security reasons","correct":false},{"id":"20cd1061d46cc8bc057253b243a29214","text":"Yes, VPC Traffic Mirroring supports direct access to the network packets flowing through your VPC by allowing you to mirror and forward them to another network interface in the same or another VPC in the same or another account","correct":true}]},{"id":"08eb44ba-042a-4ced-b833-888d2943bb07","domain":"dep-prov","question":"You are experiencing issues with an Application Load Balancer you have recently set up. You have not changed any of the default logging settings. In this situation, which of the following monitoring tools is the most appropriate to help with troubleshooting client requests?","explanation":"You can use Amazon CloudWatch to retrieve statistics about data points for your load balancers and targets as an ordered set of time-series data, known as metrics. You can use these metrics to verify that your system is performing as expected and they are enabled by default. Access Logs capture detailed information about requests sent to your load balancer but are disabled by default. You can use request tracing to track HTTP requests but this functionality requires that access logging be enabled. You can use AWS CloudTrail to capture detailed information about the calls made to the Elastic Load Balancing API and store them as log files in Amazon S3. You can use these CloudTrail logs to determine which calls were made, the source IP address where the call came from, who made the call, when the call was made, and so on. CloudTrail logs are not appropriate for troubleshooting client requests.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html","title":"Monitor Your Application Load Balancers"}],"answers":[{"id":"6c5c81f47915de5f03d2577e8fae1c34","text":"Access logs","correct":false},{"id":"8c6ded942a243b91e65d037ab4e21f7d","text":"CloudTrail logs","correct":false},{"id":"50ed91980adb1dac23689554eb719277","text":"CloudWatch metrics","correct":true},{"id":"6a292fdb897093b64ef80b39e7db0a4a","text":"Request tracing","correct":false}]},{"id":"582d1290-8099-4ef5-8f80-ea6043cc32d7","domain":"security-comp","question":"You are a Sys Ops Administrator for your organization. During a routine security audit, you discovered several vulnerabilities in the operating systems of your EC2 fleet. Your EC2 fleet consists of over 250 instances. How would you resolve the security issues within your EC2 fleet in the most effective manner?","explanation":"AWS Systems Manager Patch Manager automates the process of patching managed instances with both security related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. Amazon GuardDuty is a security monitoring service that analyzes VPC Flow Logs, CloudTrail Events, and DNS logs. It would not be effective in applying patches to EC2. Amazon Inspector could help identify EC2 with security vulnerabilities, but the findings generated by Amazon Inspector depend on your choice of rules packages included in each assessment template, the presence of non-AWS components in your system, and other factors. You are responsible for the security of applications, processes, and tools that run on AWS services. Similar to Amazon Inspector, AWS Config would not help in applying patches to your entire fleet.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html","title":"Systems Manager"}],"answers":[{"id":"20ebac0f0e1509a9f9b321e86861fdff","text":"Deploy Amazon GuardDuty which will create a unique finding ID for each vulnerability in CloudWatch. Automate security patch updates with Lambda to the instances that are associated with a GuardDuty finding ID.","correct":false},{"id":"6256bbd45632da9b813aac4c9411f564","text":"Have Amazon Inspector assess the instances and send metric data to CloudWatch. Set up a CloudWatch Event to trigger a Lambda function that will install the patch to instances with the vulnerability.","correct":false},{"id":"2161bb4bdfc887d3c242cee80467ae30","text":"Deploy the security patch using RunCommand with AWS Systems Manager for the entire fleet of EC2 instances.","correct":true},{"id":"b966c9e66f79563a38283cf5493db53e","text":"Use AWS Config to identify the EC2 instances with vulnerabilities based on security rules. Isolate these instances and install the patch updates to fix the vulnerabilities.","correct":false}]},{"id":"daab371e-09f1-4d56-ae18-ac01147c8d31","domain":"automation","question":"Your organisation is growing their AWS footprint and wants to build a dashboard for their hybrid infrastructure.  They use a mix of on-premises Linux and Windows machines, and new AWS EC2 instances. There are around 1500 on-premises VMs which your CTO ambitiously wants to manage with a centralised configuration tool.  How can your organization simplify the management of the patching and inventory of both the on-premises and cloud instances from one central AWS account?","explanation":"Microsoft patching is only available for on-premises instances under the 'advanced-instances' tier of Systems Manager.  The standard Systems Manager tier also only enables you to register a maximum of 1,000 servers per AWS account per AWS Region. If you need to register more than 1,000 servers or VMs in a single account and Region, then you need to use the advanced-instances tier. Since there are over 1,000 servers, and a mix of Windows and Linux workloads, the organisation needs to enable Systems Manager Advanced-Instances before it can perform inventory and patching of the whole hybrid fleet using SSM.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html","title":"Setting Up AWS Systems Manager for Hybrid Environments"}],"answers":[{"id":"59d2c9186d25dac05a2935b57bd9c708","text":"Enable AWS Systems Manager Advanced-Instances and use Systems Manager to inventory and patch the instances via the SSM agent.","correct":false},{"id":"dc3a64c07de1193f3276dfd13990fbc3","text":"Enable AWS Systems Manager Enterprise to inventory and patch instances via the SSM agent.","correct":false},{"id":"b99a0eb336123271439fdc9e65a6eace","text":"You can patch on-premises Linux VMs using AWS Systems Manager, however patching of on-premises Windows instances is not supported using AWS Systems Manager.","correct":false},{"id":"34c2b7a9390e3561fd7641103bc12575","text":"Use AWS Systems Manager Managed Instances to inventory and patch instances via the SSM agent.","correct":true}]},{"id":"114b6b8e-801b-43bf-8229-6086cb2bac9f","domain":"data-man","question":"Your team is migrating a MySQL database (version 5.6) from on-premises to AWS. It is known that the database load is very unpredictable and read-intensive. You need to select a database service that will automatically scale its compute capacity, based on the application's needs, and require the least amount of administration and management overhead. Which AWS Database service would you choose?","explanation":"Amazon Aurora Serverless is the correct answer as it can automatically start up, shut down, and scale the compute capacity and is suitable for unpredictable and read-intensive requests. Aurora Serverless also requires minimal management as AWS configure and manage the DB service and platform. MySQL on EC2 is eliminated as it doesn't meet any of the autoscaling or management requirements. Although RDS MySQL and Aurora Cluster are managed by AWS, they do not have the ability to automatically scale their compute capacity.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html","title":"Using Amazon Aurora Serverless"}],"answers":[{"id":"51e3de37586339e9534e5b4d38015b1f","text":"Amazon Aurora Serverless","correct":true},{"id":"88cafd371970b1e442de7b45206d1688","text":"MySQL in RDS","correct":false},{"id":"e503e981db6c49edcd41880be77c5369","text":"Amazon Aurora Cluster","correct":false},{"id":"58dbd4ac6f5e40d5300757915fa51606","text":"MySQL on EC2","correct":false}]},{"id":"066a63c9-1a0c-454f-8eeb-628657c4b7b3","domain":"security-comp","question":"As a security administrator for your company, the development team has asked for your advice on protecting their web product running on AWS against SQL injection attacks. Recently, there have been several cases where attackers have tried to insert certain malicious SQL queries to extract data from a database that stores confidential customer data. The development team manages and runs the database on EC2 running behind a load balancer. What advice would you give to the team to proactively protect against these kinds of attacks?","explanation":"There are several firewall services that AWS has provided including AWS WAF, AWS Shield, and AWS Firewall Manager. But in this case an ACL with WAF would be the most appropriate. AWS Firewall Manager simplifies your administrative and maintenance tasks across multiple accounts and resources for AWS WAF. AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to a load balancer. You can block/allow all requests except the ones your specify. AWS Shield Advanced would protect against DDOS attacks. AWS Config would only provide notifications and thus would be a reactive solution to attacks.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager?"}],"answers":[{"id":"3b888c9ab75dfc1fd31b6fdcec298c41","text":"Use AWS Config to monitor the application. Set a rule to notify the development team when a malicious attack occurs.","correct":false},{"id":"5cfd6d4faff387749033b7f3e493f870","text":"Create a WAF Access Control List (ACL) with a rule to explicitly block the SQL injection attacks. Attach the ACL to the load balancer.","correct":true},{"id":"c84b52c0e32a45fc862e101beb233230","text":"Activate AWS Shield Advanced. Although costly, it will protect the application with a 24/7 response team from AWS, and full system and financial restoration after an attack.","correct":false},{"id":"3e1ffae7dc069fcf4bdce431a89b4792","text":"Create a rule in AWS Firewall Manager to explicitly block the IP addresses that were listed as the attackers.","correct":false}]},{"id":"4999e9be-1508-4e8c-bf2f-c663e9e4792b","domain":"security-comp","question":"You are the security administrator for your company tasked with setting up proper IAM permissions for your company's AWS users. Your CFO has tried to create an IAM user for someone in the finance department. The CFO has asked you why she is getting an access denied message when her IAM policy has the following statement:\r {\r\n  \"Version\": \"2012-10-17\",\r\n  \"Statement\": {\r\n    \"Effect\": \"Allow\",\r\n    \"Action\": \"iam:CreateUser\",\r\n    \"Resource\": \"*\"\r\n  }\r\n} \r How would you troubleshoot this issue?","explanation":"Use permissions boundaries to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. The CFO may have a permissions boundary only allowing it to access certain AWS services, not including IAM. Editing the IAM policy to add the CreateRole and CreateGroup API calls would not matter if there is an active permissions boundary. The change in Effect to Deny would have the opposite desired outcome. The resource policy is not relevant as it would have no impact on IAM permissions.","links":[{"url":"https://docs.aws.amazon.com/en_pv/IAM/latest/UserGuide/access_policies_boundaries.html","title":"Permissions Boundaries for IAM Entities"}],"answers":[{"id":"29cfaa54d99cf8d21527cc4cbbc91dbc","text":"Edit the IAM policy to allow the CreateRole and CreateGroup API calls.","correct":false},{"id":"57770d7d7e08acce1c636dc781add121","text":"Check the resource policy of what the IAM User is attempting to access. Make sure there is no DENY actions on the resource policy.","correct":false},{"id":"77af352e9767ed879035e1f548900d8a","text":"Change the Effect:Allow to Effect:Deny.","correct":false},{"id":"8c3a9236f32602e84fc9dc7586ec3a2d","text":"Check the permissions boundary to ensure its permissions overlap with permissions in the IAM policy.","correct":true}]},{"id":"10c87ec7-567f-4119-8817-e484af01182e","domain":"high-avail","question":"Your development team has written an application to pick up and process images taken from an SQS queue. The app is growing in popularity and your manager wants to do this as cheaply as possible. What is the best way to achieve cheap and timely processing?","explanation":"The solution must respond in a timely manner to increases in workload and running during ‘off-peak’ hours is not appropriate. Using a lambda to perform the scaling is not necessary since it is supported out-of-box by AWS’ Autoscaling service. Since the application is growing in demand there isn’t a defined capacity to pre-purchase via Reserved Instances.  Therefore the best solution is to use Autoscaling to scale in and out based on queue length.","links":[{"url":"https://docs.aws.amazon.com/en_pv/autoscaling/ec2/userguide/as-using-sqs-queue.html","title":"Scaling Based on Amazon SQS"}],"answers":[{"id":"905df3db934191d38ccbd1dbec2e459a","text":"Deploy the app to EC2 and use lambda to stop and resize the instance based on the SQS queue length","correct":false},{"id":"a9f53d8ff41f6f104afe7dfd446f8cf0","text":"Deploy the app into an Autoscaling Group and scale in and out based on an SQS queue length","correct":true},{"id":"e00965d574d10c8a9d991c96eb786139","text":"Deploy the app into an Autoscaling Group and run the processing during off-peak hours","correct":false},{"id":"d3dd710c9e556214f0ae216bd74a6982","text":"Deploy the app into an Autoscaling Group and use reserved instances (RI) to pre-purchase a year’s worth of processing capacity","correct":false}]},{"id":"44323aa9-db88-4c8e-8594-5af3edab91b8","domain":"networking","question":"You're consulting for a consumer electronics company that markets its products globally. A new customer-facing application will be deployed in seven AWS Regions worldwide. Business logic will be handled by microservices deployed on EC2 instances in each region. The data layer will be hosted on Amazon Aurora in a single AWS Region. Which architecture will provide the highest performing solution for end users?","explanation":"A Route 53 latency routing policy will send requests to the destination with the lowest latency, generally resulting in the best performance. A Route 53 geolocation routing policy will probably not provide better performance than a latency routing policy. Even though targets may be physically closer, they may involve more network hops. Geolocation policies are generally used to serve localized content. Application Load Balancers work well for microservice architectures since targets can be registered as a specific port on an EC2 instance. CloudFront path patterns are for routing different file types, not for distinguishing origins in different regions.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy"},{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html","title":"What Is an Application Load Balancer?"}],"answers":[{"id":"915fca3a9a0bf0eaf51de3fe684ef9e0","text":"Configure an Amazon Route 53 record set for the application with a geolocation routing policy. Implement an ELB Application Load Balancer in front of the EC2 instances in each region","correct":false},{"id":"9a5a8c536cd4c0209770d8e9b9897abd","text":"Create an Amazon Route 53 record set for the application with a latency routing policy. Deploy an ELB Application Load Balancer in front of the EC2 instances in each region","correct":true},{"id":"562b2939dfd8d3940e82593f75d73725","text":"Implement an Amazon Route 53 record set for the application with a geolocation routing policy. Use an ELB Network Load Balancer in front of the EC2 instances in each region","correct":false},{"id":"8f8597399845c09e57ce08b6791b3ab0","text":"Deploy the EC2 instances behind an ELB Network Load Balancer in each region and set each one up as an Amazon CloudFront origin. Create path patterns to route all requests to the load balancer in the desired region","correct":false}]},{"id":"225c564a-15fc-45f8-b734-2bd99e1fbb77","domain":"dep-prov","question":"You are experiencing issues with HTTP traffic going through a Network Load Balancer you have recently set up. Access logging is enabled on the NLB and VPC Flow Logs have been configured. In this situation, which of the following monitoring tools is the most appropriate to help with troubleshooting HTTP client requests?","explanation":"You can use VPC Flow Logs to capture detailed information about the traffic going to and from your Network Load Balancer. You can use access logs to capture detailed information about TLS requests made to your load balancer, which you can use to analyze traffic patterns and to troubleshoot issues with your targets. On Network Load Balancers, access logs only capture TLS requests, not HTTP requests, so they are not appropriate in this case. Request tracing is not a valid monitoring tool for Network Load Balancers. You can use AWS CloudTrail to capture detailed information about the calls made to the Elastic Load Balancing API and store them as log files in Amazon S3. You can use these CloudTrail logs to determine which calls were made, the source IP address where the call came from, who made the call, when the call was made, and so on. CloudTrail logs are not appropriate for troubleshooting client requests.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-monitoring.html","title":"Monitor Your Network Load Balancers"}],"answers":[{"id":"6c5c81f47915de5f03d2577e8fae1c34","text":"Access logs","correct":false},{"id":"8c6ded942a243b91e65d037ab4e21f7d","text":"CloudTrail logs","correct":false},{"id":"6a292fdb897093b64ef80b39e7db0a4a","text":"Request tracing","correct":false},{"id":"8db8c2b0bbd0ff71d1d15bb32f69e3b8","text":"VPC Flow Logs","correct":true}]},{"id":"55cd4e20-346b-4ab5-920b-bf4720db7ee5","domain":"automation","question":"You are working as a SysOps Administrator for your company and are working on writing a CRON job on an application running on EC2. The CRON expression requires the instance to provide its public IP address to pass to another application running on a second EC2 instance. How would you obtain the IP address?","explanation":"Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you're writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application. User data are the parameters you specify when configuring your instance. Instance store is a type of instance and the AMI does not contain the IP address.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html","title":"Instance Metadata and User Data"}],"answers":[{"id":"6248d35cdbd86a27a1dd1517a621d1d2","text":"From the instance AMI data.","correct":false},{"id":"6544d86f167272ed59ebf831074f047f","text":"From the instance store data using the curl command.","correct":false},{"id":"45dbd3d67172bc00ea7cf86e81faf0f5","text":"From the instance metadata using the curl command.","correct":true},{"id":"263ce3ebb0453f2bf28edc6a6d3bbf05","text":"From the instance user data using the curl command.","correct":false}]}]}}}}
