{
    "data": {
        "createNewExamAttempt": {
            "attempt": {
                "id": "0565aa7d-b969-407f-a50c-25ea2c0acb53"
            },
            "exam": {
                "id": "0088e682-8c3d-4dce-8274-d20a84540347",
                "title": "AWS Certified DevOps Engineer - Professional 2019",
                "duration": 10800,
                "totalQuestions": 75,
                "questions": [
                    {
                        "id": "781e2a28-ff7a-4712-bf0c-1ae69dec243f",
                        "domain": "SDLCAutomation",
                        "question": "You are part of a development team that has decided to compile release notes directly out of a CodeCommit repository, the version control system in use. This step is to be automated as much as possible. Standard GitFlow is used as the branching model with a fortnightly production deploy at the end of a sprint and occasional hotfixes. Select the best approach.",
                        "explanation": "Following GitFlow's standard release procedures, a release branch is merged into master. That commit on master must be tagged for easy future reference to this historical version. Both release and hotfix branches are temporary branches and would require ongoing updates of the CodeCommit trigger. Feature branches are used to develop new features for the upcoming or a distant future release and might be discarded (e.g. in case of a disappointing experiment). CodeCommit does not provide a generate release notes feature.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html",
                                "title": "Manage Triggers for an AWS CodeCommit Repository"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html#codecommit_event_type",
                                "title": "CodeCommit Events"
                            },
                            {
                                "url": "https://aws.amazon.com/blogs/devops/build-serverless-aws-codecommit-workflows-using-amazon-cloudwatch-events-and-jgit/",
                                "title": "Build Serverless AWS CodeCommit Workflows using Amazon CloudWatch Events and JGit"
                            },
                            {
                                "url": "https://nvie.com/posts/a-successful-git-branching-model/",
                                "title": "A successful Git branching model"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codecommit/latest/APIReference/Welcome.html",
                                "title": "AWS CodeCommit API Reference"
                            },
                            {
                                "url": "https://forums.aws.amazon.com/thread.jspa?messageID=756611",
                                "title": "CodeCommit Lambda triggers fire off separate events for each commit?"
                            }
                        ],
                        "answers": [
                            {
                                "id": "601038d6d016fa6b4637092d2a68db75",
                                "text": "Configure a trigger by choosing the 'Delete branch or tag' repository event that invokes a Lambda function when development for a sprint is finished, i.e. the last feature-* branch has been deleted. In that Lambda, retrieve the latest git merge commit message before the deletion and append it to the release notes text file stored in an S3 bucket.",
                                "correct": false
                            },
                            {
                                "id": "625fdc89350c8e0422b2c105b66be63a",
                                "text": "Create a trigger for your CodeCommit repository using the 'Push to existing branch' event and apply that to any release and hotfix branch. Add an Amazon SNS topic as the target and have a Lambda listen to it. In that function, filter out specific commit type changes such as style, refactor and test that are not relevant for release notes. Store all other commit messages in a DynamoDB table and, at release time, run a query to collate the release notes.",
                                "correct": false
                            },
                            {
                                "id": "daf0713b7db34409528f3e191adf166e",
                                "text": "Use the 'generate release notes' feature of CodeCommit by running the 'create-release-notes' command with the --from <datetime> (use the start of the sprint) or --previousTag <tagName> option in the AWS CLI. Create a Lambda to execute this on a regular schedule (i.e. every 2 weeks) using CloudWatch Events with a cron expression.",
                                "correct": false
                            },
                            {
                                "id": "94ff89937241b8005e348ff0550a1947",
                                "text": "Setup up an Amazon CloudWatch Event rule to match CodeCommit repository events of type 'CodeCommit Repository State Change'. Look for 'referenceCreated' events with a 'tag' referenceType that are created when a production release is tagged after a merge into 'master'. In a Lambda function, use the CodeCommit API to retrieve that release commit message and store it in a static website hosting enabled S3 bucket.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "d4166e07-35b5-4196-b355-a04793003f88",
                        "domain": "HAFTDR",
                        "question": "You currently have a lot of IoT weather data being stored in a DynamoDB database. It stores temperature, humidity, wind speed, rainfall, dew point and air pressure. You would like to be able to take immediate action on some of that data. In this case, you want to trigger a new high or low temperature alert and then send a notification to an interested party. How can you achieve this in the most efficient way?",
                        "explanation": "Using a DynamoDB stream is the most efficient way to implement this. It allows you to trigger the lambda function only when a temperature record is created, thus saving Lambda from triggering when other records are created, such as humidity and wind speed.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/",
                                "title": "DynamoDB Streams Use Cases and Design Patterns"
                            }
                        ],
                        "answers": [
                            {
                                "id": "bfa4fc6ae9d9c31b4764750964fc2583",
                                "text": "Use CloudWatch custom metrics to plot your temperature readings and generate an event alert if it breaches your high and low thresholds.",
                                "correct": false
                            },
                            {
                                "id": "3cca31ba4defd9f00540ba540543c0af",
                                "text": "Modify your IoT devices to also log their data to Kinesis Data Firehose and trigger a Lambda function which will check for new high or low temperatures. Send a SNS notification.",
                                "correct": false
                            },
                            {
                                "id": "295bff0a7cd575c34ecae937a4f12f8e",
                                "text": "Use a DynamoDB stream and Lambda trigger only on a new temperature reading. Send a SNS notification if a record is breached.",
                                "correct": true
                            },
                            {
                                "id": "86d964e70883ad4a58b196f134df686c",
                                "text": "Write an application to use a DynamoDB scan and select on your Sort key to determine the maximum and minimum temperatures in the table. Compare them to the existing records and send an SNS notification if they are breached. Run the application every minute.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "6c131b00-36b0-41c4-a384-b2865d782a9f",
                        "domain": "PoliciesStandards",
                        "question": "The security officer of the company you work for has mandated that from now on all production database credentials are to be changed every 90 days. You've decided to automate this for a cluster of Db2 instances and want to use AWS Secrets Manager. This database is used by several of your applications but owned by an external party who has given you three separate DB users for different environments (DEV, TEST, PROD). You can change their passwords yourself but not create new users. Select the correct answer that describes the best way to proceed.",
                        "explanation": "Secrets Manager already natively knows how to rotate secrets for supported Amazon RDS databases. However, it also can enable you to rotate secrets for other databases or third-party services. It provides a secure API that enables the programmatic retrieval of secrets to ensure that it can't be compromised by someone examining your code and configurations stored in a version control system. Secrets Manager invokes the secrets rotation Lambda function each time with the same secretId and clientTokenRequest. Only the Step parameter changes with each call. This helps prevent you from having to store any state between steps.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html",
                                "title": "Rotating Your AWS Secrets Manager Secrets"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets-one-user-one-password.html",
                                "title": "Rotating AWS Secrets Manager Secrets for One User with a Single Password"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets-lambda-function-overview.html",
                                "title": "Overview of the Lambda Rotation Function"
                            }
                        ],
                        "answers": [
                            {
                                "id": "857599feabd6fa9df995006ef2a15739",
                                "text": "You should ask the external party for a DB user with at least two credential sets or the ability to create new users yourself. Otherwise, you might encounter client sign-on failures. The risk is because of the time lag that can occur between the change of the actual password and - when using Secrets Manager - the change in the corresponding secret that tells the client which password to use.",
                                "correct": true
                            },
                            {
                                "id": "e30a5db26a1d4130fbfdc496ea15db8f",
                                "text": "Update the apps to retrieve the DB credentials from AWS Secrets Manager. You will also need to configure Secrets Manager with a custom Lambda function that is called several times by it when rotation is triggered, each time with different parameters. It is expected to perform several tasks throughout the process of rotating a secret. The task to be performed for each request is specified by the 'Step' parameter in the request. Every step is invoked with a unique 'clientTokenRequest' parameter.",
                                "correct": false
                            },
                            {
                                "id": "0698b583ffe36fca819f464eaaffe7a9",
                                "text": "AWS Secrets Manager allows you to automatically rotate secrets for some Amazon RDS databases, Redshift and DocumentDB. Db2 is not a supported RDS database engine and therefore, you cannot use AWS Secrets Manager to rotate your secrets and should use the AWS Systems Manager Parameter Store instead.",
                                "correct": false
                            },
                            {
                                "id": "2c8938e46fe2a94adbb22a51927db17e",
                                "text": "You can implement the DB credentials change with an AWS Lambda function that changes the DB user's password and updates the application's DB connection configurations. Use CloudWatch events to trigger this every 3 months.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "9e3970ae-d3c6-4365-bd1d-ece6971ce4a0",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "Which of the following AWS services allow native encryption of data, while at rest?",
                        "explanation": "EBS, S3 and EFS all allow the user to configure encryption at rest using either the AWS Key Management Service (KMS) or, in some cases, using customer provided keys.  The exception on the list is ElastiCache for Memcached which does not offer a native encryption service, although ElastiCache for Redis does.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/ebs/faqs/",
                                "title": "Amazon EBS FAQs"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html",
                                "title": "Comparing Memcached and Redis"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
                                "title": "Protecting Data Using Server-Side Encryption"
                            },
                            {
                                "url": "https://aws.amazon.com/efs/faq/",
                                "title": "Amazon EFS FAQs"
                            }
                        ],
                        "answers": [
                            {
                                "id": "e2ab7c65b21ed8cc1c3b642b5e36429e",
                                "text": "S3",
                                "correct": true
                            },
                            {
                                "id": "1e26da0c1f6b77f6ef2ad9a3b8cf5a98",
                                "text": "ElastiCache for Memcached",
                                "correct": false
                            },
                            {
                                "id": "e9880f1e01aac341f6553ec8a6a7e622",
                                "text": "Elastic File System (EFS)",
                                "correct": true
                            },
                            {
                                "id": "4b222d59a012c5cc128037a5a473cde7",
                                "text": "Elastic Block Store (EBS)",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "9a41a543-3d2a-4090-b97a-16d86d7a7ab0",
                        "domain": "SDLCAutomation",
                        "question": "CodeBuild has been configured as the Action provider for your Integration Test Action which has been added to your CodePipelines' 'Test' stage. These tests connect to your RDS test database that is isolated on a private subnet and because executing that stage alone takes nearly 2 hours, you want to run it during the night before developers come into the London office in the morning. How might you go about this?",
                        "explanation": "Enabling Amazon VPC access in your CodeBuild project requires the ID of the VPC, subnets, and security groups. You cannot use the internet gateway instead of a NAT gateway or a NAT instance because CodeBuild does not support assigning elastic IP addresses to the network interfaces that it creates, and auto-assigning a public IP address is not supported by Amazon EC2 for any network interfaces created outside of Amazon EC2 instance launches. The ordered cron expression fields are: minutes, hours, day of month, month, day of week, year.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/about-aws/whats-new/2017/03/aws-codepipeline-adds-support-for-unit-testing/",
                                "title": "AWS CodePipeline Adds Support for Unit and Custom Integration Testing with AWS CodeBuild"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html",
                                "title": "Use CodePipeline with CodeBuild to Test Code and Run Builds"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-schedule-cli.html",
                                "title": "Create a CloudWatch Events Rule That Schedules Your Pipeline to Start (CLI)"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html",
                                "title": "Schedule Expressions for Rules"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/cli/latest/reference/codebuild/start-build.html",
                                "title": "AWS CLI Command Reference: start-build"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_StartPipelineExecution.html",
                                "title": "AWS CodePipeline: API Reference: Actions: StartPipelineExecution"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codebuild/latest/userguide/vpc-support.html",
                                "title": "Use CodeBuild with Amazon Virtual Private Cloud"
                            }
                        ],
                        "answers": [
                            {
                                "id": "680092a0588f13a4230a986961172023",
                                "text": "To allow CodeBuild access to your database, you have to specify the VPC ID and the ID of the subnet of your private RDS instance. Then create a CloudWatch events rule to schedule a CodeBuild project build using the following cron expression: 3 10 * * ? *",
                                "correct": false
                            },
                            {
                                "id": "e49c56dbee3e27e947c2cda2784975fb",
                                "text": "CodeBuild supports assigning elastic IP addresses to the network interfaces that it creates. Therefore, you need to configure your build project with the allocated EIP and VPC details of your RDS instance. Call the AWS CodePipeline 'StartPipelineExecution' API with {'stage': 'Test', 'schedule': '5 23 ? * SUN-THU *' }.",
                                "correct": false
                            },
                            {
                                "id": "7a12726d6a6e0d62c071bfa26f1daf46",
                                "text": "By default, CodeBuild projects can access VPCs in the same account. Use the AWS CLI CodeBuild command 'start-build' with the --repeat=true, --hours=3, --minutes=10 and --frequency=weekdays option.",
                                "correct": false
                            },
                            {
                                "id": "f10014d4b7fb41a95a0f4102b36a8460",
                                "text": "Amazon VPC access in your CodeBuild project needs to be enabled. Specify your VPC ID, subnets, and security groups in your build project and set up a rule in Amazon CloudWatch Events to start the pipeline on a schedule. Use the following command for that: aws events put-rule --schedule-expression 'cron(10 3 ? * MON-FRI *)' --name IntegrationTests",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "b21d657a-8e37-4f2b-9641-6e37ff00825c",
                        "domain": "SDLCAutomation",
                        "question": "Your manager wants to implement a CI/CD pipeline for your new cloud-native project using AWS services, and would like you to ensure that it is performing the best automated tests that it can. He would like fast and cheap testing, where bugs can be fixed quickly. He suggests starting with individual units of your software and wants you to test each one, ensuring they perform how they are designed to perform. What kind of tests do you suggest implementing, and what part of your CI/CD pipeline will you implement them with?",
                        "explanation": "Unit tests are built to test individual units of your software and quickly identify bugs. These can be implemented with CodeBuild.",
                        "links": [
                            {
                                "url": "https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf",
                                "title": "Practicing CI/CD on AWS"
                            }
                        ],
                        "answers": [
                            {
                                "id": "437e248b28463ad899879dda97bea983",
                                "text": "Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some unit tests for the existing code base and ensure that your developers produce component tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeCommit",
                                "correct": false
                            },
                            {
                                "id": "962f2ea0d522b0054011f4e7935fd81e",
                                "text": "Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some unit tests for the existing code base and ensure that your developers produce unit tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeBuild",
                                "correct": true
                            },
                            {
                                "id": "464e499150ad3f7270dbfb985c67bcb0",
                                "text": "Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some compliance tests for current code base and ensure that your developers produce component tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeBuild",
                                "correct": false
                            },
                            {
                                "id": "673e10c0ae89a727cede17ac866f1ae4",
                                "text": "Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build source tests for current code base and ensure that your developers produce source tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeCommit",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "e96d9cdb-1a97-4abe-9aea-0f5c2016fd0f",
                        "domain": "IncidentEventResponse",
                        "question": "You run a cat video website, and have an EC2 instance running a webserver which serves out the video files to your visitors who watch them. On the same EC2 instance, you also perform some transcoding to make the files smaller or to change their resolution based on what the website visitor is requesting. You have found that as your visitors grow, this is getting harder to scale. You would like to add more web servers and some autoscaling, as well as moving the transcoding to its own autoscaling group so it autoscales up when you have a lot of videos to convert. You are also running out of disk space fairly frequently and would like to move to a storage system that will scale too, with the least amount of effort and change in the way your entire system works. What do you do?",
                        "explanation": "This is a great case for using EFS. With minimal effort you can move your cat video website to an automatically scaling storage solution which can be used by all of your EC2 instances.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/efs/",
                                "title": "EFS"
                            }
                        ],
                        "answers": [
                            {
                                "id": "fc82534fc7c30a5bbc7dd66625b426ae",
                                "text": "Implement Storage Gateway on each EC2 instance. Use the gateway to quickly move files between instances as required.",
                                "correct": false
                            },
                            {
                                "id": "b908baa5c72de0c770b146c5103275e0",
                                "text": "Implement S3. Modify your applications to serve videos from S3 as well as downloading files from S3, transcoding them and storing them there.",
                                "correct": false
                            },
                            {
                                "id": "0369b9bbb9fc65a7ad7ed7be897b6824",
                                "text": "Implement EFS. Mount your volume to each server for serving content and transcoding.",
                                "correct": true
                            },
                            {
                                "id": "c9acb0ccafd771fdd4739d4cb5146111",
                                "text": "Implement S3 Glacier to save on storage costs. Use the S3 Glacier API to retrieve and store video files as required.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "9404d153-05aa-4c0e-aeb4-5e38aa54f0d9",
                        "domain": "MonitoringLogging",
                        "question": "Your company has built an app called InstaFaceTube. It's incredibly successful. Your user-base is growing exponentially. Your manager has decided it's time to ensure the web application is as efficient as possible, running with the best possible performance. To achieve this you will have to monitor all aspects of the application and then analyze the issues to determine the root cause of any latencies, errors or issues that might be causing your application to slow down. How do you achieve this using AWS tools?",
                        "explanation": "Implementing the AWS X-Ray SDK will assist you in achieving this goal. It will help you understand how your application and its underlying services are performing and give you the information required to analyze issues and determine the root cause of issues.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/xray/latest/devguide/xray-gettingstarted.html",
                                "title": "Getting Started with AWS X-Ray - AWS X-Ray"
                            }
                        ],
                        "answers": [
                            {
                                "id": "d68e824386322cec32abb02c7d311e66",
                                "text": "Implement the Amazon Athena SDK, send sectors to Athena, view the service tables and blocklines in the Athena console.",
                                "correct": false
                            },
                            {
                                "id": "1b1c00f2d92850ce3ef939b7be776c78",
                                "text": "Implement the AWS X-Ray SDK, send segments to X-ray, view the service graph and traces in the X-Ray console.",
                                "correct": true
                            },
                            {
                                "id": "84b0ed98a0ac96d03ac7f24dbd70d85c",
                                "text": "Implement the AWS Macie SDK, send blocks to Macie, view the service graph and blocklines in the Macie console.",
                                "correct": false
                            },
                            {
                                "id": "5dd4c06446d96cea277f4219bafed0c7",
                                "text": "Implement the Amazon QuickSight SDK, send partitions to QuickSight, view the service tables and traces in the QuickSight console.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "6313796d-1496-4bfc-b9e4-fe35e00f883b",
                        "domain": "HAFTDR",
                        "question": "Your team is excited about embarking upon their first greenfield AWS project after months of lift-and-shift migration from your old datacenter into AWS.  This will be your first true infrastructure as code project.  Your team consists of eight people who all will be making changes to infrastructure over time. You would like to use native AWS tooling for writing the infrastructure templates however you are concerned about how team member changes will actually affect the resources you have running already. You don't want to accidentally destroy important AWS resources due to a developer or engineer changing a CloudFormation property whose update property requires replacement. How can you ensure that engineers are aware of the impact of their updates before they implement them, and protect important stateful resources such as EBS volumes and RDS instances against accidental deletion?",
                        "explanation": "CloudFormation Change sets will let you submit your modified stack template, it will compare it for you and show you which stack settings and resources will change. You can then execute that change set if you are happy with the changes that will occur.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html",
                                "title": "Updating Stacks Using Change Sets"
                            }
                        ],
                        "answers": [
                            {
                                "id": "3264bb68f07bc74ca58c6288d8f1074e",
                                "text": "Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use CloudFormation Modify Set to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.",
                                "correct": false
                            },
                            {
                                "id": "a0fa6661560c21c0020855fcc08e5013",
                                "text": "Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use diff in the Linux terminal to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.",
                                "correct": false
                            },
                            {
                                "id": "08659d45b30150542b63689978cf172d",
                                "text": "Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they first create a CloudFormation Change Set. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.",
                                "correct": true
                            },
                            {
                                "id": "1f2e9869f312d47386abc4a295c498d1",
                                "text": "Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use CloudFormation Drift Check to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "d68edb14-2da6-4852-8ca7-ac8f6f9b78fd",
                        "domain": "IncidentEventResponse",
                        "question": "You have decided to install the AWS Systems Manager agent on both your on-premises servers and your EC2 servers. This means you will be able to conveniently centralize your auditing, access control and provide a consistent and secure way to remotely manage your hybrid workloads. This also results in all your servers appearing in your EC2 console, not just the servers hosted on EC2. How are you able to tell them apart in the console?",
                        "explanation": "Hybrid instances with the Systems Manager agent installed and registered to your AWS account will appear with the 'mi-' prefix in EC2.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html",
                                "title": "Setting Up AWS Systems Manager for Hybrid Environments"
                            }
                        ],
                        "answers": [
                            {
                                "id": "d48ada0514bb41e403b1c5400cbf75a7",
                                "text": "The ID of the hybrid instances are prefixed with 'mi-'. The ID of the EC2 instances are prefixed with 'i-'.",
                                "correct": true
                            },
                            {
                                "id": "86986700437afbb27d1a3deb0c996665",
                                "text": "The ID of the hybrid instances are prefixed with 'm-'. The ID of the EC2 instances are prefixed with 'i-'.",
                                "correct": false
                            },
                            {
                                "id": "a418f11f7d1488b0abb356ca2f8e070e",
                                "text": "The ID of the hybrid instances are prefixed with 'h-'. The ID of the EC2 instances are prefixed with 'i-'.",
                                "correct": false
                            },
                            {
                                "id": "62fb5905dab4e1724d15d5ed81dee2d6",
                                "text": "The ID of the hybrid instances are prefixed with 'v-'. The ID of the EC2 instances are prefixed with 'i-'.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "10530be3-3d3d-4b54-875f-066cc6789c22",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "Your company APetGuru has been developing its website and companion application for a year, and everything is currently deployed manually. You have EC2 servers and load balancers running your website front end, your backend is a database using RDS and some Lambda functions. Your manager thinks it's about time you work on some automation so time isn't wasted on managing AWS resources. He would also like the added benefit of being able to deploy your entire environment to a different region for redundancy or disaster recovery purposes. Your RTO is an hour, and RPO is two hours.  Which AWS services would you suggest using to achieve this?",
                        "explanation": "AWS CloudFormation Stack Sets will suit the requirements, using the same S3 bucket for source code, and a recent RDS snapshots will allow a RPO of one hour.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/",
                                "title": "Use CloudFormation StackSets to Provision Resources Across Multiple AWS Accounts and Regions | AWS News Blog"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html#USER_CopyDBSnapshot",
                                "title": "Copying a Snapshot - Amazon Relational Database Service"
                            }
                        ],
                        "answers": [
                            {
                                "id": "530f3a5b92e4090b35ff7569343314b5",
                                "text": "Automatically take an hourly snapshot and use a CloudWatch event trigger to run a Lambda function which copies the snapshot to your DR region.  Use AWS CodeDeploy Deployment Sets to build your infrastructure in your Production region. Parameterise the deployment set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CodeDeploy Deployment Set to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.",
                                "correct": false
                            },
                            {
                                "id": "bd4c9a245ec35f4087247b3a463f5d41",
                                "text": "Automatically run an AWS Step function to take an hourly snapshot then run a Lambda function to copies the snapshot to your DR region.  Use AWS CloudFormation Stack Sets to build your infrastructure in your Production region. Parameterise the stack set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CloudFormation StackSet to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.",
                                "correct": true
                            },
                            {
                                "id": "3d27ef7d6046ef9df655199e0943af4c",
                                "text": "Automatically take an hourly snapshot and use a EventWatch event trigger to run a Lambda function which copies the snapshot to your DR region.  Use AWS CloudFormation Stack Sets to build your infrastructure in your Production region. Parameterise the stack set to use the snapshot ID to deploy RDS resources from snapshot.  Use Systems Manager to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CloudFormation StackSet to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.",
                                "correct": false
                            },
                            {
                                "id": "3373d70ebc5df7fa36010e5b49e31e69",
                                "text": "Automatically take an hourly snapshot and use Cloud Scheduler trigger to run a Lambda function which copies the snapshot to your DR region.  Use AWS CodePipeline Regional Deploy Set to deploy your infrastructure in your Production region. Parameterise the deploy set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CodePipeline Regional Deploy Set to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.",
                                "correct": false
                            },
                            {
                                "id": "0d3bb19cd25dd0111cf3edda16072d8e",
                                "text": "Automatically take an hourly snapshot using AWS Step functions which then invokes a Lambda function to copy the snapshot to your DR region.  Use AWS CodeFormation Sets to build your infrastructure in your Production region. Parameterise the CodeFormation Set to use the snapshot ID to deploy RDS resources from snapshot.  Use user data to deploy to your web servers from S3 on launch.  In the case of an EC2 or RDS outage, manually update your public DNS to point to a maintenance static webpage hosted in S3.  Then use your CodeFormation Set to deploy to your DR region, using the most recently-copied snapshot.  Once the load balancer health checks pass, update Route53 to point to your new Elastic Load Balancer.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "69f350ad-db3a-4b83-bf8e-15ea0c0df866",
                        "domain": "PoliciesStandards",
                        "question": "Your company is preparing to become ISO 27001 certified and your manager has asked you to propose a comprehensive solution to log configuration and security changes in a separate audit account.  Specifically, the solution should ensure IAM users have MFA enabled, identify S3 buckets which aren't encrypted and enforce the addition of specific tagging.  Identify which option solves the problem.",
                        "explanation": "AWS Config is the only service that will meet all of the requirements in the question as it records configuration changes and snapshots the configuration at regular intervals set by you. Data aggregation means that AWS Config data from multiple accounts can be stored in a single account.  The following built in rules; s3-bucket-server-side-encryption-enabled and iam-user-mfa-enabled identify any S3 buckets not encrypted and any IAM accounts that do not have MFA enabled.  Tagging is also available for AWS Config resources that describe AWS Config rules.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/config/faq/",
                                "title": "AWS Config FAQs"
                            },
                            {
                                "url": "https://aws.amazon.com/cloudtrail/faqs/",
                                "title": "AWS CloudTrail FAQs"
                            }
                        ],
                        "answers": [
                            {
                                "id": "8c9a63c97e58123416200044d646218a",
                                "text": "Enable Enhance Logging in AWS Cloudwatch to track all security and configuration changes and view these using Cloudwatch Logs Insights.",
                                "correct": false
                            },
                            {
                                "id": "2f426c67cf1036d34927bdb5aac020ba",
                                "text": "Enable AWS Cloudtrail to check for MFA enabled IAM users, configure Server access logging in S3 to view the encryption status and use a CloudFormation Templates to add tagging.",
                                "correct": false
                            },
                            {
                                "id": "0e1d676582953a5a30bcd07fa6996329",
                                "text": "Enable AWS Config and AWS Cloudtrail to track changes in all resources and to identify IAM user API calls with MFA enabled.",
                                "correct": false
                            },
                            {
                                "id": "efd3ca0f78115a7a32bffa7905edd17b",
                                "text": "Enable AWS Config and create three default rules to check whether IAM users have MFA enabled, S3 buckets have server side encryption and tagging is added to resources.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "df9c422d-ca3c-47ad-a584-4d21f9ca0e05",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "Your company has been re-architecting its core applications as microservice based APIs.  Each API is a Docker image which is to be deployed into Amazon ECS.  The DevOps team have confirmed that the first Docker images are running locally, but they are having multiple issues when trying to ship into ECS Fargate.  They are receiving two errors, the first is; \"An error occurred when calling the RegisterTaskDefinition operation - Invalid 'cpu' setting for task\" and the second is; \"CannotPullContainerError - API error (404) - repository not found\".  You have been asked by the Head of Operations to resolve these issues.  Which of these options could be the cause of the errors?",
                        "explanation": "Whenever an 'invalid cpu setting' or 'invalid memory setting' error appears during Fargate configuration, you must refer to the table in the documentation to ensure that the values chosen in the Task Definition match the supported values. If you receive a 'repository not found' error, it points to an incorrect specification of the ECR Image location in the container definition, and you should specify the valid ARN or URI with the image name.  ECR images do not have to be registered in Docker Hub. All other answers mentioned will not resolve the issues in the question.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html",
                                "title": "Cannot Pull Container Image Error"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html",
                                "title": "Invalid CPU or Memory Value Specified"
                            }
                        ],
                        "answers": [
                            {
                                "id": "78021d89732fa38a0c0b1f9ae4b65e80",
                                "text": "Ensure that the Task Definition has CPU and Memory hard limits set only.  They will not operate correctly with soft limits.",
                                "correct": false
                            },
                            {
                                "id": "9af4c33241090a770a4aa778d094c53c",
                                "text": "Ensure that the ECS Cluster compatibility is set to 'Networking only' as you are utilising ECS Fargate.",
                                "correct": false
                            },
                            {
                                "id": "9adff74ec4c8c8a05fed10071e151688",
                                "text": "Ensure that the image is uploaded into the Elastic Container Registry (ECR) and that the full path of the repository and the image name is in the container definition.",
                                "correct": true
                            },
                            {
                                "id": "cb4b4ba444e685699e32cbb57cfafaff",
                                "text": "Ensure that your new image is uploaded into the Docker Hub container registry as all new images running on the Amazon ECS platform must be registered here first.",
                                "correct": false
                            },
                            {
                                "id": "7806d100b31ffa99c98719ad4c2808be",
                                "text": "Ensure that the Task Definition has a supported value of CPU specified.  These are expressed as either CPU units or vCPUs.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "abfe783d-622d-4a39-9ee5-f210c96b3a6b",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "A company has created three Docker containers which need to be deployed.  The first is a core application for the company which will need to accommodate thousands of Websocket connections every minute.  The second is the main corporate Website which is based on a Node.js application running behind an Nginx sidecar. The final container is a small departmental application written by a single developer using the React framework.  You have been asked to deploy each container on the most suitable, cost effective and reliable AWS platform from the options below.",
                        "explanation": "In order to make the decision which options are the best, we should start with the deployment that has the most constraints and that is the application using Websockets as these require many network connections and should be installed as multiple tasks across the ECS Cluster.  With this constraint dealt with, we can eliminate all options that don't include the ECS Cluster.  Putting all containers on one ECS Cluster would work technically, but wouldn't be cost effective and would mean an internal departmental application lives on the same cluster as the main core production applications.  When we also exclude installing Docker directly on EC2 as there is no redundancy, this leaves the only option as using ECS for the core application, Fargate for the Website and Elastic Beanstalk for the internal application.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/ecs/faqs/",
                                "title": "Amazon Elastic Container Service FAQs"
                            },
                            {
                                "url": "https://aws.amazon.com/elasticbeanstalk/faqs/",
                                "title": "AWS Elastic Beanstalk FAQs"
                            },
                            {
                                "url": "https://aws.amazon.com/fargate/faqs/",
                                "title": "AWS Fargate FAQs"
                            }
                        ],
                        "answers": [
                            {
                                "id": "31d397d0bdcff1d57aeca0011626b830",
                                "text": "Deploy the Websocket application on a managed ECS Cluster, the corporate Website in Fargate and install Docker on a single EC2 instance to run the departmental application.",
                                "correct": false
                            },
                            {
                                "id": "cc5e3933e49bdce4224537871a4f765b",
                                "text": "Deploy all three applications on a managed ECS Cluster.",
                                "correct": false
                            },
                            {
                                "id": "37d838ae967ac13002610a1398c9ffd2",
                                "text": "Deploy all of the Docker containers in Fargate.",
                                "correct": false
                            },
                            {
                                "id": "a09251d393d8cef1692aee1706d87dec",
                                "text": "Deploy the Websocket application on a managed ECS Cluster, the Corporate Website on Fargate and the Departmental application on Elastic Beanstalk.",
                                "correct": true
                            },
                            {
                                "id": "d8af6733a20ec3d6677a1e9d0016f47a",
                                "text": "Deploy the Websocket application and the corporate Website on a managed ECS Cluster and deploy the departmental application in Fargate.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "c3e23238-408e-4b66-86f9-c652a8649dc5",
                        "domain": "MonitoringLogging",
                        "question": "Your organization deals with petabytes of data which needs to be shared with a vendor. They require full access to your private S3 buckets to perform their development work. They have extensive AWS experience already. How will you give them access?",
                        "explanation": "The best way to accomplish this is to create a cross-account IAM role with permission to access the bucket, and grant the vendor's AWS ID permission to use the role.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/",
                                "title": "Provide Cross-Account Access to Objects In S3 Buckets"
                            }
                        ],
                        "answers": [
                            {
                                "id": "8f55be78212f11fd250d67174c0dc930",
                                "text": "Create an IAM Role",
                                "correct": false
                            },
                            {
                                "id": "c81b77222ee909b09dcad4f625bfe641",
                                "text": "Enable cross region replication",
                                "correct": false
                            },
                            {
                                "id": "38cbb5ba1eb48a6fd20b712b0433a77d",
                                "text": "Grant permission to the vendors AWS ID in the S3 bucket policy",
                                "correct": false
                            },
                            {
                                "id": "18e8d083cfe353b0d12b4ee66f546b75",
                                "text": "Create a cross-account IAM Role",
                                "correct": true
                            },
                            {
                                "id": "087a96fb587651ee51b6ca0824c7b131",
                                "text": "Grant the role permission to the bucket",
                                "correct": true
                            },
                            {
                                "id": "10b2007be424d0cd17f0473bf718ef66",
                                "text": "Edit the bucket policy to allow the vendor AWS ID read access",
                                "correct": false
                            },
                            {
                                "id": "9744a37e79095f8e3ca34aadf736f9b6",
                                "text": "Grant permission to vendor AWS ID to use the role",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "74ff9710-cfb7-46df-a980-0811c45f45f0",
                        "domain": "HAFTDR",
                        "question": "For a number of years, your company has been running its Billing system using Amazon Aurora for MySQL, using additional Read Replicas to run reports and to generate invoices.  Your Manager has said that RDS is now showing a RDS-EVENT-0045 error after one of the team made a configuration change, then left to go on holiday.  After restarting the replication, you briefly get an RDS-EVENT-0046 notification which quickly returns another RDS-EVENT-0045 notification and replication stops again.  Your Manager wants you to troubleshoot the problem as soon as possible, because no bills can be generated.  What steps could you take to resolve the issue?",
                        "explanation": "All of the answers have something to do with failing replication, but we can immediately discount anything which requires a change in the code as the question states it was a configuration change which caused the initial issue.  It can't be a database engine issue, because replication will only run on a transactional engine like InnoDB.  Deleting and recreating the read replicas can resolve an issue, but only if there are data inconsistencies and performing only that operation, means that those errors will come back.  In this case, the only thing it could be, is that your colleague has changed the size of max_allowed_packet.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html",
                                "title": "Troubleshooting for Aurora"
                            }
                        ],
                        "answers": [
                            {
                                "id": "1ed2f7210119575d1441c76431e4ab4d",
                                "text": "Delete and recreate the Read replicas using the same DB instance identifier.",
                                "correct": false
                            },
                            {
                                "id": "9ad744f1e763f7a5e0ede3939d14a3a3",
                                "text": "Switch the database engine from MyISAM to InnoDB and restart replication.",
                                "correct": false
                            },
                            {
                                "id": "73f213792c22de894e9255b4caa4c13e",
                                "text": "Ensure the max_allowed_packet parameter on a Read Replica is the same as that of the source DB.",
                                "correct": true
                            },
                            {
                                "id": "da283d585bf5dbb555cb1e9207f19fae",
                                "text": "Change and redeploy the code to stop writing to a Read Replica, then set read_only to 1.",
                                "correct": false
                            },
                            {
                                "id": "52956a328038ae53dea8c74318f72c79",
                                "text": "Remove any queries using SYSDATE() from the code, redeploy and restart replication.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "1c7dd43f-b2c7-49aa-b897-bd62f97ee183",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "A company has successfully deployed a two EC2 node Redis cluster, using CloudFormation.  In order for the cluster to work, Node one is started and populated with data before Node two is started. Some weeks later, a new Redis stack is created using the same template, but is populated by a different set of data.  In this case the stack consistantly fails to be created eventhough the template and parameters are identical to the original stack.  Which actions should be taken to ensure that the new cluster can successfully be created?",
                        "explanation": "If, as in this case, the order of resources is important, it should never be assumed that they will always stand up in the same order each time, the first stack may have deployed in the correct order by luck.  A 'DependsOn' resource should always be used to maintain order consistancy.  Also, the data that is populating Redis may be much larger and take more time in the second Stack.  Adding a Creation Policy with a large Timeout will ensure that data successfully completes the import and the EC2 instance is successfully created.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html",
                                "title": "Template Anatomy"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-creationpolicy.html",
                                "title": "CreationPolicy Attribute"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-dependson.html",
                                "title": "DependsOn Attribute"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html",
                                "title": "Working with Nested Stacks"
                            }
                        ],
                        "answers": [
                            {
                                "id": "a7e04cbb4e145efffd811f7596f2fbdf",
                                "text": "Use two CloudFormation templates, one for each EC2 Redis node and manually run one at at time.",
                                "correct": false
                            },
                            {
                                "id": "af31ddbdf176fd17fce491dc05ba1388",
                                "text": "Use a CloudFormation root template which uses nested stacks.  Define each Redis node in a separate template which are linked to by the root template.",
                                "correct": false
                            },
                            {
                                "id": "9a01df5c5a78cacac35b87d8678c7d74",
                                "text": "Use a single CloudFormation template and add a Creation Policy and Deletion Policy with a Retain option to maintain both EC2 Redis nodes.",
                                "correct": false
                            },
                            {
                                "id": "002f82cd165be0c2dd0d4bfba60bde24",
                                "text": "Use a single CloudFormation template and add a 'DependsOn' resource for the second EC2 node which points to the first EC2 node.",
                                "correct": true
                            },
                            {
                                "id": "d1b2270b9abfe6f978128e05ec79268d",
                                "text": "Use a single CloudFormation template and add a Creation Policy with a large Timeout, to the first EC2 node.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "5de9216b-07fe-47be-aed7-41a4da2b44bd",
                        "domain": "HAFTDR",
                        "question": "One of your colleagues has been asked to investigate increasing the performance of the main corporate Website, for customers in the Asia Pacific Region, using a CDN.  They have decided to use CloudFront to perform this function, but have encountered problems when configuring it.  You can see that CloudFront returns an InvalidViewerCertificate error in the console whenever they attempt to to add an Alternate Domain Name.  What can you suggest to your colleague to assist them in solving the issue?",
                        "explanation": "You must ensure that the certificate you wish to associate with your Alternate Domain Name is from a trusted CA, has a valid date and is formatted correctly.  Wildcard certificates do work with Alternate Domain Names providing they match the main domain, and they also work with valid Third Party certificates.  If all of these elements are correct, it may be that there was an internal CloudFront HTTP 500 being generated at the time of configuration, which should be transient and will resolve if you try again.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/troubleshooting-distributions.html",
                                "title": "Troubleshooting Distribution Issues"
                            }
                        ],
                        "answers": [
                            {
                                "id": "8eec3b58fb382642048a138370ae92cf",
                                "text": "Ensure the certificate is not a Wildcard certificate as these do not work with Alternate Domain Names.",
                                "correct": false
                            },
                            {
                                "id": "2d089a04108e374436b3174f6acaa8e3",
                                "text": "The certificate relating to the Alternate Domain Name was imported from a Third Party CA and this will not work.",
                                "correct": false
                            },
                            {
                                "id": "e4f79c0e45af152d4406e7e7123c83a8",
                                "text": "There was a temporary, internal issue with CloudFront which meant it couldn't validate certificates.",
                                "correct": true
                            },
                            {
                                "id": "6ef625f1455326a96638173bec379999",
                                "text": "Ensure that there is a trusted and valid certificate attached to your distribution.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "3072e673-774e-4865-a7ed-85fafa597eeb",
                        "domain": "SDLCAutomation",
                        "question": "Your companies Security Officer just mandated a policy whereby all in use encryption keys within your organisation must be rotated every 120 days. How does that affect your CodePipelines Artifact store?",
                        "explanation": "An artifact store for your pipeline, such as an Amazon S3 bucket, is not the source bucket for your source code and is required for each pipeline. When you create or edit a pipeline, you must have an artifact bucket in the pipeline Region, and then you must have one artifact bucket per AWS Region where you are running an action. Unlike the console, running the create-pipeline command in the AWS CLI does not create an Amazon S3 bucket for storing artifacts. The bucket must already exist. When you enable automatic key rotation for a customer managed CMK, AWS KMS generates new cryptographic material for the CMK every year.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create.html",
                                "title": "Create a Pipeline in CodePipeline"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codepipeline/latest/userguide/S3-artifact-encryption.html",
                                "title": "Configure Server-Side Encryption for Artifacts Stored in Amazon S3 for CodePipeline"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codepipeline/latest/userguide/S3-view-default-keys.html",
                                "title": "View Your Default Amazon S3 SSE-KMS Encryption Keys"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html",
                                "title": "Rotating Customer Master Keys"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codepipeline/latest/userguide/S3-rotate-customer-key.html",
                                "title": "Configure Server-Side Encryption for S3 Buckets When Using AWS CloudFormation or the CLI"
                            }
                        ],
                        "answers": [
                            {
                                "id": "7cb596ab386cdf5e14c65e9384a96895",
                                "text": "Your artifact store can either be a CodeCommit, GitHub or an Amazon ECR repository. It can also be an S3 source bucket where your source code is stored. Every 120 days, you need to generate new access keys and change the connection details to your repository if you are not using S3 as your artifact store. No further action is required if you use S3.",
                                "correct": false
                            },
                            {
                                "id": "75c9d50127886728d1cfde275bdb7915",
                                "text": "You must have a separate artifact store for each CodePipeline. When using the AWS CLI, these are automatically created during the creation of your pipelines. CodePipeline also configures default AWS-managed SSE-KMS encryption keys for your artifact store. If you enable automatic key rotation and specify a refresh rate of 120 days, AWS KMS generates new cryptographic material and saves the key's older cryptographic material so that it can be used to decrypt data that it encrypted.",
                                "correct": false
                            },
                            {
                                "id": "30285e906f24c643049a339b0551b048",
                                "text": "You can use any already existing artifact store as long as it is in the same Region as your pipeline. When you create a pipeline using AWS CloudFormation or the CLI, you must configure server-side encryption for your artifact store manually. Use an appropriate bucket policy and then create your own customer-managed SSE-KMS encryption keys. Instead of using the default Amazon S3 key, choose to use your own keys so that you can rotate these every 120 days as per your organisations security requirements.",
                                "correct": true
                            },
                            {
                                "id": "03be9a17c0b30ac5a6639484c188ed1c",
                                "text": "AWS recommends to use the same default global artifact store for all your CodePipelines. This is created for you when you create your first pipeline in any region. By default, CodePipeline uses server-side encryption with the AWS KMS-managed keys (SSE-KMS) using the default key for Amazon S3 (the aws/s3 key). This key is created and stored in your AWS account. When artifacts are retrieved from the artifact store, CodePipeline uses the same SSE-KMS process to decrypt the artifact. Change that key every 120 days.",
                                "correct": false
                            },
                            {
                                "id": "83305810e49f3d0e2dc10c7f564a2616",
                                "text": "CodePipeline Artifact stores are regional. When you use the CodePipeline console in a specific region to create a pipeline and choose 'Default location' when asked for an Artifact store, a new default one will be created for you in that region if none existed beforehand. CodePipeline creates default AWS-managed SSE-KMS encryption keys when you create a pipeline using the Create Pipeline wizard. The master key is encrypted along with object data and managed by AWS. However, you can also create and manage your own customer-managed SSE-KMS keys in AWS KMS to encrypt or decrypt artifacts in your artifact store and rotate these keys as necessary.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "8cd81d91-8288-4830-ae8b-0aaa2c7f706c",
                        "domain": "SDLCAutomation",
                        "question": "You want to use Jenkins as the build provider in your CI/CD Pipeline. Is this possible, and if so how would you implement it?",
                        "explanation": "You can select Jenkins an action provider when creating a build stage in CodePipeline. You cannot select it as a source provider within CodeBuild.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html",
                                "title": "Use CodePipeline with Jenkins"
                            }
                        ],
                        "answers": [
                            {
                                "id": "d27a4cc08449c797a8fd8b6c6b9d505e",
                                "text": "Yes it's possible. You can use a CodePipeline plugin for Jenkins and can configure a build stage which connects to your Jenkins instance.",
                                "correct": true
                            },
                            {
                                "id": "2c74a9018e02367af56b8cd6036ae2a9",
                                "text": "No it's not possible.",
                                "correct": false
                            },
                            {
                                "id": "80ab8f614bf8d08d188690809f0dc8fa",
                                "text": "Yes it's possible. CodePipeline will let you select Jenkins as a destination build provider when you are creating your pipeline.",
                                "correct": false
                            },
                            {
                                "id": "c5a84ac110e8df3c1da4255e362f2066",
                                "text": "Yes it's possible. CodeBuild will let you select Jenkins as a source provider when you are creating your build project",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "b1b39162-3af1-4aa5-9cd4-4e30338d07f6",
                        "domain": "HAFTDR",
                        "question": "Your CEO wants you to start future proofing your AWS environment, so he's asked you to look into IPv6 compatibility of your existing Load Balanced EC2 stack. You make use of both Application (ALB) and Network (NLB) load balancers in your EC2-VPC. What are your findings?",
                        "explanation": "At this moment in time only the Application Load Balancer supports IPv6 in the EC2-VPC environment. Classic Load balancers only support IPv6 if you are using an EC2-Classic environment.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/about-aws/whats-new/2017/01/announcing-internet-protocol-version-6-ipv6-support-for-elastic-load-balancing-in-amazon-virtual-private-cloud-vpc/",
                                "title": "Announcing Internet Protocol Version 6 (IPv6) support for Elastic Load Balancing in Amazon Virtual Private Cloud (VPC)"
                            }
                        ],
                        "answers": [
                            {
                                "id": "eda9ef0a0ac30b1db3a5bf8674687603",
                                "text": "Application and Network Load Balancers both support IPv6.",
                                "correct": false
                            },
                            {
                                "id": "2a74b0b6126f559f800f0f5cef044526",
                                "text": "No Load Balancers in EC2 support IPv6.",
                                "correct": false
                            },
                            {
                                "id": "4aff7beb08ea7fe4fa49ab96903dfc41",
                                "text": "Application Load balancers do not support IPv6, Network Load balancers do.",
                                "correct": false
                            },
                            {
                                "id": "12d596f99a6d191c9c1aaae49901a9e1",
                                "text": "Application Load balancers support IPv6, Network Load Balancers do not.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "100210fd-801f-4377-bbb1-131646b5bbf4",
                        "domain": "SDLCAutomation",
                        "question": "Your developers are currently storing their code in a private github repository, however your organization has recently introduced rules that everything must live within your AWS environment, so you need to find an alternative. What do you suggest that will both continue to work with your existing developer environments but also support a possible future transition into a CI/CD environment?",
                        "explanation": "CodeCommit is the best solution because it's compatible with CI/CD supporting services such as CodeBuild and CodePipeline",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/codecommit/",
                                "title": "AWS CodeCommit | Managed Source Control Service"
                            }
                        ],
                        "answers": [
                            {
                                "id": "909d969c995f3c405df90a78ad1dc185",
                                "text": "Move your repositories to your own CodeCommit EC2 instance from the AWS marketplace.",
                                "correct": false
                            },
                            {
                                "id": "efdd4ad61a489544e40886e3db7751db",
                                "text": "Move your repositories to S3.",
                                "correct": false
                            },
                            {
                                "id": "dfa1a6e5db21b8bd2df6f5674aadbbd7",
                                "text": "Move your repositories to your own Git server running in EC2.",
                                "correct": false
                            },
                            {
                                "id": "59bc8c49cd324cbe0767a72d0787fc19",
                                "text": "Move your repositories to CodeCommit.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "90ad13a4-87b3-4afb-9f86-6dd9f234f8ff",
                        "domain": "MonitoringLogging",
                        "question": "Your organisation would like to implement autoscaling of your servers, so you can spin up new servers during times of high demand and remove servers during the quiet times. Your application load mainly comes from memory usage, and so you have chosen that as your scaling metric. What do you have to do next?",
                        "explanation": "Memory utilization is not a default CloudWatch metric, you will have to publish a custom metric first.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/autoscaling/",
                                "title": "AWS Auto Scaling"
                            }
                        ],
                        "answers": [
                            {
                                "id": "e83f40180c161cdc0807d8b69b0b85ee",
                                "text": "Configure the Auto Scaling Group with scaling policy with steps, use the default metric type 'Average Memory Utilization'",
                                "correct": false
                            },
                            {
                                "id": "e49d82811a3d3e790284aadeeb45eccd",
                                "text": "Publish a custom memory utilization metric to CloudWatch, as there isn't one by default.",
                                "correct": true
                            },
                            {
                                "id": "19f84b354da6659ed799e7df04b00825",
                                "text": "Configure the Auto Scaling Group to use the default metric type 'Average Memory Utilization'",
                                "correct": false
                            },
                            {
                                "id": "228ffd56d2bbca049ca1bb76c788e09f",
                                "text": "Configure the Auto Scaling Group with a target tracking scaling policy, use the default metric type 'Average Memory Utilization'",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "bd2c5320-2840-4970-8e18-0bdd5132927b",
                        "domain": "SDLCAutomation",
                        "question": "You are using the AWS Serverless Application Model to build a serverless application on AWS. You've just completed the development of a new feature that you wish to roll out to production. Despite proper testing, you want to gradually shift customer traffic to the updated Lambda version in increments of 10% with 10 minutes between each increment until you are satisfied that it's working as expected. How can you achieve this?",
                        "explanation": "When a canary deployment preference is used, traffic will be shifted in two increments. Various available options specify the percentage of traffic that's shifted to the updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment. During traffic shifting, if any of the CloudWatch Alarms go to Alarm state, CodeDeploy will immediately flip the Alias back to old version and report a failure to CloudFormation.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
                                "title": "Gradual Code Deployment"
                            },
                            {
                                "url": "https://github.com/awslabs/serverless-application-model/blob/master/docs/safe_lambda_deployments.rst",
                                "title": "Safe Lambda deployments"
                            }
                        ],
                        "answers": [
                            {
                                "id": "a62b715c5e4b5a4687f710c050c9b7b9",
                                "text": "In the AWS SAM template, specify the Canary10Percent10Minutes Deployment Preference Type.",
                                "correct": false
                            },
                            {
                                "id": "2ff7334d621d3c3d77cff05fc776c090",
                                "text": "Set up a DeploymentPreference alarm that flips, after the traffic shifting has completed, the Lambda alias back to the old version if the CloudWatch Alarm goes to the Alarm state.",
                                "correct": false
                            },
                            {
                                "id": "aa6d852cdfebae63c93515f42d41ab90",
                                "text": "Configure a post-traffic hook Lambda function to run a sanity test that is invoked by CodeDeploy after traffic shifting completes.",
                                "correct": true
                            },
                            {
                                "id": "e9662a939975a537911f76c87947a375",
                                "text": "Define a DeploymentPreference of type 'Linear10PercentEvery10Minutes' in your AWS SAM template.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "dc3d128f-8cce-4541-b6e6-be1ec6cea96d",
                        "domain": "SDLCAutomation",
                        "question": "Your CI/CD pipeline generally runs well, but your manager would like a report of some CodeBuild metrics, such as how many builds were attempted, how many builds were successful and how many builds failed in an AWS account over a period of time. How would you go about gathering the data for you manager?",
                        "explanation": "These are default CloudWatch metrics that come with CodeBuild.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html",
                                "title": "Monitoring Builds with CloudWatch Metrics"
                            }
                        ],
                        "answers": [
                            {
                                "id": "6149c7ca08a16f697f2988509bb61427",
                                "text": "Implement a lambda function to poll the CodeBuild API to gather the data and store it in CloudWatch Logs. Write a metric filter to graph the data and generate your report.",
                                "correct": false
                            },
                            {
                                "id": "c5045cc818946fcb9c3f6d71744202b7",
                                "text": "Configure CodeBuild to log builds to CloudWatch Logs, and then write a metric filter which will graph the data points your manager requires.",
                                "correct": false
                            },
                            {
                                "id": "19e6e35ed627bf15d10ddc88af1ab9be",
                                "text": "Configure a CloudWatch custom metric to track the build information, and create custom graphs in the CloudWatch console.",
                                "correct": false
                            },
                            {
                                "id": "a1f83b40240e6af928c6cab6eeb010c1",
                                "text": "CloudWatch metrics will report these metrics by default. You can view them in the CloudWatch console.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "61e75027-a31b-4647-9a9c-f5cb8c771339",
                        "domain": "IncidentEventResponse",
                        "question": "Your application uses Kinesis Data Streams to process incoming streaming data.  A colleague has noticed that GetRecords.IteratorAgeMilliseconds metric increases up to 10 minutes when traffic hits its peak during the day, and then reduces again when traffic gets less during the night.  This is a problem as it means data processing is being delayed during the times when the most amount of data is being ingested. What changes could you make to reduce the time lag in the stream?",
                        "explanation": "An increased GetRecords.IteratorAgeMilliseconds metric means that either the KCL consumers cannot keep up processing the data from the Kinesis stream or there aren't enough shards in the stream.  Choosing both of these options will satisfy the needs of the question.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/kinesis/data-streams/faqs/",
                                "title": "Amazon Kinesis Data Streams FAQs"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html",
                                "title": "Troubleshooting Amazon Kinesis Data Streams Consumers"
                            }
                        ],
                        "answers": [
                            {
                                "id": "4e638cee5fe6dc1a0c8c05bbfc5d2192",
                                "text": "Rewrite code to handle all exceptions within processRecords.",
                                "correct": false
                            },
                            {
                                "id": "67a7d4be4c7d2f710e0f6a7ddc4f2086",
                                "text": "Increase the amount of KPL producers putting data onto the shards.",
                                "correct": false
                            },
                            {
                                "id": "3cef88c0f16832880d58c622e62f99f4",
                                "text": "Add more EC2 KCL consumers to allow each to process less shards per instance.",
                                "correct": true
                            },
                            {
                                "id": "a3a32610b2f82f78ed6560f077bba3d2",
                                "text": "Increase the retention time of the stream from 24 hours to 7 days.",
                                "correct": false
                            },
                            {
                                "id": "88282ae94db4da5f7717813516387595",
                                "text": "Increase parallelism by adding more shards per stream.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "97a8a78b-a6d1-4bf7-aaff-a59191881a65",
                        "domain": "IncidentEventResponse",
                        "question": "Your company runs a popular website for selling cars and its userbase is growing quickly. It's currently sitting on on-premises hardware (IIS web servers and SQL Server backend.)  Your managers would like to make the final push into the cloud. AWS has been chosen, and you need to make use of services that will scale well into the future. Your site is tracking all ad clicks that your customers purchase to sell their cars. The ad impressions must be then consumed by the internal billing system and then be pushed to an Amazon Redshift data warehouse for analysis. Which AWS services will help you get your website up and running in the cloud, and will assist with the consumption and aggregation of data once you go live?",
                        "explanation": "Amazon Kinesis Data Firehose is used to reliably load streaming data into data lakes, data stores and analytics tools like Amazon Redshift. Process the incoming data from Firehose with Kinesis Data Analytics in order to provide real-time dashboarding of website activity.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/kinesis/data-firehose/",
                                "title": "Streaming Data Firehose - Amazon Kinesis - AWS"
                            },
                            {
                                "url": "https://aws.amazon.com/solutions/real-time-web-analytics-with-kinesis/",
                                "title": "Real-Time Web Analytics with Kinesis | AWS Solutions"
                            }
                        ],
                        "answers": [
                            {
                                "id": "029dca01b35ce55b6a3218b46cb66d8f",
                                "text": "Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Aggregator to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.",
                                "correct": false
                            },
                            {
                                "id": "5a744ec834a3bed2345133430519b13d",
                                "text": "Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Streamer to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.",
                                "correct": false
                            },
                            {
                                "id": "ee0e1b18c96ba916c7d61c5f11a173b3",
                                "text": "Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.",
                                "correct": true
                            },
                            {
                                "id": "ed27547c1d649b6ee7f4c6106499701b",
                                "text": "Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Athena Data Stream to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.",
                                "correct": false
                            },
                            {
                                "id": "3e05395168d2e92f31097dc8eaadfb28",
                                "text": "Build the website to run in stateless EC2 instances which autoscale with traffic, and migrate your databases into Amazon RDS.  Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create a CloudTrail stream to push the data to Amazon Redshift warehouse for analysis.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "3e8ee2fb-8946-43b2-ba85-78ce64b74626",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "You are discussing error scenarios and possible retry strategies for your Step Functions machine with your colleague. Which of her claims are incorrect?",
                        "explanation": "Errors can arise because of state machine definition issues, task failures or because of transient issues. When a state reports an error, the default course of action for AWS Step Functions is to fail the execution entirely.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-errors.html",
                                "title": "AWS Step Functions: Amazon States Language - Errors"
                            }
                        ],
                        "answers": [
                            {
                                "id": "5673ef332c854836ed6963374170526a",
                                "text": "Any state can encounter runtime errors. Examples are when a Lambda function throws an exception or if a transient network issue exists. AWS Step Functions distinguishes these clearly from Failures such as state machine definition issues that are handled differently.",
                                "correct": true
                            },
                            {
                                "id": "8c65f4ebde68eb7bc8a58c7747c9f843",
                                "text": "A Retry field with an 'IntervalSeconds' and 'MaxAttempts' value of 3 and 'BackoffRate' value of 1.5 will make three retry attempts after waits of 3, 4.5 and 6.75 seconds.",
                                "correct": false
                            },
                            {
                                "id": "51e213b667c0c51b9814eb2430f2d081",
                                "text": "A Retrier must contain the 'ErrorEquals' field which is a non-empty array of strings that match Error Names, e.g. 'States.Timeout'. When a state reports an error, Step Functions scans through the Retriers and, when the Error Name appears in this array, it implements the retry policy described in this Retrier.",
                                "correct": false
                            },
                            {
                                "id": "c1c080d749a342bf5e9fc2509c11fe40",
                                "text": "When a state reports an error, the default course of action for AWS Step Functions is to log the error and perform a single retry after 1 second. If that doesn't succeed, AWS Step Functions will fail the execution entirely.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "80f664b2-633c-4476-ae74-6d249087e79f",
                        "domain": "IncidentEventResponse",
                        "question": "You are a day trader working from home with a software engineering background. You are considering making use of some cloud services to alert you of stock price changes so you know immediately when the price of certain stocks has risen or fallen more than 1%. You know you can ingest your data to a Kinesis Data stream, but how will you query it?",
                        "explanation": "You can query your streams directly from your application using SQL and Kinesis Data Analytics",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/kinesisanalytics/latest/dev/continuous-queries-concepts.html",
                                "title": "Continuous Queries - Amazon Kinesis Data Analytics for SQL Applications Developer Guide"
                            }
                        ],
                        "answers": [
                            {
                                "id": "04efd74e70cd57a041797aa4b35f74e7",
                                "text": "Use SQL with Kinesis Data Analytics",
                                "correct": true
                            },
                            {
                                "id": "e010acc5480838d8d1679bd03d0730ba",
                                "text": "Use JSON with Kinesis Data Analytics",
                                "correct": false
                            },
                            {
                                "id": "1069736ede5f32f059462eb2901e8b93",
                                "text": "Use YAML with Kinesis Data Analytics",
                                "correct": false
                            },
                            {
                                "id": "ce8028a6137a5e1bb29aa083a15ccd3c",
                                "text": "Use RDS with Kinesis Data Analytics",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "ee99751c-1258-4f81-adf9-31a8c57b2605",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "Your CloudFormation template is becoming quite large, so you have been tasked with reducing its size and coming up with a solution to structure it in a way that works efficiently. You have quite a few resources defined which are almost duplicates of each other, such as multiple load balancers using the same configuration. What CloudFormation features could you use to help clean up your template?",
                        "explanation": "Nested stacks should be used to reuse common template patterns.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#nested",
                                "title": "AWS CloudFormation Best Practices"
                            }
                        ],
                        "answers": [
                            {
                                "id": "b8d5d665b7d8b4abfc48daa77a64c021",
                                "text": "Use nested stacks. This will allow a dedicated templates to be defined for services that are used multiple times",
                                "correct": true
                            },
                            {
                                "id": "242cb2d9063dee2cfd0a6b0cabacb580",
                                "text": "Use goto policies. These allow you to refer to different sections of your template for reuse.",
                                "correct": false
                            },
                            {
                                "id": "86793dc47df7cca5d373cdb3f8fc8f61",
                                "text": "Use Intrinsic functions. This allows dedicated functions to be called and parameters passed to generate resources.",
                                "correct": false
                            },
                            {
                                "id": "8a32bed4f4f47b1c84105ed6173ba8c4",
                                "text": "Use custom resources. They can be called by your stack to define resources without having to reuse their code.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "0917c160-74a4-439b-818b-248197d8fd54",
                        "domain": "IncidentEventResponse",
                        "question": "Your organization has a few million text documents in S3 that are stored in a somewhat random manner, and the amount of files is always growing. The developer that initially wrote the system in use stored everything with a random file name with some attempt at security through obscurity. Now your CEO and CFO both need to be able to search the contents of these documents, and they want to be able to do so quickly at a reasonable cost. What managed AWS services can assist with implementing a solution for your CEO and CFO, and what would the setup process involve?",
                        "explanation": "CloudSearch by itself is enough to fulfill the requirements put forward here. CloudSearch is managed, scalable can very quick to configure and get online. In comparison it would take some time to set up EC2 and install ElasticSearch or any other search tool, and would be much more difficult to scale. This involves creating a search domain and configuring the index as required, and then setting up the access policies.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/cloudsearch/",
                                "title": "AWS | Amazon CloudSearch - Search Service in the Cloud"
                            }
                        ],
                        "answers": [
                            {
                                "id": "38b79d58ed1b5a50d6bbc91173d0c745",
                                "text": "Create a search index",
                                "correct": false
                            },
                            {
                                "id": "d912e8309bb58d3483307e1ea428bf8a",
                                "text": "Set up access policies",
                                "correct": true
                            },
                            {
                                "id": "a34f38b0356fb4da2ce3a29a66b4b415",
                                "text": "Configure your baseline",
                                "correct": false
                            },
                            {
                                "id": "562f9538310fcd7c846445ad9768ab7c",
                                "text": "Implement Amazon CloudSearch",
                                "correct": true
                            },
                            {
                                "id": "ab25d01b99c512f0cb03129afc920a07",
                                "text": "Implement ElasticSearch",
                                "correct": false
                            },
                            {
                                "id": "280209643a35ef346f22e051eafca06e",
                                "text": "Create a search domain",
                                "correct": true
                            },
                            {
                                "id": "e26c5d07fc30848902a99c95897441b4",
                                "text": "Set up IAM roles",
                                "correct": false
                            },
                            {
                                "id": "8f9d7ad9c2c6d20b41851ce8e3572aaa",
                                "text": "Configure your index",
                                "correct": true
                            },
                            {
                                "id": "705d488d75db0b38eb9811a38a863d57",
                                "text": "Implement MongoDB",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "1793c7ec-eff4-4f0d-8252-d1a464d5cd02",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "A financial services company runs their application portfolio on EC2 Linux instances with Auto Scaling to provide elastic capacity. They need to increase compute resources based on demand during month-end processing, and decrease them after all reporting has completed to avoid unnecessary costs. Each instance must have a secondary network interface in an isolated subnet for administration tasks in order to meet compliance requirements. How will the company ensure that new instances provisioned by Auto Scaling meet the compliance requirement?",
                        "explanation": "Auto Scaling supports adding hooks to the launching and terminating stages of the instance lifecycle. These hooks can send an SNS notification and hold the instance in a pending state waiting for a callback to the API. You can trigger a Lambda function from the SNS topic to create an ENI and attach it to the instance. The lifecycle hook will abandon the instance after a timeout period or if the Lambda function fails. Auto Scaling does not allow for specifying a secondary ENI in the launch configuration. The user data shell script option may work, but there will only be a set number of ENIs available in the pool, whereas the lifecycle hook option creates ENIs as needed. The shell script could possibly create the ENIs on demand. aws:createENI is not a valid AWS Systems Manager automation document action.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
                                "title": "What Is Amazon EC2 Auto Scaling?"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html",
                                "title": "Amazon EC2 Auto Scaling Lifecycle Hooks"
                            },
                            {
                                "url": "https://aws.amazon.com/blogs/compute/using-aws-lambda-with-auto-scaling-lifecycle-hooks/",
                                "title": "Using AWS Lambda with Auto Scaling Lifecycle Hooks"
                            }
                        ],
                        "answers": [
                            {
                                "id": "40c2708f1afdec5ad8e8dbf1e224bd2c",
                                "text": "Create a pool of Elastic Network Interfaces in the administrative subnet. Pass a shell script as user data to be run when Auto Scaling launches an instance. Have the shell script use the AWS CLI to choose an ENI from the pool in the administrative subnet and attach it to the instance.",
                                "correct": false
                            },
                            {
                                "id": "ad0e0d5cf6e1e64ab9d7089fc43559f7",
                                "text": "Include a shell script as user data to be run when Auto Scaling launches an instance. Have the shell script use the AWS CLI to call the AWS Systems Manager StartAutomationExecution API. Pass the aws:createENI action and the administrative subnet id as parameters to the API call.",
                                "correct": false
                            },
                            {
                                "id": "eba70943b29ff601c4b5f75eddbb07eb",
                                "text": "Set up the Auto Scaling launch configuration to include a second Elastic Network Interface for EC2 instances. Include the administrative subnet for the secondary ENI as part of the launch configuration. Allow Auto Scaling to create and attach the secondary ENI during the instance launch stage.",
                                "correct": false
                            },
                            {
                                "id": "7479f465403b12319be40ec429085a28",
                                "text": "Add a lifecycle hook to the Auto Scaling launch stage which publishes to an Amazon Simple Notification Service topic. Configure SNS to trigger an AWS Lambda function that creates an Elastic Network Interface in the administrative subnet. Have the Lambda function attach the ENI to the EC2 instance.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "bd630b5c-8d55-48ed-9cde-2a0d186b5350",
                        "domain": "MonitoringLogging",
                        "question": "You have a large amount of infrastructure, and monitoring has been neglected since it was provisioned. You want to monitor your AWS resources, your on-premises resources, applications and services. You would like to be able to retain your system and application logs, graph metrics and be alerted to critical events. Which AWS services and features will assist you in meeting this requirement?",
                        "explanation": "Amazon CloudWatch Metrics is a suitable service for graphing, Amazon CloudWatch Logs will allow you to log both AWS and on-premises resources, and Amazon CloudWatch Alarms will be suitable for alerts and notifications.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/cloudwatch/",
                                "title": "Amazon CloudWatch - Application and Infrastructure Monitoring"
                            }
                        ],
                        "answers": [
                            {
                                "id": "cd727ff9e153593f4a455b61933f1bca",
                                "text": "Amazon CloudWatch Metrics for graphing, Amazon CloudLog for logging, Amazon CloudWatch Alarms for alerts.",
                                "correct": false
                            },
                            {
                                "id": "2fb0f1a79c04ca46dfb050e4c7918475",
                                "text": "Amazon CloudWatch Metrics for graphing, Amazon CloudWatch Logs for logging, Amazon CloudWatch Alarms for alerts.",
                                "correct": true
                            },
                            {
                                "id": "03ee68867429d155452a986cc42efb65",
                                "text": "Amazon CloudTrail for graphing and logging, Amazon CloudWatch Alarms for alerts",
                                "correct": false
                            },
                            {
                                "id": "572ff814493d6190a6180e49f2deaa1a",
                                "text": "Amazon QuickSight for graphing, Amazon CloudWatch Logs for logging, Amazon CloudWatch Alarms for alerts.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "d1e18980-dcfc-4893-afce-cb19efacb6a9",
                        "domain": "IncidentEventResponse",
                        "question": "Your CEO has heard how an ELK stack would improve your monitoring, troubleshooting and ability to secure your AWS environment. Before letting you explain anything about it, he demands you get one up and running as soon as possible using whatever AWS services you need to use. How do you go about it?",
                        "explanation": "The Amazon Elasticsearch service will give you managed Elasticsearch, Logstash and Kibana without the requirement of installing, maintaining or scaling any of them and their associated infrastructure.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/elasticsearch-service/resources/articles/the-benefits-of-the-elk-stack/",
                                "title": "The benefits of the ELK stack without the operational overhead"
                            }
                        ],
                        "answers": [
                            {
                                "id": "c18a97d462ec5c25526d8376971d37b2",
                                "text": "Install CloudSearch, Logstash and Kibana on an EC2 instance.",
                                "correct": false
                            },
                            {
                                "id": "de19d8eddb68265815d3495f5ee053dd",
                                "text": "Use CloudSearch, CloudWatch Logs and CloudKibana managed services to create your ELK stack.",
                                "correct": false
                            },
                            {
                                "id": "04de12d4521b9dc826f59dc0d42f97f5",
                                "text": "Use the Amazon Elasticsearch Service.",
                                "correct": true
                            },
                            {
                                "id": "2c6e86f82d079bd47ee2f084bbe0fa2d",
                                "text": "Install Elasticsearch, Logstash and Kibana on an EC2 instance.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "ad3ca55a-a29e-4c0a-8204-2eb07854ddf2",
                        "domain": "IncidentEventResponse",
                        "question": "Your company's suite of web applications have just been overhauled to prevent some security issues and memory leaks that were slowing them all down significantly. It's working a lot more efficiently now, though your developers are still on the lookout for any network security issues the application might be leaving exposed. A weekly scan of ports reachable from outside the VPC would be beneficial. Which AWS service will you use to implement this with minimal additional overhead to the existing CI/CD process, and how will you configure it?",
                        "explanation": "Amazon Inspector is an automated security assessment service which will allow you to improve the security and compliance of your applications. A network configuration analysis checks for any ports reachable from outside the VPC. The agent is not required for this.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/inspector/",
                                "title": "Amazon Inspector - Amazon Web Services (AWS)"
                            }
                        ],
                        "answers": [
                            {
                                "id": "b5dabb6bbcab4c4cb776e9cfed03b42e",
                                "text": "Configure Host Assessments.",
                                "correct": false
                            },
                            {
                                "id": "85ffa705a973908e8e02e22ca035a00a",
                                "text": "Implement Amazon GuardDuty.",
                                "correct": false
                            },
                            {
                                "id": "27ec757b9636ff05cdc67b897d2e0b9e",
                                "text": "Implement Amazon X-ray.",
                                "correct": false
                            },
                            {
                                "id": "5263e79a1693336f8cffea30b57f14c4",
                                "text": "Configure Network Assessments.",
                                "correct": true
                            },
                            {
                                "id": "de2ce6aca6fc8725c0f4d32837d0371f",
                                "text": "Install agent.",
                                "correct": false
                            },
                            {
                                "id": "4d72a45ae986fba394b6ac15e0822b91",
                                "text": "Implement Amazon Macie.",
                                "correct": false
                            },
                            {
                                "id": "69ef6b28f0e5a5c3d28ccda2f3197473",
                                "text": "Implement Amazon Inspector.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "97b89b27-99f8-4cb9-88f3-9002eacb7713",
                        "domain": "MonitoringLogging",
                        "question": "You are part of a team working on an house valuation service that aggregates data via APIs from different external providers. Your Solution Architect specified that AWS Secrets Manager is to be used for the storage of all third-party API keys, client IDs and other secrets. One particular API requires two customer specific values - each of these is 1024 bytes long. Which of the following answer is correct?",
                        "explanation": "Secrets can be database credentials, passwords, third-party API keys, and even arbitrary text. The maximum length of a secret 7168 bytes.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html",
                                "title": "What Is AWS Secrets Manager?"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/reference_limits.html",
                                "title": "Limits of AWS Secrets Manager"
                            }
                        ],
                        "answers": [
                            {
                                "id": "8c641d9b7e20b71b0bc352b7ed6f68dc",
                                "text": "Secrets must not exceed 256 characters in length.",
                                "correct": false
                            },
                            {
                                "id": "147c08f6dcfd373071697491fcf3b414",
                                "text": "AWS Secrets Manager supports the management of database credentials (username and password) only. Max. length is 512 characters for each field.",
                                "correct": false
                            },
                            {
                                "id": "45f37012d1d473b2ab3bc21659f75224",
                                "text": "You can store up to 7168 bytes in each secret.",
                                "correct": true
                            },
                            {
                                "id": "42946ed8adcfa463fd89ecdf2afed049",
                                "text": "You will need to store the two values separately as two secrets and combine them programmatically.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "47fefdcc-2b9b-40bc-ab6d-9c70bad210a3",
                        "domain": "SDLCAutomation",
                        "question": "CodeBuild is used to manage your project builds. In the past, quite a few builds completed successfully but you are currently dealing with a failed one after a source code change. Which of the below are INVALID statements about AWS CodeBuild build phase transitions?",
                        "explanation": "PROVISIONING comes before DOWNLOAD_SOURCE and a failed PRE-BUILD phase transitions to the FINALIZING phase.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/codebuild/latest/userguide/view-build-details.html#view-build-details-phases",
                                "title": "View Build Details in CodeBuild: Build Phase Transitions"
                            }
                        ],
                        "answers": [
                            {
                                "id": "38d29cdacb55007ab2949624e436d1c0",
                                "text": "The UPLOAD_ARTIFACTS phase is always attempted, even if the BUILD phase fails.",
                                "correct": false
                            },
                            {
                                "id": "346f35bdae6107a46bb68a12559c154d",
                                "text": "CodeBuild transitions from a failed PRE-BUILD phase to the POST_BUILD phase.",
                                "correct": true
                            },
                            {
                                "id": "8e289066c9ed3349987194d5056b8fe5",
                                "text": "This failed build could be a result of a failed PROVISIONING phase which comes after the DOWNLOAD_SOURCE phase.",
                                "correct": true
                            },
                            {
                                "id": "5e1175d25bc1d48d660e2eebac6142eb",
                                "text": "If a command fails during the INSTALL phase, CodeBuild transitions to the FINALIZING phase and none of the commands in the pre_build, build, and post_build phases are run for that build's lifecycle.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "d330cf75-1b29-4c26-a96b-1bb878b00c5f",
                        "domain": "MonitoringLogging",
                        "question": "Your security conscious CEO has a strange request. She would like an email every time someone signs in to your organization's AWS Console. What is the simplest way to implement this?",
                        "explanation": "The AWS Console Sign-in is a valid event source in CloudWatch Events. This would be the simplest way to implement this requirement.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html",
                                "title": "What is Amazon CloudWatch Events?"
                            }
                        ],
                        "answers": [
                            {
                                "id": "6497f488d9ea9251133be660f2896779",
                                "text": "Configure a CloudWatch logs Insight query to pull login attempts from the CloudTrail log and send the results to an SNS queue your CEO is subscribed to.",
                                "correct": false
                            },
                            {
                                "id": "1b70fcf29e1f7aaadbf314e9a69a0dd9",
                                "text": "Configure CloudTrail to stream events to CloudWatch Logs and configure a Lambda function to run on a 1 minute schedule to find logins. Send the results to an SNS queue your CEO is subscribed to.",
                                "correct": false
                            },
                            {
                                "id": "0bde2cccb1e8f94a030f2f445adae3fc",
                                "text": "Configure an IAM trigger on each user and set the target to an SNS queue your CEO is subscribed to.",
                                "correct": false
                            },
                            {
                                "id": "623afa587682320a753c248c3d41f3c4",
                                "text": "Configure a CloudWatch event for the AWS Console Sign-in service and set the target to an SNS queue your CEO is subscribed to.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "67394e5d-b7ef-4850-b0da-9548411156f3",
                        "domain": "SDLCAutomation",
                        "question": "Vector Technologies Incorporated wants to implement a CI/CD environment for their online bill payment system. The application runs in a .NET environment on AWS Elastic Beanstalk, and the database is SQL Server. Code is stored in a GitHub repository external to AWS. Changes to the database schema are needed for most code updates, and are done with a script that is also stored on GitHub. The CI/CD process should be initiated every time new source code or a new schema update script is moved to the central repo. Which architecture will provide the capability to automatically retrieve, build, and deploy application components each time changes are made?",
                        "explanation": "Code Pipeline can be invoked every time there is a code change or new schema update script in the GitHub repository branch, both of which can be written to S3. CodePipeline can then trigger CodeBuild to use the MSBuild Windows image that has been stored in ECR to build the .NET application code. The resulting package is then placed in a build artifacts S3 folder, which is picked up by CodeDeploy to be implemented on Elastic Beanstalk. CodeDeploy can also invoke a PowerShell script to run the schema update executable. An ECS container is not needed to perform this build, and will incur additional costs. CodeBuild can work directly with the MSBuild container image in ECR. NAnt provides the capability to automate software build processes, but MSBuild is still needed to compile the .NET code. CodeDeploy won't be able to run the database schema update scripts without them being embedded in an executable.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/codepipeline/",
                                "title": "AWS CodePipeline"
                            },
                            {
                                "url": "https://aws.amazon.com/codebuild/",
                                "title": "AWS CodeBuild"
                            },
                            {
                                "url": "https://aws.amazon.com/codedeploy/",
                                "title": "AWS CodeDeploy"
                            },
                            {
                                "url": "https://aws.amazon.com/quickstart/architecture/dotnet-cicd-on-aws/",
                                "title": ".NET CI/CD on the AWS Cloud"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html",
                                "title": "CodeDeploy AppSpec File Reference"
                            }
                        ],
                        "answers": [
                            {
                                "id": "2b48c63fdb564d11a32c45fcc0205c3f",
                                "text": "Create an MSBuild container image with the required tools for compiling .NET applications and push it to Amazon Elastic Container Registry (ECR). Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use the MSBuild container image from ECR to compile the source code. Configure CodeBuild to then construct the .NET application. Also have CodeBuild create an executable to run the schema update script. Have CodePipeline trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy invoke a Powershell script to run the schema update executable.",
                                "correct": true
                            },
                            {
                                "id": "a69266d62fd4ec974f90c80873f07508",
                                "text": "Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use NAnt to compile the source code and construct the .NET application. Also have CodeBuild create an executable to run the schema update script. Configure CodePipeline to trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy invoke a Powershell script to run the schema update executable.",
                                "correct": false
                            },
                            {
                                "id": "0bace657c20367e9692de64ec05c7669",
                                "text": "Write an MSBuild container image with the required tools for compiling .NET applications and push it to Amazon Elastic Container Registry (ECR). Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script and write them to S3. Have CodePipeline trigger AWS CodeBuild to create an Amazon Elastic Container Service (ECS) container from the ECR image. Use the ECS container application to retrieve the latest code updates from S3 and construct the .NET application. Have CodePipeline trigger AWS CodeDeploy to implement the .NET application on Elastic Beanstalk. Have CodeDeploy run the schema update scripts against the database.",
                                "correct": false
                            },
                            {
                                "id": "3833bcf2af22e331e4ba509f55aa1236",
                                "text": "Use AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use NAnt to compile the source code and construct the .NET application. Configure CodePipeline to trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy run the schema update scripts against the database.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "7cd5551a-b9a0-4e59-bb53-d40d81dc8938",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "You have been ask to deploy a clustered application on a small number of EC2 instances.  The application must be placed across multiple Availability Zones, have high speed, low latency communication between each of the nodes, and should also minimise the chance of underlying hardware failure.  Which of the following options would provide this solution?",
                        "explanation": "Spread Placement Groups are recommended for applications that have a small number of critical instances which need to be kept separate from each other. Launching instances in a Spread Placement Group reduces the risk of simultaneous failures that might occur when instances share the same underlying hardware. Spread Placement Groups provide access to distinct hardware, and are therefore suitable for mixing instance types or launching instances over time. In this case, deploying the EC2 instances in a Spread Placement Group is the only correct option.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
                                "title": "Placement Groups"
                            }
                        ],
                        "answers": [
                            {
                                "id": "112a2330d77c300b357edda7c17dddb6",
                                "text": "Deploy the EC2 servers in a Cluster Placement Group",
                                "correct": false
                            },
                            {
                                "id": "f3d57c381ce0c53f5ff05f7a48d8ae15",
                                "text": "The application should deployed as a service in ECS",
                                "correct": false
                            },
                            {
                                "id": "72371c02b14e73370c1b01dd2523a1c1",
                                "text": "deploy the EC2 servers in a Spread Placement Group",
                                "correct": true
                            },
                            {
                                "id": "b1954190e825c200d890843b70d5cb38",
                                "text": "Create a new VPC with the tenancy type of host and deploy the instances in the VPC",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "5c90df95-736b-4b27-a7cf-095c4dc3d4c4",
                        "domain": "HAFTDR",
                        "question": "You run a load balanced, auto-scaled website in EC2. Your CEO informs you that due to an upcoming public offering, your website must not go down, even if there is a region failure. What's the best way to achieve this?",
                        "explanation": "A latency based routing policy will keep your website as fast as possible for your customers, and will act as redundancy should one of the regions go down.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
                                "title": "Choosing a Routing Policy"
                            }
                        ],
                        "answers": [
                            {
                                "id": "230b3ec4e1a08538bc506cca11e9c1a1",
                                "text": "Deploy your load balancers and auto-scaled website in two different availability zones. Create a Route53 GeoProximity Routing Record. Point the record to each of your Elastic LoadBalancers.",
                                "correct": false
                            },
                            {
                                "id": "f20603c9e3176b9a3bdd585343418443",
                                "text": "Deploy your load balancers and auto-scaled website in two different regions. Create a Route53 Latency Based Routing Record. Point the record to each of your Elastic LoadBalancers.",
                                "correct": true
                            },
                            {
                                "id": "20703f2de437222f1e63b4b81216e053",
                                "text": "Deploy your load balancers and auto-scaled website in two different availability zones. Create a Route53 weighted Routing Record. Point the record to each of your Elastic LoadBalancers.",
                                "correct": false
                            },
                            {
                                "id": "560436ce37b94afeee434582179ddc95",
                                "text": "Deploy CloudFront in front of your instances. It will cache requests even if a region goes down and your users will not notice.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "fc10726c-c2c6-4c3e-a514-ef41e8e8ba30",
                        "domain": "IncidentEventResponse",
                        "question": "You work for a company who sells various branded products via their popular Website.  Every time a customer completes a purchase, streaming data is immediately sent back to your endpoint and is placed onto a Kinesis Data Stream.  Most of the year, traffic remains static and no scaling of the Stream is necessary.  However, during the Black Friday period, scaling is required and this is accomplished by increasing the KCL instances and also re-sharding the Kinesis Stream.  This year, when sharding some of the streams, it was noticed that an extra shard was left after the operation finished, and this meant that if an even number of shards was requested, the number of open shards became odd.  You have been asked to troubleshoot the issue and find the cause and resolution from the list below.",
                        "explanation": "This is an issue which occurs from time to time when re-sharding.  The difference between the StartingHashKey and EndingHashKey values is normally large (depending on the number of shards you have in the stream).  Occasionally, the difference can be a very low value such as 1 and this causes the UpdateShardCount to end up with an extra shard.  This can usually be resolved by finding the ShardID with next adjacent Hash Key value, and merging the small shard with the larger shard.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/streams/latest/dev/introduction.html",
                                "title": "What Is Amazon Kinesis Data Streams?"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html",
                                "title": "Troubleshooting Amazon Kinesis Data Streams Consumers"
                            }
                        ],
                        "answers": [
                            {
                                "id": "0e5554f90414fbfc45af06379bc77584",
                                "text": "This occurs if a shard iterator expires immediately before you can use it.  This may be resolved by ensuring the DynamoDB storing the lease information has enough capacity to store this data, by increasing the write capacity assigned to the shard table.",
                                "correct": false
                            },
                            {
                                "id": "c5d9d335c1cea4d9ab165da89fc4240f",
                                "text": "This occurs when an unhandled exception is thrown in the KCL from the processRecords code, and a record is skipped. This is resolved by handling all exceptions within processRecords appropriately.",
                                "correct": false
                            },
                            {
                                "id": "4c33b9c64a3047658345b8abf257490c",
                                "text": "This occurs when the width of a shard is very small in size in relation to other shards in the stream. This is resolved by merging with any adjacent shard.",
                                "correct": true
                            },
                            {
                                "id": "8d38887be863e96d47c5353c44733dd0",
                                "text": "This occurs when a producer application writes to an encrypted stream without permissions on the KMS master key. This is resolved by assigning the correct permissions to an application to access a KMS key.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "dad35c13-8f81-4aac-a7b9-c2ed4a5f9335",
                        "domain": "HAFTDR",
                        "question": "With your company moving more internal services into AWS, your colleagues have started to complain about using different credentials to access different applications. Your team has started to plan to implement AWS SSO, connected to the corporate Active Directory system, but are struggling to implement a working solution.  Which of the following are not valid troubleshooting steps to confirm that SSO is enabled and working?",
                        "explanation": "The question states which are NOT valid troubleshooting steps, so we need to choose the ones which will not help us troubleshoot the issues. Firstly, you can use the User Principal Name (UPN) or the DOMAIN\\UserName format to authenticate with AD, but you can't use the UPN format if you have two-step verification and Context-aware verification enabled. Secondly, AWS Organisations and the AWS Managed Microsoft AD must be in the same account and the same region.  The answers which suggest the opposite are the ones which should be chosen.  The other answers are correct troubleshooting steps and therefore can not be chosen.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/singlesignon/latest/userguide/prereqs.html",
                                "title": "AWS SSO PreRequisites"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/singlesignon/latest/userguide/troubleshooting.html",
                                "title": "Troubleshooting AWS SSO Issues"
                            }
                        ],
                        "answers": [
                            {
                                "id": "eb8551b7f4293f316685247af6fbe823",
                                "text": "Implement AWS Organisations with 'All Features' enabled, deploy the AD Connector residing in your master account.",
                                "correct": false
                            },
                            {
                                "id": "0fa6f9fa1ae19cce6cead12014268484",
                                "text": "Ensure the number of AWS SSO permission sets are less than 500 and you have no more than 1500 AD groups.",
                                "correct": false
                            },
                            {
                                "id": "fe88b553bb4a39e8268dcb243738d505",
                                "text": "AWS SSO with Active Directory only allows authentication using the DOMAIN\\UserName format.",
                                "correct": true
                            },
                            {
                                "id": "ff24f8f9a780de8999f077e354ae2eef",
                                "text": "To allow authentication using a User Principal Name, enable two-step verification in Context-aware verification mode.",
                                "correct": true
                            },
                            {
                                "id": "3627bfa9871db5cdd41006b26ec5dbbe",
                                "text": "Implement AWS Organisations and deploy AWS Managed Microsoft AD in two separate accounts.  It does not matter which regions they are deployed in.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "e742c07d-c719-4c53-9615-292147e77480",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "In the Amazon States Language, InputPath, Parameters, ResultPath, and OutputPath filter and control the flow of JSON from state to state. Which of the following definitions are correct?",
                        "explanation": "The definition of OutputPath and ResultPath are swapped around, i.e. an OutputPath can filter the JSON output to further limit the information that is passed to the output while the ResultPath selects what combination of the state input and the task result to pass to the output",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html",
                                "title": "Input and Output Processing in Step Functions"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/step-functions/latest/dg/input-output-inputpath-params.html",
                                "title": "InputPath and Parameters"
                            }
                        ],
                        "answers": [
                            {
                                "id": "cfd8d208af0b74c2e7d0fdc920690032",
                                "text": "Parameters enable you to pass a collection of key-value pairs, where the values are either static values that you define in your state machine definition, or that are selected from the input using a path.",
                                "correct": true
                            },
                            {
                                "id": "9d857343a25dab2067293a92f66a0fbf",
                                "text": "InputPath selects which parts of the JSON input to pass to the task of the Task state.",
                                "correct": true
                            },
                            {
                                "id": "1063b9865beb745109392a1abe5501c8",
                                "text": "ResultPath can filter the JSON output to further limit the information that is passed to the output.",
                                "correct": false
                            },
                            {
                                "id": "84df9b15c00389ac9ecd3ba94d81a165",
                                "text": "OutputPath selects what combination of the state input and the task result to pass to the output.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "dae6c09f-c2e7-4b01-b0ec-95ed07a7bab2",
                        "domain": "HAFTDR",
                        "question": "Due to some recent performance issues, you have been asked to move your existing Product Information System to Amazon Aurora.  The database uses the InnoDB storage engine, with new products being added regularly throughout the day. As the database is read heavy, the decision has also been made to add a Read Replica during the migration process.  The changeover completes successfully, but after a few hours you notice that lag starts to appear between the Read/Write Master and the Read Replica.  What actions could you carry out to reduce this lag?",
                        "explanation": "One of the most obvious causes of Replication lag between two Aurora databases is because of settings and values, so making the storage size comparable between the source DB and Read Replica is a good start to resolving the issue, as is ensuring compatible DB Parameter settings, such as with the max_allowed_packet parameter.  Turning off the Query Cache is good for tables that are modified often which causes lag, because the cache is locked and refreshed often. No other options are correct.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html",
                                "title": "Troubleshooting for Aurora"
                            }
                        ],
                        "answers": [
                            {
                                "id": "94692ead9b9079bf92302461566b94a5",
                                "text": "Add additional replicas and the alter the code to initiate Read/Write splitting of the Database.",
                                "correct": false
                            },
                            {
                                "id": "5b3b3ef613f42a00bbda6ed80b4f3683",
                                "text": "Change the Read Replica instance class to have the same storage size as the source instance.",
                                "correct": true
                            },
                            {
                                "id": "82a38022b7a8f6e84ec39753340df001",
                                "text": "Set query_cache_type=0 in the DB Parameter Group, to disable the query cache.",
                                "correct": true
                            },
                            {
                                "id": "09fee716d530e8dbb655cb9541bb4d5c",
                                "text": "Unlike Amazon RDS for MySQL, Amazon Aurora does not exhibit replication lag.",
                                "correct": false
                            },
                            {
                                "id": "05acabf0a4eea6213f78770124a81bcd",
                                "text": "Ensure the max_allowed_packet parameter value for the Read Replica is the same as the source DB instance.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "cae0814f-fa70-443b-9c43-1cca04205b54",
                        "domain": "SDLCAutomation",
                        "question": "Your senior developer wants to be able to access any past version of the binaries that are being built as part of your CI/CD pipeline. You are using CodeBuild with CodePipeline to automate your build process. How will you achieve this?",
                        "explanation": "CodeBuild has an optional 'Namespace type', which will insert the build ID into the path to the build output zip file or folder, giving you a unique directory and binary artifact for each build that runs.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/codebuild/latest/userguide/create-project.html",
                                "title": "Create a Build Project in CodeBuild"
                            }
                        ],
                        "answers": [
                            {
                                "id": "baa367f7cda7f088cdd1fbab6a72d39b",
                                "text": "Change the artifacts packaging to zip, which will append a version number to each build.",
                                "correct": false
                            },
                            {
                                "id": "8c3218ccd025dc9444babadf01808d86",
                                "text": "Nothing needs to change, by default all artifacts are given a unique filename in S3.",
                                "correct": false
                            },
                            {
                                "id": "c52ea2b58f7db8902570649989645ace",
                                "text": "Implement a CodeBuild lambda trigger which will copy each build artifact to S3 with a unique ID.",
                                "correct": false
                            },
                            {
                                "id": "4eed75febec27e9d78dc57a1fbbc70f8",
                                "text": "Change the artifact namespace type to Build ID, which will insert the build ID into path of the output folder in S3.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "f6f2e5d0-a72e-4d44-aaea-2e2e8fa264c3",
                        "domain": "HAFTDR",
                        "question": "Your company utilizes EC2 and various other AWS services for its workloads. As the DevOps engineer it is your responsibility to ensure all company policies are implemented. You have noticed that while you are using S3 for data archival and backups, your company policy is that your backups need to reside on company owned servers, such as those you run in your local Equinix data center. You also have another company policy that backup and archival cannot traverse the internet. What do you do?",
                        "explanation": "AWS Direct Connect is the only way to access your AWS resources from a Data Center without traversing the internet, despite the encryption offered by the other solutions.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/directconnect/",
                                "title": "AWS Direct Connect"
                            }
                        ],
                        "answers": [
                            {
                                "id": "008894eae706c0e60a40a59c79342658",
                                "text": "Implement an AWS Client VPN with a company owned server in your data center. Push backups to your data center backup NAS through SSH via vpn.",
                                "correct": false
                            },
                            {
                                "id": "f42f4a5866921fe7f0288f667c043aac",
                                "text": "Provision an AWS Direct Connect connection to your local router in your data center and your local VPC. Push backups via the Direct Connect connection.",
                                "correct": true
                            },
                            {
                                "id": "b51577fa11c831eba7efec0c73952602",
                                "text": "Implement a Site to site VPN with a company owned server in your data center. Push backups to your data center backup NAS through SSH via vpn.",
                                "correct": false
                            },
                            {
                                "id": "32f6b2f1cc26546abf7ce835ae778156",
                                "text": "Install the EFS agent on your data center backup NAS, mount the volume on an EC2 server and copy the backups to the volume.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "927aeb0d-23f7-48f5-9ca7-aa76c6403385",
                        "domain": "SDLCAutomation",
                        "question": "You are contracting for APetGuru, an online pet food store. Their website works fine, but you really want to update the look and feel to be more modern. They have given you strict guidelines that their website can not be offline at all or they will lose sales. You are thinking of using a rolling deployment so some of their servers are always online during the rollout. Just before you trigger the roll out, you receive a phone call asking if you can only install your updates on a few servers first for testing purposes. It is suggested that a few customers can be redirected to the updated site and everyone else can still use the old site until you confirm the new one is working properly. Once you're happy with the operation of the updated website, you can complete the rollout to all servers. What do you do?",
                        "explanation": "A canary deployment will allow you to complete the roll out in the way you want to.",
                        "links": [
                            {
                                "url": "https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf",
                                "title": "Practicing continuous integration / continuous delivery on AWS"
                            }
                        ],
                        "answers": [
                            {
                                "id": "a93e067980f0010b2f812e5f3eef9b9f",
                                "text": "Use a Blue/Green deployment. This allows you to install the new website to the blue servers, and once you're happy with it working you can finalise the install on the green servers. If there's an issue you can roll back the blue installation.",
                                "correct": false
                            },
                            {
                                "id": "562add4453cef6369a9860f6a26dbe36",
                                "text": "Use a Batch Deployment. This allows you to install your new website to the small batch which traffic will be directed to, and will allow you to roll back the small batch if there's an error. Otherwise, you can complete the rollout on the large batch.",
                                "correct": false
                            },
                            {
                                "id": "807b35728975296049d217f0dfbb3bc8",
                                "text": "Use an In-Place Deployment. You can deploy to the \"In-Place\" servers first for testing, then once testing is verified you can continue the deployment to the \"Out-Place\" externally facing servers.",
                                "correct": false
                            },
                            {
                                "id": "864cd30cc708bcfb524df58f775ae226",
                                "text": "Use a Canary Deployment. This allows deployment to a few servers where you can observe how the website is running while still receiving a small amount of customers. If there's an error it can be rolled back. Otherwise, the rollout will continue on new servers.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "bbe1fc54-7ed1-4156-930f-446e13ca5e59",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "One of your colleagues has developed a CloudFormation template which creates a VPC and a subnet for each Availability Zone within each Region the Stack is created in.  Although this template is currently functional, it requires the Region and Availability Zones to be passed into the Stack as parameters.  You have been asked by your manager to alter the template so that you can automate all of this functionality using Intrinsic Functions, and eliminate passing in parameters.  Choose options from the list which will meet the requirements of rewriting the template.",
                        "explanation": "Intrinsic functions can not be used within the Parameters section and so these options can be immediately ruled out. Using both Fn::GetAZs to return the Availability Zones and Fn:Select to choose from the list and also using Fn::FindInMap to pull back each Region from the Mappings section are both valid options to retrieve the Regions.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html",
                                "title": "Template Anatomy"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html",
                                "title": "Intrinsic Function Reference"
                            }
                        ],
                        "answers": [
                            {
                                "id": "0f58a828daf6079e3560070d348c8d13",
                                "text": "In the Parameters section, create three individual parameters, one for each Availability Zone and use !Ref to retrieve the values in the Resources section.",
                                "correct": false
                            },
                            {
                                "id": "9fe027bb7b742b6639a50bc38d88cd6f",
                                "text": "In the Mappings section, make a list of each Availability Zone in each Region.  Then in the Resources section use Fn::FindInMap to pull back each AZ using !Ref \"AWS::Region\" as the key.",
                                "correct": true
                            },
                            {
                                "id": "c697110279709e26aa4a1219dcf5f6e2",
                                "text": "In the Resources section, use !Ref \"AWS::Region\" to return the Region, Fn::GetAZs to return a list of all Availability Zones and Fn:Select to choose each AZ from the list.",
                                "correct": true
                            },
                            {
                                "id": "3eca8a7377d618c7e76e66d97dddfd42",
                                "text": "In the Parameters section, use !Ref \"AWS::Region\" to return the Region for each parameter, Fn::GetAZs to return a list of all Availability Zones and Fn:Select to choose each AZ from the list.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "6e3db01a-b168-46b2-87a2-8ee861d18334",
                        "domain": "SDLCAutomation",
                        "question": "You have set up a new AWS CodeCommit repository for your company's development team. All but one member can connect to it from an SSH command line terminal on their local machines and you are now troubleshooting the problem for that user. What are some of the possible reasons for this not working and how can you resolve the issue?",
                        "explanation": "The simplest and easiest connection method to use with CodeCommit is HTTPS connections and Git credentials. It is supported by most IDEs and development tools. However, the SSH protocol is also supported. If you want to use SSH connections for your repository, you can connect to AWS CodeCommit without installing the AWS CLI.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html",
                                "title": "Setup for HTTPS Users Using Git Credentials"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-https-unixes.html",
                                "title": "Setup Steps for HTTPS Connections to AWS CodeCommit Repositories on Linux, macOS, or Unix with the AWS CLI Credential Helper"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-without-cli.html",
                                "title": "Setup for SSH Users Not Using the AWS CLI"
                            }
                        ],
                        "answers": [
                            {
                                "id": "136c49d59bc5936c51a8afe887d7fada",
                                "text": "The user might have previously configured his local computer to use the credential helper for CodeCommit. In this case, edit the .gitconfig file to remove the credential helper information from the file before using Git credentials. On macOS, you might need to clear cached credentials from Keychain Access.",
                                "correct": true
                            },
                            {
                                "id": "a607ed26280fc1c3e5fe316de5339c82",
                                "text": "Because CodeCommit requires AWS Key Management Service, a policy might be attached to the already existing IAM user that expressly denies the KMS actions required by CodeCommit. If required, update the users' policies and ensure that all required permissions are granted. For a new IAM user, attach the AWSCodeCommitFullAccess or another managed policy for CodeCommit access.",
                                "correct": true
                            },
                            {
                                "id": "6ebf3ce5aed5449aa5c0fc11fbd81b2b",
                                "text": "Instead of using the SSH protocol, the user tried unsuccessfully another connection method. With SSH connections, you create public and private key files on the users' local machine that Git and CodeCommit use for SSH authentication. You simply associate the public key with the IAM user and store the private key on the users' local machine. Besides the use of the CodeCommit console, this is currently the only supported connection method to use with CodeCommit.",
                                "correct": false
                            },
                            {
                                "id": "c87085b06d5895f92cf3cc838ee49464",
                                "text": "The user didn't install the AWS CLI. You cannot connect to AWS CodeCommit without installing the AWS CLI when using SSH connections for your repository. You must install the AWS CLI first, then associate the public key in OpenSSH format with the IAM user and finally add CodeCommit to the SSH configuration.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "6a6f97fb-45c8-4509-91c7-4507629d60fa",
                        "domain": "HAFTDR",
                        "question": "Your current workload with DynamoDB is extremely latency bound, and you need it to be as fast as possible. You do not have time to look at other AWS services but instead have been instructed to use features and configuration changes of the services you are currently using. What do you do?",
                        "explanation": "Implementing a DAX cluster is the ideal solution here. It meets the requirement of using the existing DynamoDB service feature, while having the ability to reduce latency from milliseconds to microseconds. DynamoDB Scan times can also be optimised by reducing the number of attributes in your table and grouping attributes as JSON blobs within a single attribute.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/dynamodb/dax/",
                                "title": "Amazon DynamoDB Accelerator (DAX)  Fully managed in-memory cache for DynamoDB"
                            },
                            {
                                "url": "https://aws.amazon.com/blogs/database/optimizing-amazon-dynamodb-scan-latency-through-schema-design/",
                                "title": "Optimize Amazon DynamoDB scan latency through schema design | AWS Database Blog"
                            }
                        ],
                        "answers": [
                            {
                                "id": "357461d5fd66eb609146fe0e65854901",
                                "text": "Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement Elasticache Redis for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS Insights to make available Insights tracing of their DynamoDB calls.",
                                "correct": false
                            },
                            {
                                "id": "4e7fca450cdf45d6ded7827b5ef88c16",
                                "text": "Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement a DAX cluster for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.",
                                "correct": true
                            },
                            {
                                "id": "1c8d38560c0975ffecdfff42f01c5965",
                                "text": "Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement a DAX cluster for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS Insights to make available Insights tracing of their DynamoDB calls.",
                                "correct": false
                            },
                            {
                                "id": "3d69fc095075fdb4cc8e97f241e0fb76",
                                "text": "Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement Elasticache Memcached for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.",
                                "correct": false
                            },
                            {
                                "id": "b5002075c11491df649c4c4f6e960472",
                                "text": "Minimise the number of separate attributes in your table by grouping for example into JSON blobs to optimise scan times.  Implement DynamoDB Cached for caching of common queries and to reduce latency on common queries.  Consider replacing certain sequential slow scan operations with parallel scans of separate segments of the table, and suggest the development team use the AWS SDK to make available X-Ray tracing of their DynamoDB calls.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "4b8b72cd-5529-4633-8218-880a0767b77e",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "International Megawidgits Inc. is a manufacturing company which currently uses a DynamoDB table to store information relating to the status of a process. The Operations Team have been asked to design a solution so that whenever a message appears in the DynamoDB table, it informs another process that this has happened.  As this process only polls an SQS queue, they have decided to transfer the DynamoDB files to SQS using an AWS Step Function.  The team would like you to identify some Best Practices when designing the Step Functions, so that they don't run into any problems in the future.  Choose which of the following are recognised Step Functions Best Practices.",
                        "explanation": "All answers are Step Functions Best Practices, apart from \"Ignoring Lambda Service Exceptions\" and \"Avoiding Defining Timeouts in State Machine Definitions\".  To avoid latency, separate polling threads and open at least 100 polls per activity ARN.  Step functions have a hard 25,000 execution limit so ongoing work should be split across multiple workflow executions to prevent this. Passing large payloads (over 32KB) between states may terminate the execution so always store the data in an S3 bucket and pass the ARN instead of the raw data.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/step-functions/latest/dg/sfn-best-practices.html",
                                "title": "Best Practices for Step Functions"
                            }
                        ],
                        "answers": [
                            {
                                "id": "8ad70e80543575b65e907fac64b130f7",
                                "text": "Use ARNs Instead of Passing Large Payloads",
                                "correct": true
                            },
                            {
                                "id": "bada4850863c4589dd962d5f23c1a9a1",
                                "text": "Avoid Latency When Polling for Activity Tasks",
                                "correct": true
                            },
                            {
                                "id": "94f823ddcf628a72b41f43bec7a134ac",
                                "text": "Ignore Lambda Service Exceptions",
                                "correct": false
                            },
                            {
                                "id": "d4cbd984c27d737a4235edd5b4ef2801",
                                "text": "Avoid Reaching the History Limit",
                                "correct": true
                            },
                            {
                                "id": "261651564821633c3ea93f09b194daff",
                                "text": "Avoid defining Timeouts in state machine definitions",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "fddf85cb-72e6-4fa1-9c80-07536a84a6e8",
                        "domain": "HAFTDR",
                        "question": "You have a new website design you would like to test with a small subset of your users. If the test is successful, you would like to increase the amount of users accessing the new site to half your users. If that's successful and your infrastructure is able to scale up correctly, you would like to completely roll over to the new design and then decommission the servers hosting the old design. Which of these methods do you choose?",
                        "explanation": "A weighted routing policy combined with an auto-scaling group will meet your requirements and will continue to scale if your tests are successful and you completely roll over to the new design.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted",
                                "title": "Weighted Routing"
                            }
                        ],
                        "answers": [
                            {
                                "id": "1a147be1638db0d199c9779bb5488bd7",
                                "text": "Install the new website design in a new AutoScaling group. Use a Weighted Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 20%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.",
                                "correct": false
                            },
                            {
                                "id": "ff9caa8ea679855cc9425d3e553bf303",
                                "text": "Install the new website design in a new AutoScaling group. Use an A/B Test Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 20%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.",
                                "correct": false
                            },
                            {
                                "id": "8d8f29ecca384ae73a4006c686df91b8",
                                "text": "Install the new website design in a new AutoScaling group. Create a Lambda function to modify your Route53 apex record to use the new AutoScaling group for 5% of the day. If that's successful then modify the function to change the apex record for half the day, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.",
                                "correct": false
                            },
                            {
                                "id": "07123797fd0770bb8b71a6b339d36bfc",
                                "text": "Install the new website design in a new AutoScaling group. Use a Weighted Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 50%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "547f6880-c168-4fa5-91d6-6822a772af6b",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "Your team has been working with CloudFormation for a while and have become quite proficient at deploying and updating your stack and their associated resources. However one morning you notice that your RDS MySQL database is completely empty. You track this down to a simple port change request that came through, asking if the default MySQL port could be changed for security reasons. What do you suspect happened?",
                        "explanation": "The database was replaced due to the AWS:RDS:DBInstance Port attribute update requirement of 'Replacement', the database will have to be restored from backups.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html",
                                "title": "Update Behaviors of Stack Resources"
                            }
                        ],
                        "answers": [
                            {
                                "id": "2cb85ef53a3961bcf3c2798f04263e7e",
                                "text": "The 'Update' attribute for AWS:RDS has the update requirement of \"Replacement\".",
                                "correct": false
                            },
                            {
                                "id": "0974d47e810ed46ba7723376856be841",
                                "text": "The developer changed the MySQL port to a port that was already in use in your VPC so it can no longer be connected to.",
                                "correct": false
                            },
                            {
                                "id": "aab0bf043065432156b116b7beed04c2",
                                "text": "The 'Port' attribute for AWS:RDS:DBInstance has the update requirement of \"Replacement\".",
                                "correct": true
                            },
                            {
                                "id": "f3e35605b25b631ab990cfcba037091c",
                                "text": "The CloudFormation stack was updated with the Replacement stack value set to true.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "19cc0253-1dad-4ed2-bc2a-95515248a9ac",
                        "domain": "SDLCAutomation",
                        "question": "You are developing a completely serverless application and store your code in a git repository. Your CEO has instructed you that under no circumstances are you allowed to spin up an EC2 instance. In fact, he's blocked access to ec2:* company-wide with an IAM policy. He does however still want you to completely automate your development process, you just can't use servers to do so. Which AWS services will you make use of to meet these requirements?",
                        "explanation": "CodeDeploy can deploy a serverless lambda function straight from your CodeCommit repository.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html",
                                "title": "What Is CodeDeploy?"
                            }
                        ],
                        "answers": [
                            {
                                "id": "d385958d5064373b0d639a8c16f6ab09",
                                "text": "Use AWS Lambda for your compute functions, use CodeDeploy to deploy the functions for you. Store your code in GitCommit and use a CodePipeline to automatically deploy the functions when you commit your code to the repository.",
                                "correct": false
                            },
                            {
                                "id": "93bfcd1da129b57627d8753c0b2d65ef",
                                "text": "Use AWS Lambda for your compute functions, use CodeDeploy to deploy the functions for you. Store your code in CodeCommit and use a CodePipeline to automatically deploy the functions when you commit your code to the repository.",
                                "correct": true
                            },
                            {
                                "id": "fbc7f2f64b4e3a232325800045b1725f",
                                "text": "Move your compute functions into S3, and use CodePipeline to deploy your code from S3 into AWS Lambda.",
                                "correct": false
                            },
                            {
                                "id": "e3ce2b50bb87a7fe18adff65e0320c08",
                                "text": "Use AWS Lambda for your compute functions, edit them directly in the Cloud9 console in the browser. Once installed they don't need to redeployed.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "8c127356-141c-4199-bfd4-f1d8e0a8560d",
                        "domain": "IncidentEventResponse",
                        "question": "You have taken over a half finished project to implement Kinesis Data Streams.  Data is being successfully and efficiently placed onto the streams using KPL based code, but when running the KCL code to read off the shards, you are experiencing issues.  The main issues relate to slower than expected reading from the shards, but you are also seeing records being skipped.  Which options allow you to resolve both of these problems?",
                        "explanation": "Many of the above solutions answer Kinesis related questions, but we specifically need to know what could cause a slow down in reading from a shard, and why records would be skipped.  For this, there are only two correct answers.  For slow reading, it could be due to the maxRecords value being set too low or the code logic, which is calling processRecords, being inefficient and causing high CPU usage or blocking.  However, there is only one correct answer for the skipping of records and that is usually due to processRecords calls having un handled exceptions.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html",
                                "title": "Troubleshooting Amazon Kinesis Data Streams Consumers"
                            }
                        ],
                        "answers": [
                            {
                                "id": "61bd51378999bcd6a2d6551c0cc12100",
                                "text": "Check the logic of the code to ensure the processRecords call isn't CPU intensive or I/O blocking.  Also, check that the processRecords call is not throwing an unhandled exception.",
                                "correct": true
                            },
                            {
                                "id": "abd17cf52b5933e670c26b2caed0264c",
                                "text": "Choose a failover time that is less than 10 seconds, to ensure reading from a shard maintains a 1 to 1 relationship.  Also, make sure the consumers do not exceed the read per-shard limit.",
                                "correct": false
                            },
                            {
                                "id": "989df0ef5880a136eb3db849ff45dc66",
                                "text": "Ensure the maxRecords value for the GetRecords call isn't set below the default setting.  Also, check that the processRecords call is not throwing an unhandled exception.",
                                "correct": true
                            },
                            {
                                "id": "b579dde17faaedc85ca4956edf187830",
                                "text": "Ensure the maxRecords value for the GetRecords call isn't set below the default setting.  Also, make sure you aren't seeing a sudden large increase in the GetRecords.IteratorAgeMilliseconds metric.",
                                "correct": false
                            },
                            {
                                "id": "c6314cbc442d2cd7efa28180d7a5abcf",
                                "text": "Ensure you have made a GetRecords call at least every 5 minutes.  Also, check that the processRecords call is not throwing an unhandled exception.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "4c02fcad-e6c2-438b-927c-a343af2e4e89",
                        "domain": "SDLCAutomation",
                        "question": "About a dozen people collaborate on a company-internal side project using CodeCommit. The developer community is spread across multiple timezones and relies on repository notifications via email. Initially, it was configured for all event types but this resulted in a lot of emails and team members complained about too much 'noise'. Since then, commit comment notifications have been turned off. How can you improve this situation?",
                        "explanation": "You can create up to 10 triggers for each CodeCommit repository. The customData field is of type string and is used for information that you want included in the Lambda function such as the IRC channel used by developers to discuss development in the repository. It cannot be used to pass any dynamic parameters. This string is appended as an attribute to the CodeCommit JSON returned in response to the trigger. You can comment on an overall commit, a file in a commit, or a specific line or change in a file. For best results, use commenting when you are signed in as an IAM user. The commenting functionality is not optimized for users who sign in with root account credentials, federated access, or temporary credentials. By default, an Amazon SNS topic subscriber receives every message published to the topic. To receive a subset of the messages, a subscriber must assign a filter policy to the topic subscription",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html",
                                "title": "Manage Triggers for an AWS CodeCommit Repository"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda-cc.html",
                                "title": "Example: Create a Trigger in AWS CodeCommit for an Existing AWS Lambda Function"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-commit-comment.html",
                                "title": "Comment on a Commit in AWS CodeCommit"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html",
                                "title": "Amazon SNS Message Filtering"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-repository-email.html",
                                "title": "Configuring Notifications for Events in an AWS CodeCommit Repository"
                            }
                        ],
                        "answers": [
                            {
                                "id": "a74a895989f0aff5ae55553849a46032",
                                "text": "For each team member, create an individual SNS topic and a CodeCommit trigger that uses a Lambda function to filter out notifications not authored by that developer. The remaining ones are send to that SNS topic. This allows users to selectivly subscribe to specific persons to follow their activities.",
                                "correct": false
                            },
                            {
                                "id": "421e56d9693bf5c7a95f304f8be452c8",
                                "text": "Create a CodeCommit trigger that invokes a Lambda function when a new comment is added to a commit. Configure it so that its 'Custom data' field is populated with the email address of the user who authored the original commit, i.e. ${commit.author.email}. In the function, use SNS to send the notification to that address.",
                                "correct": false
                            },
                            {
                                "id": "bb2ee7c9a6b15ed2c648ce414e7f5501",
                                "text": "Ask all team members to sign in to CodeCommit as IAM users.",
                                "correct": true
                            },
                            {
                                "id": "a00a4fd86554d04bbf089d9b1f302ca3",
                                "text": "Assign Amazon SNS subscription filter policies to the 'commit comment' topic subscriptions so that team members receive only a subset of the messages.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "4cfee0f4-02a3-43bc-af10-a34d888a0086",
                        "domain": "HAFTDR",
                        "question": "The Marketing Team of Mega-Widgets Inc. have recently released an advertising campaign in the APAC region, in addition to their home market of Europe.  The CTO has told you that they have started receiving complaints from Australian customers that the site located in eu-west-1 is slow, and they think by utilising the existing Disaster Recovery infrastructure in ap-southeast-2, they can speed up response time for these customers.  As time is of the essence, they have asked you to assist.  You have already confirmed that data is synchronising between both sites.  What is the quickest way to reduce the latency for these customers?",
                        "explanation": "As we are not told otherwise, we can assume that the European site is fast for people within Europe and therefore we can assume a high latency is the cause of the problem.  Route 53 Latency-Based Routing sounds like the perfect candidate when utilising the Disaster Recovery site as the main site for Asia Pacific.  CloudFront could offer some help if we only had one site and the Edge locations could cache some content, but in this case the CTO wanted to use the Disaster Recovery infrastructure already in place.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/route53/faqs/",
                                "title": "Amazon Route 53 FAQs"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Tutorials.html",
                                "title": "Tutorials"
                            }
                        ],
                        "answers": [
                            {
                                "id": "6752c886fe176d4a7c3fd5d847abf946",
                                "text": "In Route 53, create two records for www.mega-widgets.com, a latency record pointing to the European IP and a latency record pointing to the Asia Pacific IP.",
                                "correct": true
                            },
                            {
                                "id": "d718f6cb2c4337528dfde78cbe94dfad",
                                "text": "Create a CloudFront endpoint with the origin pointing to the European site and then point www.mega-widgets.com to that endpoint.",
                                "correct": false
                            },
                            {
                                "id": "0e6075956527bb40627bfab8871eab13",
                                "text": "In Route 53, create eu.mega-widgets.com and ap.mega-widgets.com and point them to the same CloudFront endpoint.",
                                "correct": false
                            },
                            {
                                "id": "883bf6152c030fbe021fafb5fb55c8d4",
                                "text": "In Route 53, create two sub-domain A records of mega-widgets.com, one with the European IP and the other with the Asia Pacific IP. Create two aliases pointing www.mega-widgets.com to both sub-domains.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "82e554d2-f867-4507-974f-04b0b9020cb9",
                        "domain": "MonitoringLogging",
                        "question": "Your company has a team of Windows Software Engineers which have recently switched from developing on-premise applications, to cloud native micro-services.  The Service Desk has been inundated with questions, but they can't troubleshoot because they don't know enough about Amazon CloudWatch.  The questions they have been receiving mainly revolve around why EC2 logs don't appear in log groups and how they can monitor .NET applications.  Choose the following options which will help troubleshoot the Amazon Cloudwatch issues.",
                        "explanation": "The question suggests we are utilising a Windows development environment, so we can discount any answers which have Linux only terms such as running shell scripts.  To run Amazon CloudWatch Application Insights for .NET and SQL Server, we will need to install the SSM Agent with the correct roles, IAM policies and Resource Groups.  We also need to ensure that the CloudWatch agent is running correctly, by starting it using the amazon-cloudwatch-agent-ctl.ps1 script, and as we are assuming defaults, Cloudwatch metrics can be found under the CWAgent namespace.  There are limits for many items in Cloudwatch, utilising Custom Metrics is not one of them.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/troubleshooting-CloudWatch-Agent.html",
                                "title": "Troubleshooting the CloudWatch Agent"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.html",
                                "title": "Amazon CloudWatch Application Insights for .NET and SQL Server"
                            }
                        ],
                        "answers": [
                            {
                                "id": "33668d47f8774c94713045bff628ba97",
                                "text": "If you are utilising CloudWatch Custom Metrics, ensure that you have not reached the default limit.",
                                "correct": false
                            },
                            {
                                "id": "815da42a7b90966156b2d66da1a16b9e",
                                "text": "Check the common-config.toml file exists and is configured correctly, and then run the following Powershell command; amazon-cloudwatch-agent-ctl.ps1 -m ec2 -a start.",
                                "correct": true
                            },
                            {
                                "id": "ffd7b413b965db5c4e66d6dc2c8a73e8",
                                "text": "Check the aws-agent.yaml file exists and is configured correctly, and then run the following shell command; aws-cw-agent.sh start.",
                                "correct": false
                            },
                            {
                                "id": "ff3e53ec436317efcebcc4a0bf3b52a8",
                                "text": "In the Amazon Cloudwatch console, select Metrics, AWS Namespaces and then your metrics should appear under 'CWAgent'.",
                                "correct": true
                            },
                            {
                                "id": "aa7402fca9b873f6b49d144c5c67440e",
                                "text": "For each EC2 instance running .NET code, install the SSM Agent and attach the AmazonEC2RoleforSSM Role.  You will also need to create a Resource Group and IAM Policy.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "7bb8566a-c105-47ad-9848-105db61926ef",
                        "domain": "PoliciesStandards",
                        "question": "Your CTO, in a very urgent manner, wants to know the uptime of all of your EC2 servers with an 'environment' tag set to 'prod'. How do you achieve this quickly?",
                        "explanation": "The Run Command feature of AWS Systems Manager is suitable for this task. All roles must be configured correctly and the SSM agent needs to be installed.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html",
                                "title": "AWS Systems Manager Run Command - AWS Systems Manager"
                            }
                        ],
                        "answers": [
                            {
                                "id": "898c6c270ab3457638bd087cfd1247e8",
                                "text": "Use the Batch Command feature",
                                "correct": false
                            },
                            {
                                "id": "9d336b9d0d3b3fc25b4610e2a7a2f90d",
                                "text": "Ensure required users are configured",
                                "correct": false
                            },
                            {
                                "id": "2b2248d6940ab012e3eb6750edcac3fd",
                                "text": "Ensure required agent is installed",
                                "correct": true
                            },
                            {
                                "id": "92292ff4cfdeb0c7acd7901a12866431",
                                "text": "Use AWS Service Catalog",
                                "correct": false
                            },
                            {
                                "id": "5435e85a50449d33e87b3cf25e3dcf23",
                                "text": "Ensure required roles are configured",
                                "correct": true
                            },
                            {
                                "id": "d165cfe0af0f1d1de31dc1657219abe7",
                                "text": "Use AWS Systems Manager",
                                "correct": true
                            },
                            {
                                "id": "23c84837eb29f8d7c5906f9701407929",
                                "text": "Use the Run Command feature",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "e98496ed-60b9-4d96-8cbb-3d3e260da18e",
                        "domain": "IncidentEventResponse",
                        "question": "Your organisation is 75% through moving its core services from a Data centre and into AWS.  The AWS stacks have been working well in their new environment but you have been told that the Data centre contract will expire in 3 months and therefore there is not enough time to re-implement the remaining 25% of services before this date. As they are already managed by Chef, you decide to move them into AWS and manage them using OpsWorks for Chef. However, when configuring OpsWorks you have noticed the following errors have appeared; \"Not Authorized to perform sts:AssumeRole\" and \"FATAL Could not find pivotal in users or clients!\". Choose the correct options to resolve the errors.",
                        "explanation": "With the \"Not Authorized to perform sts:AssumeRole\" error, you can assume its policy/role related and therefore creating a role and attaching the AWSOpsWorksCMServiceRole policy should resolve this issue. Finally, any message which states that you 'cannot find a pivotal user', requires you to add one to the default location. All other answers will not resolve the problems listed.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/en_pv/opsworks/latest/userguide/troubleshoot-opscm.html",
                                "title": "Troubleshooting AWS OpsWorks for Chef Automate"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/en_pv/opsworks/latest/userguide/welcome_opscm.html",
                                "title": "AWS OpsWorks for Chef Automate"
                            }
                        ],
                        "answers": [
                            {
                                "id": "0ddbd3212ebd514950bf4aafc59e898c",
                                "text": "Install the knife-opc command and then run the command; knife opc org user add default pivotal",
                                "correct": true
                            },
                            {
                                "id": "0b86e189ee6ecc6aefa9be08d750c26f",
                                "text": "Ensure that the EC2 instance has the AWS service agent running and has outbound Internet access with DNS resolution enabled.",
                                "correct": false
                            },
                            {
                                "id": "edd55a54eb6c02470c1f52081eed838d",
                                "text": "Create a new service role and attach the AWSOpsWorksCMServiceRole policy to the role. Verify that the service role is associated with the Chef server and it has that policy attached.",
                                "correct": true
                            },
                            {
                                "id": "c565172b94c322361ca5823ed0126630",
                                "text": "Ensure you have at least one existing EIP address free by deleting unused addresses or by asking AWS Support for an increase.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "eed3c7b0-f676-47ce-b7b3-d9258a9e718e",
                        "domain": "PoliciesStandards",
                        "question": "Your fleet of Windows EC2 instances are running well. They're automatically built from an AMI and generally don't require much interaction, except for when they need to be patched with the latest Windows updates or have new software installed or updated. Ideally, you'd like to automate the process to install or update applications and apply windows updates and patches during the quiet hours when your customers are sleeping, which means around 3am. How would you best automate this process?",
                        "explanation": "Patch Manager combined with Maintenance Windows in AWS Systems manager is the recommended way to automate this requirement.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-scheduletasks.html",
                                "title": "About Patching Schedules Using Maintenance Windows - AWS Systems Manager"
                            }
                        ],
                        "answers": [
                            {
                                "id": "b33df3531d79b8884a9ea73b9b3eb281",
                                "text": "Implement AWS Systems Manager Patch Manager and AWS Systems Manager Maintenance Windows",
                                "correct": true
                            },
                            {
                                "id": "f958a87732086d7eac94980dcc15d1af",
                                "text": "Implement AWS Systems Manager Run Commands to execute the PowerShell scripts to run updates at 3am",
                                "correct": false
                            },
                            {
                                "id": "ef3de69841ef1a22eb05e975b235c5ec",
                                "text": "Implement a Lambda function to run PowerShell update commands on a schedule using CloudWatch Events rules for each server",
                                "correct": false
                            },
                            {
                                "id": "2c14be2bb2789102f0b7f5d920b26df9",
                                "text": "Implement AWS Systems Manager Patch manager and AWS Systems Manager Run Commands",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "860e8b11-9faf-4176-8c11-716aeccf799b",
                        "domain": "HAFTDR",
                        "question": "You're assisting a developer working on a very large and read-heavy application which uses an Amazon Aurora database cluster. The feature currently being worked on requires reading but no writing, however it will be called by the application frequently and from multiple different servers so the reads need to be load balanced. Additionally your reporting team need to make ad-hoc, expensive queries which need to be isolated so that reads for production are not affected by reporting.  Which Aurora configuration fulfils both needs with minimal extra configuration?",
                        "explanation": "The Reader endpoint is appropriate in this situation. The reader endpoint provides load balanced support for read only connections to the database cluster. A custom endpoint can be used to connect to an isolated replica for report generation or ad hoc (one-time) querying,",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html",
                                "title": "Amazon Aurora Connection Management - Amazon Aurora"
                            }
                        ],
                        "answers": [
                            {
                                "id": "af531b8d353cecad595506c5239ca5f2",
                                "text": "Use the Aurora Instance endpoint for your Production system to make load-balanced reads against a read replicas, and create another custom endpoint pointing to a dedicated reporting replica for isolation of ad-hoc reporting.",
                                "correct": false
                            },
                            {
                                "id": "794b7ab3749de5b254f0dd3f2337bc74",
                                "text": "Use the Reader endpoint for your Production system to make its load-balanced reads against high-capacity read replicas, and create a custom endpoint pointing to a separate replica for isolation of ad-hoc reporting.",
                                "correct": true
                            },
                            {
                                "id": "5ff29b324290cc87e7f79502c91662fd",
                                "text": "Use the Custom endpoint for your Production system to make its reads against a single high-capacity replica, and create another custom endpoint pointing to a dedicated reporting replica for isolation of ad-hoc reporting.",
                                "correct": false
                            },
                            {
                                "id": "2b1242d9bda15b98989935f538280b8f",
                                "text": "Use the Cluster endpoint for your Production system to make its reads against high-capacity read replicas, and create an Aurora reporting endpoint pointing to a separate replica for isolation of ad-hoc reporting.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "f6b4a9c9-32fd-433d-93fd-4b61bad8db8a",
                        "domain": "PoliciesStandards",
                        "question": "You are employee #2 in a new startup, right after the founders and their first developer. You've been given the task of checking over the AWS account that has been used so far and determining what service limits you are currently breaching (if any) and producing a report on the current service limit levels. How do you obtain this information?",
                        "explanation": "Trusted advisor makes Service Limit checks free for all users at any support level.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/blogs/mt/monitoring-service-limits-with-trusted-advisor-and-amazon-cloudwatch/",
                                "title": "Monitoring Service Limits"
                            }
                        ],
                        "answers": [
                            {
                                "id": "0ee62b6775e4ea63a1288cd941f273a5",
                                "text": "Sign up for a business support plan to access the detailed billing reports in your billings console.",
                                "correct": false
                            },
                            {
                                "id": "6ca7f9bfe1536150652a1e4ecaf6789c",
                                "text": "Enable detailed billing and look at the Service Limit reporting page, which is free for all users.",
                                "correct": false
                            },
                            {
                                "id": "1812a1f14ddc45354e11146b11914c56",
                                "text": "Sign up for a business support plan to access the Service Limit Checks in Trusted Advisor",
                                "correct": false
                            },
                            {
                                "id": "de4b26655e419de8a97e82d3598b1eda",
                                "text": "Enable Trusted Advisor and look at the Service Limit page, which is free for all users.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "00ff039c-bad1-448c-8d17-e6ad8b1491f6",
                        "domain": "SDLCAutomation",
                        "question": "You manage a team of developers who currently push all of their code into AWS CodeCommit, and then CodePipeline automatically builds and deploys the application. You think it would be useful if everyone received an email when a pull request is created or updated. How would you achieve this in the simplest way?",
                        "explanation": "Notifications in the CodeCommit console is the simplest way to implement this requirement. Triggers only trigger when someone pushes to a repository, not when a pull request is created.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-repository-email.html",
                                "title": "Configuring Notifications for Events in an AWS CodeCommit Repository"
                            }
                        ],
                        "answers": [
                            {
                                "id": "439fdcb8b7e144ddacda7b944536f82b",
                                "text": "Enable triggers in the CloudCommit console. Select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.",
                                "correct": false
                            },
                            {
                                "id": "ee242f6e3c35045661e6306dbd2e0153",
                                "text": "Enable notifications in the CodeCommit console. Select 'Pull request update events' as the event type and choose or create a new SNS topic for the notifications. Subscribe everyone to the SNS topic.",
                                "correct": true
                            },
                            {
                                "id": "15c5b59dd67cddfeb10567fef856e7b1",
                                "text": "Enable triggers in the CodePipeline console. Create a trigger stage after your source stage and select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.",
                                "correct": false
                            },
                            {
                                "id": "d5af6b2263080d905efb836b99bf8b19",
                                "text": "Enable notifications in the CodePipeline console. Create a notification stage after your source stage and select 'Pull request update events' as the event type and select Amazon SNS as the trigger service. Subscribe everyone to the SNS topic.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "b8240606-8f74-4b39-ad43-d8374d4b275d",
                        "domain": "MonitoringLogging",
                        "question": "Your organization has been using AWS for 12 months. Currently, you store all of your custom metrics in CloudWatch. Per company policy, you must retain your metrics for 3 years before it is OK to discard or delete them. Is CloudWatch suitable?",
                        "explanation": "CloudWatch will retain metrics for 15 months, after which they will expire. If you need to keep metrics for longer you must pull them out of CloudWatch using their API and store them in a database somewhere else.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html",
                                "title": "CloudWatch Concepts"
                            }
                        ],
                        "answers": [
                            {
                                "id": "47ecaf50014e4dd77522488586ae375a",
                                "text": "Yes, CloudWatch will retain custom metrics for 3 years.",
                                "correct": false
                            },
                            {
                                "id": "cf00bc1c445d1daf231db593c6f998d9",
                                "text": "Yes, CloudWatch will retain custom metrics for 5 years.",
                                "correct": false
                            },
                            {
                                "id": "d829ad2942daa681d42ad4237f50827c",
                                "text": "No, CloudWatch only retains metrics for 15 months. You will have to use the API to pull all metrics and store them somewhere else.",
                                "correct": true
                            },
                            {
                                "id": "8faab65d1ba3ce5e0cb52e471493dd0b",
                                "text": "No, CloudWatch only retains metrics for 24 months. You will have to use the API to pull all metrics and store them somewhere else.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "9ec0b495-6913-472d-bf6c-7bdfc7a22bef",
                        "domain": "MonitoringLogging",
                        "question": "You have three EC2 instances running a core application, which has been performing sub-optimally since yesterday.  One of your colleagues said that they remember that the system appeared to perform in a similar way about 18 months ago, but they can't remember what the issue was.  You need to perform an indepth investigation of the current issue and you will need to view graphs of that period, with granular metrics.  Reading the logs from when the issue originally occurred would also help troubleshooting.  Which of the following options would give you the best chance of resolving the issue?",
                        "explanation": "You can immediately disregard any option with Cloudtrail as these will only contain API logs and will not record application issues.  For the remaining options, it's important to note that Cloudwatch Logs are available indefinitely by default, so any option stating that logs can't be kept can also be excluded.  Now we have to factor in the length of time in the past we are investigating.  15 months is the maximum amount of time we can retrieve metrics from the Console, this means that we need to retrieve the data from the API and process it locally.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/cloudwatch/faqs/",
                                "title": "Amazon CloudWatch FAQs"
                            }
                        ],
                        "answers": [
                            {
                                "id": "cb2703cd52452e6b2dd73d35a2beab1d",
                                "text": "View the Cloudwatch logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 60 minutes.",
                                "correct": false
                            },
                            {
                                "id": "4adf59b6cbba3d749598c710e3671bdf",
                                "text": "View the Cloudwatch logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 15 minutes.",
                                "correct": false
                            },
                            {
                                "id": "aa3088febe99800c543d7921893448b2",
                                "text": "View the Cloudtrail logs from 18 months ago and view the Cloudwatch graphs from the same time, setting the granularity at 1 minute.",
                                "correct": false
                            },
                            {
                                "id": "3a9e3e96a4470855c66e7a3f8d0ad2ef",
                                "text": "View the Cloudwatch logs from 18 months ago and use the API to retrieve the datapoints and store in a local Database for analysis.",
                                "correct": true
                            },
                            {
                                "id": "1c6bee332c95b0f8ec7ed30a6efaa04a",
                                "text": "View the Cloudwatch graphs from 18 months ago, setting the granularity at 1 minute. Unfortunately Cloudwatch logs cannot be kept for that length of time.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "7bce21b9-f986-4147-b616-c58ac141d19f",
                        "domain": "PoliciesStandards",
                        "question": "You work for a global service provider, deploying critical software 24 hours a day.  You have 3 AWS Accounts; 'POC' allows for Developers to stand up new technology and try out new ideas on an adhoc basis. 'QA' allows for automated builds and testing to be carried out as part of your CI/CD Pipeline and any problems here mean that software will not get pushed into production, so it's important that any issues are resolved quickly. The final account is 'Live' which is the main Production account.  It's the most critical and requires the best response times.  You need to choose the most appropriate and cost effective Support Plan which will satisfy your support needs but also allow for the full range of AWS Trusted Advisor Checks and Recommendations.",
                        "explanation": "The important phrase in the question is that any choice must \"...allow for the full range of AWS Trusted Advisor Checks and Recommendations\", only two support plans have all the Trust Advisor checks and these are Business and Enterprise.  Anything that contains the Basic or Developer plans will not be correct.  Also, it specifies that the 'Live' account is \"critical and requires the best possible response times\", so we could use the Enterprise plan for this, but we'll only get better response time for mission critical business applications such as Microsoft, SAP and Oracle tools, which is not the software we are deploying.  Therefore taking all of this into account, along with cost, 'POC', 'QA' and 'Live' should all be allocated the Business plan.",
                        "links": null,
                        "answers": [
                            {
                                "id": "e52ec269985d97d389e8e39403ec69fc",
                                "text": "The 'POC' Account should be allocated the Basic Support Plan, 'QA' the Business Support Plan and 'Live' the Enterprise Support Plan.",
                                "correct": false
                            },
                            {
                                "id": "b802f5ecd5a395f2c899ccdbf041d677",
                                "text": "The 'POC' Account should be allocated the Basic Support Plan and both 'QA' and 'Live' the Business Support Plan.",
                                "correct": false
                            },
                            {
                                "id": "92b9da164af4d9a8f9d10be3a3964a68",
                                "text": "The 'POC', 'QA' and 'Live' Accounts should all be allocated the Business Support Plan.",
                                "correct": true
                            },
                            {
                                "id": "4dbc42a7817c285587307fc6102d43d2",
                                "text": "The 'POC' Account should be allocated the Developer Support Plan, 'QA' the Business Support Plan and 'Live' the Enterprise Support Plan.",
                                "correct": false
                            },
                            {
                                "id": "eec4f4e8aab13e184a6c7abc550b5ded",
                                "text": "The 'POC' and 'QA' Accounts should use the Basic Support Plan, while 'Live' should use the Enterprise Support Plan.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "279ef48b-4c8c-4cf0-a14a-8144644ac2f1",
                        "domain": "MonitoringLogging",
                        "question": "You work for a company that uses a serverless architecture to process up to 4,000 events per second for its usage analytics service. At its core, it consists of a multitude of Lambdas that have been given various resource configurations, i.e. memory/CPU settings. For each function invocation, you want to monitor that allocation against the actual memory usage. Select the simplest feasible approach to achieve that.",
                        "explanation": "AWS Lambda doesnt provide a built-in metric for memory usage but you can set up a CloudWatch metric filter. The AWS Lambda console provides monitoring graphs for Invocations, Duration, Error count and success rate (%), Throttles, IteratorAge and DeadLetterErrors.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html",
                                "title": "AWS Lambda Function Configuration"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html",
                                "title": "Monitoring and troubleshooting Lambda applications using Amazon CloudWatch"
                            },
                            {
                                "url": "https://forums.aws.amazon.com/thread.jspa?threadID=226674",
                                "title": "Profiling memory usage for Lambda functions"
                            },
                            {
                                "url": "https://gist.github.com/cgoering/4de674e3e3ca8d6255ea708997cca3b0",
                                "title": "AWS Lambda memory usage metric in CloudFormation"
                            }
                        ],
                        "answers": [
                            {
                                "id": "e2113fd25833a45a179807bc633f635a",
                                "text": "Log entries written into the log group associated with a Lambda function don't include profiling info such as memory usage. You need to use AWS X-Ray for that.",
                                "correct": false
                            },
                            {
                                "id": "65883e3f656c8f5b6f91c895b44649ff",
                                "text": "Use the 'Monitoring' tab in the AWS Lambda console and add the 'Resource' monitoring graph to your dashboard.",
                                "correct": false
                            },
                            {
                                "id": "55b98a256995ac4e4c6f7afc1e1a5845",
                                "text": "AWS Lambda provides a built-in CloudWatch metric for memory usage.",
                                "correct": false
                            },
                            {
                                "id": "2f5f4e15a960c888b86e315b993df292",
                                "text": "You can set up a custom CloudWatch metric filter using a pattern that includes 'REPORT', 'MAX', 'Memory' and 'Used:'.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "7182b5ef-3d25-41f6-8251-a554cdef2719",
                        "domain": "IncidentEventResponse",
                        "question": "Your team has been planning to move functionality from an on-premises solution into AWS.  New micro-services will be defined using CloudFormation templates, but much of the legacy infrastructure is already defined in Puppet Manifests and they do not want this effort to go to waste.  They have decided to deploy the Puppet-based elements using AWS OpsWorks but have received the following errors during configuration; \"Not authorized to perform sts:AssumeRole\" and also \"The following resource(s) failed to create [EC2Instance]\".  The team have been unable to resolve these issues and have asked for your help.  Identify the reasons why these errors occur from the options below.",
                        "explanation": "There are two answers which would resolve the errors in the question.  Any time a \"not authorized\" message is displayed, it is nearly always a permissions problem and in this case it can be resolved by attaching the AWSOpsWorksCMServiceRole policy to the instance profile role for EC2. opsworks-cm.amazonaws.com should also be listed in the Trust Relationships. For the second question, this error normally dictates that the EC2 instance doesnt have sufficient network access, so we need to ensure that the instance has outbound Internet access, and that the VPC has a single subnet with DNS resolution and Auto-assign Public IP settings enabled. All other options will not resolve the errors.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/opsworks/latest/userguide/troubleshoot-opspup.html",
                                "title": "Troubleshooting OpsWorks for Puppet Enterprise"
                            },
                            {
                                "url": "https://docs.aws.amazon.com/en_pv/opsworks/latest/userguide/welcome_opspup.html",
                                "title": "AWS OpsWorks for Puppet Enterprise"
                            }
                        ],
                        "answers": [
                            {
                                "id": "8bda36311bab2d5d74bccd6e3eac3e68",
                                "text": "You cannot use OpsWorks for managing Puppet based infrastructure.  OpsWorks only operates and integrates with Chef.",
                                "correct": false
                            },
                            {
                                "id": "72c5c3db3a42b839c2ee0df93fadac14",
                                "text": "Ensure that the EC2 instance has the AWS service agent is running, has outbound Internet access and has DNS resolution enabled.",
                                "correct": true
                            },
                            {
                                "id": "8d14b74d28ed11d3f4a78d4937d00b00",
                                "text": "Ensure that the 'AWSOpsWorksCMServerRole' policy is attached to the instance profile role.",
                                "correct": true
                            },
                            {
                                "id": "a4844f346bb4f746b9766b4608cd2b9a",
                                "text": "Ensure there are no errors within the Puppet Manifests.  Fix the syntax errors and restart the Puppet Master.",
                                "correct": false
                            },
                            {
                                "id": "fd22d4d4b177eefd2129262da6a99e82",
                                "text": "Deploy in a VPC which is set to non-default tenancy with an instance type that supports dedicated tenancy.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "fbc8bae9-74cb-430a-aaf3-7c16cd49f9f9",
                        "domain": "MonitoringLogging",
                        "question": "You have built a serverless Node.js application which uses Lambda, S3 and a DynamoDB database. You'd like to log some simple metrics so you can possibly graph them at a later date, or analyze the logs for faults or errors. You aren't able to install the CloudWatch Logs agent into a Lambda function however. What do you do instead?",
                        "explanation": "console.log will work perfectly in Lambda and is the easiest way to log directly to CloudWatch Logs",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-logging.html",
                                "title": "AWS Lambda Function Logging in Node.js"
                            }
                        ],
                        "answers": [
                            {
                                "id": "32ad7a8632717703ab0efe26deafc538",
                                "text": "Use the CloudWatch Logs API, which will provide putLogEvents where you can upload logs to CloudWatch.",
                                "correct": false
                            },
                            {
                                "id": "f86fadfb6bf4f2aff4fc541fb9f962d9",
                                "text": "Use the log.console commands in Lambda, it will log your logs straight to CloudWatch Logs.",
                                "correct": false
                            },
                            {
                                "id": "d694da5d9149fb6683cce0b6ec647e92",
                                "text": "Use an API gateway configured to log to CloudWatch Logs",
                                "correct": false
                            },
                            {
                                "id": "856b826225eea424a5e5472bb8242afa",
                                "text": "Use the console.log commands in Lambda, it will log your logs straight to CloudWatch Logs.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "c124d204-9148-4798-9b7b-eb6596a9f8b3",
                        "domain": "IncidentEventResponse",
                        "question": "Your company develops an online shopping platform and would like to implement a way to recommend products to customers based on previous products they have looked at. To use this you want to record their click-stream, or the sequence of links they've clicked on as they navigate your website. At any one time, you can have thousands of users using your shopping platform. Which architecture will meet your requirements?",
                        "explanation": "Ingesting is done with Kinesis Data Streams, grouping user requests into sessions is done with Data Analytics. Data processing and storage is done with Lambda, Firehose and S3. Read the attached link and the high-level solution overview for more information.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/",
                                "title": "Create real-time click-stream sessions and run analytics with Amazon Kinesis Data Analytics, AWS Glue, and Amazon Athena"
                            }
                        ],
                        "answers": [
                            {
                                "id": "2a469128dfb0909cb7f378ca0b0804e3",
                                "text": "Ingest data with Kinesis Data Analytics. Group user requests with Kinesis Data Firehose. Process and store the data with Lambda, Kinesis Streams and S3.",
                                "correct": false
                            },
                            {
                                "id": "308394a93edfced21f133bc91188efdf",
                                "text": "Ingest data with Kinesis Data Streams. Group user requests with Kinesis Data Analytics. Process and store the data with Lambda, Kinesis Firehose and S3.",
                                "correct": true
                            },
                            {
                                "id": "8a71cd8aeec8a1da02a83f432fb0973b",
                                "text": "Ingest data with Kinesis Firehose. Group user requests with Kinesis Data Data Streams Groups. Process and store the data with EC2, Kinesis Streams and S3.",
                                "correct": false
                            },
                            {
                                "id": "2046d52f62962c7601dbac9ea1beaf56",
                                "text": "Ingest data with Kinesis Data Streams. Group user requests with Kinesis Data Analytics. Process and store the data with EC2, Kinesis Firehose and S3.",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "85ec1f79-e327-44c4-a934-46e826ec0dd3",
                        "domain": "HAFTDR",
                        "question": "You currently run an autoscaled application which is database read heavy. Due to this, you are making use of a read replica for all application reads. It's currently running at around 60% load with your user base but you expect your company growth to double the amount of users every 6 months. You need to forward plan for this and determine methods to ensure the database doesn't become a bottleneck while still maintaining some redundancy. What is the best way to approach this issue?",
                        "explanation": "More read replicas will ease the load on your current ones, and load balancing them with a weighted routing policy will ensure they're not a single point of failure for your application.",
                        "links": [
                            {
                                "url": "https://aws.amazon.com/premiumsupport/knowledge-center/requests-rds-read-replicas/",
                                "title": "How can I distribute read requests across multiple Amazon RDS read replicas?"
                            }
                        ],
                        "answers": [
                            {
                                "id": "43d1e6030522d9685ec499098adce943",
                                "text": "Monitor your database read replica usage in CloudWatch alerts. When it's close to 90% capacity perform an online resize to a larger instance type.",
                                "correct": false
                            },
                            {
                                "id": "17ef3669be9b09f4623719b5771bcb40",
                                "text": "Create another read replica and deploy a second autoscaled version of your application. Point it at your second read replica.",
                                "correct": false
                            },
                            {
                                "id": "c67fa8f440af68765fc377331fb22bb7",
                                "text": "Deploy an additional Multi-AZ RDS read replica and modify your application to use it instead.",
                                "correct": false
                            },
                            {
                                "id": "77851b21f9bb1d7557b828c6ad224a60",
                                "text": "Create more read replicas. Use a Route53 weighted routing policy to ensure the load is spread across your read replicas evenly.",
                                "correct": true
                            }
                        ]
                    },
                    {
                        "id": "276f651e-fe53-4841-8545-38c00909e9c5",
                        "domain": "PoliciesStandards",
                        "question": "After the runaway success of their first website, ACatGuru is building out its IT services to handle its tremendous growth. The CEO of ACatGuru would like to centrally manage all of its commonly deployed IT services, while still giving its employees access to deploy only the services defined by company policy allows. For example, only a developer can spin up a test stack, only a database administrator can spin up a database, and so on. There should also be restrictions on which products can be launched, with the ability to create a custom user interface for you users so they can avoid the complex AWS Management Console. Which AWS service is ideal for implementing these requirements and which features are required?",
                        "explanation": "AWS Service Catalog enables organizations to create catalogs of what can be deployed, and create custom interfaces allowing users to deploy them. They are organised as Products which are added to Portfolios to which users can be given access.",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/servicecatalog/latest/dg/what-is-service-catalog.html",
                                "title": "What Is AWS Service Catalog? - AWS Service Catalog"
                            }
                        ],
                        "answers": [
                            {
                                "id": "725903c711beec9056a0f2891a402263",
                                "text": "A Project",
                                "correct": false
                            },
                            {
                                "id": "b1820ee1cf68e2e65f263ff7bb207626",
                                "text": "AWS Organizations",
                                "correct": false
                            },
                            {
                                "id": "96564dfa4633550e1df29d07c3dba125",
                                "text": "A Portfolio",
                                "correct": true
                            },
                            {
                                "id": "526775c1622cd0e7b703eb8d4a83d657",
                                "text": "AWS Service Catalog",
                                "correct": true
                            },
                            {
                                "id": "b8f528bf9294fdda9450ad077dfcfc66",
                                "text": "A Placement",
                                "correct": false
                            },
                            {
                                "id": "3ac62db74ae690ff5ba0cdff8b04efe0",
                                "text": "A Product",
                                "correct": true
                            },
                            {
                                "id": "132976a248c1f5f10f99b4aa7d7fe9d1",
                                "text": "AWS System Catalog",
                                "correct": false
                            }
                        ]
                    },
                    {
                        "id": "25964f23-34d0-4ed9-9f24-4cee82e76511",
                        "domain": "ConfigMgmtandInfraCode",
                        "question": "You are building acatguru, which your organization describes as facebook for cats. As part of the sign-up process your users need to upload a full size profile image. You already have these photos being stored in S3, however you would like to also create thumbnails of the same image, which will be used throughout the site. How will you automate this process using AWS resources?",
                        "explanation": "S3 triggering Lambda to create thumbnails is a perfect example of how to automate this process",
                        "links": [
                            {
                                "url": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html",
                                "title": "Using AWS Lambda with Amazon S3"
                            }
                        ],
                        "answers": [
                            {
                                "id": "9d2bb40a4f2a61f4e70855a070faa693",
                                "text": "Create an S3 event trigger to execute a Lambda function when an object is created. The function will create the thumbnail from the source image and store it in a different S3 bucket.",
                                "correct": true
                            },
                            {
                                "id": "a24cc963be9859fff809b1da36be8137",
                                "text": "Configure S3 to publish its event stream to an SNS topic. Subscribe a Lambda function to the SNS topic which will trigger when a file is uploaded. The function will create the thumbnail from the source image and store it in a different S3 bucket.",
                                "correct": false
                            },
                            {
                                "id": "30a4f470d1bf4e4d01dab0088e331a6d",
                                "text": "Create an S3 bucket notification trigger to execute a Lambda function when an object is created. The function will create the thumbnail from the source image and store it in a different S3 bucket.",
                                "correct": false
                            },
                            {
                                "id": "62751bd29f8b4728e9393f35cf7bf035",
                                "text": "Use CloudTrail to monitor PUT and POST calls sent to S3 and trigger a Lambda function when you identify an upload. The function will create the thumbnail from the source image and store it in a different S3 bucket.",
                                "correct": false
                            }
                        ]
                    }
                ]
            }
        }
    }
}