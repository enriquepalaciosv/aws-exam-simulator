{"data":{"createNewExamAttempt":{"attempt":{"id":"31c1ae5e-690e-42d5-9bbc-38c112f1bf71"},"exam":{"id":"172b26a0-40b6-462a-806b-8d78cb989692","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"55b78611-5890-4835-8371-66af208d25a2","domain":"awscsapro-domain2","question":"ACME Company has decided to migrate their on-premises 800TB data warehouse and their 200TB Hadoop cluster to Amazon Redshift. The migration plan calls for staging all of the data in Amazon S3 before loading it into Redshift in order to accomplish the desired distribution across the compute nodes in the cluster. ACME has an AWS Direct Connect 500 Mbps connection to AWS. However, calculations are showing the effective transfer rate won't allow them to complete the migration during the three month time frame they have to complete the project. What migration approach should they implement to complete the project on-time and with the least amount of effort for the migration team?","explanation":"Loading multiple Snowball Edge devices concurrently over a number of waves can be accomplished in less than a month with reasonable internal network performance. Snowball Edge's onboard S3-Compatible Endpoint makes for seamless transfer from a data warehouse S3 interface, which most of the major data warehouse vendors support. Hadoop data can be copied to the S3 endpoint after mounting it on a staging workstation. Transferring 1PB over a 500Mbps Direct Connect will take approximately 8 months, so increasing it to a 1Gbps connection will complete in a minimum of 4 months. Loading concurrent Snowball devices will work, but the staging workstation won't be able to mount the data warehouse directly. The data will most likely need to be unloaded into delimited flat files on the workstation's filesystem. Using Database Migration Service won't overcome the network bandwidth issues.","links":[{"url":"https://aws.amazon.com/snowball-edge/","title":"AWS Snowball Edge"},{"url":"https://aws.amazon.com/blogs/storage/data-migration-best-practices-with-snowball-edge/","title":"Data Migration Best Practices with AWS Snowball Edge"}],"answers":[{"id":"e705d456884fba2f673d22a9e8e9b654","text":"Increase the Direct Connect capacity to a 1Gbps connection and copy the data from the data warehouse and the Hadoop cluster directly to S3 using native tools","correct":false},{"id":"8dfc8bb936df7dbaf648fa2d5760508e","text":"Attach multiple AWS Snowball devices to the on-premises network. Use a staging workstation to mount the data warehouse and the Hadoop filesystem as data sources. Use the AWS Snowball client to load the data onto the Snowball device","correct":false},{"id":"06696779bc2515fee9a6f91b4eb57c6b","text":"Reduce the project time frame by leveraging AWS Database Migration Service to load both the data warehouse and Hadoop cluster data directly from the on-premises data sources to Redshift, eliminating the S3 intermediate stage. Then, re-organize the data based on the distribution scheme afterwards","correct":false},{"id":"2b70a067963b17ed73d482ffd2fc810a","text":"Attach multiple Snowball Edge devices to the on-premises network. Load the data warehouse data with an S3 interface supported by the data warehouse platform. Mount the Hadoop filesystem from a staging workstation using native connectors and transfer the data through the AWS CLI","correct":true}]},{"id":"a976aaea-e25f-4b84-879b-4f4843809b1f","domain":"awscsapro-domain2","question":"You need to create several file systems in Amazon Elastic File System (Amazon EFS) for an application. The storage class of the EFS file systems is standard. The Amazon EFS mount helper has already been installed in all Amazon EC2 instances and you are going to mount EFS file systems using the mount helper. According to the company policy, all file systems should be encrypted in transit. How would you achieve this requirement for the EFS file systems?","explanation":"Amazon EFS supports two forms of encryption, encryption in transit and encryption at rest. Both types of encryption are not enabled by default. When mounting the Amazon EFS file systems with the mount helper in EC2 instances, you need to explicitly add a TLS option \"-o tls\" to enable the encryption in transit. A customer managed key (CMK) is required for the encryption at rest. However, no CMK is needed for the encryption in transit.","links":[{"url":"https://docs.aws.amazon.com/efs/latest/ug/encryption.html","title":"Encrypting Data and Metadata in EFS"}],"answers":[{"id":"26cc1a439bf3ff5489f933a0268b20f1","text":"When mounting the file systems using the Amazon EFS mount helper, enable Transport Layer Security (TLS) by adding the  \"-o tls\" option such as \"sudo mount -t efs -o tls fs-12345678:/ /mnt/efs\".","correct":true},{"id":"d54fdbebc155feccf6b1a973a118e9f2","text":"Encryption in transit is not enabled by default. Users can enable the encryption in transit in the AWS Management Console by selecting a customer managed key in KMS. The CMK must have a key policy to allow the AWS EFS service to fetch the key.","correct":false},{"id":"05ca59a3b9132178fb264eeebdaeac45","text":"EFS enables the encryption both in transit and at rest by default if the EFS file systems are mounted by the Amazon EFS mount helper. No extra actions are required.","correct":false},{"id":"164503f2e48af52368a3dc0ba046e275","text":"EFS file systems only support the encryption at rest with a KMS AWS managed key or customer managed key. Encryption in transit is not supported so that EFS file systems should not be used in this scenario.","correct":false}]},{"id":"0ee4566a-508e-472d-9789-3318e3284aca","domain":"awscsapro-domain5","question":"You are an AWS Solutions Architect and you maintain a CloudFormation stack that includes the resources of a network load balancer and an Auto Scaling group. The ASG has one running instance. A developer uses the instance for feature development and testing. However, after he adds some configurations and restarts an application process, the instance is terminated by the Auto Scaling group and a new instance is created. The new configurations are lost in the new instance. You need to modify the resource settings to make sure that the instance is not terminated by the ASG when application processes are restarted. Which of the following methods would best achieve this?","explanation":"The instance fails the health check in the ELB target group and is then terminated by ASG whenever the application processes are restarted. The prevent the ASG from terminating the EC2 instance you need to modify the health check type from ELB to EC2. As a result, even if the instance fails the health check in the ELB target group, it will not be terminated by the Auto Scaling group. You do not need to create an AMI or a new launch configuration to address the issue. And the custom health check script that runs every minute cannot prevent the instance from being terminated.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health checks for Auto Scaling instances"}],"answers":[{"id":"1584209c598c68a956227b3770a97fb2","text":"Create an AMI and configure a new launch configuration with the AMI. Then modify the Auto Scaling group to use the new launch configuration and launch a new instance.","correct":false},{"id":"8fad414c494de200ee3990b334d22b13","text":"Update the CloudFormation script and modify the health check type from ELB to EC2.","correct":true},{"id":"c66bf36d77a2aa4cf5ef75a4ac494df8","text":"Store all custom configuration scripts in an S3 bucket and create a new launch configuration. In its user data section, download the scripts from the S3 bucket and execute them. Whenever a new instance is launched, the configurations can be installed automatically. ","correct":false},{"id":"745390da2b4433786bf4cdf17df3d2d3","text":"Edit a health check shell script that performs some sanity checks in the EC2 instance. If the sanity checks pass, the shell script uses AWS CLI “aws autoscaling set-instance-health” to set its status to be healthy. Run the script every minute.","correct":false}]},{"id":"66d28221-31ce-4cf0-aca3-2b5a69535bb5","domain":"awscsapro-domain3","question":"You are consulting with a small Engineering firm that wants to move to a Bring-Your-Own-Device policy where employees are given some money to buy whatever computer they want (within certain standards).  Because of device management and security concerns, along with this policy is the need to create a virtualized desktop concept.  The only problem is that the specialized engineering applications used by the employees only run on Linux.  Considering current platform limitations, what is the best way to deliver a desktop-as-a-service for this client?","explanation":"AWS Workspaces added support for Linux desktops the middle of 2018.  BYOD scenarios work together well with a DaaS concept to provide security, manageability and cost-effectiveness.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/create-custom-bundle.html","title":"Create a Custom WorkSpaces Bundle - Amazon WorkSpaces"}],"answers":[{"id":"e8a33593afbd82697d0ab168304265ed","text":"Launch an EC2 Linux instance and install XWindows and Gnome as the GUI.  Configure VNC to allow remote login via GUI and load the required software.  Create an AMI and use that to launch subsequent desktops.","correct":false},{"id":"4018dbf7b4646b285f5ceaa7b49a5934","text":"Launch a Linux Workspace in AWS WorkSpaces and customized it with the required software.  Then, create a custom bundle from that image and use that bundle when you launch subsequent Workspaces.","correct":true},{"id":"0cbf457eccae17779144d4e64e92a43e","text":"Launch a Windows Workspace and install VirtualBox along with a minimal Linux image.  Within that Linux image, install the required software.  Create an image of the Windows Workspace and create a custom bundle from that image.  Use that bundle when launching subsequent Workspaces.","correct":false},{"id":"3480305b106307937ed81bba73d294ab","text":"Package the required apps as WAM packages.  When launching new Windows Workspaces, instruct users to allow WAM to auto-install the suite of applications prior to using the Workspace.","correct":false},{"id":"bf3d66705af4677c9ade8605aa6bd89a","text":"Given current limitations, running Linux GUI applications remotely on AWS is not feasible.  They should reconsider their BYOD policy decision.","correct":false}]},{"id":"8aa313fe-cd0f-4899-a2f4-e8f2fd64c245","domain":"awscsapro-domain4","question":"Your business depends on AWS S3 to host three kinds of files - images, documents and compressed installation packages. These files are accessed and downloaded by end-users from all US regions and west EU, though the compressed installation packages are downloaded rarely as users tend to access the service from their browsers instead of installing anything on their machines. Each installation package bundles several images and documents, and also includes binaries that are downloaded from a 3rd party service while creating the package files.\nThe images and documents range from a few KBs to a few hundred KBs in size and they are mostly static in nature. However, the compressed installation package files are generated every few hours because of changes done by the 3rd party service to their binaries, and some of them are as large as a few hundred GB-s. The installation package files can be regenerated from the images and documents fairly quickly if required. It is important to be able to retrieve older versions of the images and documents.\nWhich of the following storage solutions is the most cost-effective approach to design the storage for these files?","explanation":"The areas tested by this question are:\\n1. Versioning cannot be enabled at the object level. It is a bucket-level feature. This rules out the choice where we have a single bucket and selectively turn on versioning on for some objects only.\\n2. If you enable Versioning for a bucket containing large objects that are frequently created/uploaded, it will result in higher storage cost as all the previous versions will result in storage volume growing quickly because of frequent writes. In the given scenario, the compressed installation package files are large and also frequently generated (every few hours). There is no requirement to version them, as they can be quickly generated on-demand. Hence, putting them in a bucket that has Versioning enabled is not a good cost-effective solution. This rules out two choices - one where we have a single versioned bucket, the other where we enable versioning for both buckets.\\n3. Note that all options except one correctly identify the storage class requirements - the compressed installation package files should be stored as One-Zone IA because durability is not a prime requirement for these files (simply because they can be regenerated on-demand easily). They are rarely downloaded, hence IA is the correct class. Combined with low durability, One Zone IA is the most cost-effective solution. Only one option uses the incorrect storage tier for these files - note that IA is more expensive than One-Zone IA, and the question is about cost-effectiveness.\nHence, the only correct answer is the one that addresses both Versioning and Storage Class requirements correctly.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html","title":"Documentation on Object Versioning"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-howtoset","title":"Setting the Storage Class of an Object"}],"answers":[{"id":"90f6b37d063a0158682b034d578bf29b","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for both the buckets. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":false},{"id":"9e88398b12bdf0dd2fdc12a19f4962a1","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for Bucket A only. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":true},{"id":"fd1d330daf4d05b4fa4c888a0584130f","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the bucket. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to One-Zone Infrequent Access while uploading compressed installation package files","correct":false},{"id":"2e4e12f367286b12f14ae74b1fd4e350","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the image and document objects only, but not for the compressed installation package files. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to Infrequent Access while uploading compressed installation package files","correct":false}]},{"id":"2eee6f1c-96d7-4d2b-821f-4ce8acaf3de3","domain":"awscsapro-domain5","question":"You've deployed a mobile app for a dance competition television show's viewers to vote on performances. The app's backend leverages Amazon API Gateway, AWS Lambda, and Amazon RDS Oracle, with voting activity going from devices directly to API Gateway. In the middle of the broadcast, you begin receiving errors in CloudWatch indicating that the database connection pool has been exhausted. You also see log entries in CloudWatch with a 429 status code. After the show concludes, ratings for the app indicate a very poor user experience, with multiple retries needed to cast a vote. What would be the best way to increase the scalability of the app going forward?","explanation":"Placing Kineses between API Gateway and Lambda decouples the architecture, making use of an intermediary service to buffer incoming requests. The 429 status code indicates a Lambda concurrency throttling error, which you can resolve by controlling the Kinesis batch size per batch delivery. Database sharding will increase scalability, but will still have an upper limit of capacity. Increasing available Lambda memory will have no effect. Inserting a Lambda traffic manager doesn't address the database scalability issues, nor does increasing the regional Lambda concurrency limit. Modifying RDS DB Parameter Group values will require a database restart to take effect, which won't be feasible during live voting activity.","links":[{"url":"https://aws.amazon.com/blogs/architecture/how-to-design-your-serverless-apps-for-massive-scale/","title":"How to Design Your Serverless Apps for Massive Scale"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/scaling.html","title":"AWS Lambda Function Scaling"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","title":"Using AWS Lambda with Amazon Kinesis"}],"answers":[{"id":"6b6e6fb8b54db42df4d343376ccd8c60","text":"Insert Amazon Kinesis between API Gateway and Lambda, and configure Kinesis as an event source for Lambda. Set the number of records to be read from a Kinesis shard to an optimal value based on volume projections","correct":true},{"id":"8f962935878f87ebc9523dc54f505886","text":"Scale the database horizontally by creating additional instances and use sharding to distribute the data across them. Provide the Lambda function with a mapping of the sharding scheme in DynamoDB. Increase the amount of memory available to the Lambda function during execution","correct":false},{"id":"2f66b519925956d6a91d0026056904d9","text":"Have API Gateway route requests to a new Lambda function that manages traffic and retries for the voting logic Lambda function. Request that the regional function concurrency limit be increased based on volume projections","correct":false},{"id":"a4051078cf0289e689e79fe70eab1f14","text":"Create a separate Lambda function to increase the maximum DB connection value in the RDS DB Parameter Group when a CloudWatch Metrics DB connection threshold is exceeded. Invoke Lambda functions with an 'event' invocation type to retry failed events automatically","correct":false}]},{"id":"5e92e555-6e42-4128-ae19-302b81d9fe84","domain":"awscsapro-domain2","question":"A university wants to create a regional website on AWS for end-users to download past research papers from. Most of the site will be static, as it will display metadata about the whitepapers like author, topic, date, excerpts and price. It will not show any link to download the full whitepaper. There will, however, be a payment link for each whitepaper. If users click on the payment link, the Javascript browser-side code will connect directly to a 3rd-party payment processing service which will return success on completion of payment. On success, the Javascript code will make a REST call to your website back-end to fetch a pre-signed S3 URL for that whitepaper and show a new page with the download link.\nThe pre-signed download URL must be a https URL, as you must protect the whitepaper authors from their valuable IP from being pirated using man-in-the-middle attacks while being downloaded. The website itself may be served over HTTP. None of the whitepapers must be publicly accessible without payment, though the website will not deal with any authentication or user profiles.\nIdentify the most suitable solution that will strike a good balance between quickness and convenience, cost and security for building this website.","explanation":"Admittedly verbose and lengthy, this type of question will test the time-management ability along with technical knowledge. It is clearly stated in the question that the website itself does not need to be served over HTTPS, only the pre-signed download URL (which will be used to download the full whitepaper after successful payment) needs to be HTTPS. Therefore, the choice that eliminates S3 static website citing the reason that S3 static websites do not support HTTPS is incorrect. While it is actually correct that S3 static websites do not support HTTPS, the reason for not choosing S3 static websites as a solution cannot be this fact, as the requirement clearly states that the website may be served over HTTP. Only the part that needs HTTPS can be developed outside of S3 static website, as it must be, as S3 static websites are, well, static, and would not be of much help running server-side code to generate pre-signed URLs. Also, the same choice uses EC2 instances and ELB - an approach that is not as quick or convenient as using a S3 static website.\nThe choice that uses Cloudfront and Origin Access Identity (OAI) is incorrect as we cannot use OAI if the origin is an S3 static website endpoint. OAI can only be used if the origin is an S3 bucket (and not a website). Also, using Cloudfront will unnecessarily increase the cost, as the requirement states that the website is regional.\nBoth the remaining choices are actually working solutions. Both will work. However, the best answer is the one that uses S3 static website, as it is the least amount of effort. Using API Gateway with Lambda proxy integration to serve the entire website is more work compared to using an S3 static website. Even when using S3 static website, API Gateway and Lambda needs to be used to generate the pre-signed URLs after the end-user pays for the whitepapers, but that is a small part compared to the full website.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/","title":"Various ways to chain Cloudfront and S3 for hosting websites"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html","title":"Restricting Access to Amazon S3 Content by Using an Origin Access Identity"}],"answers":[{"id":"01d44a44c543778d9db261fe16c999bd","text":"Serve the entire website using API Gateway with proxy Lambda integration. When the base website is requested, the Lambda function will read an S3 bucket that hosts the metadata and whitepapers. It will parse the JSON metadata and generate HTML for listing the whitepaper metadata in a tabular format. If an end-user makes a payment, have the client-side Javascript call a different method on the API Gateway to retrieve the pre-signed URL. Generate the pre-signed URL in a second Lambda function using the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the API Gateway URL","correct":false},{"id":"5bfca7bbb3e2e03a395345f5959db09b","text":"Use S3 static website hosting to host the website, including the metadata, whitepapers and Javascript files, where the client-side Javascript code will read the metadata stored as JSON and parse it to render in a tabular format. Deploy a Cloudfront web distribution to enable https between end-users and Cloudfront, using the static website endpoint as the origin. Use Origin Access Identity to restrict access to the S3 content only to the Cloudfront web distribution. If an end-user makes a payment, have the client-side Javascript call an API Gateway over https to retrieve the pre-signed URL. Deploy a Lambda function as the API Gateway backend. Generate the pre-signed URL in the Lambda function using the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the Cloudfront endpoint","correct":false},{"id":"1ffc7a9d39eeaa99af079745d4ed6ae0","text":"S3 static website cannot be used at all as they do not support https. Use S3 to host the metadata and whitepapers. Use a couple of EC2 instances in different Availability Zones to run the website, with an ELB distributing traffic to them. The website components deployed on the EC2 instances will handle both serving the website (by reading the S3 bucket using an EC2 Role and parsing the JSON metadata to generate HTML) and the subsequent request for the full whitepaper. While responding to the request for pre-signed URL, it will use the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the ELB DNS Name","correct":false},{"id":"06ec95a0c14f828594dd852570049d26","text":"Use S3 static website hosting to host the website, including the metadata, whitepapers and Javascript files, where the client-side Javascript code will read the metadata stored as JSON and parse it to render in a tabular format. If an end-user makes a payment, have the client-side Javascript call an API Gateway over https to retrieve the pre-signed URL. Deploy a Lambda function as the API Gateway backend. Generate the pre-signed URL in the Lambda function using the API credentials of an IAM user that is saved in AWS Secrets Manager. Use a Route 53 CNAME or ALIAS record to point the website domain to the S3 Static website endpoint","correct":true}]},{"id":"2afb0db9-a43f-4e97-8272-5ce423ded162","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They notice on their RDS console that one of their mission-critical production databases has an \"Available\" listed under the Maintenance column.  They are extremely concerned that any sort of updates to the database will negatively impact their DB-intensive mission-critical application.  They at least want to review the update before it gets applied, but they are not sure when they will get around to that.  What do you suggest they do?","explanation":"For RDS, certain OS updates are marked as Required. If you defer a required update, you receive a notice from Amazon RDS indicating when the update will be performed. Other updates are marked as Available, and these you can defer indefinitely.  You can also apply the maintenance items immediately or schedule the maintenance for your next maintenance window.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html","title":"Maintaining a DB Instance - Amazon Relational Database Service"}],"answers":[{"id":"66ed278812c2a52913954afa52952b97","text":"The maintenance will be automatically performed during the next maintenance window.  They have no choice in the matter.","correct":false},{"id":"6aeada3a9046319bc858239b15031f66","text":"Backup the database immediate because the updates could come at any time.  If possible, create a Read Replica to act as a standby in case problems are introduced with the update.","correct":false},{"id":"b9f81cfac73d5c4d2871a7f969ecb9f3","text":"Disable the Maintenance Window so the updates will not be applied.","correct":false},{"id":"93df9eb71cb48a0c821fe555e35f5b62","text":"Defer the updates indefinitely until they are comfortable.","correct":true},{"id":"35c3b3d1cb8d9866e1abc96dec8bfa3f","text":"Apply the maintenance items immediately.  AWS validates each update with each customer's RDS instances using a shadow image so there is little risk here.","correct":false}]},{"id":"93badb2c-68ab-4715-b29e-1209af0c7b27","domain":"awscsapro-domain2","question":"You are a developer for a Aerospace company.  As part of an outreach and education program, the company has financed the construction of a free public service that provides weather forecasts for the sun.  Anyone can make a call to this REST service and receive up-to-date information on forecasted sun flare or sun spots that might have an electromagnetic impact here on Earth.  You are in the final stages of developing this new serverless application based on DynamoDB, Lambda and API Gateway.  During performance testing, you notice inconsistent response times for the service.  You had expected the API to be relatively consistent since its just retrieving data from DynamoDB and returning it as JSON via the API Gateway.  What might account for this variation in response time?","explanation":"Inconsistent response times can have a few different causes.  The exact nature of the testing is not explained but we can anticipate a few causes.  If you have enabled API Gateway caching, the gateway can return a result from its cache without having to go back to a supplying service or database.  This can result in various response rates depending on if an item is in the cache or not.  (The question did not specify we had slow response...just inconsistent response which could be a response faster than we expected.)  When a Lambda function is run for the first time or after an update, AWS must provision the Lambda environment and pull in any external dependencies.  This can result in a slower response time at first but faster later.  Also, if we do not have sufficient RCU for our DynamoDB table, we could run into throttling of the reads which could appear as inconsistent response times.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html","title":"AWS Lambda Execution Context - AWS Lambda"},{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"Enable API Caching to Enhance Responsiveness - Amazon API Gateway"}],"answers":[{"id":"4922e0fbbf140cd0dbb6458bbadbf268","text":"The data is being updated on DynamoDB at the exact same time you are trying to read it.","correct":false},{"id":"43fb818a88f47b6b3b136abdd40a15a6","text":"There are not enough open inbound ports in your VPC.","correct":false},{"id":"3bd10d778b04bfa32a7338a04e832d38","text":"The CloudFront distribution used by API Gateway is not deployed fully yet.","correct":false},{"id":"b03fe2e16e6904dc4b5fa34c91c1f855","text":"You are using HTTP rather than HTTPS.","correct":false},{"id":"f13e59f5ef22ace9340e24f434eb09cb","text":"You are experiencing a cold start.","correct":true},{"id":"158838df2d14636ed1429fffbcc6825e","text":"Your DynamoDB RCUs are underprovisioned.","correct":true},{"id":"4c3254ba1ddb0fb7a3ce3824707a7105","text":"You have enabled caching on the API Gateway.","correct":true}]},{"id":"b401741c-5b37-4b47-8e61-7802fbc9d7d6","domain":"awscsapro-domain1","question":"You are helping a client consolidate several separate accounts into a single account.  This consolidation will result in approximately 50 new VPCs in their one account.  They want to continue to use Route 53 for DNS but only want it accessible privately. How can you accomplish this most efficiently?","explanation":"Private Hosted Zones provide DNS services to VPCs but cannot be access from the internet.  They can be associated with VPCs either by the console, CLI or programmatically via SDK.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html","title":"Associating More VPCs with a Private Hosted Zone - Amazon Route 53"}],"answers":[{"id":"f74daa2300ac594c111ca9fce198f19c","text":"Create a central DNS server using EC2 and BIND.  Configure Route 53 to reference this DNS server as a resolver.  Update DNS records at the registrar to point to the central DNS.","correct":false},{"id":"82d5ef7e7176200aa4350ef90dd4c354","text":"Create a Public Hosted Zone within Route 53 and associate it to each VPC.  Configure a NACL on each VPC to deny inbound DNS queries (UDP port 53).","correct":false},{"id":"3cc28b12f45b3dee8f7f16a0f93d00ce","text":"Install BIND on an EC2 instance in a single VPC.  Create VPC peering connections between the DNS VPC and any new VPCs.  Configure a DHCP Option Set to assign a DNS and link that to each VPC.","correct":false},{"id":"315372936e7ffba65896da15d0f45c2d","text":"Create a Private Hosted Zone within Route 53.  As the new VPCs are created, associate them with the Private Hosted Zone.","correct":true},{"id":"cbffb64b6b43e6fe45496b6e77ce17b8","text":"Create a Private Hosted Zone within Route 53 for each respective VPC.  Configure replication between the private hosted zones to keep records in sync.","correct":false}]},{"id":"f43ec458-0ff5-4633-a57b-6bf82f60bd14","domain":"awscsapro-domain5","question":"You have a target group in an elastic load balancer (ELB) and its target type is \"instance\". You attach an Auto Scaling group (ASG) in the target group. All the instances pass the health check and have a healthy state in the target group. Due to a new requirement, the ELB target group needs to forward the incoming traffic to an IP address that belongs to an on-premise server. The ASG is no longer needed. There is already a VPN connection between the on-premise server and AWS VPC. How would you configure the target in the ELB target group?","explanation":"The target type of existing target groups cannot be changed from \"instance\" to \"IP\". Because of this, users have to create a new target group and set the target type to be \"IP\". After that, the on-premise IP address can be registered as a target. A domain name cannot be registered as a target in the target group. You also do not need to create a new elastic load balancer since you only need a new target group to register the IP address.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#target-type","title":"Target type in ELB target group"}],"answers":[{"id":"99f63c1cf5bcfbcb188328714abb8ed3","text":"Register a record set in AWS Route 53 to forward a domain name to the on-premise IP address. Modify the target group to register the domain name as its target. Remove the previous Auto Scaling group from the target group.","correct":false},{"id":"49646fca04901301d145a2a814a7e481","text":"In the elastic load balancer, create a new target group with an \"IP\" target type. Register the on-premise IP address as its target. Monitor if the target becomes healthy after some time. Remove the old target group.","correct":true},{"id":"e0b619a421d68626ffb82b1a6e1d22d5","text":"Remove the Auto Scaling group from the target group and modify the target type to be \"IP\". Attach the IP address to the target and set up the IP address and port in the health check configurations.","correct":false},{"id":"265de6fdcab5323b156774dcb949d309","text":"Create a new network load balancer with a new listener and target group. Configure the target type to be \"IP\" in the target group and attach the on-premise IP address to it. Set up the health check using the HTTP protocol.","correct":false}]},{"id":"343601ea-0262-46dc-baab-511550237b8f","domain":"awscsapro-domain2","question":"You work for a distributed large enterprise that uses Splunk as their log aggregation, management and analysis software. The company has recently shown a keen interest in adopting a microservice based architecture and wants to convert some of its applications to Docker containers. They have selected AWS ECS as the orchestration platform for these containers. They are interested only in EC2 Launch Type as the Security Team wants to harden the EC2 instances with policies they want to control. However, the Chief Architect is concerned about hardcoding the Splunk token inside the container code or configuration file. Splunk requires any logging request to include this token. The Chief Architect wants to know if there is a way to pass the production Splunk token to ECS at runtime so that the container tasks can continue logging to Splunk without exposing the production secret to all developers, and if yes, how. Select the best answer.","explanation":"ECS Task Definition file supports two ways of specifying secretOptions in its logConfiguration element - AWS Systems Manager Parameter Store and AWS Secrets Manager. This question only deals with only one of these two ways, as it does not mention AWS Secrets Manager at all. However, the real focus of the question is how KMS is used by AWS Systems Manager Parameter Store. There is actually nothing special about how Parameter Store deals with KMS. Hence, the question actually is just about knowing the ways KMS Keys can be used - they can either be customer-managed, or AWS managed or both AWS and customer-managed.\nAnother aspect tested by the question is whether to use Task Role or EC2 Instance Role while granting an ECS Cluster permission to access resources during container execution. AWS recommends using the Task Role in this scenario, hence we can eliminate the two choices that specify using EC2 Instance Roles.\nThe option that says that the encryption key must only be stored by the customer is incorrect, as we can use KMS to store the key. The options that state the only supported management options for the KMS-stored keys are either only customer-managed or only AWS managed are both incorrect. The correct option identifies that KMS keys can either be customer-managed or AWS managed.\nOne thing to note is that this question does not need any prior knowledge of Splunk. Everything that needs to be known about Splunk is stated as part of the question. The AWS SA-P exam normally does not require the candidate to know about any 3rd-party product or services, but such services can be explicitly named.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html#secrets-logconfig","title":"Injecting Sensitive Data in a Log Configuration"},{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html","title":"How AWS Systems Manager Parameter Store Uses AWS KMS"},{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html","title":"IAM Roles for Tasks"}],"answers":[{"id":"8a8783b1fab2fb72bc61295df2630a46","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can only be stored and managed by the customer - therefore the value must be encrypted by the customer before writing to the Parameter Store and decrypted by the customer after reading from the Parameter Store. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. Each EC2 node must have an associated EC2 Instance Role that allows it to read AWS Systems Manager Parameter Store","correct":false},{"id":"a161521ba68b3fbb6a60532e1bf664c0","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS and can either be customer-managed or be AWS-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. The task definition must include a task role that allows the container task to read AWS Systems Manager Parameter Store and decrypt using a KMS key, this latter permission being required only if the key is customer-managed and not AWS managed","correct":true},{"id":"c3037df22b608e2533a356125fab5322","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS but must be solely customer-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. Each EC2 node must have an associated EC2 Instance Role that allows it to read AWS Systems Manager Parameter Store and decrypt using a customer-managed KMS key","correct":false},{"id":"ee244c2ca9783d96974f0c603a4aa548","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS but must be solely AWS-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. The task definition must include a task role that allows the container task to read AWS Systems Manager Parameter Store and decrypt using an AWS-managed KMS key","correct":false}]},{"id":"05e085a9-4de3-46fe-9470-10c7f2faba57","domain":"awscsapro-domain5","question":"You are consulting with a client who is in the process of migrating over to AWS.  Their current on-prem Linux servers use RAID1 to provide redundancy.  One of the big benefits they are looking forward to with moving to AWS is the ability to create snapshots of EBS volumes without downtime.  Right now, they intend on migrating the servers over to AWS and retaining the same disk configuration.  What is your advice for them?","explanation":"Because RAID is based upon multiple volumes being in sync, taking snapshots of an individual volume that's part of a active and mounted RAID array would not create a proper backup.  You must first unmount the RAID volume and then create the snapshots of the component volumes.  This of course means any data on the RAID volume would be unavailable.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html","title":"RAID Configuration on Linux - Amazon Elastic Compute Cloud"}],"answers":[{"id":"99a9d29ef15a0ced996c1510ff6d8f6a","text":"EC2 does not support RAID configurations.","correct":false},{"id":"bee3a756d6bfddce4e9917e171a4b0e2","text":"Consider using RAID0 when on AWS for performance reasons.","correct":false},{"id":"21a6a5218cf9778e0184eed7897c54ce","text":"Consider using RAID6 rather than RAID1 on AWS for performance reasons.","correct":false},{"id":"ead9938f5d4fc4d2df30763406b6a8e5","text":"Consider using RAID10 when on AWS because it offers the best of both RAID0 and RAID1.","correct":false},{"id":"73b207bc7a947de1eed26bc058b4b67b","text":"If snapshots without downtime are the priority, do not use RAID.","correct":true}]},{"id":"338ce579-a236-4282-9670-8da3b0baf2e9","domain":"awscsapro-domain3","question":"You are helping a Retail client migrate some of their assets over to AWS.  Presently, they are in the process of moving their Enterprise Data Warehouse.  They are planning to re-host their very large Oracle data warehouse on EC2 in a high availability configuration across AZs.  They presently have several Scala scripts that process some detailed Point of Sale data that is collected each day.  The scripts perform some aggregation on the data and import the aggregate into their Oracle database.  They want to move this process to AWS as well.  Which option would be the most cost-effective way for them to do this?","explanation":"AWS Glue is a fully managed extract, translate and loading service and is compatible with Scala.  EMR could do this but represents more overhead than necessary.  Lambda is not compatible with Scala and migrating to Redshift does not bring anything in this case if the customer wants to retain their Oracle database.","links":[{"url":"https://aws.amazon.com/glue/faqs/","title":"AWS Glue Features - Amazon Web Services"}],"answers":[{"id":"04578ae8419780f9dc441d01fe11582d","text":"Create Lambda functions using your Scala scripts.","correct":false},{"id":"1a4de6676c8c078310e08aad71d9dce6","text":"Migrate the processing to AWS EMR.","correct":false},{"id":"a445a1a877009cd7c31858687a818116","text":"Import your Scala scripts into AWS SCT for processing.","correct":false},{"id":"8b01d948d5ad2f4b1c8e817c2d98e7c2","text":"Migrate the processing to AWS Glue.","correct":true},{"id":"4f1ff8b853c3ba363bdd2bda53538ab4","text":"Migrate from Oracle to Redshift and use Kinesis Firehose.","correct":false}]},{"id":"969eccbb-023f-4b5d-ba4c-d14c22eadc02","domain":"awscsapro-domain4","question":"Your company has been running its core application on a fleet of r4.xlarge EC2 instances for a year.  You are confident that the application has a steady-state performance and now you have been asked to purchase Reserved Instances (RIs) for a further 2 years to cover the existing EC2 instances, with the option of moving to other Memory or Compute optimised instance families when they are introduced.  You also need to have the option of moving Regions in the future. Which of the following options meet the above criteria whilst offering the greatest flexibility and maintaining the best value for money.","explanation":"When answering this question, it's important to exclude those options which are not relevant, first.  The question states that the RI should allow for moving between instance families and this immediately rules out Standard and Scheduled RIs as only Convertible RIs can do this.  Of the 2 Convertible RI options, on can be ruled out as it suggests selling unused RI capacity on the Reserved Instance Marketplace, but this is not available for Convertible RIs and therefore that only leaves one answer as being correct.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html","title":"Types of Reserved Instances (Offering Classes) - Amazon Elastic Compute  Cloud"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html","title":"Scheduled Reserved Instances - Amazon Elastic Compute Cloud"}],"answers":[{"id":"188dc576a2e69c4182330092ab7f3786","text":"Purchase a Scheduled RI for 3 years, then sell the unused RI on the Reserved Instance Marketplace","correct":false},{"id":"89987ea3ddc88d352336561f25d83387","text":"Purchase a 1 year Standard Zonal RI for 3 years, then sell the unused RI on the Reserved Instance Marketplace","correct":false},{"id":"b46dcc09115bb8c1683455addbc4fa46","text":"Purchase a 1 year Convertible RI for each EC2 instance, for 2 consecutive years running","correct":true},{"id":"41620e7974f0d2cca7f57d1972e30387","text":"Purchase a Convertible RI for 3 years, then sell the unused RI on the Reserved Instance Marketplace","correct":false}]},{"id":"0a4b2449-9275-4c2f-af02-0f8c51614f3a","domain":"awscsapro-domain2","question":"You are part of a business continuity team at a consumer products manufacturer.  In scope for the current project is the company web server which serves up static content like product manuals and specification sheets which customers can download.  This landscape consists only of a single NGINX web server and 5TB of local attached storage for the static content.  In the case of a failover, RTO has been defined as 15 minutes with RPO as 24 hours as the content is only updated a few times a year.  Staff reductions and budget constraints for the year mean that you need to carefully evaluate and choose the most cost-effective and most automated solution in the case of a failover.  Which of the following would be the most appropriate given the situation?","explanation":"In this case, the most cost-effective and most automated way to ensure the reliability statistics would be to migrate the static content to S3.  This option has built-in robustness and will cost less than any other option presented.","links":[{"url":"http://d36cz9buwru1tt.cloudfront.net/AWS_Disaster_Recovery.pdf","title":"Using Amazon Web Services for Disaster Recovery"}],"answers":[{"id":"2d14eb477a2f2c9dc3605ea5740297cd","text":"Create a small pilot-light EC2 instance and configure with NGINX. Configure a CRON job to run every 24 hours that syncs the data from the on-prem web server to the pilot-light EC2 EBS volumes.  Configure an Application Load Balancer to direct traffic to the on-prem web server until a health check fails.  Then, the ALB will redirect traffic to the pilot light EC2 instances. ","correct":false},{"id":"925fb4f48c2c90011e7e1f92d3412dcd","text":"Migrate the static content to an EFS share.  Mount the EFS share via NFS from on-prem to serve up the web content.  Configure another EC2 instances with NGINX to also mount the same share.  Upon fail-over, manually redirect the Route 53 record for the web server to the IP address of the EC2 instance.","correct":false},{"id":"69fd92fc5be4948bfc0128d02ed2f392","text":"Install the CloudWatch agent on the web server and configure an alarm based on a health check.  Create an EC2 replica installation of the web server and stop the instances.  Create a Lambda function that is triggered by the health check alarm which starts the dormant EC2 instance and updates a DNS entry in Route 53 pointing to the new server.","correct":false},{"id":"bceaccaea071754d3724eaf31f0f6189","text":"Migrate the website content to an S3 bucket configured for static web hosting.  Create a Route 53 alias record for the web server domain.  End-of-life the on-prem web server.","correct":true},{"id":"46b84866da301243767946743c6024a1","text":"Download and configure the AWS Storage Gateway, creating a volume which can be replicated to AWS S3.  Attach that volume to the web server via iSCSI and migrate the content to that Storage Gateway volume.  Locate an AMI from the AWS Marketplace for NGINX.  If a failover is required, manually launch the AMI and run an RSYNC between the on-prem server and the EC2 server to migrate the content.","correct":false}]},{"id":"b533b3c1-222f-4f33-99da-2c828e98ff91","domain":"awscsapro-domain5","question":"You have run out of root disk space on your Windows EC2 instance.  What is the most efficient way to solve this?","explanation":"We can easily increase the size of an EBS from the console or the CLI (using modify-volume) but then we also need to allow the OS to expand the resized volume so we can use it.  For Windows Server, we could use Disk Manager.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/expand-ebs-root-volume-windows/","title":"Expand the EBS Root Volume of Your EC2 Windows Instance"}],"answers":[{"id":"38862a719074689f75df6a20a42f7df7","text":"Use AWS System Manager Run service to remotely execute a PowerShell script using AWS Tools for PowerShell to expand the volume using the ModifyInstance command.","correct":false},{"id":"229b013eff2d5f53df7b9c3a60bd2418","text":"From the AWS Console, select Modify Volume for the EBS volume.  Enter the new size and confirm the change.  Connect to your Windows instance and use Disk Manager to extend the newly resized volume.","correct":true},{"id":"346ac05b4353d34c630eb6d233f8a35d","text":"Compress all files on the root volume using the built-in zip utility.  Modern versions of Windows will automatically unzip the files when they are accessed.","correct":false},{"id":"b893490015da5047b17b1220d43f4a1c","text":"From the AWS CLI, use the \"modify-instance\" command for EC2 to resize the volume to a larger size.  Using RDP, connect to the Windows instances and use Disk Manager to expand the volume.","correct":false}]},{"id":"54d12a9a-149b-42a7-8491-300583d5c2b8","domain":"awscsapro-domain5","question":"A client is trying to setup a new VPC from scratch.  They are not able to reach the Amazon Linux web server instance launched in their VPC from their on-prem network using a web browser.  You have verified the internet gateway is attached and the main route table is configured to route 0.0.0.0/0 to the internet gateway properly.  The instance also is being assigned a public IP address.  Which of the following would be another potential cause of the problem?","explanation":"For an HTTP connection to be successful, you need to allow port 80 inbound and allow the ephemeral ports outbound.  Additionally, it is possible that the subnet is not associate with the route table containing the default route to the internet.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html","title":"Troubleshooting Connecting to Your Instance - Amazon Elastic Compute Cloud"}],"answers":[{"id":"c9f886542d1dabe99bc64dd39c5e1615","text":"The inbound security group allows port 80 and 22 only.","correct":false},{"id":"d0f5e76f9fc753305b11c4c3a11e97ef","text":"The IAM role assigned to the LAMP instances does not have any policies assigned.","correct":false},{"id":"2bde109ce87f4a4f513679f31116184d","text":"The instance does not have an elastic IP address assigned. ","correct":false},{"id":"32cdd854c9d71059eac396dd1249830e","text":"The subnet of the instance is not associated with the main route table.","correct":true},{"id":"01b7463243eb231b840fcd4b737e044b","text":"The default route to the internet gateway is incorrect.","correct":false},{"id":"25d3393c550b8dbc2179367832573598","text":"The outbound network ACL allows port 80 and 22 only.","correct":true},{"id":"c7ca1b6a8fe855bda71123163488960b","text":"The customer has disabled the ec2-user account on the Amazon Linux instance.","correct":false}]},{"id":"95f1d7a8-c3d4-4fec-952a-72385aa8b4c8","domain":"awscsapro-domain5","question":"You are consulting for a company that performs specialized customer data analytics.  Their customers can upload raw customer data to a website and receive back demographic statistics.  Their application consists of a REST API created using PHP and Apache.  The application is self-contained and works in real-time to return results as a JSON response to the REST API call.  Because there is customer data involved, company policy states that data must be encrypted in transit and at rest.  Sometimes, there are data quality issues and the PHP application will throw an error.  The company wants to be notified immediately when this occurs so they can proactively reach out to the customer.  Additionally, many of the company's customers use very old mainframe systems that can only access internet resources using IP address rather than a FQDN.  Which architecture will meet these requirements fully?","explanation":"The requirement of a static IP leads us to a Network Load Balancer with an EIP.","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/","title":"Elastic Load Balancing Features"}],"answers":[{"id":"5e4c5230c7e08202a0ea0575d5412d57","text":"Provision a Network Load Balancer with an EIP in front of your EC2 target group.  Install the CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.  Terminate SSL on the EC2 instances.","correct":true},{"id":"0c8e5c32f081b0484e86b71651ae3642","text":"Provision a Network Load Balancer in front of your EC2 target group and terminate SSL at the load balancer using Certificate Manager.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"569eec0061a1a97be77e3bdab43a1756","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Offload SSL termination using AWS KMS.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"e53df806e37b325d7f61be27772875f1","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Enabled SSL termination on the API Gateway using Certificate Manager.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"434ce04b2c3a4d2e679d37df43de2585","text":"Provision an Application Load Balancer with an EIP in front of your EC2 target group and terminate SSL at the ALB.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"a9ed133e35b8332aea2bf603521b891a","text":"Provision an Application Load Balancer in front of your EC2 target group and offload SSL to CloudHSM.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch and configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from CloudHSM.","correct":false}]},{"id":"f7d7767b-9159-4e53-8e37-ff9bf41ace17","domain":"awscsapro-domain5","question":"You are working with a client to help them design a future AWS architecture for their web environment.  They are open with regard to the specific services and tools used but it needs to consist of a presentation layer and a data store layer.  In a brainstorming session, these options were conceived.  As the consulting architect, which of these would you consider feasible?","explanation":"The only two options which contain feasible options are the Beanstalk and S3/Dynamo methods.  One would not create a new K8s deployment for for a new web update.  CodeBuild and AWS Config are not the correct tools for how they are being suggested.","links":[{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild – Fully Managed Build Service"}],"answers":[{"id":"f5c7f9be39386b15747c0fe57d5040ba","text":"Deploy Kubernetes on an auto-scaled group of EC2 instances.  Define pods to represent the multiple tiers of the landscape.  Use ElastiCache for Memcached to offload queries from a Multi-AZ RDS instance.  To deploy changes to the landscape, create a new EKS deployment containing all the updated service containers and deploy them to replace all the previous existing tiers.  Ensure the DevOps team understands the rollback procedures.","correct":false},{"id":"c27a151a7715575fb1ebf0225c6aee09","text":"Use the AngularJS framework to create a single-page application.  Use the API Gateway to provide public access to DynamoDB to serve as the data layer.  Store the web page on S3 and deploy it using CloudFront.  When changes are required, upload the new web page to S3.  Use S3 Events to trigger a Lambda function which expires the cache on CloudFront.","correct":true},{"id":"01a7f4c09f65e41884b0a72843a8d55b","text":"Deploy an auto scaling group of EC2 instances behind an Application Load Balancer.  Provision a Mulit-AZ RDS instance to act as the data store, configuring a caching layer to offload queries from the database.  Use a User Script in the AMI definition to download the latest web assets from S3 upon boot-up.  When changes are required, use AWS Config to automatically fetch a new version of web content from S3 when a new version is created.","correct":false},{"id":"ca4cc92538f3b73d221c9b5d4378e1f8","text":"Setup a traditional three tier architecture with a CloudFormation template per tier and one master template to link in the others.  Configure a CodeBuild stack and set this stack to perform automated Blue Green deployments whenever any code change is made.","correct":false},{"id":"d6a73928290b05c25d87e26ece9e94a6","text":"Create a monolithic architecture using Elastic Beanstalk configured in the console.  Create an RDS instance outside the Beanstalk environment and configure it for multi-AZ availability.  When a new landscape change is required, use a command line script to implement the change.","correct":true}]},{"id":"e720cd54-de67-42de-ba10-593dee0582e6","domain":"awscsapro-domain3","question":"You are the Enterprise Architect in a Risk Quantification firm. The firm has a website which end-users can use to apply for loans and also track the status of their loan application if they log in. When a loan application comes in, several downstream systems need to independently process the application. Right now, the website server-side code invokes these systems one after the other, synchronously, in a tight loop. If one of these downstream systems times out or throws an exception, the entire loan application processing errors out. Even if none of these downstream systems fail, the time it takes to process a loan application is very high due to the serial nature of these systems being invoked. Your CTO wants only the loan-processing application moved to the AWS cloud and re-architected at the same time.\nThe downstream systems are all hosted on-premises and will continue to remain on-premises. They expose REST endpoints that accept POST HTTPS requests, use self-signed certificates and respond synchronously only when they are done processing an application. After re-architecture, all downstream systems must independently start processing an incoming loan application simultaneously.\nYour CTO wants to know how the loan-processing website application can be architected in the AWS Cloud, and what supporting changes will be needed in the downstream systems on-premises. He wants to minimize code changes to the downstream on-premises systems. Choose the best option","explanation":"This is an example of a verbose question with verbose answer choices. You can expect a few such questions in the exam, testing your time management skills. Try to vertically scan the answers to see which parts differ between them. Sometimes, though the answers seem big, a large part of each is identical. You can ignore those parts, as there is nothing to choose between the.\nAmong the four choices, two use SQS and two use SNS to feed the incoming loan applications to the downstream systems. You cannot automatically eliminate either SQS or SNS, as a working solution can be designed with either.\nLet us see how we can achieve this using SNS first. The basic requirement here is fan-out - a single loan application must be processed by several downstream systems, so there are multiple consumers. Hence, SNS is a natural fit. SNS supports multiple subscribers for a topic. SNS also supports HTTP/HTTPS subscribers. SNS makes POST REST API call to as many HTTP/HTTPS subscribers exist on the topic, so it fits the bill. However, there is a small problem - the requirement states that the downstream systems must be changed as little as possible. If we follow this design, we must change the HTTP Listening part of the downstream systems significantly. Because SNS is directly calling them now, SNS will use its own headers and body format. In fact, SNS POST-s two kinds of messages - one is Subscription Confirmation and one is Notification. A special HTTP header (x-amz-sns-message-type) has the right type in its value. The server side now must parse this header out and look for only the Notification type of message. The body itself will then be JSON formatted with the payload. While the server is probably used to process just the core payload (loan application data) as the HTTP body, the same will now be hidden inside a JSON field called Message inside the request body. Additionally, the downstream systems will have to deal with SNS retries, thus the loan application part must be made idempotent (if the same loan application lands twice, it will ignore the duplicates). Thus, though it is technically possible to design the solution using SNS, it will result in a lot of changes in the downstream systems. Hence, though the SNS option will work, it is not the correct answer because of this reason.\nNow, let us see how we can design this using SQS. While SQS does not support fan-out (multiple consumers for the same message), the proposed solution uses a Lambda function to achieve fan-out. The Lambda function will pick up the message, and then call the downstream systems one by one. The key to making this work is, of course, to modify the downstream systems from synchronous monolithic beasts to asynchronous servers so that they can instantly respond to the Lambda function and then continue to process the application. We will then have to provide a callback for when it is done. The solution uses an API Gateway for that purpose. Overall, the solution is elegant, and changes to the downstream systems are less than what SNS requires. Hence, SQS is the correct answer.\nNote that one version of the SNS design proposes to retain the synchronous nature of the downstream systems. That will not work as SNS will not wait more than 15 seconds for a response. The response will then be lost and the main website app will never know the results from the downstream systems.\nAlso, note that though SNS requires the HTTPS subscriber to present a trusted CA-signed certificate, there is no such requirement for Lambda because Lambda is basically your code, you can decide to trust anyone.","links":[{"url":"https://docs.aws.amazon.com/sns/latest/dg/sns-http-https-endpoint-as-subscriber.html","title":"Using Amazon SNS for System-to-System Messaging with an HTTP/S Endpoint as a Subscriber"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html","title":"Using AWS Lambda with Amazon SQS"}],"answers":[{"id":"3fb725a8e71fa96168f18e50a146b4f0","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed and (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems","correct":true},{"id":"eec2740374df8038093d636a17252168","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Override the default delivery policy on the subscriber endpoint to remove retries so that downstream systems do not have to worry about synchronous responses taking time or idempotency of retries. Make the following changes in the downstream systems - (a) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (b) Procure server certificates from a trusted Certificate Authority (CA) instead of using the self-signed certificate as SNS will not be able to POST to a server with a self-signed certificate","correct":false},{"id":"3d032e65493c0733ebe65683fb66a562","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SQS Standard Queue. Configure a Lambda listener for the queue. The Lambda function will invoke the REST APIs for all downstream systems in a loop. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Make them idempotent in case Lambda times out or errors and a given loan application re-appears in the queue only to be picked up by another Lambda instance and re-sent to the downstream systems and (c) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as your Lambda function will not be able to POST to a server with self-signed certificate","correct":false},{"id":"48d42d3290142cbfc8207e042690b35f","text":"For the website that accepts loan applications, run it on EC2 instances in an Auto Scaling Group spanning multiple Availability Zones with an Elastic Load Balancer. It should publish each incoming loan application to an SNS Topic. Configure the SNS topic to have multiple HTTPS subscribers - add each of the downstream system REST API endpoints as a subscriber. Make the following changes in the downstream systems - (a) Make them asynchronous - they should respond with HTTP 201 Accepted immediately without waiting so that SNS does not retry, and then later post the results to a new API Gateway API that will invoke a second Lambda function to update an RDS database which the loan application website can later read to provide status as needed (b) Parse SNS-specific HTTP headers and JSON body format to extract the payload correctly (c) Make them idempotent for the same loan application as SNS may retry in case of lost messages or timeouts (d) Procure server certificates from a trusted Certificate Authority (CA) instead of using self-signed certificate as SNS will not be able to POST to a server with self-signed certificate","correct":false}]},{"id":"49107f33-5b31-4d7e-a2cb-95f3ce8a2d75","domain":"awscsapro-domain1","question":"Your customer has setup AWS Organizations to help manage a collection of AWS Accounts.  They are running into a problem though and need your help.  They have created accounts for each business unit and applied SCPs to those OUs. However, they notice that root accounts in in those sub-accounts can still change root access keys and disable MFA.  How do you instruct your customer?","explanation":"Service Control Policies can control many aspects but they cannot restrict root account actions of changing root access keys or disabling MFA.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"77df34553819fdc2e31fb79762948993","text":"You can establish a trust with the top-level account and use the \"organizations:ServicePrinciple\" condition key to restrict root access at the sub-account level.","correct":false},{"id":"94a2f948d3d2f9c317a6ebb1f5a24ea5","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the specific sub-accounts.","correct":false},{"id":"3d722952f024bcec9174a311c17dcc14","text":"You can not use SCPs to restrict root account activities of changing the root password or managing MFA settings.","correct":true},{"id":"194cc2d07b5c378b62b1e090f0aea956","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the entire OU in the root account.","correct":false}]},{"id":"c3ac5de9-a343-4cde-af1b-6c9f89824d2f","domain":"awscsapro-domain5","question":"An external auditor is reviewing your process documentation for a Payment Card Industry (PCI) audit.  The scope of this audit will extend to your immediate vendors where you store, transmit or process cardholder data.  Because you do store cardholder data in the AWS Cloud, the auditor would like to review AWS's PCI DSS Attestation of Compliance and Responsibility.  How would you go about getting this document? ","explanation":"AWS Artifact provides on-demand downloads of AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use. You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls.","links":[{"url":"https://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html?icmpid=docs_artifact_console","title":"What Is AWS Artifact? - AWS Artifact"}],"answers":[{"id":"d9208942349d1c6f7dbaba3661069bc1","text":"AWS WorkDocs","correct":false},{"id":"1d16d307ee989a80e421198a01993a9c","text":"AWS IAM Console","correct":false},{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false},{"id":"09e838e873f25f954fef911d50b3d1ab","text":"AWS Pinpoint","correct":false},{"id":"63df0d05cd43af35c95cf04d92aaf685","text":"AWS Legal Services website","correct":false},{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":true},{"id":"d7cb47dd1f374d3ed079b14cc6f2cd75","text":"Submit a Support Case requesting the document","correct":false}]},{"id":"3440b6ff-6fe9-495d-a765-f69f6b82a628","domain":"awscsapro-domain3","question":"You work for a health record management company which operates a view-only portal for end-users to check their health records online. Users can also raise disputes if anything is incorrect. This 2-tier website that supports only HTTPS will be moved to the AWS Cloud. There are 5 web servers on-premises and an F5 Load Balancer that controls traffic to these web servers. SSL is terminated at the web servers. An Oracle Real Application Cluster (RAC) serves as the database tier. Due to the sensitive nature of personal health information, the web site uses mutual authentication - the server requests browsers for a valid client certificate before establishing a trusted session.\nSelect two alternate working architectures for this application to be migrated to AWS so that code changes are minimized. Choose two responses each of which can act as an independent and functional solution, and also result in minimum application code changes.","explanation":"The areas tested by this question are as follows.\nFirst, if an ELB is not terminating SSL, its listener protocol cannot be HTTPS - it has to be TCP. Additionally, AWS ELB does not support terminating client-side certificates. Therefore, if a website requires client SSL certificates, and if it also uses AWS ELB, the ELB must let the target EC2 instances to terminate SSL and validate the client certificate. This requires the protocol to be TCP/443. Both these facts (one, SSL is not terminated at the Load Balancer level, and two, mutual authentication is used) are stated explicitly in the question so that the candidate identifies at least one of them and is thus able to conclude that HTTPS is not the correct protocol choice for ELB. Hence, amongst the two choices that use ELB, the one that says TCP is the correct one.\nSecondly, RDS does not support Oracle RAC. Hence, the database tier must use EC2 instances. Thus, for the second alternate solution that uses Route 53 multi-value answer records instead of ELB, we should select the option that deploys EC2 instances for the database tier.","links":[{"url":"https://forums.aws.amazon.com/thread.jspa?threadID=109180","title":"Discussion Forums - HTTPS Client certificate validation while using client ELB"},{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"RDS FAQ-s, search for phrase - Is Oracle RAC supported on Amazon RDS"}],"answers":[{"id":"307f49d40547367c203a0fcfa1a46be8","text":"Migrate the database to RDS Oracle and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":false},{"id":"e7a1d88af1880a82062437a4c1005adf","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":true},{"id":"11a4ad1f948d7b2fd3631cb763fa8a00","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring TCP/443 as listener","correct":true},{"id":"7250b61db9c9e69abf4f9e7bd2bfb268","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring HTTPS/443 as listener","correct":false}]},{"id":"4ebf4191-8562-4f67-945d-ea5aae2f9f26","domain":"awscsapro-domain3","question":"You are consulting for a client who is trying to define a comprehensive cloud migration roadmap.  They have a legacy custom ERP system written in RPG running on an AS400 system.  RPG programmers are becoming rare so support is an issue.  They run Lotus Notes email which has not been upgraded in years and thus out of support.  They do have a web application that serves as their CRM created several years ago by a consulting group.  It is a Java and JSP-based application running on Tomcat with MySQL as the data layer hosted on a Red Hat Linux server. The company is in a real growth cycle and realizes their current platforms cannot sustain them.  So, they are about to launch a project to implement SAP as a replacement for their legacy ERP system over the next year.  What migration strategy would you recommend for their landscape that would allow them to modernize as soon as possible?","explanation":"In this case, retiring Lotus Notes is the better move because it would just prolong the inevitable by simply migrating to EC2.  The CRM system is fairly new and can be re-platformed on Elastic Beanstalk.  Due to the impending ERP upgrade, it makes no sense to do anything with the legacy ERP.  It would take lots of work to port over an RPG application to run on AWS--if it's even possible.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud | AWS Cloud Enterprise  Strategy Blog"}],"answers":[{"id":"c7be9ce93a4a7a74b44e281cb697dcc4","text":"Begin a product search for a new CRM system that is cloud-ready.  Once identified, migrate the existing CRM into the new CRM system.  Migrate Lotus Notes to Workmail using the AWS Migration Hub.  Invest in training the IT staff about AWS through a Certified AWS Training Partner.  Provision and run the various SAP environments from scratch using EKS.  Do nothing to the legacy ERP until the SAP implementation is complete.","correct":false},{"id":"480496ebe4100958e2f466291752ae2d","text":"Retire the CRM application and migrate the MySQL data over to Aurora.  Use QuickSight to provide access to the application for users.  Pay back support agreements to bring Lotus Notes back into support so it can be upgraded.  Migrate Notes email to EC2 instances.  Invest time in training Operations staff CloudFormation.  Create the complete SAP landscape as scriptable elements.  Do nothing to the legacy ERP platform until the SAP implementation is complete.","correct":false},{"id":"7579c185856bdbbcffbe49a649866488","text":"Rehost Lotus Notes mail on EC2 instances.  Refactor the CRM application to make use of Lambda and DynamoDB.  Use a third-party RPG to Java conversion tool to create Java versions of the legacy ERP to make it more supportable. Invest time in training developers continuous integration and continuous deployment concepts.  Because SAP implementations always take longer than estimated, rehost the legacy ERP system on EC2 instances so the AS400 can be retired.","correct":false},{"id":"20ef21af4ab4ecda70e90e90861b154c","text":"Retire the Lotus Notes email and implement AWS Workmail.  Replatform the CRM application Tomcat portion to Elastic Beanstalk and the data store to MySQL RDS.  Invest time in training Operations staff CloudFormation and spend time architecting the landscape for the new SAP platform.  Do nothing to the legacy ERP platform until the SAP implementation is complete.  ","correct":true}]},{"id":"c2e1b0c2-2dd7-479b-a78e-8ddd1c6d2448","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a problem.  The client has several Ubuntu Linux servers in a private subnet within a VPC.  The servers are configured to use IPv6 only and must periodically communicate to the Internet to get security patches for applications installed on them.  Unfortunately, the servers are unable to reach the internet.  An internet gateway has been deployed in the public subnet in the VPC and default routes are configured.  Which of the following could fix the issue?","explanation":"With IPv6 you only requires an Egress-Only Internet Gateway and an IPv6 route to reach the internet from within a VPC.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/configure-private-ipv6-subnet/","title":"Configure Private IPv6 Subnets"}],"answers":[{"id":"5250cb1aeb48afd18fee3003d4798203","text":"Assign IPv6 EIP's to the servers and configure a default route to the existing internet gateway","correct":false},{"id":"3bd793f68f596bb23004279fc494f9a6","text":"Implement an Egress-only Gateway in the public subnet and configure an IPv6 default route for the private subnet to the gateway.","correct":true},{"id":"8afd1250611c55c8d3301faba9716c97","text":"Implement a NAT Instance in the public subnet and configure the instance as the default route for the private subnet.","correct":false},{"id":"024977f9219f0562163fc06724bc8c99","text":"Implement a Classic Load Balancer in front of the servers and register the servers to the load balancer.","correct":false},{"id":"3528a1941badb3d7623a6df18c766d0a","text":"Create an Internet Gateway in the private subnet and configure the default route for the private subnet to the gateway.","correct":false}]},{"id":"a21ff5b0-e658-4466-9b30-ad1292dde65d","domain":"awscsapro-domain2","question":"A composite materials company is implementing a new monitoring solution on their manufacturing floor. Wi-Fi enabled IoT devices will be registered with AWS IoT Core to read data from numerous control systems. Dashboards will be created in Amazon QuickSight to present aggregate metrics to users (average, min, max, standard deviation, variance, and percentile). Drill down capabilities will also be needed for deeper analyses of exception scenarios. Which architecture will provide the most reliable and performance efficient solution for the company's monitoring needs?","explanation":"The MQTT protocol is a publish/subscribe protocol that provides clients with independent existence from one another, enhancing the reliability of the solution. HTTP is a document-centric ,request-response protocol, requiring more processing and storage overhead for IoT devices. There is no need to use Kinesis Data Analytics in this case because QuickSight can perform all of the aggregate functions required for this use case. Answer number four won't allow for data drill down because the device messages are not written to any persistent storage service.","links":[{"url":"https://aws.amazon.com/iot-core/","title":"AWS IoT Core"},{"url":"https://aws.amazon.com/blogs/compute/visualizing-sensor-data-in-amazon-quicksight/","title":"Visualizing Sensor Data in Amazon QuickSight"},{"url":"https://aws.amazon.com/quicksight/","title":"Amazon QuickSight"},{"url":"https://aws.amazon.com/athena/","title":"Amazon Athena"}],"answers":[{"id":"d3578dd1f9b95b2daf716b5298605a4d","text":"Install HTTP libraries on the IoT devices. Create an IoT Core rule that forwards the HTTP messages to an Amazon Kineses Data Firehose stream, which deposits the data into S3, and writes the data to an Amazon Kinesis Data Analytics stream to aggregate the data. Have an AWS Lambda function trigger to read the aggregate data and deposit it into S3.","correct":false},{"id":"35c4c115504645bedabfaccb200fee7f","text":"Install MQTT libraries on the IoT devices. Create an IoT Core rule that forwards the MQTT messages to an Amazon Kineses Data Analytics stream, which writes aggregate data to an Amazon Kinesis Data Streams stream. Have an AWS Lambda function trigger to read the aggregate data and deposit it into Amazon DynamoDB tables","correct":false},{"id":"2287d42764ee3a22b9bdcb617461dbc6","text":"Install MQTT libraries on the IoT devices. Create an IoT Core rule that forwards the MQTT messages to an AWS Lambda function. Have the Lambda function write the messages to an Amazon Kinesis Data Firehose stream, which deposits them into S3","correct":true},{"id":"48b8bb005b2b6181fa6fc7161df04e9d","text":"Install HTTP libraries on the IoT devices. Create an IoT Core rule that forwards the HTTP messages to an AWS Lambda function. Have the Lambda function write the messages to S3, and to an Amazon Kinesis Data Analytics stream to aggregate the data. Have an AWS Lambda function trigger to read the aggregate data and deposit it into Amazon DynamoDb tables","correct":false}]},{"id":"071d48ba-80e7-420e-969e-98cb2bcfbaa3","domain":"awscsapro-domain2","question":"Across your industry, there has been a rise in activist hackers launching attacks on companies like yours.  You want to be prepared in case some group turns its attention toward you.  The most common attack, based on forensic work security researchers have done after other attacks, seems to be the TCP Syn Flood attack.  To better protect yourself from that style of attack, what is the least cost measure you can take?","explanation":"AWS Shield Standard is offered to all AWS customers automatically at no charge and will protect against TCP Syn Flood attacks without you having to do anything - this meets the requirements of protecting TCP Syn Flood attacks at the lowest cost possible, as described in the question. A more robust solution which is better aligned to best practice would involve a load balancer in the data path, however as this would provide more functionality than required at a higher cost, is not the correct option for this question.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"aec18df10393205d72a60b52cf05bab9","text":"This type of attack is automatically addressed by AWS.  You do not need to take additional action.","correct":true},{"id":"7d8280921ead2af66cf214b774f94306","text":"Re-architect your landscape to use an application load balancer in front of any public facing services.","correct":false},{"id":"82f7ef9f4b3e8e05aef157162915fecf","text":"Subscribe to a Business or Enterprise Support Plan.  Engage AWS DDoS Response Team and arrange for a custom mitigation.","correct":false},{"id":"c50dd258b1fac18bd9368b07bf0fbc11","text":"Implement AWS Shield Advanced and configure it to generate CloudWatch alarms when malicious activity is detected.","correct":false},{"id":"6f87c7b3eda4188070a6635a49710939","text":"Implement AWS WAF and configure filters to block cross-site scripting match conditions.","correct":false}]},{"id":"e3a59454-94fa-4b98-8d8a-80882a7d0e30","domain":"awscsapro-domain5","question":"You are setting up a new EC2 instance for an ERP upgrade project.  You have taken a snapshot and built an AMI from your production landscape and will be creating a duplicate of that system for testing purposes in a different VPC and AZ.  Because you will only be testing an upgrade process on this new landscape and it will not have the user volume of your production landscape, you select an EC2 instance that is smaller than the size of your production instance.  You create some EBS volumes from your snapshots but when you go to mount those on the EC2 instances, you notice they are not available.  What is the most likely cause?","explanation":"In order to mount an EBS volume on an EC2 instance, both must be in the same AZ.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html","title":"Amazon EBS Volumes - Amazon Elastic Compute Cloud"}],"answers":[{"id":"c2f8c791750ad6ce4c8c41ac45b246a0","text":"You have reached your account limit for EBS volumes.  You will need to create a support ticket to request an increase to the limit.","correct":false},{"id":"a9e147ffe118e514fd5069020e86cca6","text":"The instance that you selected for your testing landscape is too small.  It must be equal to or larger than the source of the AMI.","correct":false},{"id":"134c64ea3d25d70a667400e778a13c1a","text":"You created them in a different availability zone than your testing EC2 instance.","correct":true},{"id":"5a9ffc3875f9e3f50c9fc2684a6006b2","text":"The original volume is encrypted and you failed to check the encryption flag when creating the new volume.","correct":false},{"id":"7a11f6b6797c8dd005c9ce25c77a37fe","text":"An SCP applied to the account you are in has restricted you from attaching EBS volumes to instances outside the original VPC","correct":false}]},{"id":"0906c4cf-83a1-4cec-b2ab-c010dcdee73f","domain":"awscsapro-domain1","question":"The alternative energy company you work for has four different business units, each of which would like to run workloads on AWS. Each business unit has it's own AWS account, and a shared services AWS account has been created. An established process for tracking software license usage exists for on-premises applications, but the finance department has concerns that the self-serve nature of the cloud may result in license overages for applications deployed on AWS. You've been tasked with setting up a governance model whereby users are only given access to a standard list of products. Which architecture will provide an effective way to implement the governance requirements and manage software license usage on AWS?","explanation":"AWS Service Catalog allows organizations to create and manage catalogs of approved products for use on AWS. Products are defined as CloudFormation Templates. Software license information can be associated with Service Catalog products through tags. AWS Step Functions can orchestrate the process of incrementing usage counts and notifying of over-usage situations when products are launched by users. AWS License Manager is a robust solution for managing software licenses, but it needs to be coupled with Service Catalog to meet the requirement for limiting access to a standard set of products. A Lambda trigger is not currently available for Service Catalog product deployments. Elastic Container Registry provides tagging at the repository level, not at the individual container image level.","links":[{"url":"https://aws.amazon.com/servicecatalog/","title":"AWS Service Catalog"},{"url":"https://aws.amazon.com/step-functions/","title":"AWS Step Functions"},{"url":"https://aws.amazon.com/blogs/mt/tracking-software-licenses-with-aws-service-catalog-and-aws-step-functions/","title":"Tracking software licenses with AWS Service Catalog and AWS Step Functions"}],"answers":[{"id":"e2441352bdc2d4db956149f0a10b6738","text":"Create Docker images for each of the standardized applications that will be deployed and register them with Amazon Elastic Container Registry (ECR). Populate ECR tags with software license metadata. Create an Amazon DynamoDB table to store software license usage counts. Whenever a container is launched in Amazon Elastic Container Service, trigger an AWS Lambda function to increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"62788b95bb6b249e6511d14023a20364","text":"Implement AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Create an Amazon DynamoDB table to store software license usage counts. Trigger an AWS Lambda function to run each time a Service Catalog product is launched. Have the Lambda function increment license counts in the DynamoDB table and send notifications when overage thresholds are met","correct":false},{"id":"33db838110cfb8b002590cbff630b825","text":"Create Amazon Machine Images for all of the instance configurations that will be deployed. Implement AWS License Manager license configurations and attach them to the AMIs. Create AWS CloudFormation StackSets for the AMIs in the shared AWS account and make them available to users in each business unit","correct":false},{"id":"41d11d1142468e8b0d2a29674d1eaa2c","text":"Deploy AWS Service Catalog and setup the portfolio of standard products in the shared AWS account. Populate Service Catalog product tags with software license information. Create an Amazon DynamoDB table to store software license usage counts. Have Amazon CloudWatch detect when a user deploys a Service Catalog product. Launch an AWS Step Functions process to increment license counts in the DynamoDB table, and send notifications when overage thresholds are met","correct":true}]},{"id":"f3fef147-7b9d-45b6-8b2b-d943c90e8920","domain":"awscsapro-domain5","question":"You are assisting a company in the migration of their container-based web landscape over to Amazon.  They have a total of 21 containers which comprise their DEV, QA and Production environments.  All environment are identical in design and size.  Each environment consists of 3 web servers, 3 app servers and 1 datastore server.  Given the landscape, which of the provided options would be best for them to minimize maintenance?","explanation":"Deploying containers via ECS is a good option but we would want to use the EC2 hosted path.  Fargate is generally used for transient workloads and our datastore would be something we'd want to persist.  We might be able to deploy the data store with RDS, but the question does not make it clear if the data store is an RDS-supported database.  It could be a NoSQL data store or some other database unsupported by RDS.  Similarly, a MEAN stack under Elastic Beanstalk might not be compatible with our landscape either.","links":[{"url":"https://aws.amazon.com/ecs/resources/","title":"Resources for Amazon ECS - run containers in production"}],"answers":[{"id":"e0ea997f77cb156d35ec716cf772c49c","text":"Deploy the web, app and database containers using ECS.  Make use of Fargate for the underlying ECS infrastructure.","correct":false},{"id":"c1354e6d48fedccbf7b4e9c18854d980","text":"Redeploy the web landscape on a MEAN stack under Elastic Beanstalk, making use of auto-scaling groups to right-size the respective environments.  ","correct":false},{"id":"619957021a43a829fbb6228467323ca1","text":"Deploy the web, app and database servers using ECS on EC2.  Purchase 1-year reserved instance contracts for the required EC2 instances.","correct":true},{"id":"f05469f4c7578263f4271e7514c338ef","text":"Deploy the web and app servers in each environment using ECS.  Provision an RDS instance for each environment.  Use AWS Systems Manager to provide a common management console.","correct":false}]},{"id":"599dee9a-6ae7-4c85-a7c6-49edc6ae7d6b","domain":"awscsapro-domain5","question":"A development team is comprised of 20 different developers working remotely around the globe all in different timezones.  They are currently practicing Continuous Delivery and desperately want to mature to true Continuous Deployment.  Given a very large codebase and distributed nature of the team, enforcing consistent coding standards has become the top priority.  Which of the following would be the most effective to address this problem and get them closer to Continuous Deployment?","explanation":"Including an automated style check prior to the build can move them closer to a fully automated Continuous Deployment process.  A style check only before UI testing is too far in the SDLC.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"628453003287afe2200912bb38d0456b","text":"After integrating and load testing, run a code compliance check against the binary created during the build.","correct":false},{"id":"db4ecdbd1c7c8fda5d3e0792a15411ab","text":"Include code style check in the build stage of the deployment pipeline using a linting tool.  ","correct":true},{"id":"1f40591b9d9dbe7a2371e5e82ec05997","text":"Introduce a peer review step into their deployment pipeline during the daily stand-up, requiring sign off for each commit.","correct":false},{"id":"e60e97c4fb6cbf6c1dcf3e806624762f","text":"Require all developers to use the Pair Programming feature of Cloud9.  The commits must be signed by both developers before merging.","correct":false},{"id":"f4cd7f15eb32d8ddd77234b38d0b35b8","text":"Incorporate a code style check right before user interface testing to ensure standards are being followed.","correct":false},{"id":"ec61b60c7eeb3bf9ca9c4149c09c5f3d","text":"Issue a department directive that standards must be followed and require the developers to sign the document.","correct":false}]},{"id":"1727d24a-6bf1-4fc6-b99b-24c0bc4552e9","domain":"awscsapro-domain2","question":"NextGen Appliances Corporation is developing a new smart refrigerator that sends telemetry data to a backend application running on AWS. The refrigerator will also be able to receive commands from a mobile dashboard app that will be provided with the product. Features of the mobile dashboard app include data query, analytics, notifications, and settings commands. The mobile dashboard app will be developed using AWS Amplify and will be deployed to an Amazon S3 bucket configured for web hosting. The software on the refrigerator will be developed with the AWS IoT Device SDK, and will communicate with AWS IoT Core. How should the company architect the backend application to provide management capabilities for the refrigerator from the mobile dashboard app?","explanation":"The smart refrigerator solution has an AWS IoT Core rule set up to route messages to a Lambda function which stores event data in DynamoDB and sends desired notifications to an SNS topic for viewing in the mobile dashboard app. The mobile dashboard app can be developed in AWS Amplify, and deployed to an S3 bucket configured for web hosting. CloudFront can be used to provide public access to the bucket. The app can send RESTful API requests to the backend through API Gateway to a Lambda function that performs the DynamoDB queries and sends commands to the refrigerator through IoT Core. Analytics requests can be routed to QuickSight Mobile, which uses IOT Analytics as its data source. The mobile app won't be able to send settings commands to IoT Core directly. API Gateway and Lambda need to be involved for that. QuickSight can't use Kinesis Data Analytics as a data source. The data will need to be written to S3 for QuickSight queries. IoT Analytics will provide analytics capabilities more suited to this use case than general solutions like Kinesis Data Analytics and Redshift. Amazon Pinpoint is used for sending personalized marketing messages, not general notifications. ","links":[{"url":"https://aws.amazon.com/iot-analytics/","title":"AWS IoT Analytics"},{"url":"https://aws.amazon.com/amplify/","title":"AWS Amplify"},{"url":"https://docs.aws.amazon.com/quicksight/latest/user/using-quicksight-mobile.html","title":"Using the Amazon QuickSight Mobile App"},{"url":"https://aws.amazon.com/solutions/smart-product-solution/?did=sl_card&trk=sl_card","title":"Smart Product Solution"}],"answers":[{"id":"5840ee0fec04ee6fca146272a5e75892","text":"Write IoT Core messages to Amazon Kinesis Data Streams with Amazon Kinesis Data Analytics as one consumer and Amazon EC2 instances with Auto Scaling as a second consumer. Have the EC2 instances write product event data to Amazon DynamoDB and publish notifications to an Amazon Simple Notification Service topic. Configure the mobile dashboard app to call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses Kinesis Data Analytics as its source.","correct":false},{"id":"f0ebf0f93a7448dab82c61c11dcc2c4a","text":"Deliver IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and publishes notifications to an Amazon Simple Notification Service topic. Also write IoT Core messages to AWS IoT Analytics. Have the mobile dashboard app send settings commands to AWS IoT Core. Configure the AWS Mobile Hub NoSQL Database option and register the DynamoDB event table for app data queries. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses IoT Analytics as its source.","correct":false},{"id":"57c04a93b4f460af88471726dcca35b2","text":"Send IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and publishes notifications to an Amazon Simple Notification Service topic. Also write IoT Core messages to AWS IoT Analytics. Have the mobile dashboard app call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses IoT Analytics as its source.","correct":true},{"id":"af8854f867f77abde2a38d7b3d1d84b7","text":"Route IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and sends notifications to Amazon Pinpoint. Also write IoT Core messages to an Amazon S3 bucket to be loaded into Amazon Redshift by AWS Glue. Have the mobile dashboard app call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses Redshift as its source.","correct":false}]},{"id":"74aec97e-c092-4588-8da4-43dca3ddd0eb","domain":"awscsapro-domain5","question":"You are trying to help a customer figure out a puzzling issue they recently experienced during a Disaster Recovery Drill.  They wanted to test the failover capability of their Multi-AZ RDS instance.  They initiated a reboot with failover for the instance and expected only a short outage while the standby replica was promoted and the DNS path was updated.  Unfortunately after the failover, they could not reach the database from their on-prem network despite the database being in an \"Available\" state.  Only when they initiated a second reboot with failover were they again able to access the database.  What is the most likely cause for this?","explanation":"The routes for all subnets in an RDS subnet group for a Multi-AZ deployment should be the same to ensure all master and stand-by units can be reached in the event of a failover.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/avoid-route-table-issues-rds/","title":"Avoid route table issues RDS Multi-AZ fail over"}],"answers":[{"id":"f637d9886a1ea7b52d608046e233e504","text":"The subnets in the subnet group did not have the same routing rules.  The standby subnet did not have a valid route back to the on-prem network so the database could not be reached despite being available.","correct":true},{"id":"1105b24fe7a3c1eb0a86d0fa9a3cdc62","text":"This was most likely an AWS error.  They should submit a support ticket with the RDS instance identifier and the approximate time of the failover test via the Support Center.","correct":false},{"id":"46b9b5188d94533190c30de816355165","text":"They used the AWS Console to issue the reboot.  You can only force a failover of RDS by using the AWS CLI and adding the --force-failover parameter to the \"aws rds reboot-db-instance\" command.","correct":false},{"id":"1dbb40d7bdb88fbe54fead30b1bf5f12","text":"There was a lag in the state update on the AWS console showing \"Available\".  If they would have waited longer, it likely would have changed to \"Degraded\".  A failover can take 30 minutes or more because AWS automatically creates a snapshot before promoting the standby.","correct":false},{"id":"260ae29233a6047971894eff9d05fa55","text":"They initiated a failover with an IAM account that did not have sufficient rights to perform the reboot.  This resulted in an incomplete failover that was only corrected by executing the failover again to reset the DNS entries.","correct":false}]},{"id":"0c3d85ee-ff65-46e7-86d4-9fb7dcd21176","domain":"awscsapro-domain4","question":"Per the requirements of a government contract your company recently won, you must encrypt all data at rest.  Additionally, the material used to generate the encryption key cannot be produced by a third-party because that could result in a vulnerability.  You are making use of S3, EBS and RDS as data stores, so these must be encrypted.  Which of the following will meet the requirements at the least cost?","explanation":"When possible, making use of KMS is much more cost-effective than CloudHSM.  We can import our own key material into KMS for creating Customer Master Keys.  Because KMS works natively with the services we will be using, we save on any sort of custom integration that CloudHSM would have required.","links":[{"url":"https://aws.amazon.com/kms/faqs/","title":"FAQs | AWS Key Management Service (KMS) | Amazon Web Services (AWS)"},{"url":"https://aws.amazon.com/cloudhsm/faqs/","title":"AWS CloudHSM FAQs - Amazon Web Services"}],"answers":[{"id":"473891e603ed3e2091e789a13094399c","text":"Use AMS KMS to create a customer-managed CMK.  Create a random 256-bit key and encrypt it with the wrapping key.  Import the encrypted key with the import token.  When creating S3 buckets, EBS volumes or RDS instances, select the CMK from the dropdown list.","correct":true},{"id":"9052cdd68b68935da91384397a18351c","text":"Use AMS KMS to create a 256-bit encryption key.  Use a grant to only allow access by S3, RDS and EBS.  When creating an S3 bucket, select the SSE-KMS option and pick the key from the dropdown.  For EBS and RDS, use the CLI to assign the KMS key when creating those instances.","correct":false},{"id":"66a66f8efdd420438db68069645b1ae7","text":"Initialize a CloudHSM instance.  Use it to generate custom encryption keys for each service you will use.  When creating an S3 bucket, EBS volume or RDS instance, select the custom CloudHSM key from the dropdown in the setup wizard.","correct":false},{"id":"a4e27f3b361ede72810f71b15168297e","text":"Generate a public and private key pair.  Upload the public key via the EC2 dashboard.  When creating EBS volumes, select encryption and select this public key.  When creating S3 buckets, implement a bucket policy which requires encryption at rest only, rejecting other files.  Create an RDS instance and select the public key from the dropdown in the setup wizard.","correct":false}]},{"id":"951e49a8-9395-4b49-b623-1fb9e88a9639","domain":"awscsapro-domain5","question":"Your production web farm consists of a minimum of 4 instances.  The application can run on any instance type with at least 16GB of RAM but you have selected m5.xlarge in your current launch configuration.  You have defined a scaling policy such that you scale out when the average CPU across your auto scaling group reaches 70% for 5 minutes.  When this threshold is reached, your launch configuration should add more m5.xlarge instances.  You notice that auto scaling is not working as it should when your existing instances reach the scaling event threshold of 70% for 5 minutes.  Since you are deployed in a heavily used region, you suspect there are capacity issues.  Which of the following would be a reasonable way to solve this issue?","explanation":"If you do not have capacity reserved via a zonal RI or on-demand capacity reservation, it is possible that the AZ is out of available capacity for the type of instance you need.  You can reserve capacity or you can also increase the possible instance types in hopes that some other similarly equipped instance capacity is available.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html","title":"Types of Reserved Instances (Offering Classes) - Amazon Elastic Compute  Cloud"}],"answers":[{"id":"04c8568f7761456b829145690b8d0cba","text":"Provision some regional reserved instances of m5.xlarge to ensure you have capacity when you need it. ","correct":false},{"id":"04211a7288d23e07315243e9b492dcb4","text":"Provision some zonal reserved instances of m5.xlarge to ensure you have capacity when you need it. ","correct":true},{"id":"a791a52f0967a913925bc47c7ae1c0a6","text":"Version the launch configuration to include additional instances types that also have at least 16GB of RAM.","correct":false},{"id":"989bc9eb46719d5cf9d9b69d191ee8d8","text":"Lower the CPU threshold to 60% so the scaling event triggers earlier and therefore has a better chance of getting resources.","correct":false},{"id":"ea27906009e93ed8060db15ee28fa8ae","text":"Reduce the warm-up and cooldown period in the scaling policy to allow more time to provision resources.","correct":false}]},{"id":"ef90df82-b36a-41f5-9294-904edfb5830e","domain":"awscsapro-domain1","question":"You are helping a client design their AWS network for the first time.  They have a fleet of servers that run a very precise and proprietary data analysis program.  It is highly dependent on keeping the system time across the servers in sync.  As a result, the company has invested in a high-precision stratum-0 atomic clock and network appliance which all servers sync to using NTP.  They would like any new AWS-based EC2 instances to also be in sync as close as possible to the on-prem atomic clock as well.  What is the most cost-effective, lowest maintenance way to design for this requirement?","explanation":"DHCP Option Sets provide a way to customize certain parameters that are issued to clients upon a DHCP request.  Setting the NTP server is one of those parameters.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_DHCP_Options.html","title":"DHCP Options Sets - Amazon Virtual Private Cloud"}],"answers":[{"id":"d5ea0e4895105bc1137c03c8bfb28ebe","text":"Configure a DHCP Option Set with the on-prem NTP server address and assign it to each VPC.  Ensure NTP (UDP port 123) is allowed between AWS and your on-prem network.","correct":true},{"id":"40efb671ff85878c28d1fa99329f15c7","text":"Configure your Golden AMI to use Amazon Time Sync Server at 169.254.169.123 and require this AMI to be used.  Use AWS Config to periodically audit the NTP configuration of all AWS assets.","correct":false},{"id":"2b2197df0ae72d4d462432d7c2f65c98","text":"Create a bridged network tunnel from the on-prem time server to the VPCs on AWS.  Configure the VPC route tables to route NTP (UDP 123) over the tunnel.","correct":false},{"id":"c950359dab3520d6c8c8f4e16a925860","text":"Deploy a third-party time server from the AWS Marketplace.  Configure it to sync from the on-prem time server.  Ensure NTP (UDP port 123) is allow inbound in the NACLs for the VPC containing the third-party server.","correct":false},{"id":"d94eaf4f8670e4d3f42fbae47250e3e6","text":"Create a dedicated host instance on AWS and place it within a transit VPC.  Configure the server to run NTP as a stratum-2 server.  Ensure NTP (UDP port 123) is allowed inbound and outbound in the Security Groups local to the stratum-2 server.","correct":false}]},{"id":"312b233b-ecc8-4e70-ae91-665159c7f77b","domain":"awscsapro-domain2","question":"You work for an automotive parts manufacturer as a Cloud Solutions Architect and you are in the middle of a design project for a new quality vision system.  To \"help out\", your parent company has insisted on contracting with a very expensive consultant to review your application design.  (You suspect that the consultant has more theoretical knowledge than practical knowledge however.)  You explain that the system uses video cameras and special polarizing filters to identify defects on fuel injectors.  As the part passes each station, an embedded RFID serial number is read and included with the PASS/FAIL vision test result in a JSON record written to DynamoDB.  The DynamoDB table is exported to Redshift on a monthly basis.  If a flaw is detected, the part can sometimes be reworked and sent back through the process--but it does retain its unique RFID tag.  Only the latest tests need to be kept for the part.  The consultant reviews your design and seems slightly frustrated that he is unable to recommend any improvement.  Then, he smiles and asks \"How are you ensuring idempotency?  In case a part is reprocessed?\"    ","explanation":"Idempotency or idempotent capability is a design pattern that allows your application to deal with the potential of duplicate records.  This can happen when interfaces fail and some records need to be reprocessed.  In this case, we are using a unique RFID serial number as our identifier for the part.  In DynamoDB, we would just overwrite the record with the latest record using a UpdateItem SDK method.  For Redshift, an UPSERT function allows us to either insert as a new record or update if a record of the same key already exists.  Redshift can do this using a merge operation with a staging table.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateItem.html","title":"UpdateItem - Amazon DynamoDB"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-upsert.html","title":"Use a Staging Table to Perform a Merge (Upsert) - Amazon Redshift"}],"answers":[{"id":"9dc68479e32f89bfe0afde00f41ae6c3","text":"For the target DynamoDB table, you have defined the unique RFID string as the partition key.  When copying to Redshift, you use table merge method to perform a type of UPSERT operation.","correct":true},{"id":"d87a9f08720c0fd91dee999f81f6f0ed","text":"You could change your design to write the message first to an SQS queue with FIFO enabled.  The records would then be guaranteed to process in the order they arrived.","correct":false},{"id":"5a65801823c8e80af868a9ca05e34e18","text":"You will be using API Gateway and Lambda for the insert into DynamoDB so scaling is not a concern. The part can be processed as many times as needed and Lambda will scale as needed.","correct":false},{"id":"8f594d180a595027ddef4e33f2784b0f","text":"You will be using CloudWatch to monitor the DynamoDB tables for capacity concerns.  If needed, you can enable DynamoDB auto scaling to accommodate the extra volume that reprocessing might introduce.","correct":false}]},{"id":"9ff9e447-acaa-4dae-9bbf-08db786af693","domain":"awscsapro-domain2","question":"You are a AWS Solutions Architect, and your team is working on a new Java based project. After the application is deployed in an EC2 instance in the development AWS account, an encrypted EBS snapshot is created. You need to deploy the application in the production AWS account using the EBS snapshot. For security purposes, a different customer managed key (CMK) in key management service (KMS) is required to encrypt the EBS volume in the new EC2 instance. What is the best method to achieve this requirement?","explanation":"In order to share an encrypted EBS snapshot between AWS accounts, you must also share the customer managed key (CMK) used to encrypt the snapshot. You can then copy the snapshot to a new one encrypted with another encryption key. You cannot directly copy an EBS volume between accounts. An EBS snapshot encrypted by the default AWS key is not allowed to be shared with another account.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html","title":"Sharing an Amazon EBS Snapshot"}],"answers":[{"id":"5751e42f86f4695e7309214d370c4d2b","text":"Share the snapshot and the encryption key to the production AWS account. Copy the snapshot to a new snapshot in the production account and encrypt it using another CMK in KMS. Create an AMI using the snapshot and launch a new EC2 instance.","correct":true},{"id":"536a3b4a4f859146660d3b6dc2036650","text":"Create an AMI from the EBS snapshot and copy the AMI from the development account to the production account. Launch an Auto Scaling group with the AMI and encrypt it with a new customer managed key.","correct":false},{"id":"941319ca17d0d8915ed822083e1dc2d4","text":"Re-encrypt the EBS snapshot with the default EBS encryption key in the development account. Share both the snapshot and the default EBS key to the production account. In the production account, create a volume from the snapshot and attach the volume to a new EC2 instance.","correct":false},{"id":"16c8a2b39dcb6723939653ed8deb9f90","text":"Share the CMK with the production account. In the development account, create a volume from the EBS snapshot. Copy the volume to the production account and encrypt it with another customer managed key. Launch a new EC2 instance and attach the new EBS volume to it.","correct":false}]},{"id":"945797fb-5147-44fc-a50c-aaa636c3705b","domain":"awscsapro-domain3","question":"Your organisation currently runs an on-premise Windows file server.  Your manager has requested that you utilise the existing Direct Connect connection into AWS, to provide a method of storing and accessing these files securely in the Cloud.  The method should be simple to configure, appear as a standard file share on the existing servers, use native Windows technology and also have an SLA.  Choose an option which meets these needs.","explanation":"To choose the correct option, we can start by eliminating services which don't have an SLA, in this case only Storage Gateway doesn't have an SLA so we can remove that as an option.  Next we can rule out EFS and S3 as they don't use native Windows technology or provide a standard Windows file share, therefore the only correct answer is to use Amazon FSx for Windows File Server.","links":[{"url":"https://aws.amazon.com/fsx/windows/faqs/","title":"Amazon FSx for Windows File Server FAQs"},{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs"},{"url":"https://aws.amazon.com/efs/faq/","title":"Amazon EFS FAQs"}],"answers":[{"id":"437caccc8e39a279a232207ec3ca741a","text":"Map an SMB share to the Windows file server using Amazon FSx for Windows File Server and use RoboCopy to copy the files across","correct":true},{"id":"5a76242c7735a5846218b183930c3a41","text":"Map an Amazon Elastic File System (EFS) share to the Windows file server and use RoboCopy to copy files across","correct":false},{"id":"af082c9581ff0bb5e1c9cc6daf7d72e0","text":"Write a Powershell script which uses the CLI to synchronise the files into an S3 bucket","correct":false},{"id":"e953bc59adceffe1274c4bc66d83b365","text":"Create an AWS Storage Gateway for Files server and map the generated SMB share to the Windows file server, then synchronise the files","correct":false}]},{"id":"6d93e859-e1a9-468f-9a05-61a2dbc2be9c","domain":"awscsapro-domain5","question":"You manage a group of EC2 instances that host a critical business application.  You are concerned about the stability of the underlying hardware and want to reduce the risk of a single hardware failure impacting multiple nodes.  Regarding Placement Groups, which of the following would be the best course of action in this case?","explanation":"Spread Placement Groups ensure your instances are each placed on separate underlying hardware so this reduces the risk of a single hardware failure taking down multiple instances.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-spread","title":"Placement Groups - Amazon Elastic Compute Cloud"}],"answers":[{"id":"3bde2d7fff7ead39c4035ab0600275f1","text":"You would use the AWS CLI to move the existing instances into a diversified placement group.","correct":false},{"id":"e346d1667489b5726c0367eff4ea4a34","text":"You would use the AWS CLI to move the existing instances into a spread placement group.","correct":true},{"id":"e6d129a06320300aaf95634a6691b7cb","text":"You would the AWS Console to move the existing instances into a clustered placement group.","correct":false},{"id":"c8b809a2fef3f21146774b82e8f03f12","text":"You would move the instances onto a Dedicated Host.","correct":false},{"id":"b6153af57a0b8805fc28d93a2859fb9e","text":"You cannot move existing instances into a new placement group.  You would create AMIs from the existing instances and redeploy them into a clustered placement group.","correct":false}]},{"id":"1520156f-0918-4ab4-a759-ce33a931c744","domain":"awscsapro-domain5","question":"Your company has an online shopping web application. It has adopted a microservices architecture approach and a standard SQS queue is used to receive the orders placed by the customers. A Lambda function sends orders to the queue and another Lambda function fetches messages from the queue and processes them. On some occasions the message in the queue cannot be handled properly. For example, when an order has a deleted production ID, the message cannot be consumed successfully and is returned to the queue. The problematic messages in the queue keep growing and the ability to process normal messages is affected. You need a mechanism to handle the message failure and isolate error messages for further analysis. Which method would you choose?","explanation":"It is not a good idea to adjust the retention period or simply delete the messages that fail to be processed as the question asks for a mechanism to isolate the messages for further troubleshooting. A redrive policy should be used to auto-forward error message to a dead letter queue. Then you can analyze the contents of messages to diagnose the producer’s or consumer’s issues. One thing to note is that a standard queue can only have another standard queue as the dead letter queue. Therefore a FIFO dead letter queue is incorrect as this scenario uses a standard SQS queue and requires a standard dead letter queue.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","title":"Amazon SQS dead-letter queues"}],"answers":[{"id":"3c8453a6c57faf61e761f771bab6f1af","text":"Create a standard queue as the dead letter queue and configure a redrive policy to put error messages to the dead letter queue. Analyze the contents of messages in the dead letter queue to diagnose the issues.","correct":true},{"id":"f7b3898cfcb4851b120c9b14d044ab90","text":"Decrease the message retention period of the queue to 1 day. When the messages are not processed properly and put back in the queue, they can be quickly deleted when the retention period expires.","correct":false},{"id":"64fd54fe78b534f0eac9222a6e32747d","text":"Create a FIFO (First-In-First-Out) queue as the dead letter queue and use a redrive policy to forward problematic messages to this new queue. Create a Lambda function to read the message contents in the FIFO queue for further analysis.","correct":false},{"id":"a7eb53a7df09334677590165f666c58f","text":"Modify the error handling logic of the Lambda function to delete the messages whenever the processing is unsuccessful with an error or exception. The error messages do not return to the queue and the normal message handling is not blocked.","correct":false}]},{"id":"401cbed4-e977-4303-9344-586af01a4180","domain":"awscsapro-domain2","question":"You have been contracted by a manufacturing company to create an application that uses DynamoDB to store data collected in a automotive part machining process.  Sometimes this data will be used to replay a process for a given serial number but that's always done within 7 days or so of the manufacture date.  The record consists of a MACHINE_ID (partition key) and a SERIAL_NUMBER (sort key).  Additionally, there is a CREATE_TIMESTAMP attribute that contains the creation timestamp of the record and a DATA attribute that contains a BASE64 encoded stream of machine data.  To keep the DynamoDB table as small as possible, the industrial engineers have agreed that records older than 30 days can be purged on a continual basis.  Given this, what is the best way to implement this with the least impact on provisioned throughput.","explanation":"Using DynamoDB Time to Live feature is a perfect way to purge out old data and not consume any WCU or RCU.  Other methods of deleting records would impact the provisioned capacity units.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","title":"Time To Live - Amazon DynamoDB"}],"answers":[{"id":"ef62d6ec88d117a0ac0cb7c99cd1abbd","text":"Use DynamoDB Streams to trigger a Lambda function when the record ages past 30 days.  Use the DynamoDB SDK in the Lambda function to delete the record.","correct":false},{"id":"0a945c4865c940ffaddaeade6f6bbdaf","text":"Use Step Functions to track the lifecycle of DynamoDB records.  Once 30 days has elapsed, branch to a Delete step and trigger a Lambda function to remove the record.","correct":false},{"id":"7c29d6c8effb51be4df54033ce45d01f","text":"Enabled Lifecycle Management on the DynamoDB table.  Create a rule that deletes any records where CREATE_TIMESTAMP attribute is greater than 30 days old.","correct":false},{"id":"41f68b5f48ef97cc437ebfe50ae10882","text":"Use AWS Batch to execute a daily custom script which queries the DynamoDB table and deletes those records where CREATE_TIMESTAMP is older than 30 days.  ","correct":false},{"id":"081f76fb4967f9d10a3799ae400ad898","text":"Update the table to add a attribute called EXPIRE  Change the application to store EXPIRE as CREATE_TIMESTAMP + 30 days.  Enable Time to Live on the DynamoDB table for the EXPIRE attribute.","correct":true}]},{"id":"b303f8e0-2c68-44aa-93bb-45b987b17d95","domain":"awscsapro-domain3","question":"You are helping a client build some internal training documentation to serve as architectural guidelines for their in-house Solutions Architects.  You suggest creating something inspired by the AWS Well-Architected Framework.  The client agrees and wants you to come up with some examples of each pillar.  Which of the following are examples of the Reliability pillar?","explanation":"The Reliability pillar includes five design principles:  Test recovery procedures, Automatically recovering from failure, Scaling horizontally to increase aggregate system availability, Manage change in automation and Stop guessing capacity.  By being able to closely monitor resource utilization, we can increase the Reliability and efficiency to right-size capacity.","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"3eb456756ecb767ab18179d87ec49a6b","text":"We can drive improvement through lessons learned from all operational events and failures. Share what is learned across teams and through the entire organization.","correct":false},{"id":"e82a19c63c4c9fedccc997eece7eccdc","text":"With virtual and automatable resources, we can quickly carry out comparative testing using different types of instances, storage, or configurations.  This will allow us to experiment more often.","correct":false},{"id":"c257f1eec7a5a7f1eae9338f4de45cb0","text":"On AWS, we'll be able to monitor demand and system utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over or under-provisioning.  We can stop guessing on capacity needs.","correct":true},{"id":"c57c8249b3885f2078d8dac40d695dec","text":"We can design workloads to allow components to be updated regularly, making changes in small increments that can be reversed if they fail.","correct":false}]},{"id":"dd8b46c7-d1d5-4326-a092-927b9333fd2a","domain":"awscsapro-domain5","question":"You are helping a company transition their website assets over to AWS.  The project is nearing completion with one major portion left.  They want to be able to direct traffic to specific regional EC2 web servers based on which country the end user is located.  At present, the domain name they use is registered with a third-party registrar.  What can they do?","explanation":"You can use Route 53 if the domain is registered under a third-party registrar.  When using Geolocation routing policies in Route 53, you always want to specify a default option in case the country cannot be identified.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy - Amazon Route 53"}],"answers":[{"id":"381d1621ab4c93aca8cc780f05e98c50","text":"Initiate a domain transfer request with the current registrar.  Once the request goes through, create a public hosted zone in Route 53.  Create SRV records for each regional EC2 instance using a Geolocation routing policy.  Create an alias record for the top-level domain and link that to the SRV records.","correct":false},{"id":"81efc0b140e618c378c9e9bc59dd4ca8","text":"Create a public hosted zone for the domain in Route 53.  Update the DNS entries in the registrars database to use AWS DNS Servers as defined in the NS record on Route 53.  Create A-type records for all EC2 instances. Configure CNAME records for the main FQDN that point to regional A records using a Geolocation routing policy.  Create another CNAME record as a default route.","correct":true},{"id":"294ce2854fbe727bd4f4917543d45bec","text":"Create a private hosted zone for the domain in Route 53.  Update the DNS record entries in the registrars database to use AWS DNS Servers.  Once the DNS changes are fully propagated across the internet and the TTL has expired, convert the private hosted zone to a public hosted zone.  Create A-type records for all the regional EC2 instances and configure a Geo-proximity policy for each record, ensuring the bias across all records sums to 100.","correct":false},{"id":"724dab26998c86ab7ddc7faa063285b8","text":"You cannot use Route 53 routing policies unless AWS is the registrar of record for your domain.  A workaround could be to configure your own top-level DNS server using BIND.  Ensure the NS and SOA records point to this instances.  Create A-type records pointing to the IP addresses of the regional EC2 web servers.  Dynamically redirect requests using customized BIND rules and a third-party IP geolocation database.","correct":false}]},{"id":"a4d41d3b-abbe-4121-8e1d-5567b1ec7294","domain":"awscsapro-domain5","question":"You are an AWS administrator and you need to maintain multiple Amazon Linux EC2 instances. You can SSH to the instances with a .pem key file created by another colleague. However, as the colleague will leave the company shortly, the SSH key pair needs to be changed to a new one created by you. After the change, users should be able to access the instances only with the new .pem key file. The old key should not work. How would you get the key pair replaced properly?","explanation":"You do not need to launch new instances as you can simply paste the public key content in the \".ssh/authorized_keys\" file to enable the new key pair. You cannot directly change the key through AWS Management Console by clicking the \"change SSH key\" button. You are also not allowed to change the SSH key when stopping and starting instances. Users can only select an SSH key pair when they launch a new instance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#replacing-key-pair","title":"Adding or Replacing a Key Pair for Your Instance"}],"answers":[{"id":"4fa9ce5fdb3a2215ed8798d2e734fd42","text":"In AWS Management Console, create a new key pair and download the .pem file at a safe place. Select the Amazon EC2 instances and click \"change SSH key\" to get the key replaced. The EC2 instances are automatically restarted after the change.","correct":false},{"id":"be5d2e07f838b7752b235431feaae361","text":"The SSH key pair cannot be changed after EC2 instances are launched. Take AMIs or EBS snapshots from existing instances, terminate the instances, launch new ones with the AMIs/snapshots and select another SSH key pair.","correct":false},{"id":"b216d43c3e1e71b67af45a138184b181","text":"Create a new SSH key pair, get the public key information from it, paste it in the \".ssh/authorized_keys\" file in the Amazon Linux EC2 instances and remove the old public key.","correct":true},{"id":"521ec8da4f5fa8bee591740d44a8427e","text":"Create a new key pair locally or through AWS EC2 service. Take AMIs of the instances as backups. Stop the instances and choose the new key pair when restarting the instances. Notify the team to start using the private key for SSH connections.","correct":false}]},{"id":"3f7fa126-1155-4aa3-802d-e9eeb75f5e5a","domain":"awscsapro-domain3","question":"You work for a Clothing Retailer and have just been informed the company is planning a huge promotional sale in the coming weeks.  You are very concerned about the performance of your eCommerce site because you have reached capacity in your data center.  Just normal day-to-day traffic pushes your web servers to their limit.  Even your on-prem load balancer is maxed out, mostly because that's where you terminate SSL and use sticky sessions.  You have evaluated various options including buying new hardware but there just isn't enough time.  Your company is a current AWS customer with a nice large Direct Connect pipe between your data center and AWS.  You already use Route 53 to manage your public domains.  You currently use VMware to run your on-prem web servers and sadly, the decision was made long ago to move the eCommerce site over to AWS last.  Your eCommerce site can scale easily by just adding VMs, but you just don't have the capacity.  Given this scenario, what is the best choice that would leverage as much of your current infrastructure as possible but also allow the landscape to scale in a cost-effective manner?","explanation":"A Target Group for an ALB can contain instances or IP addresses.  In this case, we can define the private IP addresses of our on-prem web servers along side the private IP addresses of any EC2 instances we spin up.  The caveat is that we can only use private IP addresses when defining a target group in this way.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancing-via-ip-address-to-aws-on-premises-resources/","title":"New – Application Load Balancing via IP Address to AWS & On-Premises  Resources | AWS News Blog"}],"answers":[{"id":"4df24111c113846bfe0505ad0c84d9a3","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define two target groups:  one containing the public IP addresses of your on-prem load balancer and one including an auto scaling group of additional EC2 instances created from the imported AMI.  Assign both target groups to the ALB using the same listener port.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"3e47f65e4524f53faba23e6995b592f5","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":true},{"id":"6d3db4c52e96931f925f17fe8e9fd50f","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using public IP addresses of your on-prem web servers and additional EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"77592781918fa63474b5efbd5cc9555f","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an NLB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the NLB as an alias record.","correct":false}]},{"id":"580790f0-3491-4b4c-a9b8-42b36a787cf5","domain":"awscsapro-domain2","question":"A new project needs a simple, scalable Amazon Elastic File System (EFS) to be used by several Amazon EC2 instances located in different availability zones. The EFS file system has a mount target in each availability zone within a customized VPC. You have already attached a security group to EC2 instances that allows all outbound traffic. In the meantime, how would you configure the EFS volume to allow the ingress traffic from these EC2 instances?","explanation":"EC2 instances connect to the Amazon EFS file system through the mount targets using the Network File System (NFS) port. The mount targets use a security group to control the access from EC2 instances. It should have an inbound rule to allow the NFS port (TCP:2049). The source of the security group would be the EC2 security group rather than the VPC CIDR. Either IAM role or network ACL cannot be used to control inbound access to an EFS system.","links":[{"url":"https://docs.aws.amazon.com/efs/latest/ug/network-access.html","title":"Security Groups for Amazon EC2 Instances and EFS Mount Targets"}],"answers":[{"id":"b346d4a177db41a1cfb51b80ae95708a","text":"Add an inbound rule in the network ACL to allow the ingress traffic from the NFS TCP port 2049. The source of the rule should be 0.0.0.0/0. Apply the network ACL in all the subnets where the EFS mount targets exist.","correct":false},{"id":"10fa29df072c2421e53a66048a92cbe3","text":"Make sure the attached IAM role in EC2 instances has the \"elasticfilesystem:*\" permission. The ingress traffic in the EFS volume is automatically allowed if the IAM role has enough permissions to interact with Elastic File Systems.","correct":false},{"id":"070a2bfc6b40165751997ed4d5d4e295","text":"Attach a security group to the mount targets in all availability zones. Allow the NFS port 2049 for its inbound rule. Identify the EC2 security group name as the source in the rule of the security group.","correct":true},{"id":"0fb49dc49facb19b17db23bdff392a1d","text":"Configure a new security group to allow inbound traffic from the VPC CIDR IP range such as 10.10.0.0/16. Associate the security group with the EFS file system directly to allow the ingress traffic from EC2 instances in the VPC.","correct":false}]},{"id":"11c28d09-1ccf-46ac-a56e-3998bff9c4e4","domain":"awscsapro-domain2","question":"You bought a domain name \"example.com\" from GoDaddy which is a domain registrar and the domain name will expire in several months. You plan to start using AWS Route 53 to manage the domain and resolve its DNS queries. The transferred domain in Route 53 should be automatically renewed every year so that the domain name will never expire and you do not need to renew it manually. Which method would you use to transfer the domain name properly?","explanation":"Users can register their new domain names in Route 53 or transfer existing domain names from other registrars such as GoDaddy to Route 53. After the transfer, the domain can automatically renew every year if the Auto Renew feature is enabled. To transfer the domain name, you do not need to wait until the domain name expires. And you cannot register the same domain name in both GoDaddy and Route 53 at the same time.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-transfer-to-route-53.html","title":"Transferring Registration for a Domain to Amazon Route 53"}],"answers":[{"id":"d8a7c0a25d22a5534a30c668de1be1d1","text":"Register the same domain name \"example.com\" in Route 53 three months before it expires in GoDaddy. Enable the feature of Transfer Lock in Route 53 to prevent it from being transferred to another registrar. Do not renew the original domain name in GoDaddy.","correct":false},{"id":"bd0fd22629d8016f6941db326aeb4ba4","text":"Wait until the domain name expires in GoDaddy and then register the domain in AWS Route 53 by clicking the \"Register Domain\" button in AWS management console. Turn on the features of Auto Renew and Transfer Lock for the new domain.","correct":false},{"id":"32c733a4bf01f533d38704f340cb7eb5","text":"Login in the GoDaddy admin account, unlock the domain transfer and request the domain transfer to Route 53. Accept the domain transfer in Route 53 and extend the expiration date to 10 years as transferred domains cannot automatically renew.","correct":false},{"id":"e27ac2c31c859905f6a51f22ab3a34bf","text":"Confirm that the domain is transferable in GoDaddy. In the Route 53 AWS Management Console, click \"Transfer Domain\" to transfer registration for the domain name from GoDaddy to Route 53. Enable the automatic renewal for this domain name.","correct":true}]},{"id":"4b00251a-a278-4d88-b715-955b4752a79a","domain":"awscsapro-domain2","question":"You'd like to create a more efficient process for your company employees to book a meeting room.  Which of the following is the most efficient path to enabling this improved business experience?","explanation":"With Alexa for Business, you can enlist Alexa-enabled devices to perform tasks for employees like retrieve information, start conference calls and book meeting rooms.","links":[{"url":"https://aws.amazon.com/blogs/business-productivity/announcing-room-booking-for-alexa-for-business/","title":"Announcing Room Booking for Alexa for Business | Business Productivity"}],"answers":[{"id":"a723a332c2ed052968becb9b6824e2a4","text":"Sign-up for AWS Alexa for Business. Create conference rooms in the console and place an Alexa device in each conference room.","correct":true},{"id":"e43eb2c75536fac2d714b862f4fec490","text":"Invest in a voice-to-text API from the AWS Marketplace.  Create a custom Lambda function that calls the API and books a conference room.  Equip each conference room with Amazon Dash buttons and configure them to invoke the Lambda function.","correct":false},{"id":"5445641c568edaf760839d9f5bb7169c","text":"Configure an Alexa device with a custom skill backed by a Lambda function.  Use Amazon Lex to convert the audio sent to the Lambda function into an actionable skill.  ","correct":false},{"id":"12cb35ea295a9a96bfba94741cb4f0df","text":"Sign-up for Amazon Chime.  Create conference rooms in the console and place speakerphones in each conference room.","correct":false}]},{"id":"34351bd0-7925-4246-bb61-c64bbf4d5baf","domain":"awscsapro-domain4","question":"An application in your company that requires extremely high disk IO is running on m3.2xlarge EC2 instances with Provisioned IOPS SSD EBS Volumes. The EC2 instances have been EBS-optimized to provide up to 8000 IOPS. During a period of heavy usage, the EBS volume on an instance failed, and the volume was completely non-functional. The AWS Operations Team restored the volume from the latest snapshot as quickly as possible, re-attached it to the affected instance and put the instance back into production. However, the performance of the restored volume was found to be extremely poor right after it went live, during which period the latency of I/O operations was significantly high. Thousands of incoming requests timed out during this phase of poor performance.\nYou are the AWS Architect. The CTO wants to know why this happened and how the poor performance from a freshly restored EBS Volume can be prevented in the future. Which answer best reflects the reason and mitigation strategy?","explanation":"Data gap cannot be the reason for high disk I/O latency. Whether the data being requested is on the disk or not cannot be responsible for the extended period of high disk I/O latency, as all operating systems index the contents in some way. They do not scan the whole disk to conclude that something is missing. Hence, the choice that suggests data gap as the reason is eliminated.\nEBS Optimization works straight away after a freshly restored volume is attached to an EBS optimized instance. Hence, the choice that suggests that EBS Optimization takes some time to kick in is eliminated.\nThere is nothing called set-up-cache command. The option that suggests that there is an inbuilt caching mechanism that needs to be activated is completely fictional, and is eliminated.\nThe only correct option is the one that correctly states that every new block read from a freshly restored EBS Volume must first be downloaded from S3. This is because EBS Snapshots are saved in S3. Remember that EBS Snapshots are incremental in nature. Every time a new snapshot is taken, only the data that changed is written to that particular snapshot. Internally, it maintains the pointers to older data that was written to S3 as part of previous snapshots. These blocks of data continue to reside on S3 even after an EBS Volume is restored, and is read the first time they are accessed. Linux utilities like dd or fio can be used after restoring an EBS Volume to read the whole volume first to get rid of this latency problem when the instance is put back in production.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-initialize.html","title":"Initializing Amazon EBS Volumes"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html","title":"Amazon EBS–Optimized Instances"}],"answers":[{"id":"ae32d192c3829287819a74ded72e0da7","text":"The latest snapshot did not have the most current data. It only had the data from the last time a snapshot was taken. The requests timed out because of this data gap. To mitigate this, increase the frequency of taking EBS snapshots.","correct":false},{"id":"d3a1c3b2669127dabe2eaf2c490fcd30","text":"A freshly restored EBS Volume needs pre-warming to activate the inbuilt caching mechanism. To fix this, update the restoration process to run the set-up-cache command on the freshly restored EBS Volume first before the instance is put back in production. Also, include random I/O tests to ensure that desired I/O levels are reached before putting the instance back to production.","correct":false},{"id":"1a47200401a925a2ad1df3d3286e26dc","text":"When a data block is accessed for the first time on a freshly restored EBS Volume, EBS has to download the block from S3 first. This increases the I/O latency until all blocks are accessed at least once. To fix this, update the restoration process to run tools to read the entire volume before putting the instance back to production.","correct":true},{"id":"51e87b2dbdfcb476b026779380119b06","text":"A freshly restored EBS Volume cannot utilize EBS Optimization Instances straight away, as the network traffic and EBS traffic traverse the same 10-gigabit network interface. Only after the entire volume is scanned by an asynchronous process, EBS Optimization kicks in. This increases the I/O latency until the volume is ready to utilize EBS Optimization. To fix this, update the restoration process to wait and run random I/O tests on a freshly restored EBS Volume. Put the instance back to production only after the desired I/O levels are reached.","correct":false}]},{"id":"d2b0a9d5-1875-4a55-968d-3a2858601296","domain":"awscsapro-domain2","question":"You currently are using several CloudFormation templates. They are used to create stacks that include the resources of VPC subnets, Elastic Load Balancers, Auto Scaling groups, etc. You want to deploy all the stacks with a root stack so that all the resources can be configured at one time. Meanwhile, you need to isolate information sharing to within this stack group, which means other stacks outside of the stack group can not import its resources. For example, one stack creates a VPC subnet resource and this subnet can only be referenced by the stack group. What is the best way to implement this?","explanation":"As the stack outputs should be limited within the stack group, nested stacks should be chosen. The export stack outputs cannot prevent other stacks to use them. The AWS::CloudFormation::Stack resource type is used in nested stacks to provide dependencies. The DependsOn attribute is not used for configuring nested stacks.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html","title":"Exporting stack output values"}],"answers":[{"id":"fe3bf9eee211c59e9c64ad17a2d32e27","text":"Export output values for each child stack if needed. Create a parent stack to use the exported values from child stacks to deploy and manage all resources at one time.","correct":false},{"id":"e44006dac54e63b93a8804a4e632eeb5","text":"Upload the root and all child stack templates to an S3 bucket under the same directory. Use the \"DependsOn\" attribute in the root template to add dependencies. When the root stack is created, all the child stacks are created first. ","correct":false},{"id":"e208251ba3e5646edab96df0da85794c","text":"Upload stack templates to an S3 bucket. Create a root CloudFormation stack to use the uploaded templates with the resource type of \"AWS::CloudFormation::Template\". Configure the \"TemplateURL\" field with the template location in S3.","correct":false},{"id":"aca679ff150a8a8176faf99cc057e825","text":"Create nested stacks with the \"AWS::CloudFormation::Stack\" resources. Use the outputs from one stack in the nested stack group as inputs to another stack in the group if needed.","correct":true}]},{"id":"49f16801-2cc1-48c8-a517-f9192f516318","domain":"awscsapro-domain3","question":"A tire manufacturing company needs to migrate a .NET simulation application to the AWS cloud. The application runs on a single Windows Application Server in their datacentre. It reads large quantities of data from local disks that are attached to the on-premises Application Server. The output from the application is small in size and posted in a queue for downstream processing. On the upstream side, the data acting as the input for the .NET simulation app is generated in the tire-testing Lab during the daytime by processes running on a Linux Lab Server. This data is then copied from the Linux Lab Server to the Windows Application Servers by a nightly process that also runs on the Linux Lab Server. This nightly process mounts the Application Server disks using a Samba client, this is made possible by the Application Server also acting as a Windows File Share Server. When the nightly process runs, it overwrites the input data from last night because of disk space constraint on the Application Server. This is undesirable as the data is permanently lost on a daily basis.\nThe migration is being undertaken because the .NET simulation application needs more CPU and RAM. The company does not want to spend on expensive hardware any more. However, the nightly process is not migrating, nor is the Linux Lab Server. The code of the simulation applications, as well as the nightly process, may change a little as a result of the migration, but leadership wants to keep these changes to a minimum. They also want to stop losing the daily test data and keep it somewhere for possible analytical processing later on.\nAs the AWS architect hired to shepherd this migration and many more possible migrations in the future, which of the following architectures would you choose as the best one, considering the minimization of code changes as the topmost goal, followed by cost-effectiveness as the second but important priority? The data has no security requirement.","explanation":"The two parts of this question are - (a) Do I use EFS or EBS for storing the data from the Lab Servers? (b) Do I copy data from each night to S3 using a NAT Gateway (thereby using the public internet) or a VPC Endpoint (thereby using the private network to copy)?\nThe answer to the first question is EBS because Windows EC2 instances cannot mount EFS, as EFS only supports Linux EC2 instances.\nThe answer to the second question is VPC Endpoint because NAT Gateways are very costly - they are charged 24-7 for just running, in addition to having data transfer rates. S3 VPC Endpoints are a cost-effective mechanism to copy data to S3. Note that the S3 put-object cost will be the same for both cases. The question tries to distract the candidate by stating that there is no security requirement, trying to confuse the candidate into selecting NAT Gateway in case they perceive the only distinction between NAT Gateway and S3 VPC Endpoint to be the usage of public network versus private.\nThis is an example of highly verbose question describing a complex scenario. There will definitely be quite a few such questions in the AWS SA-P exam that are challenging in terms of time management. You may use vertical scanning of the answer choices to spot the differences first. That way, you can focus on determining which of the variations is correct because you would know what is different between them.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html","title":"EFS Support for Windows EC2 Instances"},{"url":"https://aws.amazon.com/vpc/pricing/","title":"Search for NAT Gateway Pricing here"}],"answers":[{"id":"220d73c7cceb53661a402d59038118ee","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"fabdb7e05e5bb1c78dd6e166134202ca","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using a NAT Gateway, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"c7311a2612bfc63a966317d468a5b4dd","text":"Use Windows EC2 instances for running the simulation applications. Store the data from the on-premises Lab Servers in AWS Elastic File System (EFS), modifying the nightly process to use an NFS client instead of Samba client. Create a VPN connection between on-premises and AWS so that the nightly process can access the EFS share. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":false},{"id":"3ae5ae41663ec801882e10e7fa394613","text":"Use Windows EC2 instances for running the simulation applications. Mount general purpose EBS disks on each of these instances to store the data from Lab Servers. Create a VPN connection between on-premises and AWS so that the nightly process can access the EBS disks the same way it accesses the Application Server Disks currently. Also, modify the simulation application to move the data each night, after the calculations are complete, to an S3 bucket using an S3 VPC Endpoint, deleting it from its own disks after the copy is complete. Modify the nightly application to skip deleting data from last night as the data would have already moved to S3 by the time it runs.","correct":true}]},{"id":"8765bd56-057b-488c-9a0a-f5bd413dd240","domain":"awscsapro-domain5","question":"Due to new corporate policies on data security, you are now required to use encryption at rest for all data.  You have some EC2 Linux instances on AWS that were created without encryption for the root EBS volume.  What can you do that meet the requirement and reduce administrative overhead?","explanation":"AWS does support encrypted root volumes but conversion from unencrypted root to an encrypted root requires a bit of a process. You must first create an AMI then copy that newly created AMI to the same region, specifying that you want to encrypt the EBS volumes during the copy.  You can then create a new instance with an encrypted root volume from the copied AMI.  You can use either a generated key from KMS or your own CMK imported into KMS.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html","title":"AMIs with Encrypted Snapshots - Amazon Elastic Compute Cloud"},{"url":"https://aws.amazon.com/blogs/aws/new-encrypted-ebs-boot-volumes/","title":"New – Encrypted EBS Boot Volumes | AWS News Blog"}],"answers":[{"id":"180e9aecdb74f204b1df00ffe6fa8b56","text":"Stop the instances and create AMIs from the instances.  Copy the AMIs to the same region and select the \"Encrypt target EBS snapshots\".  Redeploy the instances using the AMI copies you made with encrypted root volumes.","correct":true},{"id":"9fbbb2e71386cb7f7a7ed77129a1a960","text":"Create an encrypted EFS instance and mount-points in the respective subnets.  Log into the instance and mount an encrypted EFS mount-point.  Copy all the root files over to the EFS mount point.  Edit the FSTAB file to mount the EFS mount point as the root volume instead of the root EBS device and reboot.","correct":false},{"id":"4430b7492a6c058c3574ce4e8ea43955","text":"Stop the instances and temporarily detach the EBS volumes.  Attach the root volumes to another EC2 instance and mount them a data volume.  Use a encryption tool like GPG or OpenPGP to recursively encrypt all the files on the mounted root volumes.  Detach and reattach the encrypted EBS volumes to the original instances and restart.  Import the encryption keys in KMS as a CMK.","correct":false},{"id":"1e30ccf0b75e9e70fd76c6e041510c75","text":"At present, EC2 does not support encrypted root volumes.  Create new encrypted EBS data volumes and attach the new volumes to the existing instances.  Use RSYNC to migrate all the non-OS data over to the encrypted data volumes.","correct":false},{"id":"50c17b27f0bd0390ae321943f7db5c3d","text":"Create a certificate in CMS for the encryption key.  Stop the instances and temporarily detach the root volumes.  Via the AWS CLI, enable encryption on the root volumes using the \"ebs modify-volume\" argument with the flag of \"encryption=<CMS ARN>\" to specify the certificate.","correct":false}]},{"id":"f19a95ac-c0b9-4d00-a84a-67f71b7e2a76","domain":"awscsapro-domain2","question":"You are advising a client on some recommendations to increase performance of their web farm.  You notice that traffic seems to usually spike on the days after public holidays and unfortunately the responsiveness of the web server as collected by a third-party analytics company reflects a customer experience that is slower than targets.  Of these choices, which is the best way to improve performance with minimal cost?","explanation":"Of these options, only one meets the question requirements of performance at minimal cost.  Simply scheduling a scale event during a known period of traffic is a perfectly valid way to address the requirement and does not incur unnecessary cost. CloudTrail records API access and is not suitable for network alarms.  Route 53 would not be able to \"consolidate\" dynamic and static web resources.","links":[{"url":"https://docs.aws.amazon.com/auto scaling/ec2/userguide/schedule_time.html","title":"#N/A"}],"answers":[{"id":"247053f8b211aeace0894f849838ef6f","text":"Configure a scheduled scaling policy to increase server capacity on days after public holidays.  ","correct":true},{"id":"3c6b1f2e20a3204df3886f680991b76d","text":"Use CloudTrail and SNS to trigger a Lambda function to scale the web farm when network traffic spikes over a configured threshold.  Create an additional Internet Gateway and split the traffic equally between the two gateways using an additional route table.  ","correct":false},{"id":"8a2f77fe64a24871e80eae971ce2c877","text":"Create replicas of the existing web farm in multiple regions.  Migrate static assets to S3 and use cross-region replication to synchronize across regions.  Create CloudFront distributions in each region.  Use Route 53 to direct traffic to the closest CloudFront alias based on a geolocation routing policy.","correct":false},{"id":"24c302b442af96b7c1eedb04e2c4069b","text":"Configure a dynamic scaling policy based on network traffic or CPU utilization.  Migrate static assets from EBS volumes to S3.  Configure two Cloudfront distributions--one for static content and one for dynamic content.  Use Route 53 to consolidate both Cloudfront distributions under one alias.","correct":false}]},{"id":"cc441e7c-594e-401f-a82c-84cb2984739d","domain":"awscsapro-domain2","question":"Your team is developing a web application. The application is hosted on EC2 instances and an application load balancer distributes the application traffic to the servers. A domain name called \"example.com\" is hosted in Route53. You want to resolve domain names \"test1.example.com\" and \"test2.example.com\" to \"server.example.com\". And domain name \"server.example.com\" should be resolved to the application load balancer DNS name. How would you configure the record sets in Route53?","explanation":"When a domain name needs to be routed to another one, a CNAME record should be used. In the \"server.example.com\" record set, the Alias target would be the ELB domain name. Alias records are suitable when you want to redirect queries to selected AWS resources such as S3, CloudFront and ELB. For A type records, the targets can only be IPv4 addresses. The targets cannot be domain names or ELB DNS names.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html","title":"Choosing Between Alias and Non-Alias Records"}],"answers":[{"id":"6c304905717410886f3dd80373f65f18","text":"Create two CNAME records for \"test1.example.com\" and \"test2.example.com\". Direct both domain names to \"server.example.com\". In the record set for \"server.example.com\", configure an Alias record to route the traffic to the ELB DNS name as its target.","correct":true},{"id":"0ffd21f78c499ada42e566a7a9000aea","text":"Create two CNAME records \"test1.example.com\" and \"test2.example.com\" to direct DNS queries to the domain name \"server.example.com\". In the \"server.example.com\" record set, configure an A type record to route the traffic to the ELB DNS name as its target.","correct":false},{"id":"3ae1eacef0f314b0f1b810cd51aebfa8","text":"Add two Alias records for \"test1.example.com\" and \"test2.example.com\". And direct both records to \"server.example.com\". In the \"server.example.com\" record set, configure an Alias record and use the application load balancer CNAME as its target.","correct":false},{"id":"4b3bea9937caf31b398fa22c5aa2e255","text":"Create two A type records for \"test1.example.com\" and \"test2.example.com\". Route both records to the domain name \"server.example.com\". In the record set for \"server.example.com\", configure a CNAME record to distribute the traffic to the ELB DNS name.","correct":false}]},{"id":"5af539b7-b132-4a3a-bc80-406c620e7325","domain":"awscsapro-domain1","question":"A food service business has begun an initiative to migrate all applications and data to the AWS cloud. Governance needs to be established before any migrations can occur. Business units such as sales, marketing, and product management have fluctuating infrastructure capacity and security requirements, while other business units like finance, operations, and human resources have more static demand. Security policies and compliance needs vary by project group within each business units. Each business unit is responsible for it's own cost center, and the finance group would like cost reporting to be as streamlined as possible. Which AWS account structure will best satisfy the company's governance needs?","explanation":"Leveraging AWS Organizations to manage an account structure with a core Organizational Unit and Organizational Units for each business unit provides flexibility for future organizational changes. Creating an account for each project group facilitates security policy differences within business units, and limits the exposure of a single security event. Managing differing security requirements by project group in a single account will require more governance maintenance. Creating billing, shared services, and log archive accounts in multiple Organizational Units will result in duplication of services, and can be done at the core level.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-laying-the-foundation/introduction.html","title":"Laying the Foundation: Setting Up Your Environment for Cost Optimization"},{"url":"https://aws.amazon.com/solutions/aws-landing-zone/?did=sl_card&trk=sl_card","title":"AWS Landing Zone"}],"answers":[{"id":"a8f7d8fbb7c6c3a1a14c91577dff42e1","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Place business units with similar security requirements in shared Organizational Units. Create accounts for each business unit in the shared Organizational Units. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"a66d8391267460b5800c5c3d07921767","text":"Use AWS Organizations to create Organizational Units for each business unit. Create a billing account, a shared services account, and a log archive account in each Organizational Unit. Create accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"bd400ff0d22480599228a0442d2bb8d4","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Create an Organizational Unit for each business unit that contains accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":true},{"id":"03705913700b8d76205d4203c58dc5e1","text":"Use AWS Organizations with a single Organizational Unit to consolidate costs. Create a billing account, a shared services account, and a log archive account in the Organizational Unit. Create individual accounts for each business unit. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs","correct":false}]},{"id":"3e23b8a9-ce64-430f-936f-9a318a85da75","domain":"awscsapro-domain4","question":"To be sure costs of AWS resources are allocated to the proper budgets, you are trying to come up with a way to allocate the AWS bill to the proper cost centers.  Which of the following would be most effective for your organization?","explanation":"Using Service Catalog is a good way to automatically enforce and apply a tagging strategy and it requires no special effort from the product consumers.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/tagoptions-launching.html","title":"Launching a Product with TagOptions - AWS Service Catalog"}],"answers":[{"id":"874d7ac46d1adeb7650db0db8dc9d2c1","text":"Use an SCP at the organizational level to require a cost center tag be applied to every resource.  Activate the cost center tag in the Billing Console and allocate costs based on that.","correct":false},{"id":"37c28a1741830444dec6469aa71f605c","text":"Deploy products within AWS Service Catalog and only allow users to deploy resources using the catalog.  Use TagOptions to provide the users a list from which they can select their cost center.  Activate the cost center tag in the Billing Console.","correct":true},{"id":"e2f5a49c87b9eac8673cfe95968f124d","text":"Use AWS Batch to periodically run a custom Lambda function that scans all resources and deletes any without proper tagging for cost center.","correct":false},{"id":"f065124beaeb3f8cde1976516f7fb97a","text":"Make use of AWS Artifact to analyse the spending pattern over the month and identify the IAM users responsible for the most costs.  Cross-reference that with the cost centers to which IAM users belong.","correct":false},{"id":"683fce4e1eed553dd4cfa34806ffcbc6","text":"Use API Gateway to create a proxy for the API of the resources your users will deploy.  Insert some custom logic using VTL to automatically append a cost center tag to the request based on the cost center of the IAM user making the request.","correct":false}]},{"id":"63a6def8-9b52-4d89-8248-6079ca1393e2","domain":"awscsapro-domain3","question":"You are helping a client prepare a business case for cloud migration.  One of the required parts of the business case is an estimation of AWS costs per month.  The client has about 200 VMs in their landscape under VMware vCenter.  Due to security concerns, they will not allow any external agents to be installed on their VMs for discovery.  How might you most efficiently gather information about their VMs to build a cost estimate with the least amount of effort? ","explanation":"The Application Discover Service uses agent-based or agentless collection methods.  Agentless collection is only available for those customers using VMware.  The AWS Application Discovery Agentless Connector is delivered as an Open Virtual Appliance (OVA) package that can be deployed to a VMware host. Once configured with credentials to connect to vCenter, the Discovery Connector collects VM inventory, configuration, and performance history such as CPU, memory, and disk usage and uploads it to Application Discovery Service data store.  This data can then be used to estimate monthly costs.","links":[{"url":"https://aws.amazon.com/application-discovery/faqs/?nc=sn&loc=6","title":"AWS Application Discovery Service FAQs"}],"answers":[{"id":"0a144eb3993e48693aab4c9744b6acb2","text":"Use AWS OpsWorks to remotely pull hardware, network connection and performance of the VMs.  Export the collected data from OpsWorks in Excel format.  Use the collected data to align current VMs with similar EC2 instance types and calculate an estimated monthly cost.","correct":false},{"id":"09d153439d976dabcdd13a4a2f8a4a5f","text":"Use Application Discovery Service to gather details on the network connections, hardware and performance of the VMs.  Export this data as CSV and use it to approximate monthly AWS costs by aligning current VMs with similar EC2 instances types.","correct":true},{"id":"b279286ed54b2394c29d2fbd0061b4c4","text":"Provision an S3 bucket for data collection.  Use SCT to scan the existing VMware landscape for VM hardware, network connection and performance parameters.  Retrieve the SCT CSV data from the data collection bucket and use it to align EC2 instance types with existing VM parameters.  Use this cross-reference to calculate estimated monthly costs for AWS.","correct":false},{"id":"30cbd0a822a4905f8a795dcb7cc3d31e","text":"Use a custom script to iteratively log into each VM and pull network, hardware and performance details of the VM.  Write the data out to S3 in CSV format.  Use that data to select corresponding EC2 instance sizes and calculate estimated monthly cost.","correct":false}]},{"id":"3d98c88b-ccae-4d7a-804a-b329b8afbdb5","domain":"awscsapro-domain1","question":"You work for a freight truck operating company that operates a website for tracking the realtime location of trucks. The website has a 3-tier architecture with web and application tiers communicating with a PostgreSQL database. Under average load, the website requires 8 web servers and 3 application servers, with average CPU consumption at 85%. Though the experience will suffer, the application can still operate with 6 web servers and 2 application servers; however, in that case, the CPU usage is close to 100%. You are deploying this application in us-west-2 region which has four AZs (Availability Zones). Select the architecture that provisions maximum availability and the ability to withstand the loss of up to two Availability Zones at the same time, being cost-efficient as well.","explanation":"The key to answering this kind of question is using a simple mathematical formula. If the requirement states that the least number of Availability Zones which will still be functioning after a catastrophic loss is X, then the servers I need per AZ is n such that n * X = minimum number of EC2 Instances required by the application.\nLet us apply this formula to the web-tier. The minimum number needed for the application to function is 6. If I am spreading my instances across 4 AZ-s, then X = 2 because I must be able to withstand the loss of two AZ-s. Therefore, n * 2 = 6. Thus, n = 3. Which means I need 3 web servers in each of my AZ-s. Let us reverse calculate with that number to be sure. If I have 3 web servers per AZ, and I have 4 AZ-s, I have 12 web servers running. Now if 2 AZ-s fail, I will still have 6 web servers running. Thus, my application will still perform, which is the requirement.\nThere are a few additional interesting aspects to this kind of problems. First, note that some of the answer choices use 3 AZ-s instead of all 4. This is because the exam wants to penalize inattentive reading, in case you miss that detail. Second, using the above formula sometimes results in multiple correct answers. In that case, we should choose the one with the lower total number of EC2 instances because that will be the lower-cost option. Thirdly, some of the answer choices would be correct if you are trying to make the architecture withstand the failure of 1 AZ (and not 2). This again is trying to penalize inattentive reading in case you are in a hurry, read the question partially, and try to find the correct answer based on 1 AZ going down instead of two, which is the stated requirement.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html","title":"AWS Regions and Availability Zones"},{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html#arch-AutoScalingMultiAZ","title":"Distributing Instances Across Availability Zones"}],"answers":[{"id":"698838baa59961738cfcdc08cbdcd653","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning 3 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 6 EC2 Instances in each Availability Zone for the web-tier. Deploy 2 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS with Read Replicas.","correct":false},{"id":"3bff2d20d9b822da03cdfc315586449e","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning all 4 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 2 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":false},{"id":"0d286ae78f17320b7244d3c53be500ba","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning 3 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 3 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":false},{"id":"2a80c20c82cfb8e8bd358b60ee1952a5","text":"Deploy the web-tier and app-tier on EC2 Instances in their own Auto Scaling Groups, each Group spanning all 4 AZ-s. Use an Elastic Load Balancer for each Auto Scaling Group. Deploy 3 EC2 Instances in each Availability Zone for the web-tier. Deploy 1 EC2 Instance in each Availability Zone for the app-tier. Deploy the PostgreSQL database in RDS in Multi-AZ mode.","correct":true}]},{"id":"431e43bc-ccbc-480f-9915-210bc7773d2b","domain":"awscsapro-domain5","question":"You are in the process of migrating a large quantity of small log files to S3 for long-term storage.  To accelerate the process and just because you can, you have created quite sophisticated multi-threaded distributed process deployed across 100 VMs which can load hundreds of thousands of files at one time.  For some reason, the process seems to be throttled somewhere along the chain.  You try many things to try to uncover the source of the throttling but nothing works.  Reluctantly, you decide to turn off the KMS encryption setting for your S3 bucket and the throttling goes away.  You turn AMS-KMS back on and the throttling is back. Given the troubleshooting steps, what is the most likely cause of the throttling and how can you correct it?","explanation":"Through a process of elimination, it seems you have identified the variable that is causing the throttling.  KMS, like other AWS services, does have rate limiters which can be increased via Support Case.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/limits.html","title":"Limits - AWS Key Management Service"}],"answers":[{"id":"60ff77ab365c15bb11771e94e3dc271d","text":"You have exceeded the number of API calls for your account.  You must create a new account.","correct":false},{"id":"05aeb0bc36d7b53aa30bf9e22b6cd120","text":"You are maxing out your PUT requests to S3.  You need to change over to multi-part upload as a workaround.","correct":false},{"id":"1dd4f25e52404e18ddec0b8711a82a13","text":"You are hitting the KMS encrypt request account limit.  You must request a limit increase via a Support Case.","correct":true},{"id":"fe629daf7473efc279d7d8ee6f5a5806","text":"You are maxing out your SYNC requests to S3.  You need to request a limit increase via a Support Case.","correct":false},{"id":"01148eae3319190a0b228c6d02c9572c","text":"You are maxing out your network connection.  You must split the traffic over multiple interfaces.","correct":false}]},{"id":"dc82c397-347d-4f69-bb06-03822238c7a0","domain":"awscsapro-domain1","question":"You are consulting for a large multi-national company that is designing their AWS account structure.  The company policy says that they must maintain a centralized logging repository but localized security management.  For economic efficiency, they also require all sub-account charges to roll up under one invoice.  Which of the following solutions most efficiently addresses these requirements?","explanation":"Service Control Policies are an effective way to broadly restrict access to certain features of sub-accounts.  Use of a single separate logging account is an effective way to create a secure logging repository.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"871870fa49beedfb95106595e4a1c9f4","text":"Configure billing for each account to load into a consolidated RedShift instance.  Create a centralized security account and establish trust relationships between each sub-account.  Configure admin roles within IAM of each sub-account for local administrators.  Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  ","correct":false},{"id":"74a6c6df518100b16da3f16e870b5d5c","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Create localized IAM policies to restrict modification of CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create a centralized security account and establish trust relationships between each sub-account.","correct":false},{"id":"0c9b5a803a99a3d2ef53869b6857c0e0","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use ACLs to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.  Establish trust relationships between the Consolidated Billing account and all sub-accounts.","correct":false},{"id":"cbec34b5388f7f183659e82c20fb3abf","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use an SCP to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.","correct":true}]},{"id":"e50e5c98-f9f8-48fd-80fc-6a8741d17482","domain":"awscsapro-domain1","question":"You have just completed setup of Direct Connect from your data center to VPCs in us-east-1.  You would also like to leverage that Direct Connect for communication between your data center and other VPCs in other regions.  What is the simplest way to do this?","explanation":"You can use an AWS Direct Connect gateway to connect your AWS Direct Connect connection over a private virtual interface to one or more VPCs in your account that are located in the same or different regions. You associate a Direct Connect gateway with the virtual private gateway for the VPC, and then create a private virtual interface for your AWS Direct Connect connection to the Direct Connect gateway. You can attach multiple private virtual interfaces to your Direct Connect gateway.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html","title":"Direct Connect Gateways - AWS Direct Connect"}],"answers":[{"id":"15955693a61b06cc43a38a2ac5a2ede9","text":"Setup a Transitive VPC and configure virtual private gateways between the us-east-1 VPC and other VPCs in other regions.","correct":false},{"id":"17077e5e04411e1e4e4b8ab017ae6c18","text":"Add a new route to the VPC peer in the desired region in the Customer Gateway.  Ensure that BGP routing is enable to propagate the route.","correct":false},{"id":"d9271b66eb35f235c0b61fd11f3af41d","text":"Associate the Direct Connect gateway with the virtual private gateway for the VPC, and then create a private virtual interface for your Direct Connect connection to the Direct Connect gateway.","correct":true},{"id":"38573789b2722bdd38529167996196d1","text":"You cannot use a Direct Connect gateway in one region to reach another region.  You must order a second Direct Connect link from a Partner in the desired regions.","correct":false}]},{"id":"1db0c4e1-44ef-4a3a-8e71-6dce38e4a0bb","domain":"awscsapro-domain3","question":"You are considering a migration of your on-prem containerized web application and CouchBase database to AWS.  Which migration approach has the lowest risk and lowest ongoing administration requirements after migration?","explanation":"A lift-and-shift approach when containers are involved is often a very easy and low-risk way to migrate to the cloud.  ECS is a good option of you already have a container landscape.  Fargate provides more automated scale and management, but AWS wants users to treat Fargate as an ephemeral platform, so an application like CouchBase that requires persistent storage would not work well.  Our best option for least management is ECS on an EC2 cluster.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud | AWS Cloud Enterprise  Strategy Blog"},{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_GetStarted.html","title":"Getting Started with Amazon ECS using Fargate - Amazon Elastic Container  Service"}],"answers":[{"id":"e2ca91149ec9ad6a9ca978f52b63f9a7","text":"Use SCT to read the existing Couchbase schema and recreate it in DynamoDB.  Use DMS to initially migrate the data from Couchbase and keep it in sync.  Import the web application into ECS using a Fargate cluster.  Update the ECS web application to use DynamoDB.  Once the AWS landscape is proven, do a final commit from the web application container state to the latest version in the registry.  Wait until ECS completes the update of the new container and change DNS entries to point to the new AWS landscape.","correct":false},{"id":"e1b011698db39a5bd75f9561625d28a0","text":"Import the containers into Elastic Container Registry.  Deploy the web application and database on ECS using an EC2 cluster.  Once the AWS version is proven, do a final commit of the container state to the latest version in the registry and use Force New Deployment on the ECS console for the service.  Change over DNS entries to point to the new AWS landscape.","correct":true},{"id":"77591c2773a9058fb91669c7bb4104ac","text":"Provision sufficient sized EC2 instances to host the web application and Couchbase.  Manually install the web application and Couchbase on the EC2 instances and configure rsync and DMS to synchronize the web server and database respectively.  Once the AWS environment is proven, change the DNS entries to point to the new AWS landscape.  Mange the instances going forward with AWS Config.","correct":false},{"id":"ecd3b08ae8761671f56d59cca00dba2e","text":"Use Server Migration Service to migrate the on-prem servers into AWS as AMIs.  Configure data volume replication to synchronize both the web server and database AMIs.  Run in parallel for no longer than 90 days.  When the new environment is proven, change over the DNS entry to point to the new AWS landscape.","correct":false}]},{"id":"2b66bd04-756d-4e2f-a628-1e9b76a57066","domain":"awscsapro-domain2","question":"A media company is producing a live streaming video broadcast of a sporting event for a customer. The announcers will be delivering play-by-play analysis in English. The broadcast will be aired over the Internet, and will require real-time subtitles in Spanish. The company has decided to run all aspects of the production on AWS. Which architecture will provide the functionality needed to deliver the broadcast with real-time subtitles?","explanation":"AWS Elemental MediaLive is a broadcast-grade live video processing service that lets you create high-quality video streams for delivery to televisions and internet-connected devices. Storing it's output in S3 can trigger a Lambda function to extract the unsigned PCM audio from the video segments. A second Lambda function can use Amazon Transcribe to convert the audio to text, which can then be run through Amazon Translate to create the Spanish subtitles. The first Lambda function can send the Spanish subtitle files, the manifests, and the video files to AWS Elemental MediaPackage for distribution through CloudFront. AWS Elemental MediaTailor is used to insert targeted advertising into video streams, not enhance video with subtitles. Amazon Comprehend provides text sentiment analysis, not speech to text conversion.","links":[{"url":"https://aws.amazon.com/medialive/","title":"AWS Elemental MediaLive"},{"url":"https://aws.amazon.com/transcribe/","title":"AWS Transcribe"},{"url":"https://aws.amazon.com/translate/","title":"AWS Translate"},{"url":"https://aws.amazon.com/mediapackage/","title":"AWS Elemental MediaPackage"},{"url":"https://aws.amazon.com/solutions/live-streaming-with-automated-multi-language-subtitling/?did=sl_card&trk=sl_card","title":"Live Streaming with Automated Multi-Language Subtitling"}],"answers":[{"id":"a8d998390ff1bff69d4a734c8c8747e6","text":"Feed the live video into AWS Elemental MediaLive and deliver it's output to Amazon S3. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Invoke a second Lambda function, which uses Amazon Transcribe to convert the audio files to text and return the text to the first Lambda function. Have the first Lambda function use Amazon Translate to create the Spanish transcript. Send the subtitle files, manifests, and video files to AWS Elemental MediaPackage. Create an Amazon CloudFront distribution with MediaPackage as its origin.","correct":true},{"id":"58eec57699d2f82a27ad049e949eb09e","text":"Send the live video to AWS Elemental MediaLive and store it's output in Amazon S3. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Have the Lambda function call Amazon Transcribe to convert the audio files to text, and then use Amazon Translate to create the Spanish transcript. Use AWS Elemental MediaTailor to insert the subtitles into the video segments. Send the video files to AWS Elemental MediaStore. Create an Amazon CloudFront distribution with MediaStore as its origin.","correct":false},{"id":"2d6eb42c2e56159dbe9d0eb1c06b9681","text":"Transmit the live video to AWS Elemental MediaLive and deliver it's output to Amazon Kinesis Video Streams. Configure S3 and Amazon Comprehend as consumers of the stream. Have Comprehend write the text files to a different S3 bucket than the video files, and trigger a Lambda function on that bucket to have Amazon Translate create the Spanish transcripts. Use AWS Elemental MediaTailor to insert the subtitles into the video segments. Send the video files to AWS Elemental MediaStore. Create an Amazon CloudFront distribution with MediaStore as its origin.","correct":false},{"id":"1cc4dd5872e2deead228ed9c6651c445","text":"Deliver the live video to AWS Kinesis Data Streams and make Amazon S3 the consumer. Trigger a Lambda function to extract the audio from the video segments and save the audio files in S3. Invoke a second Lambda function, which uses Amazon Comprehend to convert the audio files to text and return the text to the first Lambda function. Have the first Lambda function use Amazon Translate to create the Spanish transcript. Send the subtitle files, manifests, and video files to AWS Elemental MediaPackage. Create an Amazon CloudFront distribution with MediaPackage as its origin.","correct":false}]},{"id":"c01346c8-d230-4b52-b53d-78cdbfbc7794","domain":"awscsapro-domain4","question":"You work for an Insurance Company as an IT Architect. The development team for a microservices-based claim-processing system has created containerized applications to run on ECS. They have spun up a Fargate ECS cluster in their development VPC inside a private subnet. The containerized application uses awslogs driver to send logs to Cloudwatch. The ECS task definition files use private ECR images that are pulled down to ensure that the latest image is running always. The cluster is having connectivity problems as it cannot seem to connect with ECR to pull the latest images and neither can it connect with Cloudwatch to log. The development team has approached you to help troubleshoot the issue. What is a possible reason for this and what is the best way to fix it?","explanation":"ECS Fargate clusters can be deployed in a private subnet. Hence, we can safely eliminate the choice that says that ECS Fargate clusters must be deployed in a public subnet only.\nECS Fargate clusters do not need the user to control the ECS Agent Version on the nodes, as Fargate is serverless by design. Hence, we can safely eliminate the choice that deals with ECS Agent Version.\nThis leaves two options. One proposes using NAT Gateway. The other proposes using ECS Interface VPC Endpoint. Both are working solutions. However, one of them makes a false claim - it states that ECS Fargate clusters connect to ECR or Cloudwatch only over the internet. That is not true, as it can connect either using a public or a private network. Hence, the only fully correct choice is the one that uses ECS Interface VPC Endpoint","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/vpc-endpoints.html","title":"Amazon ECS Interface VPC Endpoints (AWS PrivateLink)"},{"url":"https://aws.amazon.com/blogs/compute/setting-up-aws-privatelink-for-amazon-ecs-and-amazon-ecr/","title":"Setting up AWS PrivateLink for Amazon ECR"}],"answers":[{"id":"d103c9eb11e007e2a9410cfb2e1bc1ba","text":"The version of the ECS agent may be too old. To fix this, upgrade the ECS Agent version in the cluster nodes to be compatible with connectivity requirements","correct":false},{"id":"d73a8a48aaef0ddbe725f06264c3c6a3","text":"ECS Fargate clusters can connect to ECR (to pull down latest private images) or Cloudwatch (to log) only over the internet. Hence, if it is deployed in a private subnet, it needs a route to a NAT Gateway which must be connected to an Internet Gateway. To fix this issue, deploy a NAT Gateway in a public subnet of the VPC and add appropriate routes to the Routing Table","correct":false},{"id":"f54cb63446f49ab92455024f17ae67b8","text":"ECS Fargate clusters must be deployed in a public subnet so that it can use the Internet Gateway to communicate with ECR or Cloudwatch. To fix this, redeploy the cluster in a public subnet","correct":false},{"id":"43901acf8308ae39f9cf7006afa72751","text":"ECS Fargate clusters can connect to ECR (to pull down latest private images) or Cloudwatch (to log) using either private or public network. Hence, if it is deployed in a private subnet, deploy an ECS Interface VPC Endpoint in the same subnet for connecting to internal services. Add appropriate routes to the Routing Table","correct":true}]},{"id":"2c688b4f-f267-472d-a68f-db7c9070bfae","domain":"awscsapro-domain5","question":"An application has a UI automation test suite based on Selenium and the testing scripts are stored in a GitHub repository. The UI tests need a username and password to login to the application for the testing. You check the test scripts and find that the credentials are saved in the GitHub repository using plain text. This may bring in some potential security issues. You suggest saving the username and password in a secure, highly available and trackable place. Which of the following methods is the easiest one?","explanation":"AWS Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. The credentials can be stored as ciphertext. The service is highly scalable, available, and durable. It also integrates with CloudTrail so the usage is easy to track. DynamoDB, DocumentDB and S3 are not designed to store parameters. These services need more configurations and are not as simple as AWS Parameter Store.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html","title":"AWS Systems Manager Parameter Store"}],"answers":[{"id":"33442a701fc2a058eea89266ea439ee4","text":"Create a table in DynamoDB that has a primary key for username and a sort key for password. Select the encryption type as \"KMS - AWS managed CMK\". Enable the on-demand read/write capacity mode.","correct":false},{"id":"d540b786961136abf5dfd186e84fcca0","text":"Save the username and password in AWS Parameter Store. Select the SecureString type to encrypt the data with the default key in Key Management Service (KMS). Modify the scripts to fetch the parameter values through AWS SDK.","correct":true},{"id":"26e06a6fc6a107b0dd5aac356ad3908d","text":"Edit a JSON file to store the username and password and upload the file to an S3 bucket. Encrypt the S3 bucket with SSE-S3. Modify the S3 bucket policy to only allow the testing machines to get the file.","correct":false},{"id":"fed8d54a1ce26b883744ac61a13e5ac9","text":"Configure an Amazon DocumentDB cluster with the d5.r5.large instance class. Create a schema for username and password. Enable Auto Scaling for the cluster and set up the default number of instances to be 3.","correct":false}]},{"id":"73708d6f-e6cb-4b8f-90d9-723a2961496e","domain":"awscsapro-domain2","question":"Your team is architecting an application for an insurance company.  The application will use a series of machine learning methods encapsulated in an API call to evaluate claims submitted by customers.  Whenever possible, the claim is approved automatically but in some cases were the ML API is unable to determine approval, the claim is routed to a human for evaluation.  Given this scenario, which of the following architectures would most aligned with current AWS best practices?","explanation":"Formerly, AWS recommended SWF for human-involved workflows.  Now AWS recommends Step Functions be used as it requires less programmatic work to build workflows and is more tightly integrated into other AWS services.","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html","title":"Starting a State Machine Execution in Response to Amazon S3 Events - AWS  Step Functions"}],"answers":[{"id":"fa669ed2f0666420f19ad9c8836509f6","text":"Take in the claims into an SQS queue.  Create a Lambda function to poll the SQS queue, fetch the claim and submit the API call.  Use another Lambda function to evaluate the API results and if the claim is not approved, place the claim in a dead letter queue.  Train a person to periodically log into the SQS console and read the dead letter queue for review.","correct":false},{"id":"dfc86c259de4881886ffdac5b8106777","text":"Create a State Machine using Step Functions and a Lambda function for calling the API.  Intake the claims into an S3 bucket configured with a CloudWatch Event.  Trigger the Step Function from the CloudWatch Event.  Create an Activity Task after the API check to email an unapproved claim to a human.","correct":true},{"id":"837fa3e7c6b347eddfa7fefd2a092017","text":"Create a workflow using Simple Workflow Service and an EC2 fleet to host worker and decider programs.  Create worker programs for each processing step and ML API call.  Create decider program to receive the output of the API and decide if the claim is approved.  For unapproved claims, create a worker program use the WorkMail SDK to place the unapproved claim into a mailbox to be reviewed by a human.","correct":false},{"id":"5298caaea3a55734efa8c62e637d0d40","text":"Use Kinesis to take in the claims and save them on S3 using Firehose.  Use Sagemaker to analyze the claims on S3 as a training set and devise a decider function.  Save the approved claims to another S3 bucket setup with an Event to trigger an SES message to a reviewer.","correct":false}]},{"id":"abc17e7b-6b75-45e3-a5c0-ea0f55d4df97","domain":"awscsapro-domain2","question":"Your client is a software company starting their initial architecture steps for their new multi-tenant CRM application.  They are concerned about responsiveness for companies with employees scattered around the globe.  Which of the following ideas should you suggest to help with the overall latency of the application?","explanation":"CloudFront can cache both static and dynamic content.  By setting a high TTL, we allow CloudFront to serve content longer before having to refresh from the origin.  Additionally, Lambda@Edge can intercept the request and direct the requester to a region based on the geographic origin of the request.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/11/lambda-at-edge-now-supports-content-based-dynamic-origin-selection-network-calls-from-viewer-events-and-advanced-response-generation/","title":"Lambda@Edge Now Supports Content-Based Dynamic Origin Selection, Network  Calls from Viewer Events, and Advanced Response Generation"}],"answers":[{"id":"0fc3951d630f285646b22cdd30f43eee","text":"Architect the system to use as many static objects as possible with high TTL.  Use CloudFront to retrieve both static and dynamic objects.  POST and PUT new data through CloudFront.","correct":true},{"id":"afa9743126cd2b0644654f2439a2aa0a","text":"Install the application on several regions around the globe.  Use RDS cross-region read replication for PostgreSQL to ensure a strongly consistent data store.","correct":false},{"id":"35483961564002569ee69763e24961fa","text":"Install the application in several regions around the globe.  As new customers and users are on-boarded, pre-cache their user data in CloudFront for that region.  Use AWS Batch to routinely expire the cache to ensure the latest updates are visible.","correct":false},{"id":"35638855dc45f62b3801906fd9a6d87c","text":"Install key parts of the application in multiple AWS regions chosen to balance latency for geographically diverse users.  Use Lambda@Edge to dynamically select the appropriate region based on the users location.","correct":true},{"id":"702a6122527d0830134717e0e7323bd0","text":"Store the data in a DynamoDB Global Table.  Use an auto scaling ElastiCache cluster with Memcached as a caching layer.  Distribute static elements of the application via CloudFront.  Use Route 53 Weighted routing to dynamically route users to the nearest region.","correct":false}]},{"id":"b77449a6-88a8-4ac9-ae75-664623acccd3","domain":"awscsapro-domain4","question":"Which of the following activities will have the most cost impact (increase or decrease) on your AWS bill?","explanation":"Provisioning an EIP to a running instance or using Placement Groups or App Mesh all do not cost anything.  OpsWorks Stacks on EC2 does not cost anything but using it for on-prem systems does cost a small amount.  The only thing on this list that would increase your AWS bill is adding a Route 53 hosted zone.","links":[{"url":"https://aws.amazon.com/route53/pricing/","title":"AWS | Amazon Route 53 | Pricing"}],"answers":[{"id":"568c5ec8d576ba0532c9e440440a3be9","text":"Provision an Elastic IP and associate it to a running instance.","correct":false},{"id":"a1597dd35c1fb3560a6831b69b72bd26","text":"Add a new Route 53 hosted zone.","correct":true},{"id":"fb2023a84d6c95591c6f756c641eae0d","text":"Deploy existing reserved instances into a Placement Group.","correct":false},{"id":"f80a8cefb8a1936309b0092f00c241b2","text":"Begin using AWS OpsWorks Stacks on EC2 to manage your landscape","correct":false},{"id":"f6e98aff4272a541442980d4782b0524","text":"Start using AWS App Mesh to improve the stability of your existing service landscape.","correct":false}]},{"id":"c1333471-d052-4710-bcdb-facadc095d70","domain":"awscsapro-domain5","question":"You are setting up a corporate newswire service for a global news company.  The service consists of a REST API deployed on EC2 instances where customers can retrieve the latest news articles in real-time that happen to contain their company name.  This allows companies to monitor all news sources for stories where they are mentioned.  Because of the worldwide reach of the new site, you want to position servers around the globe.  You want to publish one subdomain name globally (api.domain.com) and have the requesters directed to the nearest region based on latency.  In each region, you want to be able to accommodate blue-green deployments without downtime as well.  What steps do you take?","explanation":"We want to use weighted routing records for local instances so we have the ability to adjust weights and shift traffic during blue-green deployments.  Latency-based routing would take care of funneling requests to the site with the lowest latency.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html","title":"How Health Checks Work in Complex Amazon Route 53 Configurations - Amazon  Route 53"}],"answers":[{"id":"b850f1c18972d022213271c5d673e07f","text":"First setup weighted routing records for the local instances in the region in Route 53.  Assign equal weights with all sharing the same regional subdomain name (us-east-2.api.domain.com).  Next, create latency alias records by creating multiple entries for api.domain.com--each pointing to the regional subdomains.","correct":true},{"id":"a60aaaf971cbd546c1ec57e08ea38274","text":"We would first create geo-spatial records for the local resources in each region (us-east-2.api.domain.com) and assign equal weights.  Next, we create latency-based routing records for the top level subdomain (api.domain.com) and direct those to the regional records as an alias.  We must also disable Health Check on the latency record to ensure the localized Health Check is used.","correct":false},{"id":"6e56101de6ed2ca97550d5025ddf559a","text":"Using Route 53, we first create the top-level api.domain.com with a geolocation policy.  We then create latency-based routing records for the instances in each region (us-east-2.api.domain.com).  Next, we configure the countries closest to each region in the geolocation policy to direct them to the regional records.","correct":false},{"id":"41e30b8e30cbd737ced85953d7e3e939","text":"Use CloudFormation to create a distribution of the website.  Create an alias record for the subdomain (api.domain.com) in Route 53 and assign it to the CloudFront distribution.  To ensure no lag in news retrieval, set the maximum TTL on the CloudFront distribution to 0.","correct":false}]},{"id":"6fcb65b9-6213-4b86-b7d8-48feb4dee16d","domain":"awscsapro-domain2","question":"You have registered a domain name (example.com) and created a hosted zone in Route 53. The network team configures several record sets that route the traffic to AWS resources including CloudFront distributions and Elastic Load Balancers. You need to collect the information of DNS queries that Route 53 receives for this particular hosted zone and save the data in a secure and highly available place. Which option would you choose to record the required information?","explanation":"You can configure Amazon Route 53 to log the query information to a CloudWatch Logs log group in the US East Region. Then you can use CloudWatch Logs tools to access the query logs. The logs from Route 53 cannot be directly forwarded to an S3 bucket or CloudWatch Metrics. VPC Flow Logs can capture IP traffic information going to and from network interfaces but the logs do not contain the DNS query data.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/query-logs.html","title":"Logging DNS Queries"}],"answers":[{"id":"e5ce673695e34b533c1aa4dea4673838","text":"In Amazon Route 53 management console, forward the Route 53 query logs to CloudWatch Metrics. Save the metrics data in CloudWatch Logs and ensure the log events in the log group never expire so that the logs are not deleted by CloudWatch.","correct":false},{"id":"4d3fcf4269f935cb4ecc5e67b1c8777d","text":"Create a CloudWatch Log group in US East (N. Virginia) Region and publish the DNS query logs to the log group. Configure an IAM policy to grant Route 53 permission to publish query logs to the log group.","correct":true},{"id":"b329ad4dfc36a45f3f0e594f9b27dfd9","text":"Select the Route 53 hosted zone in each AWS region and configure it to forward its query logs to an S3 bucket. The S3 bucket should have a bucket policy that allows the Route 53 service to put objects. Each AWS region should be assigned a folder in the S3 bucket.","correct":false},{"id":"2fdbd173f2e3d6aef24c02df40c9dcb8","text":"Find out the AWS resources including the origins of CloudFront distributions and the targets of Elastic Load Balancers. Locate the VPCs of these resources and enable VPC Flow Logs. Save the logs in an S3 bucket that contains the DNS query records from Route 53.","correct":false}]},{"id":"bc9f1b35-bf56-4bf1-b563-9bd2d864e4bc","domain":"awscsapro-domain2","question":"A global digital automotive marketplace is using Lambda@Edge function with CloudFront to redirect incoming HTTP traffic to custom origins based on matching custom headers or client IP addresses with a list of redirection rules. The Lambda@Edge function reads these rules from a file, rules.json, which it fetches from an S3 bucket. The file changes every day because several teams in the company uses the file for different purposes, including but not limited to, (a) the security team uses the file to honeypot potential malicious traffic (b) the engineering team uses the file to do A-B testing on new features, (c) the product team experiments with new mobile platforms by redirecting traffic from a specific kind of mobile device to a specific set of server farms, etc.. As a result, the file can be as big as 200 KB. Recently, the response time of the website has degraded. On investigation, you have found that this Lambda@Edge function is taking too long to fetch the rules.json file from the S3 bucket. The existing CI-CD pipeline deploys the file to a versioning-enabled S3 bucket when any change is committed to source control. Any change in rules.json must reflect within 1 hour at all Cloudfront Edge locations. Select two options from the ones below that will not work in improving the latency of fetching this file?","explanation":"A key to answering this question is to not miss the fact that it asks which two of the answers will not help. AWS SA-P exam can occasionally frame the question with a not. Also, knowledge of how Lambda@Edge functions work with CloudFront is important for the exam.\nThere will be no improvement in the fetching time if we reconfigure the S3 bucket as a static website. In fact, doing so might add a layer of redirection during routing.\nLambda@Edge does not guarantee the persistence of global variables in memory between invocations. While it might be possible to use global variables for a short time as cache, provided the code does not make any assumptions about the guarantee of persistence, it is a bad idea to solely depend on Lambda@Edge memory between invocations. AWS does not guarantee using the same container instance for any number of requests, though it will try to re-use a warmed up instance for the same function invocation landing on the same edge node. If it is re-using the same container instance from the one used by the last Lambda@Edge function, the global variable trick will work. However, as the option clearly says that such usage is guaranteed (which is false and will not work), it is one of the answer choices to select in this case.","links":[{"url":"https://aws.amazon.com/blogs/networking-and-content-delivery/leveraging-external-data-in-lambdaedge/","title":"Leveraging external data in Lambda@Edge"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-limits.html#limits-lambda-at-edge","title":"Limits on Lambda@Edge"}],"answers":[{"id":"ffa2541e9e07a612a02728b1135014f9","text":"Include the rules.json file in the Lambda@Edge deployment package. Change the CI-CD pipeline to deploy a new Lambda@Edge version every time the file changes. Change the Lambda@Edge function code to read the file locally instead of reading it from S3. This will improve the latency of fetching the file.","correct":false},{"id":"058638ecef0dd62ee83782f5cbbba63b","text":"Reconfigure the S3 bucket as a static website. Use the website endpoint to download the file instead of directly accessing the bucket from the Lambda@Edge function. This will cause HTTP GET requests to be cached by S3, thus improving the latency of fetching the file","correct":true},{"id":"fd6d6c32ad84f6d5856477cd8f27d361","text":"Define a separate cache behaviour for *.json in your Cloudfront web distribution, setting the origin as the S3 bucket. Change the Lambda@Edge function code to use the Cloudfront download URL instead of downloading the file directly from S3. This way, the file will be cached by Cloudfront avoiding expensive round trip time to S3 each time. Set the Cloudfront TTL to 45 minutes.","correct":false},{"id":"5a14a6d0367c2a6daf2d4faccf1fbdb3","text":"Change the Lambda@Edge code to save the contents of the rules.json file in a global variable so that it is cached in Lambda@Edge memory, with a TTL of 55 minutes, persisted between invocations. Lambda@Edge guarantees persistence of variables in memory between invocations.","correct":true}]},{"id":"af6501cb-5a69-4af7-a5e7-cf03d6ce09c1","domain":"awscsapro-domain1","question":"As an AWS Solutions Architect, you are responsible for the configuration of your company's Organization in AWS. In the Organization, the Root is connected with two Organizational Units (OUs) called Monitor_OU and Project_OU. Monitor_OU has AWS accounts to manage and monitor AWS services. Project_OU has another two OUs as its children named Team1_OU and Team2_OU. Both Team1_OU and Team2_OU have invited several AWS accounts as their members. To simplify management, all the AWS accounts under Team1_OU and Team2_OU were added with a common administrative IAM role which is supposed to be used by EC2 instances in their accounts. For security concerns, this role should not be deleted or modified by users in these AWS accounts. How would you implement this?","explanation":"The requirement of this scenario is that the IAM role should not be modified by IAM principals in Team1_OU and Team2_OU. The best place to implement this is in SCP as it provides central management. Since Team1_OU and Team2_OU can inherit the SCP policy from Project_OU, only Project_OU needs to attach the SCP that denies the action. In the meantime, the Root should have a default full access policy. It is improper to use IAM policies or Permissions Boundary to achieve this requirement as they may be easily changed at a later stage by other IAM users, and it is also complicated to implement if there is a large number of IAM principals.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"How SCP works in AWS Organization?"}],"answers":[{"id":"46f9090f957b494e2e0075ceef9bac8b","text":"For the AWS accounts in Team1_OU and Team2_OU, configure a Permissions Boundary in IAM principals to prevent them from making modifications to this particular IAM role. As a result, all IAM principals will not have IAM policies to potentially change the role.","correct":false},{"id":"77abf7e62575051a27444694461acccd","text":"The nodes of Root, Project_OU and Monitor_OU in the Organization should allow all actions. For AWS accounts in Team1_OU and Team2_OU, attach an IAM policy that denies modifying the IAM role to IAM users, groups and roles.","correct":false},{"id":"e5acb8782282566185828b0ca39813fc","text":"Configure a new SCP policy to prevent IAM principals from deleting or modifying the IAM role. Attach the SCP policy to Project_OU, Team1_OU and Team2_OU. Configure a default full access SCP policy to Monitor_OU.","correct":false},{"id":"f868081ae3429a7ed8318636537ec834","text":"Make sure that Root node has a SCP policy that allows all actions. Create a SCP policy that restricts IAM principals from changing this particular IAM role. Attach the SCP policy to Project_OU.","correct":true}]},{"id":"a7c939f1-277e-469f-a209-9b290e8136c9","domain":"awscsapro-domain5","question":"Your company has contracted with a third-party Security Consulting company to perform some risk assessments on existing AWS resources.  As part of a routine list of activities, they inform you that they will be launching a simulated attack on one of your EC2 instances.  After the Security Group performed all their activities, they issue their report.  In their report, they claim that they were successful at taking the EC2 instance offline because it stopped responding soon after the simulated attack began.  However, you're quite certain that machine did not go offline and have the logs prove it.  What might explain the Security company's experience?","explanation":"AWS Shield and other counter-measure technologies work to protect all AWS customers from DDoS attacks.  Unless AWS was aware of the test time and expected duration, its likely the traffic was blocked as suspicious.  AWS Firewall Manager is used to manage WAF ACLs and not dynamically blacklist IPs.  Similarly, VPC Flow Logs cannot automatically implement NACL changes as described here. Despite being a permitted service, traffic suspected of being malicious will still be blocked","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/penetration-testing/","title":"Submit a Penetration Testing Request"}],"answers":[{"id":"82a05cb45adab6d248655e827de16c6f","text":"AWS Firewall Manager is dynamically adding a blacklist entry for the Security Company's testing machine because it sees the traffic as a threat.","correct":false},{"id":"f3f963b71e307f8c28109631df115418","text":"The EC2 instance is using an ENI and the Security Company temporarily exceeded the throughput limit resulting in a throttling of their connection.","correct":false},{"id":"7d4826f179b8dc854c9cfb6e43678373","text":"The VPC Flow Logs record the spike in suspicious traffic and implement an update to the inbound NACL to block the remote IP address.","correct":false},{"id":"e3facacbe52b6423f9cf2e700d8e0b81","text":"The Security Company's traffic was seen as a threat and blocked dynamically by AWS.  AWS must grant permission before any penetration testing is done.","correct":true}]},{"id":"33803c8a-b588-4dca-8067-e500383254f3","domain":"awscsapro-domain4","question":"You work for a retail services company that has 8 S3 buckets in us-east-1 region. Some of the buckets have a lot of objects in them. There are Lambda functions and EC2-hosted custom application code where the names of these buckets are hardcoded. Your manager is worried about disaster recovery. As part of her business continuity plan, she has requested you to set up Cross-Region Replication of these S3 buckets to us-west-1, ensuring that the replicated objects are using a less expensive Storage Class because they would not be accessed unless disaster strikes. You are worried that in the event of failover due to the entire us-east-1 region being unavailable, the application code, once deployed in us-west-1, must continue to work while trying to access the S3 buckets in the new region. She has also requested you to start taking periodic snapshots of EBS Volumes and make these snapshots available in the us-west-1 region so that EC2 instances can be launched in us-west-1 using these snapshots if needed. How would you ensure that (a) the launching of EC2 instances works in us-west-1 and (b) your application code works with the us-west-1 S3 buckets?","explanation":"This question presents two problems - (1) how to ensure that EBS snapshots are created periodically and are also made available in a different region for launching required EC2 instances in case of failure of the primary region (2) how to deal with application code where S3 bucket names are hardcoded and whether this hardcoding will impact disaster recovery while trying to run in a different region. Both of these problems are real-life issues AWS customers face when designing and planning their disaster recovery solutions.\n(1)Remember that Data Lifecycle Manager can only schedule snapshot creation in the same Region. If we want to copy that snapshot into a different region, we must write our own scripts or Lambda functions for doing that. Hence, the choices that state that DLM can be used to directly create the snapshot into different regions are eliminated. Additionally, only root volume snapshots can be used to create an AMI. Non-root EBS Volume snapshots cannot be used to generate an AMI. Hence, the choices that specify using non-root volume snapshots are eliminated.\n(2)Remember that S3 bucket names are globally unique. Hence, one cannot create a second S3 bucket in the DR Region with the same name as the bucket in the primary region. Hence, the options that hint the creation of S3 buckets by the same name are eliminated. This results in a problem if S3 names are hardcoded in the application - that application will simply not run in a new region, it will fail. Hence, it is best to avoid hardcoding, and fetch the S3 bucket name from a key-value storage service like AWS Systems Manager Parameter Store at runtime. Creating this Parameter Store in each region and storing the correct bucket names in them can help in designing this non-hardcoded solution. Additionally, enabling Cross-Region Replication does not copy pre-existing content. Hence, the choices that suggest that pre-existing content will be automatically copied are eliminated.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-launch-snapshot.html","title":"Launching an Instance from a Backup"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/ec2/copy-snapshot.html","title":"Copy-snapshot documentation"}],"answers":[{"id":"824dc187ab89444593b50521f60b8ff3","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":true},{"id":"fae496411f2d461e0926abfdf8ad8b64","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":false},{"id":"02e8aed40d51e4bf9256f1ed436aa069","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false},{"id":"fc1a2efe00ef610249a55dadb0dd64fe","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false}]},{"id":"758841a6-0db9-4a02-a85b-699092182451","domain":"awscsapro-domain4","question":"You have just finished a contract with your client where you have helped them fully migrate to AWS.  As you are preparing to transition out of the account, they would like to integrate their current help desk software, Jira, with the AWS support platform to be able to create and track tickets in one place.  Which of the following do you recommend?","explanation":"You must have subscribe to at least the Business Support Plan to gain access to the AWS Support API. ","links":[{"url":"https://aws.amazon.com/premiumsupport/compare-plans/","title":"AWS Support - Compare all support plans"}],"answers":[{"id":"8b3759a4758ab061f15798717c42c8e0","text":"Subscribe to the Platinum Support Plan and direct them to the AWS Support API documentation","correct":false},{"id":"0846705e92c2fcc07e439ddc2698dd15","text":"Use API Gateway to create a proxy service for the AWS Support API to allow third-party access.  Direct them to the AWS Support API documentation.","correct":false},{"id":"55d01f0a938121dd662f6d3068168eec","text":"Subscribe to the Developer Support Plan and direct them to the AWS Support API documentation","correct":false},{"id":"6c97334691882e8f846aad300e6a7811","text":"Subscribe to the Business Support Plan and direct them to the AWS Support API documentation","correct":true},{"id":"7e007e2c40ad802633ef945f34b23230","text":"It is currently not possible to integrate third-party products into the AWS Support system.  Offer to contract with them to perform manual updates between Jira and AWS Support cases.","correct":false}]}]}}}}
