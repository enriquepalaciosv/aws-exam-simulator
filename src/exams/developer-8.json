{"data":{"createNewExamAttempt":{"attempt":{"id":"5555847b-018e-40eb-8a0d-bc860125bc7f"},"exam":{"id":"af47e42f-dd6f-427b-83ae-b2335e966b59","title":"AWS Certified Developer - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"6969665b-60e1-4b4b-88db-b3a70e483f9a","domain":"security","question":"Which of the following does Cognito use to manage sign-up and sign-in functionality for mobile and web applications?","explanation":"Cognito User Pools are like a directory, allowing users sign-up and sign-in. Identity pools are used to grant temporary access to unauthenticated guests. IAM users are user account entities which allow you to interact with AWS resources. IAM groups are collections of IAM users and are used to specify permissions for multiple users.","links":[{"url":"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html","title":"Cognito User Pools"}],"answers":[{"id":"a07a0085c7712cdcacc4b2cc1359a351","text":"Identity Pools","correct":false},{"id":"6b92da4fed3ac1c921485e4de62af19b","text":"IAM Users","correct":false},{"id":"cb40a6c9df6d32418e5f67e35ed37fd0","text":"User Pools","correct":true},{"id":"2a3ae7e866e21a59b0cdd627c7f7da55","text":"IAM Groups","correct":false}]},{"id":"7b9da250-0466-4f74-a5f4-a48611e4ad52","domain":"deployment","question":"Which of the following could you NOT achieve using the Amazon SQS Extended Client Library for Java?","explanation":"You can use Amazon S3 and the Amazon SQS Extended Client Library for Java to manage Amazon SQS messages stored in S3. This includes specifying when messages should be stored in S3, referencing message objects stored in S3, getting them, and deleting them.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html","title":"Managing Large Amazon SQS Messages Using Amazon S3"}],"answers":[{"id":"c864c8d423d9f1ff9520db533d3c55f0","text":"Create a new S3 bucket and move a batch of SQS messages into the bucket","correct":true},{"id":"0c34c3d6deafa4117735272f76eb3ab8","text":"Delete a message object from an Amazon S3 bucket","correct":false},{"id":"d6cda81adfce3f5709327b9e75258e8a","text":"Manage large SQS Messages stored on S3","correct":false},{"id":"b236aa2fd73c138dd311cd5ab9732edb","text":"Get a message object from an S3 bucket","correct":false},{"id":"d807fc120f6730850ff6425f9e3c48a0","text":"Specify whether messages are always stored in Amazon S3 or only when the size of a message exceeds 256 KB","correct":false}]},{"id":"47e074a7-e675-4918-92bf-d9a34b82803c","domain":"security","question":"Your application needs to access content located in an S3 bucket which is residing in a different AWS account, which of the following API calls should be used to gain access?","explanation":"The STS AssumeRole API call returns a set of temporary security credentials which can be used to access AWS resources, including those in a different account","links":[{"url":"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html","title":"Providing temporary access to AWS resources"}],"answers":[{"id":"5854d57d52033e05be8f9fda06330abd","text":"STS:AttachRole","correct":false},{"id":"c2599de6f471b00ab3948981939f8315","text":"STS:GetFederationToken","correct":false},{"id":"100979100796827d7bcafe4666e4984f","text":"STS:AssumeRole","correct":true},{"id":"818a55892aa657e5ef8dae6d12ee9273","text":"IAM:AddRoleToInstanceProfile","correct":false}]},{"id":"43e8c8e5-d0e3-4502-b7e7-8c236a6625e3","domain":"mon-trb","question":"A company wants to monitor all traffic to a network interface on their bastion host. They wish to be alerted if there are more than 10 attempts to connect to the host via SSH within a one-hour time interval. What solution can the company employ to meet this requirement?","explanation":"VPC flow logs can be sent to CloudWatch Logs. A CloudWatch metric filter and alarm can be configured to send notifications when the specified criteria are satisfied. CloudTrail is not a supported destination for VPC flow logs. Amazon Inspector cannot be used to inspect network traffic in the way specified by the requirements. It performs vulnerability assessments on the host VM. Lambda functions cannot mount EBS volumes.","links":[{"url":"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html","title":"Amazon S3 Glacier Vault Lock"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-cwl.html#flow-logs-cwl-create-flow-log","title":"Creating a Flow Log That Publishes to CloudWatch Logs"}],"answers":[{"id":"343b93587a12ff88f4e59413fbbd5d96","text":"Install the Amazon Inspector agent on the bastion host. Configure CloudWatch alerts based on Amazon Inspector findings.","correct":false},{"id":"ecb0bcaac946feb8b9a9c2abcaace499","text":"Create a VPC flow log for the network interface with CloudTrail as the destination. Create a Lambda function that queries the CloudTrail logs for SSH login attempts. Trigger the Lambda function every 5 minutes with a scheduled CloudWatch event.","correct":false},{"id":"cdab2e49cfa676ebe99e79d8f77f49b3","text":"Create a Lambda function that mounts the bastion host EBS volume and sends logs to CloudWatch logs. Create a CloudWatch metric filter for destination port 22. Create a CloudWatch Alarm trigger.","correct":false},{"id":"6e51df099a849474d4791bce4aa25bb5","text":"Configure a VPC flow log with CloudWatch Logs as the destination. Create a CloudWatch metric filter for destination port 22. Create a CloudWatch Alarm trigger.","correct":true}]},{"id":"53899b37-d74d-42b3-9be9-4f9f0d37155d","domain":"security","question":"Your company has a corporate identity store used to authenticate its users. Your company also has resources running on AWS. Your admin has created IAM roles and an identity broker that sits between your corporate users and your AWS resources to manage the authentication and authorization process without needing to re-create all your users as IAM users in AWS. Your CISO asked you to summarize the AWS identity federation process to ensure compliance with your applications' security. Which of the following statements correctly describes the authentication process?","explanation":"Users might already have identities outside of AWS, such as in your corporate directory. However, those users might need to work with AWS resources (or work with applications that access those resources). If so, these users also need AWS security credentials in order to make requests to AWS. The process can be summarized as follows: 1. The enterprise user accesses the identity broker application. 2. The identity broker application authenticates the users against the corporate identity store. 3. The identity broker application has permissions to access the AWS Security Token Service (STS) to request temporary security credentials. 4. Enterprise users can get a temporary URL that gives them access to the AWS APIs or the Management Console. A sample identity broker application for use with Microsoft Active Directory is provided by AWS. Details on page 22 of the URL link.","links":[{"url":"https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf?refid=em_","title":"AWS Security Best Practices"}],"answers":[{"id":"fe4c072b395ee1d0521094c0d1b4a005","text":"The identity broker application has permissions to access the AWS Security Token Service (STS) to request temporary security credentials. The enterprise user accesses the identity broker application. The identity broker application authenticates the users against the corporate identity store. Enterprise users can get a temporary URL that gives them access to the AWS APIs or the Management Console.","correct":false},{"id":"2870fde359fa2a54b4e99faccf44cb32","text":"The enterprise user accesses the identity broker application.  Enterprise users can get a temporary URL that gives them access to the AWS APIs or the Management Console. The identity broker application authenticates the users against the corporate identity store. The identity broker application has permissions to access the AWS Security Token Service (STS) to request temporary security credentials.","correct":false},{"id":"9b6e91665534238e195af79b94bb3233","text":"The enterprise user accesses the identity broker application. The identity broker application authenticates the users against the corporate identity store. Enterprise users can get a temporary URL that gives them access to the AWS APIs or the Management Console. The identity broker application has permissions to access the AWS Security Token Service (STS) to request temporary security credentials.","correct":false},{"id":"26461a0c133d31dd86bb803278256573","text":"The enterprise user accesses the identity broker application. The identity broker application authenticates the users against the corporate identity store. The identity broker application has permissions to access the AWS Security Token Service (STS) to request temporary security credentials. Enterprise users can get a temporary URL that gives them access to the AWS APIs or the Management Console.","correct":true}]},{"id":"3a11539f-9ee0-4b96-a649-ec7635e18dc8","domain":"deployment","question":"What is the size of one unit of read capacity?","explanation":"A read capacity unit is 4KB in size.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html","title":"Throughput Capacity for Reads and Writes"}],"answers":[{"id":"d54d09275c9dfc95026a3a52d2f66173","text":"3 KB","correct":false},{"id":"35ef8d3515745f7c6916d7644fb5d6e5","text":"4 KB","correct":true},{"id":"bf361755334066f22d019854dd2be686","text":"1 KB","correct":false},{"id":"e7363d80bd01165d7bb0c75f5add39c0","text":"5 KB","correct":false}]},{"id":"52809d95-a072-4314-8781-a45d93534ad5","domain":"development","question":"You are importing an existing API to API Gateway. Which format is supported for API definition files?","explanation":"The Import API feature supports OpenAPI v2.0 and OpenAPI v3.0 definition files. Swagger is a common tool set that is originally defined the OpenAPI v2.0 specification.  AWS use the name interchangeably with OpenAPI v2.0 .  RAML is supported in a different tool (API Gateway Importer).","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-import-api.html","title":"Importing APIs"},{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-export-api.html","title":"Export an API from API Gateway"},{"url":"https://en.wikipedia.org/wiki/OpenAPI_Specification#History","title":"Swagger & OpenAPI"}],"answers":[{"id":"9a0df1d09b45d123c4f21aa4caf2c205","text":"OpenAPI v3.0","correct":true},{"id":"c31c335ef37283c451b18ba0dd317de1","text":"Angular","correct":false},{"id":"51546bb736b0f9021b167202e967a6cb","text":"RAML","correct":false},{"id":"a15aa8cb8ce8298ee404e93ab4afdf0b","text":"OpenAPI v2.0","correct":true},{"id":"336ff1e9aa6177ea7a71984fa8c241b9","text":"Swagger","correct":true}]},{"id":"4216cacb-ee8a-42e5-acff-9b1ea39357e6","domain":"development","question":"A developer is implementing an IoT application using DynamoDB as the data store for device event data. An application requirement is to automatically purge all event data older than 30 days. What is the optimal option to implement this requirement?","explanation":"Time to Live (TTL) for Amazon DynamoDB is functionality that enables automatic deletion of items after a specified expiration time. The expiration time is defined by a timestamp in the TTL attribute. When Time to Live (TTL) is enabled on a table in Amazon DynamoDB, a background job checks the TTL attribute of items. The background job compares the current time in epoch time format to the time stored in the Time to Live attribute of an item to determine whether the item is expired. If the epoch time value stored in the attribute is less than the current time, the item is marked as expired and then deleted.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html","title":"Time to Live: How It Works"}],"answers":[{"id":"ac0b210d52da3eacb34d48327798c599","text":"Create a new DynamoDB table every 30 days. Delete the old DynamoDB table.","correct":false},{"id":"89ff2a65dcb0d5ab07793957e16ed650","text":"Enable TTL on the DynamoDB table and store the expiration timestamp in the TTL attribute in the epoch time format.","correct":true},{"id":"c408a205dcdc64e12ad635afc658fa61","text":"Implement a Lambda function to perform a query on the table and delete items with timestamp greater than 30 days. Use CloudWatch events to trigger Lambda function.","correct":false},{"id":"0a4018a4dc09457f483a0fb75a49f140","text":"Enable DynamoDB streams on the table. Implement Lambda function to read events from the stream and delete expired items.","correct":false}]},{"id":"36bd2e7a-adf7-4dae-ad6f-5e04d3ca873e","domain":"security","question":"You work for a large government agency which is conducting research for a top secret defense project. You are using SQS to handle messaging between components of a large, distributed application. You need to ensure that confidential data relating to your research is encrypted by the messaging system, which of the following services can you use to centrally manage your encryption keys?","explanation":"You can use a CMK to encrypt and decrypt up to 4 KB (4096 bytes) of data. Typically, you use CMKs to generate, encrypt, and decrypt the data keys that you use outside of AWS KMS to encrypt your data. This strategy is known as envelope encryption. CMKs are created in AWS KMS and never leave AWS KMS un-encrypted. To use or manage your CMK, you access them through AWS KMS.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys","title":"KMS Concepts"}],"answers":[{"id":"0e8433f9a404f1f3ba601c14b026d321","text":"HTTPS","correct":false},{"id":"4a4df63c87b4f42081b846d9b9189984","text":"KMS","correct":true},{"id":"ea52c36203c5f99c3ce2442d531b1a22","text":"SSL","correct":false},{"id":"fa9c36d7e57eae51ce84cfd30a7346a3","text":"Systems Manager Parameter Store","correct":false},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":false}]},{"id":"dc3e7895-b954-4fa8-8d4f-faffeca401d2","domain":"security","question":"Which of the following methods will allow you to *securely* upload/download your data to the S3 service? Pick all that apply.","explanation":"You can securely upload/download your data to Amazon S3 via SSL endpoints using the HTTPS protocol.","links":[{"url":"https://aws.amazon.com/s3/faqs/#security","title":"S3 Security"}],"answers":[{"id":"2b716d646634dd42d3d1ab628b210081","text":"SSL endpoints using the HTTPS protocol","correct":true},{"id":"937a8b8e84ad5481f1983a1842154e18","text":"HTTP endpoints using HTTP protocol","correct":false},{"id":"1019a747b087f11f97ef6a2bf46a1978","text":"HTTP endpoints using HTTPS protocol","correct":true},{"id":"d7ad40729fa333427d4d8c3032d43fdf","text":"SSL endpoints using HTTP protocol","correct":false}]},{"id":"742534cf-2038-4f0d-b859-216cca30339e","domain":"deployment","question":"Which of the following statements relating are correct in relation to DynamoDB?","explanation":"A local secondary index maintains an alternate sort key for a given partition key value.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html","title":"Local Secondary Indexes"}],"answers":[{"id":"b9f028903456b0bc0b4706cf7c6bea8f","text":"You can add a Global Secondary Index to an existing table","correct":true},{"id":"63fb8d2672eb66b5bcaf5267421cacb2","text":"A Local Secondary Index is an index that has the same partition key as the table, but a different sort key to the table.","correct":true},{"id":"661c4d893c5ff8ef12919a23394749c8","text":"A Local Secondary Index is an index that has the a different partition key and a different sort key to the table.","correct":false},{"id":"1e76bf9bca3dc7acaf4a05fe6bedabc2","text":"You can add a Local Secondary Index to an existing table","correct":false}]},{"id":"4a430e9e-4943-4d13-9511-ba93d66d9eaf","domain":"development","question":"A developer is configuring CodeDeploy to deploy an application to an EC2 instance. The application's source code is stored within AWS CodeCommit.\n\nWhat permissions need to be configured to allow CodeDeploy to perform the deployment to EC2?","explanation":"CodeDeploy interacts with EC2 via the CodeDeploy Agent, which must be installed and running on the EC2 instance. During a deployment the CodeDeploy Agent running on EC2 pulls the source code from CodeCommit. The EC2 instance accesses CodeCommit using the permissions defined in its instance profile role; therefore, it is the EC2 instance itself that needs CodeCommit access.\n\nThe specific CodeCommit permission needed to pull code is `codecommit:GitPull`.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control-permissions-reference.html","title":"CodeCommit Permissions"},{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-configure.html","title":"Configure an Amazon EC2 Instance to Work with CodeDeploy"}],"answers":[{"id":"a5f1b9fe7264b6a78932a24f195b1064","text":"Create an IAM Policy with an acton to allow `codecommit:CreatePullRequest` on the required repository. Attach the policy to CodeDeploy's Service role.","correct":false},{"id":"3801756a25b44dc5e1eaaf3a55f6c0fe","text":"Create an IAM policy with an acton to allow `codecommit:GitPull` on the required repository. Attach the policy to the EC2 instance profile role.","correct":true},{"id":"ae4d1eac013f4a67d18129084085b41b","text":"Create an IAM policy with an acton to allow `codecommit:CreatePullRequest` on the required repository. Attach the policy to the EC2 instance profile role.","correct":false},{"id":"dcaaa5b141981d140fe32ea458888500","text":"Create an IAM Policy with an acton to allow `codecommit:GitPull` on the required repository. Attach the policy to CodeDeploy's Service role.","correct":false}]},{"id":"a2d52dd6-9826-449b-b96b-fc7f4edb46eb","domain":"development","question":"You need to announce emergency downtime for a production AWS web application. This downtime notification requires different sets of instructions for different devices. All of the application users signed up to receive SNS notifications from the downtime topic when they began using the application and they are currently subscribed to this topic. What are appropriate ways for you to provide timely, device-specific instructions to end users when announcing this downtime?","explanation":"Using the SNS JSON message generator, you can choose the appropriate endpoint types and edit the generated code to send different text to the different endpoint types.","links":[{"url":"https://docs.aws.amazon.com/sns/latest/dg/mobile-push-send-custommessage.html","title":"Platform-Specific Payloads in Messages to Mobile Devices"}],"answers":[{"id":"41ffaa9f13c5d08d4448e6f04dcc2fc6","text":"Create a different topic for each subscription type, then send one topic to SMS endpoints and the other topic to email endpoints.","correct":false},{"id":"a533b37c270c3d5b92e043a2fe21c3b3","text":"Send multiple messages to the topic and ask users to please ignore message formats that don't pertain to them.","correct":false},{"id":"954d71e78bf180ff122f720542c09639","text":"It's not possible to send SNS messages manually.","correct":false},{"id":"20231f9d3f6d52b54459fecad9bf9b14","text":"Send a single message, but customize the text in the SNS message field so that each device gets only the information that is appropriate for them.","correct":true}]},{"id":"88bcdf37-e9ac-4a5a-9561-a0b40b3d5942","domain":"deployment","question":"You have deployed your application on EC2 using Elastic Beanstalk. You would like to configure your application to send data to X-Ray. Where should you install the X-Ray daemon?","explanation":"To relay trace data from your application to AWS X-Ray, you can run the X-Ray daemon on your Elastic Beanstalk environment's Amazon EC2 instances. Elastic Beanstalk platforms provide a configuration option that you can set to run the daemon automatically. You can enable the daemon in a configuration file in your source code or by choosing an option in the Elastic Beanstalk console. When you enable the configuration option, the daemon is installed on the instance and runs as a service.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-beanstalk.html","title":"Running the X-Ray daemon on AWS Elastic Beanstalk"}],"answers":[{"id":"815e2ca172c43e652a7d680047998174","text":"Install the X-Ray daemon on a Docker container running on your EC2 instance","correct":false},{"id":"b869cc613e5695b1535e3509f2226824","text":"Install the X-Ray daemon on the EC2 instances inside your Elastic Beanstalk environment.","correct":true},{"id":"020611c7739a39c03183a066978fdad7","text":"Install the X-Ray daemon on the EC2 instances located in your own data center","correct":false},{"id":"2e3b28f6b658014b7006069d657a21b0","text":"Manually provision a new EC2 instance and install the X-Ray daemon on the new instance","correct":false}]},{"id":"4b9c267c-f41d-4325-919e-7863e0abb6f3","domain":"development","question":"A three-tier web application is deployed using CloudFormation template. How can the CloudFormation developer ensure that the database resource is saved for backup purposes upon stack deletion?","explanation":"The DeletionPolicy attribute can be used to preserve a specific resource when its stack is deleted. The DeletionPolicy Retain option can be used to ensure AWS CloudFormation keeps the resource without deleting the resource.  The Stack Termination Protection feature enables protection against accidental deletion of an entire stack, not preservation of a specific resource. Similarly, the 'cloudformation:DeleteStack' Action applies to entire stack(s).","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html","title":"DeletionPolicy Attribute"}],"answers":[{"id":"eee064b70f79422e8511f18b251253ef","text":"Create IAM Policy with Effect of Deny for 'cloudformation:DeleteStack' Action.","correct":false},{"id":"6983aeb4c0d42bb293c8ec93966aedbb","text":"Set the DeletionProtection to True in the CloudFormation template.","correct":false},{"id":"558cc27970e2b64a29bdc85f381b8cb9","text":"Set the DeletionPolicy to Retain in the CloudFormation template.","correct":true},{"id":"8093dacb1a766d0f91fa60e48baa87ed","text":"Set Stack Termination Protection to Enable.","correct":false}]},{"id":"97b2f9cc-962b-4c75-82c3-a44815644776","domain":"mon-trb","question":"You are developing a new application using Lambda, API Gateway, S3 and DynamoDB. You would like to record information about incoming and outgoing HTTP requests as well as latency incurred by each component. You have multiple versions of the application to cater for your Development, UAT, Performance Test and Production environments. What is the most efficient way to collect this information and group it according to which environment it relates to?","explanation":"AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, micro-services, databases and HTTP web APIs. When you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segment document as annotations and metadata. Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations","title":"X-Ray Annotations and Metadata"},{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html","title":"Searching for Traces in the AWS X-Ray Console with Filter Expressions"}],"answers":[{"id":"d2ae15c70b6c5a546c571203b1a18474","text":"Use CloudTrail to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false},{"id":"c57540f9c7ee62b718518b73d8ea536c","text":"Use X-Ray to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":true},{"id":"7596c1ed59077a183e47b002a53bb84a","text":"Use CloudFormation to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false},{"id":"8f0f2ab2fa58b6ddef1f41eb2fe56872","text":"Use CloudWatch to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false}]},{"id":"8b887631-86bd-436d-adee-4e2ba3b02111","domain":"security","question":"You have an application running on multiple EC2 instances, however every time an instance fails, your users complain that they lose their session. What can you do to prevent this from happening?","explanation":"There are various ways to manage user sessions including storing those sessions locally to the node responding to the HTTP request or designating a layer in your architecture which can store those sessions in a scalable and robust manner. Common approaches used include utilizing Sticky sessions or using a Distributed Cache for your session management. In order to address scalability and to provide a shared data storage for sessions that can be accessed from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution for this is to leverage an In-Memory Key/Value store such as ElastiCache.","links":[{"url":"https://aws.amazon.com/caching/session-management/","title":"Session management in AWS"}],"answers":[{"id":"76fc6bddee6b0f8d088ea5cbe4e57160","text":"Store session state in S3","correct":false},{"id":"b225818943ba4680b8e7dc9d9c376359","text":"Store session state in RDS","correct":false},{"id":"89230492f141a4f85234c624287bb96a","text":"Store session state in ElastiCache","correct":true},{"id":"ef4fd36fa55c3c499f3fffa82a0c95e8","text":"Store session state in on the Elastic Load Balancer","correct":false},{"id":"b193b1caff1bda86125cc326ca1058ac","text":"Store session state on a dedicated EC2 instance","correct":false}]},{"id":"031bc6f7-17b3-47aa-b449-a97b2ae63aa6","domain":"deployment","question":"Which section of the AWS Serverless Application Model template would you use to describe the configuration of a Lambda function and an API Gateway endpoint, if you were deploying your application using AWS SAM?","explanation":"Use the Transform section to describe your Serverless functions when using the serverless application model. Under the Transform section, you define the resources you want to deploy.","links":[{"url":"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/appendix-appendix-sam-templates-and-cf-templates.html","title":"CloudFormation Resources"},{"url":"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-template-basics.html","title":"AWS SAM Template Concepts"}],"answers":[{"id":"ba0e0cde1bf72c28d435c89a66afc61a","text":"Sam","correct":false},{"id":"e93acb146e114b5dfa6ce2d12dcb96e4","text":"Functions","correct":false},{"id":"2ff4148554480a37f85efd299df04850","text":"Transform","correct":true},{"id":"7df96b18c230f90ada0a9e2307226338","text":"Templates","correct":false}]},{"id":"e516825d-bec2-463c-9a64-63be7bc91509","domain":"refactoring","question":"You are developing a serverless application which runs on Lambda, DynamoDB and API Gateway. The application needs to support an average of 5,000 requests per second. During testing, the Test Team want to test for peaks of 2.5 x the average load (12,500 requests per second). Shortly after testing begins, your application crashes with API Gateway generating a 429 error code. What could be the reason for this?","explanation":"By default, API Gateway limits the steady-state request rate to 10,000 requests per second. The 429 error means that the application is generating too many requests and is being throttled.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html","title":"API Request Throttling"}],"answers":[{"id":"c44bbd8c68bc540a9f4ec1eb6aef1f14","text":"Your Lambda function has run out of memory, you need to increase CPU capacity in order to increase memory capacity","correct":false},{"id":"71cfc755a5cb85f74a6c94a83c5a102b","text":"Your Lambda function has run out of CPU, you need to increase the memory allocation in order to increase CPU capacity","correct":false},{"id":"998c50932ec426f49890c7568b789ab0","text":"Your tests have caused the application to hit the default limit of 10,000 requests per second for API Gateway","correct":true},{"id":"e33a82eb6c47467dbddcc3cebfdd8dae","text":"Your tests have caused the application to hit the default limit of 10,000 requests per second for Lambda","correct":false}]},{"id":"43f82a5f-84eb-4bda-9424-b03f27fa79ab","domain":"security","question":"What is the recommended approach to configuring a mobile application to allow users to sign-in and sign-up to your application via Facebook?","explanation":"Cognito is the preferred Web ID Federation mechanism in AWS","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html","title":"Cognito For Mobile Apps"}],"answers":[{"id":"a9d9e07070663e9217c3fcc296f07259","text":"Use Cognito as an Identity Broker between your application and the Web Identity Provider","correct":true},{"id":"51e84803699eae0c33a7fd3c998881df","text":"Use a custom Lambda function to act as an Identity Broker between your application and the Web Identity Provider","correct":false},{"id":"09bea5739247e8d082cd79bd90905562","text":"Use encrypted AWS credentials within your application code and store them locally on the device","correct":false},{"id":"0aed15971d7cc88de6e66d66abc629f5","text":"Use IAM as an Identity Broker between your application and the Web Identity Provider","correct":false}]},{"id":"15af205c-dba5-4175-8236-b5a7af3e70ec","domain":"deployment","question":"You are a developer for a news, entertainment, lifestyle, and fashion website. User traffic has steadily increased month-over-month, and you are now tasked with cost optimizing the website. The website is currently served from an EC2 instance that is part of an auto-scaling group behind an elastic load balancer. Your manager and CTO have approved a complete re-structuring of the websites architecture in order to accommodate future growth. How would you optimize your application in the MOST cost-effective way?","explanation":"Serverless approaches are ideal for applications where load can vary dynamically. Using a serverless approach means no compute costs are incurred when there is no end user traffic, while still offering instant scale to meet high demand, such as a flash sale on an e-commerce site or a social media mention that drives a sudden wave of traffic. Compared to traditional infrastructure approaches, it is also often significantly less expensive to develop, deliver, and operate a web or mobile backend when it has been architected in a serverless fashion. The question allows architectural restructuring and serverless would be the best approach. Moving the application on-premise would not be cost effective as it would change your from variable to operational costs. Changing the scale-in policy would help reduce costs but would not be as cost effective as going serverless. Implementing CloudFront would increase costs overall.","links":[{"url":"https://d0.awsstatic.com/whitepapers/optimizing-enterprise-economics-serverless-architectures.pdf","title":"Optimizing Enterprise Economics with Serverless Architectures"}],"answers":[{"id":"ca56c5360ae4b7c7063a9f870f717088","text":"Implement CloudFront in front of the EC2 instance as the origin.","correct":false},{"id":"1a981591e9c0499dbcb77019a1598dbc","text":"Edit the scale-in policy within auto scaling to terminate instances aggressively when demand is low.","correct":false},{"id":"59ba0fde539e6373d2e3b860d2ce4aca","text":"Move the application on-premise. You'll be able to fully manage the application, and purchase additional servers when appropriate.","correct":false},{"id":"f4f5510799286ba3bec7cef229ad2bc8","text":"Move the website to a serverless application. Use S3 to host the website. Use a combination of Lambda and API Gateway to support dynamic API requests.","correct":true}]},{"id":"0d9472d0-587b-4961-9457-e37e75979b7c","domain":"development","question":"You are working on a Serverless application written in Python and running in Lambda. You have uploaded multiple versions of your code to Lambda, but would like to make sure your test environment always utilizes the latest version. How can you configure this?","explanation":"When you create a Lambda function, there is only one version: $LATEST. You can refer to the function using its Amazon Resource Name (ARN). There are two ARNs associated with this initial version, the qualified ARN which is the function ARN plus a version suffix e.g. $LATEST. Or the unqualified ARN which is the function ARN without the version suffix. The function version for an unqualified function always maps to $LATEST, so you can access the latest version using either the qualified ARN with $LATEST, or the unqualified function ARN. Lambda also supports creating aliases for each of your Lambda function versions. An alias is a pointer to a specific Lambda function version, aliases will not be updated automatically when a new version of the function becomes available.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-intro.html","title":"Introduction To Lambda Function Versioning"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda Function Versioning and Aliases"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/how-to-manage-versioning.html","title":"Lambda Versioning"}],"answers":[{"id":"cc67dedc36d230ab3574dc07b41b7443","text":"Configure the alias to automatically update to point to the latest version of the code every time it is updated","correct":false},{"id":"02f663ae252fea90c711171a5572d211","text":"Reference the function using a qualified ARN and the $LATEST suffix","correct":true},{"id":"07c1095b47175c42406ba59e91205281","text":"Reference the function using an unqualified ARN","correct":true},{"id":"d483464867986a2d830ac23858944891","text":"Create an alias to point to the correct version of your code","correct":false},{"id":"0311fc386b14ce5afd0843ecf4a6ff16","text":"Create another function to automatically update your function alias to point to the latest version of the code every time it is updated","correct":false}]},{"id":"52dc8f6c-a34e-48cd-afbf-8a3b5ecd7e34","domain":"refactoring","question":"Your application is experiencing a large number of failed requests when making calls to the S3 API. Which of the following best describes the approach used by AWS SDKs for regulating flow control when retrying failed API requests?","explanation":"Numerous components on a network, such as DNS servers, switches, load balancers, and others can generate errors anywhere in the life of a given request. The usual technique for dealing with these error responses in a networked environment is to implement retries in the client application. In addition to simple retries, each AWS SDK implements exponential backoff algorithm for better flow control. The idea behind exponential backoff is to use progressively longer waits between retries for consecutive error responses.","links":[{"url":"https://docs.aws.amazon.com/general/latest/gr/api-retries.html","title":"Error Retries and Exponential Backoff in AWS"}],"answers":[{"id":"401eaf63ec9626a306617946768f9d43","text":"By default, the request is continuously retried until it is successful","correct":false},{"id":"1515fbc0c63bfb67a5ec940447b2480c","text":"AWS uses bandwidth throttling to manage flow control","correct":false},{"id":"78d0dc3ec6366e5ac975d5108a27106f","text":"Feedback Based Flow Control is used to avoid contention when retrying failed requests","correct":false},{"id":"c57cdded61f3bd3c9d54c8575f424941","text":"AWS uses Exponential Backoff to manage error retries","correct":true}]},{"id":"ed0618e6-a6db-4b7e-8564-384ae63b0e89","domain":"development","question":"You are working for a small but busy veterinary surgery and you need to design a new DynamoDB table to store information relating to customers, their pets, and any medications that are currently being prescribed. Which of the following attributes would be a good choice for a partition key, in order to achieve maximum provisioned throughput efficiency?","explanation":"When selecting a partition key, you want to distribute the workload evenly across as many partitions as you can, to maximize provisioned throughput of your DynamoDB table. The partition key determines which partition the record will be stored on. To achieve maximum provisioned throughput, choose a partition key with a unique attribute like Customer ID, Product ID, email address, phone number etc. A partition key design that does not distribute I/O requests evenly can create hot partitions which result in throttling and uses your provisioned I/O capacity inefficiently. Values such as Medication, Species and Registration date, are not unique and in some cases may have only a few possible values which could result in hot partitions and inefficient use of provisioned throughput.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html","title":"Designing Partition Keys to Distribute Your Workload Evenly"}],"answers":[{"id":"353bd6f65060d17097c3b03141e79cce","text":"Medication","correct":false},{"id":"d37c2bf1bd3143847fca087b354f920e","text":"Customer ID","correct":true},{"id":"e1520b5997a532c7889f6e8883920ab8","text":"Species","correct":false},{"id":"22ffd0379431f3b615eb8292f6c31d12","text":"Registration date","correct":false}]},{"id":"c1362509-8ba6-4411-80d7-a3dfff7b1c29","domain":"mon-trb","question":"As you retrieve information from DynamoDB, you receive a ProvisionedThroughputExceededException error. Further investigation shows that you're not exceeding your table's read capacity throughput. What is causing this error?","explanation":"DynamoDB distributes capacity evenly across all available partitions. If a given partition is consuming more than its share of throughput, this error will be raised.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html","title":"ProvisionedThroughputExceededException"}],"answers":[{"id":"d133230e1647cdbd9c08bcbe4c2148eb","text":"The table is warming up and this process consumes throughput.","correct":false},{"id":"d1b67149a21e4fba2199fd83ca4b3a86","text":"You are exceeding an individual partition's throughput capacity, even if you're not exceeding the overall table throughput capacity.","correct":true},{"id":"2151c5ca19006af967ad87fd46b71782","text":"You have too many sort keys on your table.","correct":false},{"id":"7bee1d9584711c70ca477468c92d24d4","text":"AWS metrics do not run in real time. This error will disappear.","correct":false}]},{"id":"616a7b1c-bc17-42a4-b361-74af9a86607f","domain":"development","question":"A financial services company is implementing a payments processing application utilizing DynamoDB tables for its data store. To process payments, the application needs to perform a write operation on a sequence of items, and roll back and reverse all operations in case of any one faulty operation.  What is the best method to accomplish this requirement?","explanation":"DynamoDB transactions feature provides ability to group multiple items into a single atomic transaction and perform all-or-nothing coordinated operations.  This can be done programmatically using the TransactWriteItems operation. The BatchWriteItem operation does not meet the question requirements as it does not guarantee that the actions will be performed on all items as a single atomic coordinated operation. It is possible that only some of the actions in the batch succeed while the others do not. Updating the payments application is not the ideal solution.  It requires application code change and tracking all connected operations and reversing them as required is not trivial to implement.  Using the native transaction ability provided by DynamoDB is a better option.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html","title":"Amazon DynamoDB Transactions: How It Works"}],"answers":[{"id":"758e6d47da6512d38526cfcfa39bb5c1","text":"Update the application to manage and perform roll-back operations.","correct":false},{"id":"cb748237c6e223d0f4506cff55c6b259","text":"Use the TransactWriteItems operation.","correct":true},{"id":"3b08e25699a7082805a9be123c9acbbb","text":"DynamoDB does not support atomic transactions.  Use relational database (such as RDS) that supports atomic transactions.","correct":false},{"id":"aa5647f8e2c8e7147097b79d2b2a5555","text":"Use the BatchWriteItem operation.","correct":false}]},{"id":"daa8b2ee-b810-4e35-947d-9ad26196189d","domain":"mon-trb","question":"You can use X-Ray with applications running on which platforms? ","explanation":"X-Ray works with Lambda, EC2, API Gateway, Elastic Beanstalk and ECS","links":[{"url":"https://aws.amazon.com/xray/faqs/","title":"X-Ray FAQs"}],"answers":[{"id":"81b22456f78954c460ce2f531b5e048f","text":"EC2","correct":true},{"id":"3d6cbd7db2a4fa389808ea6f4a5fc1bc","text":"Elastic Beanstalk","correct":true},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":true},{"id":"c8f63ecaff5e983a2441126a241c4cfa","text":"ECS","correct":true},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":false}]},{"id":"f7fd0d7a-5d45-4f72-aa8e-d9a65270360f","domain":"refactoring","question":"You are developing an online hotel booking application which makes an number of requests to different back end applications to get quotes for travel related add-on services. You are using API gateway handle all the API calls and you notice that the majority of requests are for the same 5 or 6 services. How can you optimize the configuration to ensure the best performance for your application?","explanation":"You can enable API caching to cache your endpoint's responses, this reduces the number of calls made to your endpoint and improves the latency of requests to your API.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"API Caching"}],"answers":[{"id":"9a86454fefc71c7430655ce2e18ac716","text":"Configure a CloudFront CDN in front of the API Gateway to cache the most frequent HTTP requests","correct":false},{"id":"b255bca6117b1096664ab7b1415fae80","text":"Add an ElastiCache cluster in front of your database to cache the most frequently accessed data","correct":false},{"id":"336a729744a0b4682222e3a4a1cdc750","text":"Implement API Caching to cache the endpoint's response for the most popular requests","correct":true},{"id":"59a69a5bb20a3dbe9214ef93c38041f7","text":"Configure auto-scaling for the API Gateway","correct":false}]},{"id":"56a3a6cc-4c72-4b6c-a06c-4c310d107296","domain":"development","question":"An application successfully updates an existing object in S3. When checking the file contents, the developer does not see the updated file contents. What is the cause of this issue?","explanation":"Amazon S3 offers eventual consistency for overwrite PUTS and DELETES in all Regions. Amazon S3 provides high availability and high durability by replicating bucket objects across multiple availability zones and servers. This means that any updates to objects must replicate across all servers storing the data. This can take some time. Therefore, any updates to existing objects (using POST or DELETE), will take some time to be propagated across all of S3, and hence are eventually consistent.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html","title":"Introduction to Amazon S3"}],"answers":[{"id":"57d934f14c5f166d5f0604507d795cc0","text":"S3 Bucket Versioning was not enabled.","correct":false},{"id":"47ad6c66cb5b231f66170a797fbb7782","text":"Overwrite PUTS in S3 have eventual consistency.","correct":true},{"id":"a693ecd0f4b5da724ee692d2059fc4c3","text":"S3 bucket policy permissions were not correct.","correct":false},{"id":"4756256f32016be9d23caa16a75bb9c5","text":"HTTP 200 response code was not received.","correct":false}]},{"id":"c6126f50-a373-47bf-8245-6037dcea0b5a","domain":"security","question":"Your e-commerce application needs to use database connection strings to access a database containing product and customer data. Which of the following is a secure and scalable way to manage this?","explanation":"Using secure string parameters in Parameter Store is an appropriate way to avoid hard coding a password in your template code. This ensures that sensitive runtime parameters are kept as secure as you keep other secrets, while also keeping them separate from your deployment code.","links":[{"url":"https://aws.amazon.com/systems-manager/features/","title":"Systems Manager - Parameter Store"},{"url":"https://aws.amazon.com/blogs/mt/using-aws-systems-manager-parameter-store-secure-string-parameters-in-aws-cloudformation-templates/","title":"Using AWS Systems Manager Parameter Store Secure String parameters"}],"answers":[{"id":"7cb38c21a4fc2e2ad04aca2dfe89bb59","text":"Add encrypted IAM credentials to the application server and use an IAM role to access the database","correct":false},{"id":"62fc1a48d6c466824dfef123b5407ab3","text":"Store the encrypted credentials in an S3 bucket","correct":false},{"id":"1c2a034c75f70a59fff1e639175239dd","text":"Allow the EC2 instance to access the database using an instance role","correct":false},{"id":"0d2bb1f681b1226217a023c4cb989aaa","text":"Hard code the connection strings in the application code","correct":false},{"id":"65b9be4196a653fc5de9a6427a2f168a","text":"Store the credentials in Parameter Store","correct":true}]},{"id":"ee1f9799-b2a7-445f-bb89-932b725a8374","domain":"deployment","question":"A business-critical application is deployed using CloudFormation. The team would like to prevent accidental deletion of the stack. How can this be achieved most efficiently?","explanation":"Termination Protection stack option can be enabled to prevent accidental deletion of an entire CloudFormation stack. It is possible to use IAM policy to prevent deletion of a CloudFormation stack, however, this is not the optimal solution from operations and management perspective. The DeletionPolicy CloudFormation attribute applies to individual resources, not an entire stack. There is no DeletionProtection attribute in CloudFormation.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html","title":"Protecting a Stack From Being Deleted"}],"answers":[{"id":"6983aeb4c0d42bb293c8ec93966aedbb","text":"Set the DeletionProtection to True in the CloudFormation template.","correct":false},{"id":"eee064b70f79422e8511f18b251253ef","text":"Create IAM Policy with Effect of Deny for 'cloudformation:DeleteStack' Action.","correct":false},{"id":"8093dacb1a766d0f91fa60e48baa87ed","text":"Set Stack Termination Protection to Enable.","correct":true},{"id":"558cc27970e2b64a29bdc85f381b8cb9","text":"Set the DeletionPolicy to Retain in the CloudFormation template.","correct":false}]},{"id":"004b0e0e-f5a4-4354-b130-46786f367c11","domain":"security","question":"A developer is running an application on an Amazon EC2 instance that requires access to an Amazon S3 bucket. An administrator creates a role that includes policies that grant read permissions to the bucket and that allow the developer to launch the role with an Amazon EC2 instance. The instance is launched with the created role attached. What additional step is required for the application running on the instance to access the objects in the bucket?","explanation":"Applications that run on an Amazon EC2 instance and that need access to AWS resources such as Amazon S3 buckets or an Amazon DynamoDB table must have security credentials in order to make programmatic requests to AWS. In this case, no other steps are necessary since the application running on the instance will have the necessary permissions by assuming the role attached to the EC2 instance. Since the developer is not using the bucket (the application on the instance is) granting access to the developer will have no impact. There is no need to share credentials with the bucket policy.","links":[{"url":"https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf?refid=em_","title":"AWS Security Best Practices"}],"answers":[{"id":"723ec04523ad8f60803dd2ae402f886f","text":"The developer must share his/her credentials with the bucket policy.","correct":false},{"id":"1bc25b995801c49945fc1175ebfaf95d","text":"The administrator must grant the developer permissions to access the bucket.","correct":false},{"id":"d57505894b0e8e826335fc1b6ca1f88c","text":"No other steps are necessary. The application running on the instance will be able to access the bucket.","correct":true},{"id":"ff86c6c144ca49f130e8a6024c26d48c","text":"Create an IAM policy that allows the developer permissions to access the bucket. Attach the policy to the developer's IAM User.","correct":false}]},{"id":"18db8cf0-407c-4547-a64f-eabdcbf3566a","domain":"development","question":"Which of the following DynamoDB features allows Items to be automatically deleted at a given date and time?","explanation":"DynamoDB TTL allows each Item to include a date and time at which DynamoDB will automatically delete the Item.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html","title":"Time to Live: How It Works"}],"answers":[{"id":"f15851a368334eb82668e066053bb738","text":"DynamoDB Timeout","correct":false},{"id":"52d5592ad24a2cda379ce38e9c218d65","text":"DynamoDB TTL","correct":true},{"id":"fdce91249547ab22d875d26aad3493bd","text":"DynamoDB auto-delete","correct":false},{"id":"b0d30a23fde41c669e0592b4be4d6093","text":"DynamoDB Exponential Backoff","correct":false}]},{"id":"764c68cc-0596-485a-b5a0-4bb272444d02","domain":"deployment","question":"Which of the following services enables you to automatically build, test and release new software whenever a developer makes an update to their code?","explanation":"CodeBuild only builds your code, it won't deploy it to your environment. CloudFormation is used to deliver Infrastructure As Code. CodeCommit manages your source code. CodeDeploy can be used to deploy code, but in isolation, it cannot create an automated release process. CodePipeline automates the build, test, and can be used to deploy phases of your release process every time there is a code change, based on the release model you define.","links":[{"url":"https://aws.amazon.com/codepipeline/","title":"AWS CodePipeline"}],"answers":[{"id":"56c20df941e069e1a35a1871f662298a","text":"CodeCommit","correct":false},{"id":"07929940c38c1e9e91ecf67c01ccfa73","text":"CodeDeploy","correct":false},{"id":"f8d1a2a80933c71d9a65e3d825976bda","text":"CodeBuild","correct":false},{"id":"5f6f48261d96567b3014b43c23382021","text":"CodePipeline","correct":true},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":false}]},{"id":"efa5e898-d746-4a5f-b112-11861aa90108","domain":"deployment","question":"A developer is making changes to the CloudFormation template used to deploy an application. They would like to know if any existing resources will be deleted or replaced before applying the template updates. What service feature will enable this?","explanation":"CloudFormation Change sets enable the preview of proposed changes to a stack in order to assess the impact on running resources. This functionality allows the developer to check if any existing resources will be deleted or replaced upon application of the CloudFormation template.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"e3692dda22879fd00506eaa434a3913b","text":"CloudFormation StackSets","correct":false},{"id":"400f40520b42783d45e80c35c9b18641","text":"CloudFormation Registry","correct":false},{"id":"b5b91cf0fb04ab0cccd3dc8d197bfdaf","text":"CloudFormation Rolling Updates","correct":false},{"id":"e340c6d6c2f5866020158726104d63d7","text":"CloudFormation Change Sets","correct":true}]},{"id":"66c6afb4-04e0-4eda-aa5b-745c464d5cad","domain":"mon-trb","question":"You deployed a new Lambda function a few days ago and your code seems to be executing successfully, however when you check CloudWatch there isn't any log data for your function. What could be the reason for this?","explanation":"A service needs to have permissions to write log data to CloudWatch logs, Lambda is associated with an execution role which needs to grant the relevant IAM permissions","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html","title":"Using CloudWatch"}],"answers":[{"id":"122cf71e8d5e76b7cfa4409633b0e4bf","text":"There is an issue with S3 in your region","correct":false},{"id":"dfa1603ecab090efaf75db7e3a0456e3","text":"Your code is taking too long to execute, it could be that your function does not have enough compute resources to generate the log files","correct":false},{"id":"0261bd378f9288b971d24a9076d27d22","text":"The execution role for the Lambda function did not grant permissions to write log data to CloudWatch Logs","correct":true},{"id":"ec216d5f36d3705950b7957bbb31d565","text":"The CloudWatch agent has stopped","correct":false}]},{"id":"3fc48fe5-2386-4e89-ba3e-045136c37de4","domain":"development","question":"You have a legacy application located in your production data centre, which frequently accesses files stored in S3. Due to a significant increase in workload, your application servers are now generating a huge number of requests to your S3 bucket, with many requests now failing. What can you do to improve the situation?","explanation":"Numerous components on a network, such as DNS servers, switches, load balancers, and others can generate errors anywhere in the life of a given request. The usual technique for dealing with these error responses in a networked environment is to implement retries in the client application. Retrying the request with Exponential Backoff technique increases the reliability of the application and reduces operational costs for the developer.","links":[{"url":"https://docs.aws.amazon.com/general/latest/gr/api-retries.html","title":"Error Retries And Exponential Backoff In AWS"}],"answers":[{"id":"40d21c12e8e9a47ba266648c0cd43681","text":"Install a faster network interface in your application servers","correct":false},{"id":"895f33b72cf7d11e0e8f0d9fa8ee793c","text":"Configure your application to use Exponential Backoff","correct":true},{"id":"dc46f742e30fa33e5e1c551a3057eecf","text":"Migrate the data to DynamoDB","correct":false},{"id":"b84f4c83038ab1b006eadbd1c591ccaa","text":"Configure your application to read and write to multiple S3 buckets","correct":false}]},{"id":"a94b2c88-352b-4c39-83fc-7edfa8190fca","domain":"development","question":"Your application communicates using messages in an SQS queue. You have noticed recently that you are seeing a large number of empty responses where no messages exist in the queue. You want to make sure that your application is responsive as possible, but the cost of the solution is also a concern. What can you do to ensure your application is both cost-effective and responsive?","explanation":"In almost all cases, Amazon SQS long polling is preferable to short polling. Long-polling requests let your queue consumers receive messages as soon as they arrive in your queue while reducing the number of empty ReceiveMessageResponse instances returned. In general, you should use maximum 20 seconds for a long-poll timeout. Because higher long-poll timeout values reduce the number of empty ReceiveMessageResponse instances returned, try to set your long-poll timeout as high as possible.","links":[{"url":"https://aws.amazon.com/sqs/faqs/","title":"SQS FAQs"}],"answers":[{"id":"cef7badd821b93647928923359f863e4","text":"Use a FIFO queue","correct":false},{"id":"b2ccbf04a8f468dbcaad76b35f8f076b","text":"Configure multiple queues","correct":false},{"id":"66a35e1198bcce83d8654873aab562c6","text":"Configure multiple queues with short polling","correct":false},{"id":"f11916957f37c9cd171f126572d34864","text":"Use short polling","correct":false},{"id":"2247660601daf03d12f2fb4a1fccbf55","text":"Use long polling","correct":true}]},{"id":"c11f4354-0409-46a1-a058-1e377939c655","domain":"development","question":"You are in a development team working on a popular serverless web application which allows users to book late availability flights and hotels at a significant discount. You occasionally receive complaints that the website is running slowly. After some investigation, you notice that at the time of the complaints, DynamoDB reported a ProvisionedThroughputExceeded error. Which of the following approaches is a recommended way to handle this error?","explanation":"Increasing Lambda capacity will not fix the issue because the problem is with DynamoDB. As the error only appears occasionally, the first thing to do is to ensure that the application is using Exponential Backoff to improve flow control. Increasing the capacity on the DynamoDB table could be considered but only if the problem persists.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff","title":"DynamoDB Error Handling"}],"answers":[{"id":"e7668448048601413de55468a4091b5d","text":"Increase the RAM capacity of the Lambda function","correct":false},{"id":"cbf47ddd16d15ef42c8c335fe895c69f","text":"Increase the CPU capacity of the Lambda function","correct":false},{"id":"528fd1044e587b850c48dc8d209cfe11","text":"Increase the read/write capacity of the DynamoDB table","correct":false},{"id":"7f6f787080ee787e0f7554f770e0c693","text":"Ensure your application is using Exponential Backoff","correct":true}]},{"id":"bc69cb14-0b50-4173-925a-a9c2fdb00bb5","domain":"mon-trb","question":"You have multiple applications running on a large number of EC2 instances. You need to access the application logs from a single central location, what should you do?","explanation":"You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources. CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can create CloudWatch custom metrics for your EC2 instance statistics by creating a script through the AWS Command Line Interface and then monitor that metric by pushing it to CloudWatch. However, custom metrics are only metrics or findings reported by running a script, they are not pushing log files into CloudWatch.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html","title":"What Is Amazon CloudWatch Logs?"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-custom-metrics/","title":"Custom Metrics and CloudWatch"}],"answers":[{"id":"458bb7239f53c3a09ada45ce8d2eb321","text":"CloudWatch custom metrics","correct":false},{"id":"208fa683260008af411c8bfe394bb8bf","text":"Write a script to send the application logs to CloudWatch. Install the script on each of your application servers","correct":false},{"id":"a8c600be214ced26950e704d39c3ca21","text":"CloudWatch Logs","correct":true},{"id":"f0d36f4f7323e9b431c82349dcc3ab89","text":"CloudTrail Logs","correct":false}]},{"id":"c90384c4-a4e3-44cd-909d-9c75fd296455","domain":"security","question":"You are running an application on an EC2 instance. The application needs to be able to access an S3 bucket to read and write data. Which of the following is the best approach to enabling the EC2 instance to access your bucket?","explanation":"Storing credentials in EC2, in the code or in databases is not recommended. Using an IAM role with the requisite permissions and associating that with your EC2 instance is the recommended approach.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html","title":"Using an IAM Role To Allow EC2 Access To S3"}],"answers":[{"id":"cae4292072ddb87ec39678934d1edc5d","text":"Use an IAM role with permissions to read and write to the bucket","correct":true},{"id":"53f6cd0aebbfa4fcbc2c8102ece8c9d8","text":"Store an access key and secret access key in a DynamoDB table","correct":false},{"id":"9a58eab1d5aec409abbb6212545b5d45","text":"Store AWS credentials within the application code","correct":false},{"id":"2e9ca3aed8c9e4854687f9556f8e5b1a","text":"Store AWS credentials locally on the EC2 instance","correct":false}]},{"id":"9abb4110-4b7b-11ea-b77f-2e728ce88125","domain":"development","question":"You want to add a cross-origin resource sharing (CORS) configuration to one of your S3 buckets. Which of the following tabs should you choose to do so?","explanation":"To add a CORS configuration to your S3 bucket, you have to click the 'Permissions' tab and choose 'CORS configuration'. The 'Properties' tab is for configuring object settings such as versioning, transfer acceleration, and logging. The 'Management' tab is for managing object replication, analytics, and storage lifecycle. If you want to simplify bucket access by creating endpoints, you choose 'Access points'.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-cors-configuration.html","title":"How Do I Add Cross-Domain Resource Sharing with CORS?"}],"answers":[{"id":"902c305d02b28c2f6199601e7128ed36","text":"Access points","correct":false},{"id":"9fc2d28c05ed9eb1d75ba4465abf15a9","text":"Properties","correct":false},{"id":"fe4dbcab9b910577e5035e97ac068dae","text":"Management","correct":false},{"id":"d08ccf52b4cdd08e41cfb99ec42e0b29","text":"Permissions","correct":true}]},{"id":"a0b993eb-5210-4269-b055-8cb0f8ee0192","domain":"security","question":"Your mobile application needs to read data from DynamoDB. What is the best way to give mobile devices permissions to read from DynamoDB?","explanation":"Web identity federation removes the need for creating individual IAM users. Instead, users can sign in to an identity provider and then obtain temporary security credentials from the AWS Security Token Service.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WIF.html","title":"Using Web Identity Federation"}],"answers":[{"id":"baa876447171bf9fb69ccfb1811d73e1","text":"Issue an access key and secret access key to each user.","correct":false},{"id":"8b6a4ec1dbecce8cd70cbe48c62b5c6c","text":"Create an IAM role for your users.","correct":false},{"id":"85b1805591d84001d38b3d9942fee4fe","text":"Connect your application to an EC2 instance with permission to read from DynamoDB.","correct":false},{"id":"9fb0f3fc8b9734347e97c0b47d69bbba","text":"Create an IAM role that can be assumed by an app that allows federated users.","correct":true}]},{"id":"1bebd0ce-024b-4592-9a82-af0d2a38f135","domain":"deployment","question":"What action does CloudFormation take if a creation of a stack fails?","explanation":"If stack creation fails, AWS CloudFormation rolls back any changes by deleting the resources that it created.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html","title":"How Does AWS CloudFormation Work?"}],"answers":[{"id":"134c4b16aa963269be7f346441209384","text":"It stops the creation of the resource and any other resources in the template.","correct":false},{"id":"38a8b80a58bb9367d4f7106ea09c7f94","text":"It rolls back the stack and deletes any resources that have been created.","correct":true},{"id":"9bfaf040c6904ecc9d4771bb81398d24","text":"It creates a SNS notification.","correct":false},{"id":"cd021a755a83b7333bd29e58e4cf61fc","text":"It skips the creation of the resource and continues with creations of other resources in the template.","correct":false}]},{"id":"4750f3bd-de92-4efb-ad08-06c9ea71eccb","domain":"refactoring","question":"Kinesis allows consumer applications to consume records in which order?","explanation":"Kinesis gives you the ability to consume records according to a sequence number applied when data is written to the Kinesis shard","links":[{"url":"https://aws.amazon.com/kinesis/data-streams/faqs/","title":"Kinesis FAQ"}],"answers":[{"id":"e71e8f5a50819b4773c68991d0c9f602","text":"Records are processed in no particular order","correct":false},{"id":"cb1ab21304b72197113dbe18c491e9c2","text":"According a sequence number assigned when the record is written to the stream","correct":true},{"id":"b1e05f0db70da1b8dee8592d942ed2e1","text":"According to the timestamp assigned when the record is written to the stream","correct":false},{"id":"689f6f887e0c13fb07b437de23303cac","text":"Last In First Out","correct":false}]},{"id":"88a77a58-b901-4605-a657-98cdc90dff65","domain":"deployment","question":"A developer needs to compile Java code to produce a deployment artifact. Which Amazon service can the developer use for this task?","explanation":"Amazon CodeBuild is a service that compiles source code, runs tests, and produces software packages that are ready to deploy. Amazon CodeCommit is a source control service that hosts Git-based repositories. Amazon CodeDeploy is a deployment service that automates software deployments. Amazon CodePipeline is a continuous delivery service that helps you automate your release pipelines.","links":[{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild"}],"answers":[{"id":"5f6f48261d96567b3014b43c23382021","text":"CodePipeline","correct":false},{"id":"56c20df941e069e1a35a1871f662298a","text":"CodeCommit","correct":false},{"id":"f8d1a2a80933c71d9a65e3d825976bda","text":"CodeBuild","correct":true},{"id":"07929940c38c1e9e91ecf67c01ccfa73","text":"CodeDeploy","correct":false}]},{"id":"03ea5729-a486-470f-8cf7-a006c62c2045","domain":"deployment","question":"You have been asked to run your in-house application code using Lambda. Which of the following services could you use to deploy your code?","explanation":"You cannot deploy code using CodeCommit or CodeBuild. All of the other services can be used to deploy code in a Serverless environment","links":[{"url":"https://aws.amazon.com/blogs/infrastructure-and-automation/deploying-aws-lambda-functions-using-aws-cloudformation-the-portable-way/","title":"Deploying Lambda Functions Using CloudFormation"},{"url":"https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/","title":"Deploying Lambda Functions Using CodeDeploy"},{"url":"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html","title":"AWS SAM"}],"answers":[{"id":"2565605f16beacf2f11c0ac6e7510e80","text":"AWS Serverless Application Model","correct":true},{"id":"56c20df941e069e1a35a1871f662298a","text":"CodeCommit","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":true},{"id":"f8d1a2a80933c71d9a65e3d825976bda","text":"CodeBuild","correct":false},{"id":"07929940c38c1e9e91ecf67c01ccfa73","text":"CodeDeploy","correct":true}]},{"id":"868cb94f-ce60-4590-bfd2-60e8295cc413","domain":"security","question":"You are developing a online-banking website which will be accessed by a global customer base. You are planning to use CloudFront to ensure users experience good performance regardless of their location. The Security Architect working on the project asks you to ensure that all requests to CloudFront are encrypted using HTTPS. How can you configure this?","explanation":"Viewer Protocol Policy defines the protocols which can be used to access CloudFront content","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html","title":"Requiring HTTPS for Communication Between Viewers and CloudFront"}],"answers":[{"id":"c40022183e6d5dd97e4c778332064ed2","text":"Set the Viewer Protocol Policy to redirect HTTP to HTTPS","correct":true},{"id":"052066815497ced9f9852e55d66c6782","text":"Set the Request Protocol Policy to redirect HTTP to HTTPS","correct":false},{"id":"ae41866f6b2df14a344847e9629076db","text":"Set the User Protocol Policy to redirect HTTP to HTTPS","correct":false},{"id":"659099387a57b1f316abf3c6afac459d","text":"Set the Session Protocol Policy to redirect HTTP to HTTPS","correct":false}]},{"id":"b170b108-ed74-40ba-a811-383fa049982d","domain":"mon-trb","question":"You receive a \"timed out\" error message when running a command using the AWS CLI. What could be a possible reason for this?","explanation":"If you see issues when running list commands on a large number of resources, the default page size of 1000 might be too high. This can cause calls to AWS services to exceed the maximum allowed time and generate a \"timed out\" error. You can use the --page-size option to specify that the AWS CLI request a smaller number of items from each call to the AWS service. The CLI still retrieves the full list, but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call. This gives the individual calls a better chance of succeeding without a timeout. Changing the page size doesn't affect the output; it affects only the number of API calls that need to be made to generate the output.","links":[{"url":"https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html","title":"AWS CLI Pagination"}],"answers":[{"id":"9a732fb1da270d734f8c6fd13ec818f7","text":"The AWS service that you are trying to call is taking too long to respond","correct":false},{"id":"6671b1dd34271b2e7f713a895d4dc800","text":"The AWS service that you are trying to call is unreachable","correct":false},{"id":"afdc0b8222f758f2d5101f7516eac7b5","text":"Your network connection is too slow","correct":false},{"id":"693a2ee98a96c9712d1720d141c29c5b","text":"You have run a command which is trying to return a large number of items and has exceeded the maximum allowed time to return results","correct":true}]},{"id":"a4afc791-647b-40b6-8f2a-29feb6142a0d","domain":"development","question":"ABC corp runs a web application that uses API Gateway to provide their developer customers with access to data. To reduce load on their upstream systems, the ABC corp have enabled API Gateway caching. A small number of developer customers still need access to results directly from the integration endpoint. To prevent all developer customers from bypassing the cache, ABC corp has also enabled the requirement for cache invalidation to require authorization.\n\nWhat must a developer customer do to return a result that is not cached from the API Gateway?","explanation":"Setting a Cache-Control: max-age=0 HTTP header as part of the request tells API Gateway that you want a response directly from the integration endpoint, rather than a cached response. This header can be interpreted as the client stating the maximum age a cached result can be is 0 seconds - equivalent to saying it cannot be cached at all.\n\nAs the cache is configured to require authorization to be invalidated, the request must be signed with a user or role that allows the execute-api:InvalidateCache action to be performed on the API Gateway resource. An example of this policy is found in the documentation for API Gateway Caching. We have also included a link to how to sign a request using AWS Signature Version 4.\n\nIt is recommended that you require authorization to invalidate a cached response; otherwise, if a significant number of requests perform an invalidation, the cache is no longer helping reduce load on upstream systems.\n\nCalling flush-stage-cache is incorrect because this would delete all data in the entire API cache, rather than just for the response the client has requested. Called often, this will likely result in the cache not having sufficient data to be effective.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"API Gateway Cache developer guide"},{"url":"https://docs.aws.amazon.com/apigateway/api-reference/signing-requests/","title":"Signing API Gateway requests"}],"answers":[{"id":"0f148da68f4442eace5c89fe939ba9ac","text":"Call the flush-stage-cache command before making the request.","correct":false},{"id":"54f0ac8f9360a2dfe63f22a0a503ae7e","text":"Call the /execute-api-invalidate-cache API Gateway endpoint before sending their request.","correct":false},{"id":"4f0d8679da16e6c12618e0c2b85739be","text":"Include a Cache-Control: max-age=0 HTTP header in their request.","correct":true},{"id":"3dd3c69d480989554365826eaef7be2f","text":"Include a ?execute-api-invalidate-cache query string in their request.","correct":false},{"id":"cb28453003b461189ca4aec138e2d1bf","text":"Sign their request with a user or role that has the required execute-api:InvalidateCache permissions to invalidate the cache.","correct":true}]},{"id":"cd7c8d61-2ce7-401a-956f-c01c33305ae2","domain":"deployment","question":"Which of the following statements is correct?","explanation":"EBS-backed instances can be stopped and restarted without losing the data on the volume.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/instance-store-vs-ebs/","title":"Differences between EBS and Instance Store"}],"answers":[{"id":"b335920b9f6cd5d6425a787a6183b378","text":"If you want to use auto-scaling, you must use an EBS-backed instance.","correct":false},{"id":"18eb677e835b4b7214b432d085e121fb","text":"An EBS backed instance can be stopped and restarted.","correct":true},{"id":"36e118d467e9fcf94627a5fa1bc11446","text":"Instance-store backed instances can be stopped and restarted.","correct":false},{"id":"91fff5146ee33778a9158bd16a8ba469","text":"An Amazon VPC requires that instances be backed with EBS.","correct":false}]},{"id":"235fa5cd-42b5-4017-af9a-e62d0503651a","domain":"security","question":"You are working on a Lambda function which needs to access data in RDS, which of the below are valid approaches for securely storing the encrypted database connection strings and other secrets which your function needs to use?","explanation":"Parameter Store provides secure storage for configuration data, connection strings, passwords and secrets management. None of the other options are secure.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html","title":"AWS Systems Manager Parameter Store"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/tutorial-env_console.html","title":"Create a Lambda Function Using Environment Variables To Store Sensitive Information"}],"answers":[{"id":"53708b71a9f9d0c3994b3bd1d470b254","text":"Use DynamoDB to store the encrypted connection string and secrets","correct":false},{"id":"913d4a9f910c7f6b836d623b39131480","text":"Use Systems Manager Parameter Store","correct":true},{"id":"ca7c47bd30833fb42e5bc78f6c7583be","text":"Use Lambda Environment Variables","correct":true},{"id":"f2523ec5429fa4e1be124d650a69a0f5","text":"Store the encrypted connection string and other secrets in S3","correct":false}]},{"id":"fe61a353-eb18-40dc-9e65-30f7106d3e6e","domain":"mon-trb","question":"Your application is running on EC2 and on Linux virtual machines in your own data center. You would like to configure your application to send data to X-Ray for troubleshooting and performance analysis. Which of the following steps will you need to complete?","explanation":"You need the X-Ray SDK and the X-Ray daemon on your EC2 instances and on-premises systems, you then need to instrument your application to send the required data to X-Ray","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html","title":"X-Ray Developer Guide"}],"answers":[{"id":"da1eaa5b340d64b20d5b526dc0de4b85","text":"Install the X-Ray SDK and the X-Ray daemon, then instrument your application to send data to X-Ray.","correct":true},{"id":"ff31ac1442cb9f28ad8f65a358816a25","text":"Install the AWS SDK and the X-Ray CLI, then instrument your application to send data to X-Ray.","correct":false},{"id":"998a1f353d1923ec24b66b6ae427d343","text":"Install the X-Ray daemon, then instrument your application to send data to X-Ray.","correct":false},{"id":"6ffb986aa2b49eb856b38668f9a44fc2","text":"Install the AWS CLI, then instrument your application to send data to X-Ray.","correct":false}]},{"id":"32d5da0f-0ed7-436c-98cf-a16053badf54","domain":"development","question":"Your application runs on Lambda and you would like to enable your functions to communicate with EC2 instances in your private subnet. How can you enable this?","explanation":"In order to enable Lambda to communicate with your private VPC, you need to add VPC config information. You do not need to add a NAT or Internet gateway and it is not possible to launch a Lambda function inside your own VPC or subnet. - Please be aware that in September 2019, AWS announced that they are simplifying VPC networking for Lambda functions, with changes planned to be rolled out on a per region basis. However the exams do generally run at least 6-12 months behind any new updates or announcements, so unfortunately, you may still see exam questions referring to the old way of doing things.","links":[{"url":"https://aws.amazon.com/blogs/compute/announcing-improved-vpc-networking-for-aws-lambda-functions/","title":"Announcing improved VPC networking for AWS Lambda functions"}],"answers":[{"id":"d77b6ba1a84152571b9b01b57aa78774","text":"Add an internet gateway to your VPC","correct":false},{"id":"4f2ebe62076f1314a2b7ece1aae5f495","text":"Update your Lambda function with the relevant VPC config information","correct":true},{"id":"a8f43c529989f897c2749cb29caeb890","text":"Add a NAT gateway to the subnet","correct":false},{"id":"9b7ce49759154447fc7d7666f84adf68","text":"Launch the Lambda function in the same subnet as your EC2 instances","correct":false}]},{"id":"5953c122-dbdd-4d25-a8fe-3e1fd23a6c8f","domain":"mon-trb","question":"An application developer finds that performing a scan operation on a large DynamoDB table is taking a long time to execute.  What can be used to improve the performance and decrease the execution time of the scan operation?","explanation":"Parallel scans can be used by multiple worker threads in an application to perform a scan of a DynamoDB table much faster. Filter expression in a scan operation only filters the results.  Scan operation still performs the same amount of read operations. Projection expression is used to limit the attributes returned by the scan operation.  It reduces the size of the payload of the scan operation, but it does not affect the speed of the scan operation. Pagination can be used to divide the result set into multiple pages, but does not increase the performance of the scan operation.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan","title":"Parallel Scan"}],"answers":[{"id":"b68d0e7d10bb7958f9b4b5fa43f06ac3","text":"Use of parallel scans.","correct":true},{"id":"7eeca829a717872bbf53c3ff8a69e41a","text":"Use of pagination.","correct":false},{"id":"62fef50451cb13a49660c69ae13e7932","text":"Use of filter expression.","correct":false},{"id":"e262b00e3caa1a25ec14571777714e9c","text":"Use of projection expression.","correct":false}]},{"id":"3a01c37a-6939-444b-89b0-f73b1f232601","domain":"deployment","question":"You are developing a social media messaging and photo-sharing application which consists of a web front end, with persistent data stored in S3 and RDS. Which of the following instance pricing models should you choose to make running this application as cost-effective as possible?","explanation":"Reserved instances provide a significant discount compared to running instances On-Demand. You can take advantage of Spot Instances to run and scale applications such as stateless web services, image rendering, big data analytics, and massively parallel computations. Dedicated Instances are Amazon EC2 instances that run in a VPC on hardware that is dedicated to a single customer.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/when-to-use-spot-instances.html","title":"When To Use Spot Instances"},{"url":"https://aws.amazon.com/ec2/pricing/reserved-instances/","title":"When To Use Reserved Instances"},{"url":"https://aws.amazon.com/ec2/pricing/dedicated-instances/","title":"When To Use Dedicated Instances"},{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/when-to-use-spot-instances.html","title":"Cost Optimization White Paper"}],"answers":[{"id":"a057af2fca16fcc4c6997976ea994e95","text":"Use Spot instances for the web servers","correct":true},{"id":"04be777131f511081730fe2b34662123","text":"Use dedicated instances for the web servers","correct":false},{"id":"1b65fa924bf38ff7e68d41f3495434bd","text":"Use reserved instances for the web servers","correct":false},{"id":"86ed636cf8296c0bedacc99ec10c39f0","text":"Use dedicated instances for the database servers","correct":false},{"id":"7e5d7958c67ef51985894e5f8b2a25a8","text":"Use Spot instances for the database","correct":false},{"id":"60e31915c1c4910787bf03781555aa97","text":"Use reserved instances for the database","correct":true}]},{"id":"b419a913-a5f1-4ad2-8c06-3b579643d073","domain":"development","question":"Upon creating your code repository, you remember that you want to receive recommendations on improving the quality of the Java code for all pull requests in the repository. Which of the following services provide this ability?","explanation":"When creating your repository, you have the option of enabling 'Amazon CodeGuru Reviewer for Java.' This will automate reviews of your code to spot problems that can be hard for you to detect, in addition to the recommendations to fix the code. CodeGuru Reviewer would have been the correct answer, but its not specific to the type of code (always look for the *most* correct answer in exams.) Creating the repository itself requires the use of CodeCommit, which includes the 'Amazon CodeGuru Reviewer for Java' option; thats why CodeCommit is the wrong answer. Finally, CodeBuild is for building code, rather than improving it.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/pull-requests.html","title":"Working with Pull Requests in AWS CodeCommit Repositories"}],"answers":[{"id":"56c20df941e069e1a35a1871f662298a","text":"CodeCommit","correct":false},{"id":"b9a2b9145e469fee4b348e257067e441","text":"CodeGuru Reviewer for Java","correct":true},{"id":"f7f128d00b13b324ebd29c42f1037cf8","text":"CodeGuru Reviewer","correct":false},{"id":"f8d1a2a80933c71d9a65e3d825976bda","text":"CodeBuild","correct":false}]},{"id":"cd8831e1-d079-43e5-a45d-4d0770135faa","domain":"refactoring","question":"You are working on a flight booking application which runs on a number of EC2 instances. Recently one of your servers crashed which meant all of your users lost their sessions and had to log in again. Many of your customers have complained that they had to start their session again from the beginning because your application does not store session state anywhere. Which of the following could you use to persist session state and stop this from happening?","explanation":"Many applications store session state data in memory. However, this approach doesn't scale well. After the application grows beyond a single web server, the session state must be shared between servers. DynamoDB provides an effective and scalable solution for sharing session state across web servers.","links":[{"url":"https://docs.aws.amazon.com/sdk-for-net/v3/ndg/web-dynamodb-session.html","title":"Managing Session State with Amazon DynamoDB"}],"answers":[{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":false},{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true},{"id":"248c1b0bcb74394beb4a08030c4a6847","text":"EBS","correct":false}]},{"id":"777f1a0d-0acd-4907-be1f-ceb4266d3170","domain":"development","question":"You are developing a Lambda function which takes an average of 20 seconds to execute. During performance testing, you are trying to simulate peak loads, however soon after the testing begins, you notice that requests are failing with a throttling error. What could be the problem?","explanation":"When requests come in faster than your function can scale, or when your function is at maximum concurrency, additional requests fail with a throttling error (429 status code).","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/limits.html","title":"Lambda Limits"},{"url":"https://aws.amazon.com/about-aws/whats-new/2017/05/aws-lambda-raises-default-concurrent-execution-limit/","title":"Default Concurrent Execution Limit"}],"answers":[{"id":"396cb0f44930bc3d21771b4f8c6e7afd","text":"You have reached the limit of concurrent executions for Lambda","correct":true},{"id":"5f877189f6b1d52e35220d6fac989254","text":"Your application does not have permission to invoke the Lambda function","correct":false},{"id":"f33d22d5ee65a1e9f86c1631c889d199","text":"The Lambda function is taking too long to execute","correct":false},{"id":"e7f379a6b188c5064bd7c1bf17994433","text":"You haven't allocated enough memory to your function","correct":false},{"id":"67713e6254fbdabc3504f77f455cf500","text":"The deployment package is too large","correct":false}]},{"id":"0c35721a-5f70-44fa-b450-3a734cffe32d","domain":"development","question":"You are working on an application which runs inside a Docker container. All your images are stored in a repository named mydockerrepo AWS ECR. Which of the following commands could you use to pull the Docker image to your local workstation?","explanation":"If you would like to run a Docker image that is available in Amazon ECR, you can pull it to your local environment with the docker pull command.","links":[{"url":"https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html","title":"Pulling an Image From ECR To Your Local Machine"}],"answers":[{"id":"d2b7e2822cfd2c34e140e5a2b38a8844","text":"docker clone aws_account_id.dkr.ecr.us-west-2.amazonaws.com/mydockerrepo:latest","correct":false},{"id":"3de38211ddfad5912196a8d85b693d6b","text":"docker push aws_account_id.dkr.ecr.us-west-2.amazonaws.com/mydockerrepo:latest","correct":false},{"id":"2adf07d0af0f83b4c0e540b293950cae","text":"docker pull aws_account_id.dkr.ecr.us-west-2.amazonaws.com/mydockerrepo:latest","correct":true},{"id":"90dc63a057ad01f414275df9fe070ea1","text":"docker get aws_account_id.dkr.ecr.us-west-2.amazonaws.com/mydockerrepo:latest","correct":false}]},{"id":"832e5d50-044a-49a0-8e39-b67b7325b242","domain":"refactoring","question":"You have software on an EC2 instance that needs to access both the private and public IP address of that instance. What's the best way for the software to get that information?","explanation":"To view all categories of instance metadata from within a running instance, use the following URI: http://169.254.169.254/latest/meta-data/","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html","title":"Instance Metadata and User Data"}],"answers":[{"id":"239df81185d957b59d31a19adbc9322c","text":"Have the software use cURL or GET to access the instance metadata.","correct":true},{"id":"2ee1ba64e47acbacf84d2d788ee960f6","text":"Have the software use cURL or GET to access the instance user data.","correct":false},{"id":"d7f090ce9cc095c4e147cb891beeb897","text":"Call the EC2 API.","correct":false},{"id":"62683b06759958dafa2f12da5889a9b3","text":"Reference the instance metadata for the private IP and the instance user data for the public IP.","correct":false}]},{"id":"f71363df-7941-4a84-93e1-7a5ed2ef1c08","domain":"security","question":"You have some sensitive data that you would like to encrypt. You want to be sure that once the data is encrypted, nobody but you will be able to use the encryption key to decrypt your files. Your head of security has asked you to make sure that the key used to encrypt your files is itself encrypted under another key. Which AWS technology enables this?","explanation":"When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key, and then encrypting the data key under another key.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping","title":"Envelope Encryption"}],"answers":[{"id":"8afab6f0bb19c0796620267f6a74644b","text":"Encrypt the master key with the data key","correct":false},{"id":"834369e6ea722aa59f60696c106ba827","text":"Re-encrypt the data key with the master key","correct":false},{"id":"fdf5291377e4e56d4ee76f0328313232","text":"Use envelope encryption to encrypt the data key with another key","correct":true},{"id":"c5c0340c8e4f6673916050e3cbc5e2b2","text":"Store the encryption key in an encrypted S3 bucket","correct":false},{"id":"bd2c15ddb6cb6e3fee2adc749979e1e1","text":"Store the encryption keys in CloudHSM","correct":false}]},{"id":"2863369f-3c52-497f-887b-1e6d5a13e5fe","domain":"security","question":"A developer needs to share an EBS volume with a second AWS account. What actions need to be performed to accomplish this task in the most optimal way?","explanation":"It is not possible to directly share an EBS volume with another account. In order to accomplish the required task, it is required to create an EBS volume snapshot and grant permissions to that snapshot to the second AWS account. Although EBS volume snapshots are stored in S3, they are not in a user-visible bucket. Sharing a private AMI with a second account does not meet the specific requirement as defined in the question.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html","title":"Sharing an Amazon EBS Snapshot"}],"answers":[{"id":"f2cb6e39eb1e99f97c84895d0c6b0cc8","text":"Create an IAM policy granting necessary actions on the specific EBS volume. Add the second AWS account ID in the Principal element.","correct":false},{"id":"823ae734c13e3b7ed146146637c1df42","text":"Create an EBS volume snapshot. Modify EBS snapshot permissions and add the second AWS account ID to share the snapshot. In the second AWS account, create an EBS volume from the snapshot.","correct":true},{"id":"d477177fe8b8bb5a16854f7c5632856d","text":"Create an EBS volume snapshot. Modify S3 bucket policy granting the second AWS account access to the S3 object of the snapshot. In the second AWS account, create an EBS volume from the S3 object.","correct":false},{"id":"683e6c4bef943d4c2e5871fcc39e1ebd","text":"Create an AMI from the EC2 instance. Modify image permissions and add a second AWS account ID to share the AMI. Ensure 'create volume' permissions are added. In the second AWS account, create an EC2 instance using the shared AMI.","correct":false}]},{"id":"8f888c41-8c19-4f1a-8682-7a8f8235182a","domain":"security","question":"A developer is working on a new HR application that must be able to encrypt sensitive documents, each of which is approximately 100 MB in size. The encryption needs to take place within the HR application, and each document must be encrypted using a unique key. The developer has decided to use envelope encryption, and KMS to manage their keys.\n\nWhat KMS operation should be called for each document, to most efficiently meet the requirements of the HR application?","explanation":"generate-data-key returns a plaintext data key, ready to be used to encrypt a document, and a ciphertext version of the key, encrypted using the Customer Master Key. The command should be called for each document, so a different key is used for each. Once the document is encrypted, the plaintext key is securely discarded, and the encrypted data key is stored along with the encrypted document.\n\ngenerate-data-key-without-plaintext is incorrect because it is the plaintext key that is used to encrypt the document within the application.\n\ngenerate-random is incorrect as while the response could be used as a data key; a second step would also be needed to acquire the ciphertext version of the key. It is, therefore, not the best solution.\n\nencrypt is incorrect because it can only encrypt up to 4 kilobytes of data, and because the encryption process itself would take place within KMS, directly using the Customer Master Key, not within the application. To that end, using 'encrypt' directly does not fall under AWS's definition of envelope encryption.","links":[{"url":"https://docs.aws.amazon.com/cli/latest/reference/kms/generate-data-key.html","title":"KMS CLI: generate-data-key"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/kms/encrypt.html","title":"KMS CLI: encrypt"}],"answers":[{"id":"966a36d051e3b8290953bce53c3513bf","text":"generate-random","correct":false},{"id":"f71001650370495dc4bb6bccf8fc3ac9","text":"generate-data-key-without-plaintext","correct":false},{"id":"53c82eba31f6d416f331de9162ebe997","text":"encrypt","correct":false},{"id":"0b6eb26a9e685b2edba22d0b9f8534a3","text":"generate-data-key","correct":true}]},{"id":"6641f65a-c837-49a2-bbeb-11af158d44e0","domain":"mon-trb","question":"What is the maximum execution duration for a Lambda request?","explanation":"As of Oct 2018 the maximum execution duration has been increased from 300 seconds to 900 seconds (15 minutes)","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/limits.html","title":"Lambda Limits"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/","title":"Update notice- Oct 2018"}],"answers":[{"id":"533f546e5ddb63fb2c810f7cca06678f","text":"300 seconds","correct":false},{"id":"7ed53d277129b356be62369ec930e3b8","text":"500 seconds","correct":false},{"id":"a51534ea662db3ce23238035e25859e2","text":"900 seconds","correct":true},{"id":"4234838a99b7912e550babb083c205c4","text":"60 seconds","correct":false},{"id":"8d15ed7d27d83ed6229a66b1f44b7696","text":"3 minutes","correct":false}]}]}}}}
