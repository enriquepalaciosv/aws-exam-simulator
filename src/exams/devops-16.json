{"data":{"createNewExamAttempt":{"attempt":{"id":"3df77d5e-af6b-48c4-b7f9-3b383c9c4bd0"},"exam":{"id":"f5a36bd5-2939-47a1-8f8f-d2ba450b1d52","title":"AWS Certified DevOps Engineer - Professional 2019","duration":10800,"totalQuestions":75,"questions":[{"id":"9f7ba9e2-8030-4333-acf6-e2d27613e959","domain":"IncidentEventResponse","question":"You have an idea regarding your AWS account security. You would like to monitor your account for any possible attacks against your resources, such as port scans or brute force SSH and RDP attacks. If anything is detected, you want the report pushed to a Slack channel where anyone in your company can monitor and take action if it's their responsibility. How do you go about implementing this?","explanation":"Amazon GuardDuty is the best way to implement this. It can trigger Lambda functions on events which can be used to post to a Slack channel.","links":[{"url":"https://aws.amazon.com/guardduty/","title":"Amazon GuardDuty"}],"answers":[{"id":"e91bba567af0e13b250c7976157789ae","text":"Implement a Lambda function to monitor your VPC Flow Logs. For any odd requests, post the information to your Slack channel.","correct":false},{"id":"a594a49c80d0416db98a60a3f66adc4e","text":"Implement Amazon GuardDuty. On detected events, trigger a Lambda function to post the information to your Slack channel.","correct":true},{"id":"2b3ed3e5fd2a948326831b73fe30bd4a","text":"Implement AWS Inspector. On detected events, trigger a Lambda function to post the information to your Slack channel.","correct":false},{"id":"74245776eaafb5659f52d1c243b22462","text":"Implement a Lambda function to monitor your CloudTrail file. For any odd API calls or requests, post the information to your Slack channel","correct":false}]},{"id":"68f29294-fe1b-46a6-8c84-d5076c7a126e","domain":"MonitoringLogging","question":"Your customers have recently reported that your Java web application stops working sometimes. Your developers have researched the issue and noticed that there appears to be a memory leak which causes the software to eventually crash. They have fixed the issue, but your CEO wants to ensure it never happens again. Which AWS services could help you detect future leaks so you're able to fix the issue before the application crashes?","explanation":"Pushing the custom cloudwatch metric is a good idea, you could add it to a dashboard but that wont alert your developers unless they're actively checking it, which you can't rely on.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"Publishing custom metrics"}],"answers":[{"id":"cc97d942d9ce31c3147db1f7555e8fb1","text":"Push your memory usage to a custom CloudWatch metric, set it to alert your developers if it crosses a threshold.","correct":true},{"id":"b11a45539b5c62eef7c857b34b391f5e","text":"Push your memory usage to CloudTrail, have a lambda function monitor it and alert a SNS queue if it crosses a threshold.","correct":false},{"id":"c2991a8b12bcb9d0e41ed823d43ec6e8","text":"Push your memory usage to CloudWatch logs, have a lambda function monitor it and alert a SNS queue if it crosses a threshold.","correct":false},{"id":"d4ed3b2231654ab7cfe611a280ed9fa8","text":"Create a CloudWatch dashboard to monitor the memory usage of your app from a custom metric you are pushing to CloudWatch.","correct":false}]},{"id":"80f664b2-633c-4476-ae74-6d249087e79f","domain":"IncidentEventResponse","question":"You are a day trader working from home with a software engineering background. You are considering making use of some cloud services to alert you of stock price changes so you know immediately when the price of certain stocks has risen or fallen more than 1%. You know you can ingest your data to a Kinesis Data stream, but how will you query it?","explanation":"You can query your streams directly from your application using SQL and Kinesis Data Analytics","links":[{"url":"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/continuous-queries-concepts.html","title":"Continuous Queries - Amazon Kinesis Data Analytics for SQL Applications Developer Guide"}],"answers":[{"id":"e010acc5480838d8d1679bd03d0730ba","text":"Use JSON with Kinesis Data Analytics","correct":false},{"id":"04efd74e70cd57a041797aa4b35f74e7","text":"Use SQL with Kinesis Data Analytics","correct":true},{"id":"ce8028a6137a5e1bb29aa083a15ccd3c","text":"Use RDS with Kinesis Data Analytics","correct":false},{"id":"1069736ede5f32f059462eb2901e8b93","text":"Use YAML with Kinesis Data Analytics","correct":false}]},{"id":"0917c160-74a4-439b-818b-248197d8fd54","domain":"IncidentEventResponse","question":"Your organization has a few million text documents in S3 that are stored in a somewhat random manner, and the amount of files is always growing. The developer that initially wrote the system in use stored everything with a random file name with some attempt at security through obscurity. Now your CEO and CFO both need to be able to search the contents of these documents, and they want to be able to do so quickly at a reasonable cost. What managed AWS services can assist with implementing a solution for your CEO and CFO, and what would the setup process involve?","explanation":"CloudSearch by itself is enough to fulfill the requirements put forward here. CloudSearch is managed, scalable can very quick to configure and get online. In comparison it would take some time to set up EC2 and install ElasticSearch or any other search tool, and would be much more difficult to scale. This involves creating a search domain and configuring the index as required, and then setting up the access policies.","links":[{"url":"https://aws.amazon.com/cloudsearch/","title":"AWS | Amazon CloudSearch - Search Service in the Cloud"}],"answers":[{"id":"8f9d7ad9c2c6d20b41851ce8e3572aaa","text":"Configure your index","correct":true},{"id":"38b79d58ed1b5a50d6bbc91173d0c745","text":"Create a search index","correct":false},{"id":"e26c5d07fc30848902a99c95897441b4","text":"Set up IAM roles","correct":false},{"id":"280209643a35ef346f22e051eafca06e","text":"Create a search domain","correct":true},{"id":"562f9538310fcd7c846445ad9768ab7c","text":"Implement Amazon CloudSearch","correct":true},{"id":"d912e8309bb58d3483307e1ea428bf8a","text":"Set up access policies","correct":true},{"id":"a34f38b0356fb4da2ce3a29a66b4b415","text":"Configure your baseline","correct":false},{"id":"ab25d01b99c512f0cb03129afc920a07","text":"Implement ElasticSearch","correct":false},{"id":"705d488d75db0b38eb9811a38a863d57","text":"Implement MongoDB","correct":false}]},{"id":"f6f2e5d0-a72e-4d44-aaea-2e2e8fa264c3","domain":"HAFTDR","question":"Your company utilizes EC2 and various other AWS services for its workloads. As the DevOps engineer it is your responsibility to ensure all company policies are implemented. You have noticed that while you are using S3 for data archival and backups, your company policy is that your backups need to reside on company owned servers, such as those you run in your local Equinix data center. You also have another company policy that backup and archival cannot traverse the internet. What do you do?","explanation":"AWS Direct Connect is the only way to access your AWS resources from a Data Center without traversing the internet, despite the encryption offered by the other solutions.","links":[{"url":"https://aws.amazon.com/directconnect/","title":"AWS Direct Connect"}],"answers":[{"id":"008894eae706c0e60a40a59c79342658","text":"Implement an AWS Client VPN with a company owned server in your data center. Push backups to your data center backup NAS through SSH via vpn.","correct":false},{"id":"b51577fa11c831eba7efec0c73952602","text":"Implement a Site to site VPN with a company owned server in your data center. Push backups to your data center backup NAS through SSH via vpn.","correct":false},{"id":"f42f4a5866921fe7f0288f667c043aac","text":"Provision an AWS Direct Connect connection to your local router in your data center and your local VPC. Push backups via the Direct Connect connection.","correct":true},{"id":"32f6b2f1cc26546abf7ce835ae778156","text":"Install the EFS agent on your data center backup NAS, mount the volume on an EC2 server and copy the backups to the volume.","correct":false}]},{"id":"4cfee0f4-02a3-43bc-af10-a34d888a0086","domain":"HAFTDR","question":"The Marketing Team of Mega-Widgets Inc. have recently released an advertising campaign in the APAC region, in addition to their home market of Europe.  The CTO has told you that they have started receiving complaints from Australian customers that the site located in eu-west-1 is slow, and they think by utilising the existing Disaster Recovery infrastructure in ap-southeast-2, they can speed up response time for these customers.  As time is of the essence, they have asked you to assist.  You have already confirmed that data is synchronising between both sites.  What is the quickest way to reduce the latency for these customers?","explanation":"As we are not told otherwise, we can assume that the European site is fast for people within Europe and therefore we can assume a high latency is the cause of the problem.  Route 53 Latency-Based Routing sounds like the perfect candidate when utilising the Disaster Recovery site as the main site for Asia Pacific.  CloudFront could offer some help if we only had one site and the Edge locations could cache some content, but in this case the CTO wanted to use the Disaster Recovery infrastructure already in place.","links":[{"url":"https://aws.amazon.com/route53/faqs/","title":"Amazon Route 53 FAQs"},{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Tutorials.html","title":"Tutorials"}],"answers":[{"id":"0e6075956527bb40627bfab8871eab13","text":"In Route 53, create eu.mega-widgets.com and ap.mega-widgets.com and point them to the same CloudFront endpoint.","correct":false},{"id":"6752c886fe176d4a7c3fd5d847abf946","text":"In Route 53, create two records for www.mega-widgets.com, a latency record pointing to the European IP and a latency record pointing to the Asia Pacific IP.","correct":true},{"id":"883bf6152c030fbe021fafb5fb55c8d4","text":"In Route 53, create two sub-domain A records of mega-widgets.com, one with the European IP and the other with the Asia Pacific IP. Create two aliases pointing www.mega-widgets.com to both sub-domains.","correct":false},{"id":"d718f6cb2c4337528dfde78cbe94dfad","text":"Create a CloudFront endpoint with the origin pointing to the European site and then point www.mega-widgets.com to that endpoint.","correct":false}]},{"id":"79f293d1-a9b0-46ff-b2e2-bb8caa0f0ff1","domain":"IncidentEventResponse","question":"You are working in a creative industry where artists will upload various works of art to your servers before they are packaged up into a single archive each month. They are then sold to customers on a subscription basis where they receive the monthly collection from the artist(s) they support. Your servers are autoscaled, so it's difficult to know which one the artist will be uploading to at any one time. You have also decided to serve the collections straight out of S3 instead of storing them locally. What's the most convenient way to manage the uploading and packaging of the art?","explanation":"An EFS volume will ensure all files are included across all of your upload servers.","links":[{"url":"https://aws.amazon.com/efs/","title":"Amazon Elastic File System"}],"answers":[{"id":"6b6133799067d4557d61f78b2ae7e3e7","text":"Use an EFS volume across all upload servers. Package the art into a single zip or tar file with a cronjob at the end of the month and upload it to S3.","correct":true},{"id":"5ecfc0c066355a155ec75dd3eed9dff6","text":"Set up an FTP server for artists to upload to on all the EC2 instances. Package the art into a single zip or tar file with a cronjob at the end of the month and upload it to S3.","correct":false},{"id":"22aa5e2ae7e30890fad3160f0a4d2441","text":"Keep the art works on the server the artist is uploading to, run a cronjob which will tar the files and upload them to S3.","correct":false},{"id":"c81737cd1c2058ecb93efc5a2854e034","text":"Upload each file directly into S3, use S3's archival feature to package the artworks into a single file each month.","correct":false}]},{"id":"b21d657a-8e37-4f2b-9641-6e37ff00825c","domain":"SDLCAutomation","question":"Your manager wants to implement a CI/CD pipeline for your new cloud-native project using AWS services, and would like you to ensure that it is performing the best automated tests that it can. He would like fast and cheap testing, where bugs can be fixed quickly. He suggests starting with individual units of your software and wants you to test each one, ensuring they perform how they are designed to perform. What kind of tests do you suggest implementing, and what part of your CI/CD pipeline will you implement them with?","explanation":"Unit tests are built to test individual units of your software and quickly identify bugs. These can be implemented with CodeBuild.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing CI/CD on AWS"}],"answers":[{"id":"673e10c0ae89a727cede17ac866f1ae4","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build source tests for current code base and ensure that your developers produce source tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeCommit","correct":false},{"id":"437e248b28463ad899879dda97bea983","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some unit tests for the existing code base and ensure that your developers produce component tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeCommit","correct":false},{"id":"464e499150ad3f7270dbfb985c67bcb0","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some compliance tests for current code base and ensure that your developers produce component tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeBuild","correct":false},{"id":"962f2ea0d522b0054011f4e7935fd81e","text":"Start by creating a code repository in AWS CodeCommit for your software team to perform source-control.  Build some unit tests for the existing code base and ensure that your developers produce unit tests as early as possible for software as it is built.  Implement the execution of unit testing using AWS CodeBuild","correct":true}]},{"id":"ac6646a9-8e77-48b4-99fd-a477f558c477","domain":"SDLCAutomation","question":"Your CEO has come up with a brilliant idea for when your company migrates its first application to the Cloud. He is a big fan of infrastructure as code and automation, and thinks your deployments will never fail again if you spin up completely new servers, deploy your software to them, redirect your DNS to the new servers and then shut down the old servers. Unfortunately for him, you know this has existed for a long time and had already planned to implement this in the coming weeks. Which build and deployment strategy report do you grab from your desk to show him?","explanation":"Blue/Green deployments are certainly what your CEO thinks he has invented. Updating your Route53 alias record to a new load balancer","links":[{"url":"https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf","title":"AWS Blue Green Deployments"}],"answers":[{"id":"6b741114a9154860a54dfc92186ad064","text":"Build the data tier using Amazon RDS via CloudFormation. Deploy your web tier behind a load balancer, hosted on EC2 instances.  During deployments, utilise a rolling deployment strategy by deploying an entirely new load balancer and group of EC2 instances which run your new code base.  Once the build and deployment of the fresh environment is complete, testing can be performed by accessing the site from the new load balancer URL. You can use a Route 53 alias to direct people from the old load balancer to another once your testing team completes their checks.","correct":false},{"id":"0b5318c4c58764e77d91057e51dcc6ba","text":"Build the data tier using Amazon RDS via CloudFormation. Deploy your web tier behind a load balancer, hosted on EC2 instances.  During deployments, utilise a blue/green deployment strategy by deploying an entirely new load balancer and group of EC2 instances which run your new code base.  Once the build and deployment of the fresh environment is complete, testing can be performed by accessing the site from the new load balancer URL. You can use a Route 53 failover routing policy to redirect people from your old load balancer to another once your testing team completes their checks.","correct":false},{"id":"648583c850bab8423500a7340e307a54","text":"Build the data tier using Amazon RDS via CloudFormation. Deploy your web tier behind a load balancer, hosted on EC2 instances.  During deployments, utilise a Mirrored deployment strategy by deploying an entirely new load balancer and group of EC2 instances which run your new code base.  Once the build and deployment of the fresh environment is complete, testing can be performed by accessing the site from the new load balancer URL. You can use a Route 53 alias to direct people from one load balancer to another once your testing team completes their checks.","correct":false},{"id":"6ccc35631fac4729d9b44594506d7a15","text":"Build the data tier using Amazon RDS via CloudFormation. Deploy your web tier behind a load balancer, hosted on EC2 instances.  During deployments, utilise a blue/green deployment strategy by deploying an entirely new load balancer and group of EC2 instances which run your new code base.  Once the build and deployment of the fresh environment is complete, testing can be performed by accessing the site from the new load balancer URL. You can update your Route 53 simple routing policy to direct people from the old load balancer to another once your testing team completes their checks.","correct":true}]},{"id":"c124d204-9148-4798-9b7b-eb6596a9f8b3","domain":"IncidentEventResponse","question":"Your company develops an online shopping platform and would like to implement a way to recommend products to customers based on previous products they have looked at. To use this you want to record their click-stream, or the sequence of links they've clicked on as they navigate your website. At any one time, you can have thousands of users using your shopping platform. Which architecture will meet your requirements?","explanation":"Ingesting is done with Kinesis Data Streams, grouping user requests into sessions is done with Data Analytics. Data processing and storage is done with Lambda, Firehose and S3. Read the attached link and the high-level solution overview for more information.","links":[{"url":"https://aws.amazon.com/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/","title":"Create real-time click-stream sessions and run analytics with Amazon Kinesis Data Analytics, AWS Glue, and Amazon Athena"}],"answers":[{"id":"2a469128dfb0909cb7f378ca0b0804e3","text":"Ingest data with Kinesis Data Analytics. Group user requests with Kinesis Data Firehose. Process and store the data with Lambda, Kinesis Streams and S3.","correct":false},{"id":"308394a93edfced21f133bc91188efdf","text":"Ingest data with Kinesis Data Streams. Group user requests with Kinesis Data Analytics. Process and store the data with Lambda, Kinesis Firehose and S3.","correct":true},{"id":"8a71cd8aeec8a1da02a83f432fb0973b","text":"Ingest data with Kinesis Firehose. Group user requests with Kinesis Data Data Streams Groups. Process and store the data with EC2, Kinesis Streams and S3.","correct":false},{"id":"2046d52f62962c7601dbac9ea1beaf56","text":"Ingest data with Kinesis Data Streams. Group user requests with Kinesis Data Analytics. Process and store the data with EC2, Kinesis Firehose and S3.","correct":false}]},{"id":"8c127356-141c-4199-bfd4-f1d8e0a8560d","domain":"IncidentEventResponse","question":"You have taken over a half finished project to implement Kinesis Data Streams.  Data is being successfully and efficiently placed onto the streams using KPL based code, but when running the KCL code to read off the shards, you are experiencing issues.  The main issues relate to slower than expected reading from the shards, but you are also seeing records being skipped.  Which options allow you to resolve both of these problems?","explanation":"Many of the above solutions answer Kinesis related questions, but we specifically need to know what could cause a slow down in reading from a shard, and why records would be skipped.  For this, there are only two correct answers.  For slow reading, it could be due to the maxRecords value being set too low or the code logic, which is calling processRecords, being inefficient and causing high CPU usage or blocking.  However, there is only one correct answer for the skipping of records and that is usually due to processRecords calls having un handled exceptions.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html","title":"Troubleshooting Amazon Kinesis Data Streams Consumers"}],"answers":[{"id":"989df0ef5880a136eb3db849ff45dc66","text":"Ensure the maxRecords value for the GetRecords call isn't set below the default setting.  Also, check that the processRecords call is not throwing an unhandled exception.","correct":true},{"id":"61bd51378999bcd6a2d6551c0cc12100","text":"Check the logic of the code to ensure the processRecords call isn't CPU intensive or I/O blocking.  Also, check that the processRecords call is not throwing an unhandled exception.","correct":true},{"id":"b579dde17faaedc85ca4956edf187830","text":"Ensure the maxRecords value for the GetRecords call isn't set below the default setting.  Also, make sure you aren't seeing a sudden large increase in the GetRecords.IteratorAgeMilliseconds metric.","correct":false},{"id":"abd17cf52b5933e670c26b2caed0264c","text":"Choose a failover time that is less than 10 seconds, to ensure reading from a shard maintains a 1 to 1 relationship.  Also, make sure the consumers do not exceed the read per-shard limit.","correct":false},{"id":"c6314cbc442d2cd7efa28180d7a5abcf","text":"Ensure you have made a GetRecords call at least every 5 minutes.  Also, check that the processRecords call is not throwing an unhandled exception.","correct":false}]},{"id":"6e4406f4-c446-4f58-9533-e04e5b45eb0d","domain":"ConfigMgmtandInfraCode","question":"After many years of running co-located servers, your company has decided to move their services into AWS.  The prime reasons for doing this is to scale elastically and to define their new infrastructure as code using CloudFormation.  Your colleagues have been defining infrastructure in a test account but now they are ready to deploy into production, they are identifying some problems.  They have have attempted to deploy the main CloudFormation Template but they are seeing the following errors; 'Invalid Value or Unsupported Resource Property', 'Resource Failed to Stabilize' and when these errors are encountered, the stack fails to rollback cleanly.  Assist your team by choosing the best ways to troubleshoot these problems.","explanation":"Most of these answers are valid troubleshooting solutions for various CloudFormation issues, but there is only one answer for the problems listed in the question. 'Invalid Value or Unsupported Resource Property' errors appear only if there is a parameter naming mistake or that the property names are unsupported. 'Resource Failed to Stabilize' errors appear because a timeout is exceeded, the AWS service isn't available or is interrupted.  Finally, any update which fails to rollback could be because of a number of reasons, but the most popular is due to the deployment account having permissions to create stacks, but not to modify or delete stacks. The answer which includes all of these pieces of advice is correct.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html","title":"Troubleshooting AWS CloudFormation"}],"answers":[{"id":"9f3b9793c5af6485de0b1b166a4e36be","text":"Ensure that your AMI has the CloudFormation helper scripts installed and that your VPC has a defined route to the Internet.  Also ensure that the user deploying the CloudFormation stack has enough permissions to modify resources.","correct":false},{"id":"7114f5881fc7b95daef2e696e207bac9","text":"Check that a DependsOn attribute has been included in the Template, and that you are not waiting for a resource which usually takes longer than the default timeout period. Finally, ensure that the user deploying the CloudFormation stack has enough permissions to create resources.","correct":false},{"id":"c271e5866ef0c9376f5675822c410c2f","text":"Check that resources exist when specified as parameters, ensure that there is no maintenance being undertaken on any of the defined AWS services and that the user deploying the CloudFormation stack has enough permissions to modify resources.","correct":true},{"id":"a577e14c499181ee450e6306c9e57820","text":"Ensure that you do not have termination protection enabled, that a DependsOn attribute has been included in the Template and that there is no maintenance being undertaken on any of the defined AWS services.","correct":false}]},{"id":"fddf85cb-72e6-4fa1-9c80-07536a84a6e8","domain":"HAFTDR","question":"You have a new website design you would like to test with a small subset of your users. If the test is successful, you would like to increase the amount of users accessing the new site to half your users. If that's successful and your infrastructure is able to scale up correctly, you would like to completely roll over to the new design and then decommission the servers hosting the old design. Which of these methods do you choose?","explanation":"A weighted routing policy combined with an auto-scaling group will meet your requirements and will continue to scale if your tests are successful and you completely roll over to the new design.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted","title":"Weighted Routing"}],"answers":[{"id":"07123797fd0770bb8b71a6b339d36bfc","text":"Install the new website design in a new AutoScaling group. Use a Weighted Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 50%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":true},{"id":"1a147be1638db0d199c9779bb5488bd7","text":"Install the new website design in a new AutoScaling group. Use a Weighted Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 20%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false},{"id":"ff9caa8ea679855cc9425d3e553bf303","text":"Install the new website design in a new AutoScaling group. Use an A/B Test Routing policy in Route53 and use it to choose the percentage of users you would like during different testing phases. Start with 5%, then 20%, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false},{"id":"8d8f29ecca384ae73a4006c686df91b8","text":"Install the new website design in a new AutoScaling group. Create a Lambda function to modify your Route53 apex record to use the new AutoScaling group for 5% of the day. If that's successful then modify the function to change the apex record for half the day, and end with 100% of traffic going to the new AutoScaling group if tests are successful. Decommission the old EC2 servers.","correct":false}]},{"id":"927aeb0d-23f7-48f5-9ca7-aa76c6403385","domain":"SDLCAutomation","question":"You are contracting for APetGuru, an online pet food store. Their website works fine, but you really want to update the look and feel to be more modern. They have given you strict guidelines that their website can not be offline at all or they will lose sales. You are thinking of using a rolling deployment so some of their servers are always online during the rollout. Just before you trigger the roll out, you receive a phone call asking if you can only install your updates on a few servers first for testing purposes. It is suggested that a few customers can be redirected to the updated site and everyone else can still use the old site until you confirm the new one is working properly. Once you're happy with the operation of the updated website, you can complete the rollout to all servers. What do you do?","explanation":"A canary deployment will allow you to complete the roll out in the way you want to.","links":[{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing continuous integration / continuous delivery on AWS"}],"answers":[{"id":"807b35728975296049d217f0dfbb3bc8","text":"Use an In-Place Deployment. You can deploy to the \"In-Place\" servers first for testing, then once testing is verified you can continue the deployment to the \"Out-Place\" externally facing servers.","correct":false},{"id":"a93e067980f0010b2f812e5f3eef9b9f","text":"Use a Blue/Green deployment. This allows you to install the new website to the blue servers, and once you're happy with it working you can finalise the install on the green servers. If there's an issue you can roll back the blue installation.","correct":false},{"id":"562add4453cef6369a9860f6a26dbe36","text":"Use a Batch Deployment. This allows you to install your new website to the small batch which traffic will be directed to, and will allow you to roll back the small batch if there's an error. Otherwise, you can complete the rollout on the large batch.","correct":false},{"id":"864cd30cc708bcfb524df58f775ae226","text":"Use a Canary Deployment. This allows deployment to a few servers where you can observe how the website is running while still receiving a small amount of customers. If there's an error it can be rolled back. Otherwise, the rollout will continue on new servers.","correct":true}]},{"id":"0e604476-a66d-48c8-b59d-dbb5efe66d38","domain":"SDLCAutomation","question":"AWS CodeBuild is used for a project that you look after which has a known issue with its build performance. You can trace it back to an environment variable named AWS_CODEBUILD_MAX_MEM_ALLOC that is defined in multiple places. It has a value of '4096' in your build spec declaration, '2048' in the build project definition and a value of '1024' in the start build operation call. Which of the following statements is correct?","explanation":"Its value is '1024'. Do not set any environment variable with a name that starts with 'CODEBUILD_' as this prefix is reserved for internal use.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html","title":"Build Specification Reference for CodeBuild: Build Spec Syntax: env"}],"answers":[{"id":"7201fd1c5d65552aba9eb7f9f1b7c846","text":"Its value is '4096' because the value in the build spec declaration takes highest precedence, followed by the value in the build project definition while the value in the start build operation call takes lowest precedence.","correct":false},{"id":"a09a6c2e72b77a064358b20cbf7ffeea","text":"The value in the start build operation call takes highest precedence, followed by the value in the build project definition while the value in the build spec declaration takes lowest precedence. Therefore its value is determined as '1024'.","correct":true},{"id":"6d6b3a85646a3254ab985c43b3a11a20","text":"The value in the build project definition takes highest precedence, followed by the value in the start build operation call while the value in the build spec declaration takes lowest precedence. Because of this, its value is '2048'.","correct":false},{"id":"3e4a9e589e2cb112cdc5aff1387285fc","text":"The variable is undefined as the 'AWS_CODEBUILD_' prefix of its name is reserved for internal use.","correct":false},{"id":"91e162eb52d2dd94ba6973eae901a468","text":"The value of AWS_CODEBUILD_MAX_MEM_ALLOC  is '2048' as the build project definition takes highest precedence, followed by the value in the build spec declaration while the value in the start build operation call takes lowest precedence.","correct":false}]},{"id":"ee99751c-1258-4f81-adf9-31a8c57b2605","domain":"ConfigMgmtandInfraCode","question":"Your CloudFormation template is becoming quite large, so you have been tasked with reducing its size and coming up with a solution to structure it in a way that works efficiently. You have quite a few resources defined which are almost duplicates of each other, such as multiple load balancers using the same configuration. What CloudFormation features could you use to help clean up your template?","explanation":"Nested stacks should be used to reuse common template patterns.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#nested","title":"AWS CloudFormation Best Practices"}],"answers":[{"id":"242cb2d9063dee2cfd0a6b0cabacb580","text":"Use goto policies. These allow you to refer to different sections of your template for reuse.","correct":false},{"id":"8a32bed4f4f47b1c84105ed6173ba8c4","text":"Use custom resources. They can be called by your stack to define resources without having to reuse their code.","correct":false},{"id":"86793dc47df7cca5d373cdb3f8fc8f61","text":"Use Intrinsic functions. This allows dedicated functions to be called and parameters passed to generate resources.","correct":false},{"id":"b8d5d665b7d8b4abfc48daa77a64c021","text":"Use nested stacks. This will allow a dedicated templates to be defined for services that are used multiple times","correct":true}]},{"id":"547f6880-c168-4fa5-91d6-6822a772af6b","domain":"ConfigMgmtandInfraCode","question":"Your team has been working with CloudFormation for a while and have become quite proficient at deploying and updating your stack and their associated resources. However one morning you notice that your RDS MySQL database is completely empty. You track this down to a simple port change request that came through, asking if the default MySQL port could be changed for security reasons. What do you suspect happened?","explanation":"The database was replaced due to the AWS:RDS:DBInstance Port attribute update requirement of 'Replacement', the database will have to be restored from backups.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html","title":"Update Behaviors of Stack Resources"}],"answers":[{"id":"2cb85ef53a3961bcf3c2798f04263e7e","text":"The 'Update' attribute for AWS:RDS has the update requirement of \"Replacement\".","correct":false},{"id":"f3e35605b25b631ab990cfcba037091c","text":"The CloudFormation stack was updated with the Replacement stack value set to true.","correct":false},{"id":"aab0bf043065432156b116b7beed04c2","text":"The 'Port' attribute for AWS:RDS:DBInstance has the update requirement of \"Replacement\".","correct":true},{"id":"0974d47e810ed46ba7723376856be841","text":"The developer changed the MySQL port to a port that was already in use in your VPC so it can no longer be connected to.","correct":false}]},{"id":"a5ef00b0-c312-49fc-9553-6e7217f93561","domain":"HAFTDR","question":"Your organization has multiple weather stations installed around your state. You would like to move the data from your old relational database to a newer and hopefully faster NoSQL database. Which AWS solution would you choose for storing and migrating the data, and which keys do you use for your weather station ID and the timestamp of the reading?","explanation":"DynamoDB is the AWS NoSQL database. The Weather Station ID will be your Partition Key and the timestamp will be your Sort key.","links":[{"url":"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/","title":"Choosing the Right DynamoDB Partition Key"}],"answers":[{"id":"52f544e451b4aef2951330297f1b4235","text":"Use Amazon DMS to migrate data to an Amazon DynamoDB table. Select your RDS instance as the source and DynamoDB as destination.  Create a task with an object mapping rule to copy the required relational database tables to DynamoDB.  Within your object-mapping set your weather station ID as the Primary Key of your table and timestamp of the reading as the Sort key.","correct":false},{"id":"3399272a0609e8a363a59878ea5960ef","text":"Use Amazon DMS to migrate data to an Amazon ElasticCache table. Select your RDS instance as the source and ElasticCache as destination.  Create a task with an object mapping rule to copy the required relational database tables to DynamoDB.  Within your object-mapping set your weather station ID as the Primary Key of your table and timestamp of the reading as the Sort key.","correct":false},{"id":"4ea68f2e23cb18631ed94c5aefa19cf8","text":"Use Amazon DMS to migrate data to an Amazon DataTables table. Select your RDS instance as the source and DataTables as destination.  Create a task with an object mapping rule to copy the required relational database tables to DynamoDB.  Within your object-mapping set your weather station ID as the Partition Key of your table and timestamp of the reading as the Sort key.","correct":false},{"id":"a0d196957142835b951087cae863939c","text":"Use Amazon DMS to migrate data to an Amazon DynamoDB table. Select your RDS instance as the source and DynamoDB as destination.  Create a task with an object mapping rule to copy the required relational database tables to DynamoDB.  Within your object-mapping set your weather station ID as the Partition Key of your table and timestamp of the reading as the Sort key.","correct":true}]},{"id":"5125f905-4372-437c-838f-63de5bf0354f","domain":"HAFTDR","question":"Your current application uses an Aurora database, however the speeds aren't as fast as you would like for your bleeding edge website. You are too deep into developing your application to be able to change the database you are using or to implement faster or larger read replicas. Your application is read-heavy, and the team has identified there are a number of common queries which take a long time to be returned from Aurora. What recommendations would you make to the development team in order to increase your read performance and optimise the application to use Aurora?","explanation":"ElastiCache will cache common queries by holding them in memory instead of on disk, and will speed up your application considerably","links":[{"url":"https://aws.amazon.com/elasticache/faqs/","title":"Amazon ElastiCache FAQs"}],"answers":[{"id":"d2f04a86c7627f46eed3695113f9d1d9","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should switch to Aurora Serverless.","correct":false},{"id":"c639fb8842c60b552eacb76b7583ebc3","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should create a read-optimised replica and redirect all application reads to that endpoint.","correct":false},{"id":"f9d6e50f24acc22852ddfc3f2dbfb914","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should Implement ElastiCache between your application and the database.","correct":true},{"id":"fd97848b273db8bc7178cc86c812aa15","text":"You should tell your team to optimise their application by ensuring that where possible they engineer the application to make a large number of concurrent queries and transactions as this is one area that Aurora is optimised for.  In addition they should increase the database instance size to a low-latency instance.","correct":false}]},{"id":"ad3ca55a-a29e-4c0a-8204-2eb07854ddf2","domain":"IncidentEventResponse","question":"Your company's suite of web applications have just been overhauled to prevent some security issues and memory leaks that were slowing them all down significantly. It's working a lot more efficiently now, though your developers are still on the lookout for any network security issues the application might be leaving exposed. A weekly scan of ports reachable from outside the VPC would be beneficial. Which AWS service will you use to implement this with minimal additional overhead to the existing CI/CD process, and how will you configure it?","explanation":"Amazon Inspector is an automated security assessment service which will allow you to improve the security and compliance of your applications. A network configuration analysis checks for any ports reachable from outside the VPC. The agent is not required for this.","links":[{"url":"https://aws.amazon.com/inspector/","title":"Amazon Inspector - Amazon Web Services (AWS)"}],"answers":[{"id":"85ffa705a973908e8e02e22ca035a00a","text":"Implement Amazon GuardDuty.","correct":false},{"id":"5263e79a1693336f8cffea30b57f14c4","text":"Configure Network Assessments.","correct":true},{"id":"4d72a45ae986fba394b6ac15e0822b91","text":"Implement Amazon Macie.","correct":false},{"id":"69ef6b28f0e5a5c3d28ccda2f3197473","text":"Implement Amazon Inspector.","correct":true},{"id":"b5dabb6bbcab4c4cb776e9cfed03b42e","text":"Configure Host Assessments.","correct":false},{"id":"27ec757b9636ff05cdc67b897d2e0b9e","text":"Implement Amazon X-ray.","correct":false},{"id":"de2ce6aca6fc8725c0f4d32837d0371f","text":"Install agent.","correct":false}]},{"id":"0c271f46-7ab3-485d-a497-2387d04c151a","domain":"MonitoringLogging","question":"You're developing a Node.js application that uses the AWS Node.js API. You interact with lots of different AWS services, and would like to determine how long your API requests are taking in an effort to make your application as efficient as possible. It would also be useful to detect any issues that may be arising and give you an idea about how to fix them. Which AWS service can assist in this task and how would you go about achieving it?","explanation":"AWS X-Ray will produce an end to end view of requests made from your application where you can analyze the requests made as they pass through your application.","links":[{"url":"https://aws.amazon.com/xray/features/","title":"AWS X-Ray Features"}],"answers":[{"id":"d920ccd5f4c9bc34ad4acb6ba48ac523","text":"AWS AWS X-Ray, inspect the client map, trace the segment path, determine the bottlenecks.","correct":false},{"id":"4ca7aef91bd24ff0af398119f581d683","text":"Use Amazon QuickSight, inspect the client map, trace the segment path, determine the bottlenecks.","correct":false},{"id":"0d3184462bd7dde819bd2cb519ee4738","text":"Use AWS X-Ray, inspect the service map, trace the request path, determine the bottlenecks.","correct":true},{"id":"d58a545b79533928203dfa0e4a473547","text":"Use Amazon QuickSight, inspect the service map, trace the request path, determine the bottlenecks.","correct":false}]},{"id":"2783a2d1-d7aa-42d9-8a75-041a2c73010e","domain":"MonitoringLogging","question":"During a compliance audit, a deficiency is identified stating that insufficient log monitoring and alarming exists for your entire application portfolio on AWS. You have workloads running on Amazon EC2 in ten different AWS accounts and in three AWS regions. Resolving the audit deficiency requires the creation of a centralized log monitoring capability for the AWS-based applications and infrastructure, and for certain on-premises systems that they interface with. You've been tasked with creating an automated solution that will satisfy the auditors. Which architecture will you implement for the solution?","explanation":"Amazon Elasticsearch is a fully managed service that lets you collect and analyze logs and metrics, giving you a comprehensive view into your applications and infrastructure, reducing mean time to detect (MTTD) and resolve (MTTR) issues. Amazon CloudWatch collects API events from CloudTrail, networking events from VPC Flow Logs, and application and system logs from EC2 instances via the CloudWatch Logs agent. Using a single CloudWatch log group ensures that system logs share the same retention, monitoring, and access control settings. Lambda functions in each account can move log data from CloudWatch into Elasticsearch for indexing and visualization with Kibana. CloudWatch log streams are sequences of events from the same source, whereas a log group is a collection of log streams. Logstash works well for collecting logs, but is usually paired with Elasticsearch for log monitoring and analysis. AWS Systems Manager agents send status and execution information back to Systems Manager, but not the application logs, which the CloudWatch Logs agents are capable of sending.","links":[{"url":"https://aws.amazon.com/cloudwatch/","title":"Amazon CloudWatch"},{"url":"https://aws.amazon.com/elasticsearch-service/","title":"Amazon Elasticsearch Service"},{"url":"https://aws.amazon.com/solutions/centralized-logging/","title":"Centralized Logging"}],"answers":[{"id":"08a40cc3203d8aca0e7363189d9cff8e","text":"Install Logstash on an EC2 instance in an Auto Scaling Group in the primary account. Implement AWS Systems Manager agents on all EC2 instances and point them to Systems Manager in the primary account. Also install AWS Systems Manager agents on the on-premises systems and point them to Systems Manager in the primary account. Create an AWS Systems Manager Automation document in the primary account to load AWS CloudTrail, VPC Flow Log, and EC2 log data from all accounts into Logstash. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false},{"id":"bc1e38ce114c629b8597b80ee4242dc1","text":"Implement CloudWatch Logs agents on all EC2 instances and set up a single log stream. Also deploy CloudWatch Logs agents on the on-premises systems and send their logs to the same log stream. Configure Amazon CloudWatch Events to trigger an AWS Lambda function on a regular schedule. Have the Lambda function load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data from CloudWatch in all accounts into an Amazon Elasticsearch domain in the primary account. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false},{"id":"9dd2205bea19fc01a45c85453e73a8a0","text":"Deploy CloudWatch Logs agents on all EC2 instances and set up a single log group. Also install CloudWatch Logs agents on the on-premises systems and send their logs to the same log group. Configure Amazon CloudWatch Events to trigger an AWS Lambda function on a regular schedule. Have the Lambda function load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data from CloudWatch into an Amazon Elasticsearch domain in the primary account. Use CloudWatch Events to trigger Lambda functions in each of the other accounts to send CloudWatch data to Elasticsearch in the primary account. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":true},{"id":"b1ea8cd72d3e0e0233c2c1dd38b80375","text":"Configure Logstash on an EC2 instance in an Auto Scaling Group in the primary account. Implement CloudWatch Logs agents on all EC2 instances and set up a single log group. Also install CloudWatch Logs agents on the on-premises systems and send their logs to the same log group. Create an AWS System Manager Automation document in the primary account to load AWS CloudTrail, VPC Flow Log, and CloudWatch Log data in all accounts into Logstash. Create Kibana dashboards to visualize log data summaries and send alerts for identified issues.","correct":false}]},{"id":"1cce154e-46ef-4042-81de-54e423a7f043","domain":"MonitoringLogging","question":"Your chief security officer would like some assistance in producing a graph of failed logins to your Linux servers, located in your own data centers and EC2. The graph would then be used to trigger alert emails to investigate the failed attempts once it crosses a threshold. What would be your suggested method of producing this graph in the easiest way?","explanation":"The easiest way to produce these graphs would be to ensure you are streaming your logs to cloudwatch logs, create a metric filter and then graph the graph on your dashboard.","links":[{"url":"https://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/","title":"How to Monitor and Visualize Failed SSH Access Attempts to Amazon EC2 Linux Instances"}],"answers":[{"id":"a007e0393b7c2d1fc4b14392b98bab0b","text":"Install the CloudWatch Logs agent on all Linux servers, stream your logs to CloudWatch Logs and create a CloudWatch Logs insight query to create the graph.","correct":false},{"id":"fbb22151cccbae0d255548876414ad08","text":"Install the CloudWatch Logs agent on all of your servers, stream the logs to CloudWatch Logs. Create a Lambda function to parse the logs and detect failed logins, and push a custom metric data point to CloudWatch and then graph the metric on the dashboard.","correct":false},{"id":"76a62d761b77acd84a507a3000156072","text":"Install the CloudWatch Logs agent on all Linux servers, stream your logs to CloudWatch Logs and create a CloudWatch Logs Metric Filter. Create the graph on your Dashboard using the metric.","correct":true},{"id":"165852eeb9a4147840be82962d4a32c6","text":"Install the CloudWatch Logs agent on your Linux servers, stream the logs to CloudWatch Logs. Use cloudwatch events to trigger a lambda function to parse the log, and then push a custom metric data point to CloudTrail and then graph the metric on the dashboard.","correct":false}]},{"id":"7182b5ef-3d25-41f6-8251-a554cdef2719","domain":"IncidentEventResponse","question":"Your team has been planning to move functionality from an on-premises solution into AWS.  New micro-services will be defined using CloudFormation templates, but much of the legacy infrastructure is already defined in Puppet Manifests and they do not want this effort to go to waste.  They have decided to deploy the Puppet-based elements using AWS OpsWorks but have received the following errors during configuration; \"Not authorized to perform sts:AssumeRole\" and also \"The following resource(s) failed to create [EC2Instance]\".  The team have been unable to resolve these issues and have asked for your help.  Identify the reasons why these errors occur from the options below.","explanation":"There are two answers which would resolve the errors in the question.  Any time a \"not authorized\" message is displayed, it is nearly always a permissions problem and in this case it can be resolved by attaching the AWSOpsWorksCMServiceRole policy to the instance profile role for EC2. opsworks-cm.amazonaws.com should also be listed in the Trust Relationships. For the second question, this error normally dictates that the EC2 instance doesnt have sufficient network access, so we need to ensure that the instance has outbound Internet access, and that the VPC has a single subnet with DNS resolution and Auto-assign Public IP settings enabled. All other options will not resolve the errors.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/troubleshoot-opspup.html","title":"Troubleshooting OpsWorks for Puppet Enterprise"},{"url":"https://docs.aws.amazon.com/en_pv/opsworks/latest/userguide/welcome_opspup.html","title":"AWS OpsWorks for Puppet Enterprise"}],"answers":[{"id":"8bda36311bab2d5d74bccd6e3eac3e68","text":"You cannot use OpsWorks for managing Puppet based infrastructure.  OpsWorks only operates and integrates with Chef.","correct":false},{"id":"a4844f346bb4f746b9766b4608cd2b9a","text":"Ensure there are no errors within the Puppet Manifests.  Fix the syntax errors and restart the Puppet Master.","correct":false},{"id":"fd22d4d4b177eefd2129262da6a99e82","text":"Deploy in a VPC which is set to non-default tenancy with an instance type that supports dedicated tenancy.","correct":false},{"id":"8d14b74d28ed11d3f4a78d4937d00b00","text":"Ensure that the 'AWSOpsWorksCMServerRole' policy is attached to the instance profile role.","correct":true},{"id":"72c5c3db3a42b839c2ee0df93fadac14","text":"Ensure that the EC2 instance has the AWS service agent is running, has outbound Internet access and has DNS resolution enabled.","correct":true}]},{"id":"dc3d128f-8cce-4541-b6e6-be1ec6cea96d","domain":"SDLCAutomation","question":"Your CI/CD pipeline generally runs well, but your manager would like a report of some CodeBuild metrics, such as how many builds were attempted, how many builds were successful and how many builds failed in an AWS account over a period of time. How would you go about gathering the data for you manager?","explanation":"These are default CloudWatch metrics that come with CodeBuild.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html","title":"Monitoring Builds with CloudWatch Metrics"}],"answers":[{"id":"6149c7ca08a16f697f2988509bb61427","text":"Implement a lambda function to poll the CodeBuild API to gather the data and store it in CloudWatch Logs. Write a metric filter to graph the data and generate your report.","correct":false},{"id":"a1f83b40240e6af928c6cab6eeb010c1","text":"CloudWatch metrics will report these metrics by default. You can view them in the CloudWatch console.","correct":true},{"id":"c5045cc818946fcb9c3f6d71744202b7","text":"Configure CodeBuild to log builds to CloudWatch Logs, and then write a metric filter which will graph the data points your manager requires.","correct":false},{"id":"19e6e35ed627bf15d10ddc88af1ab9be","text":"Configure a CloudWatch custom metric to track the build information, and create custom graphs in the CloudWatch console.","correct":false}]},{"id":"e6b3fbb9-d87d-4dc7-b056-981294c46bdc","domain":"PoliciesStandards","question":"An error has occurred in one of the applications that your team looks after and you have traced it back to a DB connection issue. There have been no network outages and the database is up and running. A colleague tells you that all credentials are now stored in AWS Secrets Manager and suspects that the problem might be caused by a recent change in that area. Select all possible reasons for this.","explanation":"Although you typically only have one version of the secret active at a time, multiple versions can exist while you rotate a secret on the database or service. GetSecretValue has an optional VersionId parameter that specifies the unique identifier of the version of the secret that you want to retrieve. If you don't specify either a VersionStage or VersionId then the default is to perform the operation on the version with the VersionStage value of AWSCURRENT.","links":[{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/enable-rotation-other.html","title":"Enabling Rotation for a Secret for Another Database or Service"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_retrieve-secret.html","title":"Retrieving the Secret Value"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_overview.html","title":"Overview of Managing Access Permissions to Your Secrets Manager Secrets"}],"answers":[{"id":"4add2a29b600d7ccdc5e4633595bd593","text":"The DB credentials and connection details in Secrets Manager have been encrypted using the default Secrets Manager CMK for the account. The application and secret are in different AWS accounts though and no cross-account access has been granted.","correct":true},{"id":"67a9e9da426f34e831b5de27c46e3b56","text":"The AWS credentials that are used for the call to Secrets Manager in the client-side component embedded in the application to retrieve the database password don't have the secretsmanager:DescribeSecret permission on that secret.","correct":true},{"id":"ef3ec50fbea476c0443043b89119ef3b","text":"The GetSecretValue API call in the application didn't include the version of the secret to return.","correct":false},{"id":"8525c502b705804602b85cbe128c73a3","text":"When secret rotation is configured in Secrets Manager, it causes the secret to rotate once as soon as you store the secret. This can lead to a situation where the old credentials are not usable anymore after the initial rotation. It is possible that the team forgot to update the application to retrieve the secret from Secrets Manager.","correct":true}]},{"id":"b8ae52d2-9638-47a9-b2b0-69c471a35eab","domain":"MonitoringLogging","question":"Your organization runs a large amount of workloads in AWS and has automated many aspects of its operation, including logging. As the in-house devops engineer, you've received a ticket asking you to log every time an EC2 instance state changes. Normally you would use CloudWatch events for something like this, but CloudWatch logs aren't a valid target in CloudWatch Events. How will you solve this?","explanation":"CloudWatch Events can use a Lambda function as a target, which will solve this issue.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/LogEC2InstanceState.html","title":"Tutorial: Log the State of an Amazon EC2 Instance Using CloudWatch Events"}],"answers":[{"id":"1fc1f373fd2b33a9a55daeb93aa03e7b","text":"Parse the EC2 CloudWatch changelog with a Lambda function each minute, and log the results to a separate log for instance state changes","correct":false},{"id":"a4e104c9dd0f3e6a03765a0680648eed","text":"Use CloudWatch Events, but use a Lambda function target. Write a Lambda function which will perform the logging for you.","correct":true},{"id":"a68e57ea4a6bd4cfa8f565ef7cbc637e","text":"Create a Lambda function and run it on a schedule with CloudWatch Events. Make the Lambda function parse your CloudTrail logs for EC2 instance state changes and log them to another CloudWatch Logs log.","correct":false},{"id":"b482ab43e71b8137c1bb08d0904d0576","text":"Use a CloudWatch dashboard, which will log EC2 state changes to CloudWatch logs if you create a text widget.","correct":false}]},{"id":"3ab068b8-2849-4c61-8ffd-d4048963f8db","domain":"HAFTDR","question":"Due to the design of your application, your EC2 servers aren't treated as cattle as advised in the cloud world, but as pets. As such, you need DNS entries for each of them. Managing each DNS entry is taking a long time, especially when you have lots of servers, some of which may last a day, a week or a month. You don't want your Route53 records to be messy, and you would prefer some kind of automation to add and remove them. Which method would you choose to solve this in the best way?","explanation":"Tagging your instance with the required DNS record is a great way to help you automate the creation of Route53 records. A Lambda function can be triggered from a CloudWatch Events EC2 start/stop event and can add and remove the Route53 records on your behalf. This will meet your requirements and automate the creation and cleanup of DNS records.","links":[{"url":"https://aws.amazon.com/blogs/compute/building-a-dynamic-dns-for-route-53-using-cloudwatch-events-and-lambda/","title":"Building a Dynamic DNS for Route 53 using CloudWatch Events and Lambda"}],"answers":[{"id":"d6e2786b315c66a437523d7a8fb675c1","text":"Make your instance ID the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use CloudTrail API call logs to detect when an instance is started or stopped and trigger the Lambda function.","correct":false},{"id":"b1f933ed5c535439e1e097db065978eb","text":"Tag your instance with the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use CloudTrail API call logs to monitor when an instance is started or stopped and trigger the Lambda function.","correct":false},{"id":"338d60d516c4c48ee9e575f75fe01c98","text":"Tag your instance with the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use a CloudWatch Events rule to monitor when an instance is started or stopped and trigger the Lambda function.","correct":true},{"id":"29aa731344e5dd9aad6885f0b8c7274c","text":"Make your instance ID the DNS record required. Deploy a Lambda function which can add or remove DNS records in Route53 based on the DNS tag. Use a CloudWatch Events rule to detect when an instance is started or stopped and trigger the Lambda function.","correct":false}]},{"id":"3072e673-774e-4865-a7ed-85fafa597eeb","domain":"SDLCAutomation","question":"Your companies Security Officer just mandated a policy whereby all in use encryption keys within your organisation must be rotated every 120 days. How does that affect your CodePipelines Artifact store?","explanation":"An artifact store for your pipeline, such as an Amazon S3 bucket, is not the source bucket for your source code and is required for each pipeline. When you create or edit a pipeline, you must have an artifact bucket in the pipeline Region, and then you must have one artifact bucket per AWS Region where you are running an action. Unlike the console, running the create-pipeline command in the AWS CLI does not create an Amazon S3 bucket for storing artifacts. The bucket must already exist. When you enable automatic key rotation for a customer managed CMK, AWS KMS generates new cryptographic material for the CMK every year.","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create.html","title":"Create a Pipeline in CodePipeline"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/S3-artifact-encryption.html","title":"Configure Server-Side Encryption for Artifacts Stored in Amazon S3 for CodePipeline"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/S3-view-default-keys.html","title":"View Your Default Amazon S3 SSE-KMS Encryption Keys"},{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html","title":"Rotating Customer Master Keys"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/S3-rotate-customer-key.html","title":"Configure Server-Side Encryption for S3 Buckets When Using AWS CloudFormation or the CLI"}],"answers":[{"id":"30285e906f24c643049a339b0551b048","text":"You can use any already existing artifact store as long as it is in the same Region as your pipeline. When you create a pipeline using AWS CloudFormation or the CLI, you must configure server-side encryption for your artifact store manually. Use an appropriate bucket policy and then create your own customer-managed SSE-KMS encryption keys. Instead of using the default Amazon S3 key, choose to use your own keys so that you can rotate these every 120 days as per your organisations security requirements.","correct":true},{"id":"75c9d50127886728d1cfde275bdb7915","text":"You must have a separate artifact store for each CodePipeline. When using the AWS CLI, these are automatically created during the creation of your pipelines. CodePipeline also configures default AWS-managed SSE-KMS encryption keys for your artifact store. If you enable automatic key rotation and specify a refresh rate of 120 days, AWS KMS generates new cryptographic material and saves the key's older cryptographic material so that it can be used to decrypt data that it encrypted.","correct":false},{"id":"7cb596ab386cdf5e14c65e9384a96895","text":"Your artifact store can either be a CodeCommit, GitHub or an Amazon ECR repository. It can also be an S3 source bucket where your source code is stored. Every 120 days, you need to generate new access keys and change the connection details to your repository if you are not using S3 as your artifact store. No further action is required if you use S3.","correct":false},{"id":"83305810e49f3d0e2dc10c7f564a2616","text":"CodePipeline Artifact stores are regional. When you use the CodePipeline console in a specific region to create a pipeline and choose 'Default location' when asked for an Artifact store, a new default one will be created for you in that region if none existed beforehand. CodePipeline creates default AWS-managed SSE-KMS encryption keys when you create a pipeline using the Create Pipeline wizard. The master key is encrypted along with object data and managed by AWS. However, you can also create and manage your own customer-managed SSE-KMS keys in AWS KMS to encrypt or decrypt artifacts in your artifact store and rotate these keys as necessary.","correct":true},{"id":"03be9a17c0b30ac5a6639484c188ed1c","text":"AWS recommends to use the same default global artifact store for all your CodePipelines. This is created for you when you create your first pipeline in any region. By default, CodePipeline uses server-side encryption with the AWS KMS-managed keys (SSE-KMS) using the default key for Amazon S3 (the aws/s3 key). This key is created and stored in your AWS account. When artifacts are retrieved from the artifact store, CodePipeline uses the same SSE-KMS process to decrypt the artifact. Change that key every 120 days.","correct":false}]},{"id":"9e3970ae-d3c6-4365-bd1d-ece6971ce4a0","domain":"ConfigMgmtandInfraCode","question":"Which of the following AWS services allow native encryption of data, while at rest?","explanation":"EBS, S3 and EFS all allow the user to configure encryption at rest using either the AWS Key Management Service (KMS) or, in some cases, using customer provided keys.  The exception on the list is ElastiCache for Memcached which does not offer a native encryption service, although ElastiCache for Redis does.","links":[{"url":"https://aws.amazon.com/ebs/faqs/","title":"Amazon EBS FAQs"},{"url":"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html","title":"Comparing Memcached and Redis"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html","title":"Protecting Data Using Server-Side Encryption"},{"url":"https://aws.amazon.com/efs/faq/","title":"Amazon EFS FAQs"}],"answers":[{"id":"1e26da0c1f6b77f6ef2ad9a3b8cf5a98","text":"ElastiCache for Memcached","correct":false},{"id":"4b222d59a012c5cc128037a5a473cde7","text":"Elastic Block Store (EBS)","correct":true},{"id":"e9880f1e01aac341f6553ec8a6a7e622","text":"Elastic File System (EFS)","correct":true},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true}]},{"id":"dae6c09f-c2e7-4b01-b0ec-95ed07a7bab2","domain":"HAFTDR","question":"Due to some recent performance issues, you have been asked to move your existing Product Information System to Amazon Aurora.  The database uses the InnoDB storage engine, with new products being added regularly throughout the day. As the database is read heavy, the decision has also been made to add a Read Replica during the migration process.  The changeover completes successfully, but after a few hours you notice that lag starts to appear between the Read/Write Master and the Read Replica.  What actions could you carry out to reduce this lag?","explanation":"One of the most obvious causes of Replication lag between two Aurora databases is because of settings and values, so making the storage size comparable between the source DB and Read Replica is a good start to resolving the issue, as is ensuring compatible DB Parameter settings, such as with the max_allowed_packet parameter.  Turning off the Query Cache is good for tables that are modified often which causes lag, because the cache is locked and refreshed often. No other options are correct.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html","title":"Troubleshooting for Aurora"}],"answers":[{"id":"82a38022b7a8f6e84ec39753340df001","text":"Set query_cache_type=0 in the DB Parameter Group, to disable the query cache.","correct":true},{"id":"05acabf0a4eea6213f78770124a81bcd","text":"Ensure the max_allowed_packet parameter value for the Read Replica is the same as the source DB instance.","correct":true},{"id":"09fee716d530e8dbb655cb9541bb4d5c","text":"Unlike Amazon RDS for MySQL, Amazon Aurora does not exhibit replication lag.","correct":false},{"id":"94692ead9b9079bf92302461566b94a5","text":"Add additional replicas and the alter the code to initiate Read/Write splitting of the Database.","correct":false},{"id":"5b3b3ef613f42a00bbda6ed80b4f3683","text":"Change the Read Replica instance class to have the same storage size as the source instance.","correct":true}]},{"id":"ffda164a-53b8-4683-82d5-422d4a4910b0","domain":"ConfigMgmtandInfraCode","question":"Your Manager has informed you that all 300 of the EC2 Linux instances in the company VPC will be audited next week and in preparation, a number of shell commands must be run on each of the servers, with results posted into a secure S3 bucket.  This task has to be completed by tomorrow.  What are the quickest ways to deploy and execute the commands and to retrieve the data?","explanation":"Systems Manager gives the ability to run a list of shell commands or a script, by choosing from a list of servers or by tags.  To do this you can run the Command Document called 'AWS-RunRemoteScript' to run a script located in an S3 bucket or GitHub repository or the Command Document 'AWS-RunShellScript' which will allow individual shell commands to be run on a set of servers.  Either of these options would be the quickest and most effective way of running the audit script on the 300 servers in the shortest time.","links":[{"url":"https://aws.amazon.com/systems-manager/faq/","title":"AWS Systems Manager FAQs"}],"answers":[{"id":"9c4a809d938990c6c1f061c48223d0df","text":"In AWS Systems Manager, run the Automation Document called 'AWSSupport-TroubleshootSSH',  supply the shell commands in a list and enable 'Write results to Cloudwatch'","correct":false},{"id":"977b9ab61a47ae98fc0fd7f109d46b26","text":"In AWS Systems Manager, run the Automation Document called 'AWSSupport-DeployScript', add the shell command to a script and store this in S3 and enable 'Write results to Cloudwatch'","correct":false},{"id":"341a4cd5e00a3dfea4b7da604cc91b3f","text":"In AWS Systems Manager, run the Command Document called 'AWS-RunShellScript', supply the shell commands in a list and enable 'Write command output to an Amazon S3 bucket'","correct":true},{"id":"8a3910c5b9b55101e20ab80338a96fd8","text":"In AWS Systems Manager, run the Command Document called 'AWS-RunRemoteScript', add the shell command to a script and store this in S3. Enable 'Write command output to an Amazon S3 bucket'","correct":true},{"id":"1a4604679274c8f0b28e31aa3958d321","text":"Run the script locally using the command 'ssh -i server.pem ec2-user@remoteserver-n \"bash -s\" < /scripts/audit.sh' replacing remoteserver-n with each IP of the 300 servers which will run the local script on the remote servers.","correct":false}]},{"id":"98efc6ca-1ded-4c51-94dd-7aa4ff3e835e","domain":"SDLCAutomation","question":"Your manager has asked you to investigate different deployment types for your application. You currently use Elastic Beanstalk so you are restricted to what that service offers. Your manager thinks it would be best to only deploy to a few machines at a time, so if there are any failures only the few that have been updated will be affected, making a rollback easier. To start with, your manager would like the roll-outs to occur on 2 machines at a time but would like the option to increase that to 3 at a time in the future. Deployments are done during quiet periods for your application, so reduced capacity is not an issue.  Which deployment type do you implement, and how can you get the most reliable indication that deployments have not introduced errors to the servers?","explanation":"The rolling deployment policy is suitable. Additional batch is not specified as a requirement.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html","title":"Elastic Beanstalk Deployment Policies and Settings"}],"answers":[{"id":"7257c3141e580ca3d648f8ccd2e3e77e","text":"Implement Elastic Beanstalk's Two-at-once deployment policy. Within the Two-at-once update policy set update based on Health with batch size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":false},{"id":"afd11d75ca714f8fd9bc5435eb0279b3","text":"Implement Elastic Beanstalk's Immutable deployment policy. Within the update policy set batch size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":false},{"id":"3ab638a2611327523fa489ffd5358081","text":"Implement Elastic Beanstalk's Rolling deployment policy. Within the Rolling update policy set Rolling based on Health with batch size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":true},{"id":"88b8bcbc9a5f8d43bd4e059cf5b1ee0f","text":"Implement Elastic Beanstalk's Rolling with additional batch deployment policy. Within the policy, set update based on Health with size of two.  Use Elastic Beanstalk's Enhanced Health Reporting to analyse web logs and Operating System metrics within the target servers as this will give the best guarantee that a deployment was successful.","correct":false}]},{"id":"df9c422d-ca3c-47ad-a584-4d21f9ca0e05","domain":"ConfigMgmtandInfraCode","question":"Your company has been re-architecting its core applications as microservice based APIs.  Each API is a Docker image which is to be deployed into Amazon ECS.  The DevOps team have confirmed that the first Docker images are running locally, but they are having multiple issues when trying to ship into ECS Fargate.  They are receiving two errors, the first is; \"An error occurred when calling the RegisterTaskDefinition operation - Invalid 'cpu' setting for task\" and the second is; \"CannotPullContainerError - API error (404) - repository not found\".  You have been asked by the Head of Operations to resolve these issues.  Which of these options could be the cause of the errors?","explanation":"Whenever an 'invalid cpu setting' or 'invalid memory setting' error appears during Fargate configuration, you must refer to the table in the documentation to ensure that the values chosen in the Task Definition match the supported values. If you receive a 'repository not found' error, it points to an incorrect specification of the ECR Image location in the container definition, and you should specify the valid ARN or URI with the image name.  ECR images do not have to be registered in Docker Hub. All other answers mentioned will not resolve the issues in the question.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html","title":"Cannot Pull Container Image Error"},{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html","title":"Invalid CPU or Memory Value Specified"}],"answers":[{"id":"9af4c33241090a770a4aa778d094c53c","text":"Ensure that the ECS Cluster compatibility is set to 'Networking only' as you are utilising ECS Fargate.","correct":false},{"id":"78021d89732fa38a0c0b1f9ae4b65e80","text":"Ensure that the Task Definition has CPU and Memory hard limits set only.  They will not operate correctly with soft limits.","correct":false},{"id":"cb4b4ba444e685699e32cbb57cfafaff","text":"Ensure that your new image is uploaded into the Docker Hub container registry as all new images running on the Amazon ECS platform must be registered here first.","correct":false},{"id":"9adff74ec4c8c8a05fed10071e151688","text":"Ensure that the image is uploaded into the Elastic Container Registry (ECR) and that the full path of the repository and the image name is in the container definition.","correct":true},{"id":"7806d100b31ffa99c98719ad4c2808be","text":"Ensure that the Task Definition has a supported value of CPU specified.  These are expressed as either CPU units or vCPUs.","correct":true}]},{"id":"279ef48b-4c8c-4cf0-a14a-8144644ac2f1","domain":"MonitoringLogging","question":"You work for a company that uses a serverless architecture to process up to 4,000 events per second for its usage analytics service. At its core, it consists of a multitude of Lambdas that have been given various resource configurations, i.e. memory/CPU settings. For each function invocation, you want to monitor that allocation against the actual memory usage. Select the simplest feasible approach to achieve that.","explanation":"AWS Lambda doesnt provide a built-in metric for memory usage but you can set up a CloudWatch metric filter. The AWS Lambda console provides monitoring graphs for Invocations, Duration, Error count and success rate (%), Throttles, IteratorAge and DeadLetterErrors.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html","title":"AWS Lambda Function Configuration"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html","title":"Monitoring and troubleshooting Lambda applications using Amazon CloudWatch"},{"url":"https://forums.aws.amazon.com/thread.jspa?threadID=226674","title":"Profiling memory usage for Lambda functions"},{"url":"https://gist.github.com/cgoering/4de674e3e3ca8d6255ea708997cca3b0","title":"AWS Lambda memory usage metric in CloudFormation"}],"answers":[{"id":"55b98a256995ac4e4c6f7afc1e1a5845","text":"AWS Lambda provides a built-in CloudWatch metric for memory usage.","correct":false},{"id":"e2113fd25833a45a179807bc633f635a","text":"Log entries written into the log group associated with a Lambda function don't include profiling info such as memory usage. You need to use AWS X-Ray for that.","correct":false},{"id":"2f5f4e15a960c888b86e315b993df292","text":"You can set up a custom CloudWatch metric filter using a pattern that includes 'REPORT', 'MAX', 'Memory' and 'Used:'.","correct":true},{"id":"65883e3f656c8f5b6f91c895b44649ff","text":"Use the 'Monitoring' tab in the AWS Lambda console and add the 'Resource' monitoring graph to your dashboard.","correct":false}]},{"id":"9a41a543-3d2a-4090-b97a-16d86d7a7ab0","domain":"SDLCAutomation","question":"CodeBuild has been configured as the Action provider for your Integration Test Action which has been added to your CodePipelines' 'Test' stage. These tests connect to your RDS test database that is isolated on a private subnet and because executing that stage alone takes nearly 2 hours, you want to run it during the night before developers come into the London office in the morning. How might you go about this?","explanation":"Enabling Amazon VPC access in your CodeBuild project requires the ID of the VPC, subnets, and security groups. You cannot use the internet gateway instead of a NAT gateway or a NAT instance because CodeBuild does not support assigning elastic IP addresses to the network interfaces that it creates, and auto-assigning a public IP address is not supported by Amazon EC2 for any network interfaces created outside of Amazon EC2 instance launches. The ordered cron expression fields are: minutes, hours, day of month, month, day of week, year.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/03/aws-codepipeline-adds-support-for-unit-testing/","title":"AWS CodePipeline Adds Support for Unit and Custom Integration Testing with AWS CodeBuild"},{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html","title":"Use CodePipeline with CodeBuild to Test Code and Run Builds"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-schedule-cli.html","title":"Create a CloudWatch Events Rule That Schedules Your Pipeline to Start (CLI)"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html","title":"Schedule Expressions for Rules"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/codebuild/start-build.html","title":"AWS CLI Command Reference: start-build"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_StartPipelineExecution.html","title":"AWS CodePipeline: API Reference: Actions: StartPipelineExecution"},{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/vpc-support.html","title":"Use CodeBuild with Amazon Virtual Private Cloud"}],"answers":[{"id":"7a12726d6a6e0d62c071bfa26f1daf46","text":"By default, CodeBuild projects can access VPCs in the same account. Use the AWS CLI CodeBuild command 'start-build' with the --repeat=true, --hours=3, --minutes=10 and --frequency=weekdays option.","correct":false},{"id":"e49c56dbee3e27e947c2cda2784975fb","text":"CodeBuild supports assigning elastic IP addresses to the network interfaces that it creates. Therefore, you need to configure your build project with the allocated EIP and VPC details of your RDS instance. Call the AWS CodePipeline 'StartPipelineExecution' API with {'stage': 'Test', 'schedule': '5 23 ? * SUN-THU *' }.","correct":false},{"id":"f10014d4b7fb41a95a0f4102b36a8460","text":"Amazon VPC access in your CodeBuild project needs to be enabled. Specify your VPC ID, subnets, and security groups in your build project and set up a rule in Amazon CloudWatch Events to start the pipeline on a schedule. Use the following command for that: aws events put-rule --schedule-expression 'cron(10 3 ? * MON-FRI *)' --name IntegrationTests","correct":true},{"id":"680092a0588f13a4230a986961172023","text":"To allow CodeBuild access to your database, you have to specify the VPC ID and the ID of the subnet of your private RDS instance. Then create a CloudWatch events rule to schedule a CodeBuild project build using the following cron expression: 3 10 * * ? *","correct":false}]},{"id":"8cd81d91-8288-4830-ae8b-0aaa2c7f706c","domain":"SDLCAutomation","question":"You want to use Jenkins as the build provider in your CI/CD Pipeline. Is this possible, and if so how would you implement it?","explanation":"You can select Jenkins an action provider when creating a build stage in CodePipeline. You cannot select it as a source provider within CodeBuild.","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html","title":"Use CodePipeline with Jenkins"}],"answers":[{"id":"80ab8f614bf8d08d188690809f0dc8fa","text":"Yes it's possible. CodePipeline will let you select Jenkins as a destination build provider when you are creating your pipeline.","correct":false},{"id":"2c74a9018e02367af56b8cd6036ae2a9","text":"No it's not possible.","correct":false},{"id":"c5a84ac110e8df3c1da4255e362f2066","text":"Yes it's possible. CodeBuild will let you select Jenkins as a source provider when you are creating your build project","correct":false},{"id":"d27a4cc08449c797a8fd8b6c6b9d505e","text":"Yes it's possible. You can use a CodePipeline plugin for Jenkins and can configure a build stage which connects to your Jenkins instance.","correct":true}]},{"id":"8e6340c5-f419-4d6c-af85-cbbe05be55a1","domain":"PoliciesStandards","question":"You want to utilise SAML and ADFS to allow staff to authenticate with the AWS console using your company Active Directory credentials.  Details of how to do this have been posted on some technical Blogs, but the advice in the articles appear to contradict each other.  From the options, choose the correct process to enable federated sign-in to the AWS console with AD.","explanation":"Only one solution will actually correctly work for SAML and ADFS based federation and that is the option utilising Identity Providers, Roles and configuring of ADFS Relay Party and Custom Claim Rules.  We can discount utilising Amazon Single Sign-On service as it's not SAML based.  The other two options are flawed in their design and will not work.","links":null,"answers":[{"id":"5883b5069bb8b53f7891891f202ebc59","text":"Create and configure an IAM SAML Identity Provider, create a role with a SAML Trusted Entity, Configure AD, Configure ADFS with Relay Party, Create Custom Claim Rules","correct":true},{"id":"6640fd057d573df93b0a72926390951d","text":"Create an AWS account for each user, ensure that each AWS account has an AD account with the same name, put all users into a SAML group, Configure AD and ADFS","correct":false},{"id":"9d24399e083ec0a93ea5b882150363e2","text":"Install a SAML and ADFS replication application running on an EC2 instance, configure AD to synchronize with the replication application","correct":false},{"id":"429b105c841989e4c36ecca2cf06149c","text":"Configure the AWS Single Sign-On service and link it to an on-premise Domain Controller","correct":false}]},{"id":"e7f37856-b31b-4997-872b-99ede9470dda","domain":"SDLCAutomation","question":"Your organization is currently using CodeDeploy to deploy your application to 20 EC2 servers which sit behind a load balancer. It's making use of the CodeDeployDefault.OneAtATime deployment configuration. Your manager has decided to speed up deployment by deploying to as many servers as possible at once, as long as at least five of them remain in service at any one time. How do you achieve this, ensuring that it will scale?","explanation":"Setting the minimum healthy host as a Number and 5 will work as desired. A percentage set to 25% will work for the current 20 servers, but it will not scale down if any servers are removed.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html","title":"Working with Deployment Configurations in CodeDeploy"}],"answers":[{"id":"09c0a847baf9482b080d3e333ecca413","text":"Create a custom deployment configuration, specifying the maximum healthy host as a \"Number\" and set it to 5","correct":false},{"id":"8b33708bd673179fc3d5e2da7d7eed70","text":"Create a custom deployment configuration, specifying the maximum healthy host as a \"Percentage\" and set it to 25%","correct":false},{"id":"c58aab48dd8ec648df3d39b4264533d8","text":"Create a custom deployment configuration, specifying the minimum healthy host as a \"Percentage\" and set it to 25%","correct":false},{"id":"9cad758b5b1009c5dcdc6df71e58852b","text":"Create a custom deployment configuration, specifying the minimum healthy host as a \"Number\" and set it to 5","correct":true}]},{"id":"430df32b-2784-4be3-83cc-ce359d07d95a","domain":"PoliciesStandards","question":"You work for a medical imaging company, dealing with X-rays, MRI's, CT scans and so on. The images and other related patient reports and documents are stored in various S3 buckets in the US West region. Your organization is very security conscious and wants to ensure that while the S3 buckets are locked down, there's no other way that the documents are being shared internally or externally other than the approved methods already in place. Audits are also important, so whatever methods of data protection are in place must work together with this, as well as providing actionable alerts if there any observed issues. How do you best achieve this? Which AWS services can help?","explanation":"Amazon Macie is a security service that uses machine learning to discover personally identifiable information in your S3 buckets. It also provides you with dashboards and alerts that show how your private data is being accessed.","links":[{"url":"https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html","title":"What Is Amazon Macie?"}],"answers":[{"id":"2e08a3430fb173fad3d2a91519ecc6e5","text":"Don't store sensitive data in S3, the public cloud is not secure enough. Look into moving storage in-house.","correct":false},{"id":"4535ec14d07cb09bc414719290a94777","text":"Write a lambda function which is triggered when new data is uploaded into your S3 buckets to apply an S3 policy to ensure the data is secure.","correct":false},{"id":"2cd9c2c19e59cae06d93cba7c9861016","text":"Write a lambda function to monitor CloudTrail API calls to S3 and trigger an SNS notification if anything out of the ordinary is detected.","correct":false},{"id":"4a68228f60a7ed4f82aa09dbc1d18425","text":"Implement Amazon Macie across your S3 buckets.","correct":true}]},{"id":"bbe1fc54-7ed1-4156-930f-446e13ca5e59","domain":"ConfigMgmtandInfraCode","question":"One of your colleagues has developed a CloudFormation template which creates a VPC and a subnet for each Availability Zone within each Region the Stack is created in.  Although this template is currently functional, it requires the Region and Availability Zones to be passed into the Stack as parameters.  You have been asked by your manager to alter the template so that you can automate all of this functionality using Intrinsic Functions, and eliminate passing in parameters.  Choose options from the list which will meet the requirements of rewriting the template.","explanation":"Intrinsic functions can not be used within the Parameters section and so these options can be immediately ruled out. Using both Fn::GetAZs to return the Availability Zones and Fn:Select to choose from the list and also using Fn::FindInMap to pull back each Region from the Mappings section are both valid options to retrieve the Regions.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"Template Anatomy"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html","title":"Intrinsic Function Reference"}],"answers":[{"id":"c697110279709e26aa4a1219dcf5f6e2","text":"In the Resources section, use !Ref \"AWS::Region\" to return the Region, Fn::GetAZs to return a list of all Availability Zones and Fn:Select to choose each AZ from the list.","correct":true},{"id":"9fe027bb7b742b6639a50bc38d88cd6f","text":"In the Mappings section, make a list of each Availability Zone in each Region.  Then in the Resources section use Fn::FindInMap to pull back each AZ using !Ref \"AWS::Region\" as the key.","correct":true},{"id":"0f58a828daf6079e3560070d348c8d13","text":"In the Parameters section, create three individual parameters, one for each Availability Zone and use !Ref to retrieve the values in the Resources section.","correct":false},{"id":"3eca8a7377d618c7e76e66d97dddfd42","text":"In the Parameters section, use !Ref \"AWS::Region\" to return the Region for each parameter, Fn::GetAZs to return a list of all Availability Zones and Fn:Select to choose each AZ from the list.","correct":false}]},{"id":"b2d2d9d6-3ab6-4949-85a3-de49e4783ab9","domain":"ConfigMgmtandInfraCode","question":"In the past, your organisation has had security breaches due to unauthorised changes in infrastructure.  To reduce these, the InfoSec Team have implemented AWS Config so that all changes are recorded.  Any changes to the infrastructure are then monitored by the Service Desk.  However, the following errors; \"We are unable to complete the request at this time. Try again later or contact AWS Support\" are appearing in the console.  The Service Desk Manager has asked you for more information on why these errors are happening.  Choose the correct reasons from below.","explanation":"There are only two issues that would cause this error to appear in AWS Config.  The first is if you make multiple calls to the AWS Config API within a minute and the default rate limiting will then implement a temporary block.  The CloudTrail logs will show the following; \"You have exceeded the maximum request rate. Try again at a later time.\" and you should stop calling the API as frequently.  The second option will be if you exceed the set Aggregator limit (which is 50 by default).  CloudTrail will show; \"The configuration aggregator could not be created because the account already contains '50' configuration aggregators. Consider deleting configuration aggregators or contact AWS Config to increase the limit.\"  No other options listed accurately describe the message.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/config-console-error/","title":"How can I troubleshoot AWS Config console error messages?"},{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"}],"answers":[{"id":"72a656f0e09c8f5382bf1978c403720f","text":"This error is related to the Aggregator limit being reached, and you should contact AWS Support to increase the Configuration Aggregator limit from the default value.","correct":true},{"id":"c2b54e290aa3e1f5ba5b69b19e872a6a","text":"This error is related to the ConformancePackStatusReason, and you should check it to know more about the reason for failure.","correct":false},{"id":"bb66cca2e29dc015297e99a6c201a347","text":"This error is related to switching to a different region when Remediation is in Progress.","correct":false},{"id":"8d84028643f27b004e826c24087d49db","text":"This error is related to exceeding the rate limiting if you use the API call GetResourceConfigHistory or ListDiscoveredResources with a Lambda function.","correct":false},{"id":"92344dfafeacc6f8c91cc9d5159ce1ff","text":"This error is related to calling the StartConfigRulesEvaluation API more than once per minute.","correct":true}]},{"id":"f6b4a9c9-32fd-433d-93fd-4b61bad8db8a","domain":"PoliciesStandards","question":"You are employee #2 in a new startup, right after the founders and their first developer. You've been given the task of checking over the AWS account that has been used so far and determining what service limits you are currently breaching (if any) and producing a report on the current service limit levels. How do you obtain this information?","explanation":"Trusted advisor makes Service Limit checks free for all users at any support level.","links":[{"url":"https://aws.amazon.com/blogs/mt/monitoring-service-limits-with-trusted-advisor-and-amazon-cloudwatch/","title":"Monitoring Service Limits"}],"answers":[{"id":"1812a1f14ddc45354e11146b11914c56","text":"Sign up for a business support plan to access the Service Limit Checks in Trusted Advisor","correct":false},{"id":"de4b26655e419de8a97e82d3598b1eda","text":"Enable Trusted Advisor and look at the Service Limit page, which is free for all users.","correct":true},{"id":"6ca7f9bfe1536150652a1e4ecaf6789c","text":"Enable detailed billing and look at the Service Limit reporting page, which is free for all users.","correct":false},{"id":"0ee62b6775e4ea63a1288cd941f273a5","text":"Sign up for a business support plan to access the detailed billing reports in your billings console.","correct":false}]},{"id":"d68edb14-2da6-4852-8ca7-ac8f6f9b78fd","domain":"IncidentEventResponse","question":"You have decided to install the AWS Systems Manager agent on both your on-premises servers and your EC2 servers. This means you will be able to conveniently centralize your auditing, access control and provide a consistent and secure way to remotely manage your hybrid workloads. This also results in all your servers appearing in your EC2 console, not just the servers hosted on EC2. How are you able to tell them apart in the console?","explanation":"Hybrid instances with the Systems Manager agent installed and registered to your AWS account will appear with the 'mi-' prefix in EC2.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html","title":"Setting Up AWS Systems Manager for Hybrid Environments"}],"answers":[{"id":"d48ada0514bb41e403b1c5400cbf75a7","text":"The ID of the hybrid instances are prefixed with 'mi-'. The ID of the EC2 instances are prefixed with 'i-'.","correct":true},{"id":"a418f11f7d1488b0abb356ca2f8e070e","text":"The ID of the hybrid instances are prefixed with 'h-'. The ID of the EC2 instances are prefixed with 'i-'.","correct":false},{"id":"86986700437afbb27d1a3deb0c996665","text":"The ID of the hybrid instances are prefixed with 'm-'. The ID of the EC2 instances are prefixed with 'i-'.","correct":false},{"id":"62fb5905dab4e1724d15d5ed81dee2d6","text":"The ID of the hybrid instances are prefixed with 'v-'. The ID of the EC2 instances are prefixed with 'i-'.","correct":false}]},{"id":"97b89b27-99f8-4cb9-88f3-9002eacb7713","domain":"MonitoringLogging","question":"You are part of a team working on an house valuation service that aggregates data via APIs from different external providers. Your Solution Architect specified that AWS Secrets Manager is to be used for the storage of all third-party API keys, client IDs and other secrets. One particular API requires two customer specific values - each of these is 1024 bytes long. Which of the following answer is correct?","explanation":"Secrets can be database credentials, passwords, third-party API keys, and even arbitrary text. The maximum length of a secret 7168 bytes.","links":[{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html","title":"What Is AWS Secrets Manager?"},{"url":"https://docs.aws.amazon.com/secretsmanager/latest/userguide/reference_limits.html","title":"Limits of AWS Secrets Manager"}],"answers":[{"id":"45f37012d1d473b2ab3bc21659f75224","text":"You can store up to 7168 bytes in each secret.","correct":true},{"id":"147c08f6dcfd373071697491fcf3b414","text":"AWS Secrets Manager supports the management of database credentials (username and password) only. Max. length is 512 characters for each field.","correct":false},{"id":"42946ed8adcfa463fd89ecdf2afed049","text":"You will need to store the two values separately as two secrets and combine them programmatically.","correct":false},{"id":"8c641d9b7e20b71b0bc352b7ed6f68dc","text":"Secrets must not exceed 256 characters in length.","correct":false}]},{"id":"26b5e2d7-e00f-4d7b-b086-c09e029d1de9","domain":"HAFTDR","question":"The world wide cat news powerhouse, Meow Jones, has hired you as a DevOps Database consultant. They're currently using legacy in-house PostgreSQL databases which cost a considerable amount to maintain the server fleet, as well as operational costs for staff, and further hardware costs for scaling as the industry grows. You are tasked in finding an AWS solution which will meet their requirements. They require high throughput, push button scaling, storage auto-scaling and low latency read replicas. Any kind of automatic monitoring and repair of databases instances will also be appreciated. Which AWS service(s) do you suggest?","explanation":"Amazon Aurora will fit the needs perfectly, and the Database Migration Service can assist with the migration.","links":[{"url":"https://aws.amazon.com/rds/aurora/details/postgresql-details/","title":"Amazon Aurora Features: PostgreSQL-Compatible Edition"}],"answers":[{"id":"0a663a4b84931c9af9abadcccde84f3f","text":"Keep the current PostgreSQL databases and implement an ElastiCache to cache common queries and reduce load on your in-house databases to save on upgrade costs.","correct":false},{"id":"6129915cbe4cbc9e1ddb0b74808beded","text":"Amazon Aurora, AWS Database Migration Service","correct":true},{"id":"9802017b44b6d60fc846a356c1fa9e9a","text":"A cluster of Amazon RDS PostgreSQL instances, AWS Database Migration Service.","correct":false},{"id":"e96297fedff7f3062d962bbd0a387cb4","text":"An auto-scaled, load balanced EC2 fleet running PostgreSQL with data shared via EFS volumes.","correct":false}]},{"id":"b299b67c-9e64-46b9-8397-b3d92e5a8e9a","domain":"SDLCAutomation","question":"You have multiple teams of developers and at the moment they all have the ability to start and stop any EC2 instance that they can see in the EC2 console, which is all of them. You would really like to implement some security measures so they can only start and stop the instances based on their cost center. What AWS features would you use to achieve this?","explanation":"You can simplify user permissions to resources by using tags and policies attached to roles. the aws:PrincipalTag is a tag that exists on the user or role making the call, and an iam:ResourceTag is a tag that exists on an IAM resource. In this case we want the CostCenter tag on the resource to match the ConstCenter tag assigned to the developer.","links":[{"url":"https://aws.amazon.com/blogs/security/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles/","title":"Simplify granting access to your AWS resources by using tags"}],"answers":[{"id":"1597cc0291688fb61d2e070e6fefc038","text":"Implement tags and restrict access by comparing the aws:PrincipalTag and the iam:ResourceTag in a policy attached to your developer role and seeing if they match.","correct":true},{"id":"d106bcfe8a672539187a3453ec7498d0","text":"Implement roles which you can assign to each resource which will allow a developer to start or stop the instance if they are also assigned to it.","correct":false},{"id":"3c58e079022c3f48db6068b4e061539c","text":"Implement tags and restrict access by comparing the iam:PrincipalTag and the aws:ResourceTag in a policy attached to your developer role and seeing if they match.","correct":false},{"id":"3e3f004e8462587e3c0ee0b9692cde3b","text":"Implement EC2 policies which you can assign to each resource which will allow a developer to start or stop the instance if they are also assigned to it.","correct":false}]},{"id":"5e50532a-5d17-4a6f-a367-db3eb2c87698","domain":"SDLCAutomation","question":"Your organization has been using CodePipeline to deploy software for a few months now and it has been smoothly for the majority of releases, but when something breaks during the build process it requires lots of man hours to determine what the problem is, roll back the deployment and fix it. This is frustrating both management and your customers. Your supervisor would like to assign one developer to test the build works successfully before your CodePipeline proceeds to the deploy stage so you don't encounter this issue again. How would you implement this?","explanation":"CodePipeline allows for manual approval steps to be implemented for exactly this reason","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html","title":"Add a Manual Approval Action to a Pipeline in CodePipeline"}],"answers":[{"id":"ca05b5864508a85819911de3b31a9c02","text":"Ask the assigned developer to run a local build first to test all changes, and then commit it to the repository which is then deployed to production only when they know there are no errors.","correct":false},{"id":"adac0bfeeb378f849ee1f8575b660f83","text":"Create a test deploy stage as well as a manual approval stage in CodePipeline. Once the assigned developer checks the testing deploy worked, they can authorize the pipeline to continue and deploy to production.","correct":true},{"id":"61692576c930a7b4f4bb412881fa1829","text":"Configure SQS to email the assigned developer when CodePipeline has deployed to production. This will provide an immediate notification so they can check if there are any errors in testing. Then they can push the changes to the production git branch.","correct":false},{"id":"fb383cd098c54e0f94598a5029e3c99e","text":"Configure SES to email the assigned developer when CodePipeline has deployed to production. This will provide an immediate notification so they can check if there are any errors in testing. Then they can push the changes to the production git branch.","correct":false}]},{"id":"1793c7ec-eff4-4f0d-8252-d1a464d5cd02","domain":"ConfigMgmtandInfraCode","question":"A financial services company runs their application portfolio on EC2 Linux instances with Auto Scaling to provide elastic capacity. They need to increase compute resources based on demand during month-end processing, and decrease them after all reporting has completed to avoid unnecessary costs. Each instance must have a secondary network interface in an isolated subnet for administration tasks in order to meet compliance requirements. How will the company ensure that new instances provisioned by Auto Scaling meet the compliance requirement?","explanation":"Auto Scaling supports adding hooks to the launching and terminating stages of the instance lifecycle. These hooks can send an SNS notification and hold the instance in a pending state waiting for a callback to the API. You can trigger a Lambda function from the SNS topic to create an ENI and attach it to the instance. The lifecycle hook will abandon the instance after a timeout period or if the Lambda function fails. Auto Scaling does not allow for specifying a secondary ENI in the launch configuration. The user data shell script option may work, but there will only be a set number of ENIs available in the pool, whereas the lifecycle hook option creates ENIs as needed. The shell script could possibly create the ENIs on demand. aws:createENI is not a valid AWS Systems Manager automation document action.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html","title":"What Is Amazon EC2 Auto Scaling?"},{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html","title":"Amazon EC2 Auto Scaling Lifecycle Hooks"},{"url":"https://aws.amazon.com/blogs/compute/using-aws-lambda-with-auto-scaling-lifecycle-hooks/","title":"Using AWS Lambda with Auto Scaling Lifecycle Hooks"}],"answers":[{"id":"7479f465403b12319be40ec429085a28","text":"Add a lifecycle hook to the Auto Scaling launch stage which publishes to an Amazon Simple Notification Service topic. Configure SNS to trigger an AWS Lambda function that creates an Elastic Network Interface in the administrative subnet. Have the Lambda function attach the ENI to the EC2 instance.","correct":true},{"id":"eba70943b29ff601c4b5f75eddbb07eb","text":"Set up the Auto Scaling launch configuration to include a second Elastic Network Interface for EC2 instances. Include the administrative subnet for the secondary ENI as part of the launch configuration. Allow Auto Scaling to create and attach the secondary ENI during the instance launch stage.","correct":false},{"id":"40c2708f1afdec5ad8e8dbf1e224bd2c","text":"Create a pool of Elastic Network Interfaces in the administrative subnet. Pass a shell script as user data to be run when Auto Scaling launches an instance. Have the shell script use the AWS CLI to choose an ENI from the pool in the administrative subnet and attach it to the instance.","correct":false},{"id":"ad0e0d5cf6e1e64ab9d7089fc43559f7","text":"Include a shell script as user data to be run when Auto Scaling launches an instance. Have the shell script use the AWS CLI to call the AWS Systems Manager StartAutomationExecution API. Pass the aws:createENI action and the administrative subnet id as parameters to the API call.","correct":false}]},{"id":"dad35c13-8f81-4aac-a7b9-c2ed4a5f9335","domain":"HAFTDR","question":"With your company moving more internal services into AWS, your colleagues have started to complain about using different credentials to access different applications. Your team has started to plan to implement AWS SSO, connected to the corporate Active Directory system, but are struggling to implement a working solution.  Which of the following are not valid troubleshooting steps to confirm that SSO is enabled and working?","explanation":"The question states which are NOT valid troubleshooting steps, so we need to choose the ones which will not help us troubleshoot the issues. Firstly, you can use the User Principal Name (UPN) or the DOMAIN\\UserName format to authenticate with AD, but you can't use the UPN format if you have two-step verification and Context-aware verification enabled. Secondly, AWS Organisations and the AWS Managed Microsoft AD must be in the same account and the same region.  The answers which suggest the opposite are the ones which should be chosen.  The other answers are correct troubleshooting steps and therefore can not be chosen.","links":[{"url":"https://docs.aws.amazon.com/singlesignon/latest/userguide/prereqs.html","title":"AWS SSO PreRequisites"},{"url":"https://docs.aws.amazon.com/singlesignon/latest/userguide/troubleshooting.html","title":"Troubleshooting AWS SSO Issues"}],"answers":[{"id":"3627bfa9871db5cdd41006b26ec5dbbe","text":"Implement AWS Organisations and deploy AWS Managed Microsoft AD in two separate accounts.  It does not matter which regions they are deployed in.","correct":true},{"id":"0fa6f9fa1ae19cce6cead12014268484","text":"Ensure the number of AWS SSO permission sets are less than 500 and you have no more than 1500 AD groups.","correct":false},{"id":"eb8551b7f4293f316685247af6fbe823","text":"Implement AWS Organisations with 'All Features' enabled, deploy the AD Connector residing in your master account.","correct":false},{"id":"ff24f8f9a780de8999f077e354ae2eef","text":"To allow authentication using a User Principal Name, enable two-step verification in Context-aware verification mode.","correct":true},{"id":"fe88b553bb4a39e8268dcb243738d505","text":"AWS SSO with Active Directory only allows authentication using the DOMAIN\\UserName format.","correct":true}]},{"id":"69f350ad-db3a-4b83-bf8e-15ea0c0df866","domain":"PoliciesStandards","question":"Your company is preparing to become ISO 27001 certified and your manager has asked you to propose a comprehensive solution to log configuration and security changes in a separate audit account.  Specifically, the solution should ensure IAM users have MFA enabled, identify S3 buckets which aren't encrypted and enforce the addition of specific tagging.  Identify which option solves the problem.","explanation":"AWS Config is the only service that will meet all of the requirements in the question as it records configuration changes and snapshots the configuration at regular intervals set by you. Data aggregation means that AWS Config data from multiple accounts can be stored in a single account.  The following built in rules; s3-bucket-server-side-encryption-enabled and iam-user-mfa-enabled identify any S3 buckets not encrypted and any IAM accounts that do not have MFA enabled.  Tagging is also available for AWS Config resources that describe AWS Config rules.","links":[{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"},{"url":"https://aws.amazon.com/cloudtrail/faqs/","title":"AWS CloudTrail FAQs"}],"answers":[{"id":"2f426c67cf1036d34927bdb5aac020ba","text":"Enable AWS Cloudtrail to check for MFA enabled IAM users, configure Server access logging in S3 to view the encryption status and use a CloudFormation Templates to add tagging.","correct":false},{"id":"efd3ca0f78115a7a32bffa7905edd17b","text":"Enable AWS Config and create three default rules to check whether IAM users have MFA enabled, S3 buckets have server side encryption and tagging is added to resources.","correct":true},{"id":"0e1d676582953a5a30bcd07fa6996329","text":"Enable AWS Config and AWS Cloudtrail to track changes in all resources and to identify IAM user API calls with MFA enabled.","correct":false},{"id":"8c9a63c97e58123416200044d646218a","text":"Enable Enhance Logging in AWS Cloudwatch to track all security and configuration changes and view these using Cloudwatch Logs Insights.","correct":false}]},{"id":"fdf88463-93d7-415a-8d3c-3bc428f88120","domain":"IncidentEventResponse","question":"You currently work for a local government department which has cameras installed at all intersections with traffic lights around the city. The aim is to monitor traffic, reduce congestion if possible and detect any traffic accidents. There will be some effort required to meet these requirements, as lots of video feeds will have to be monitored. You're thinking about implementing an application that will user Amazon Rekognition Video, which is a Deep learning video analysis service, to meet this monitoring requirement. However before you begin looking into Rekognition, which other AWS service is a key component of this application?","explanation":"Amazon Kinesis Video Streams makes it easy to capture, process and store video streams which can then be used with Amazon Rekognition Video.","links":[{"url":"https://aws.amazon.com/rekognition/video-features/","title":"Amazon Rekognition  Video - AWS"}],"answers":[{"id":"bd6f317b2c9dca8a10917f4bc718f74d","text":"Amazon Kinesis Camera Streams","correct":false},{"id":"35b2a3e15fd22f4f37f605c348df07b6","text":"Amazon Kinesis Data Streams","correct":false},{"id":"d508d8ca67770d1a3837779bbaffacbb","text":"Amazon Kinesis Video Streams","correct":true},{"id":"fd61365f43b7f51920e8e3acc7f0f3a4","text":"Amazon Kinesis Video Analytics","correct":false}]},{"id":"25964f23-34d0-4ed9-9f24-4cee82e76511","domain":"ConfigMgmtandInfraCode","question":"You are building acatguru, which your organization describes as facebook for cats. As part of the sign-up process your users need to upload a full size profile image. You already have these photos being stored in S3, however you would like to also create thumbnails of the same image, which will be used throughout the site. How will you automate this process using AWS resources?","explanation":"S3 triggering Lambda to create thumbnails is a perfect example of how to automate this process","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html","title":"Using AWS Lambda with Amazon S3"}],"answers":[{"id":"62751bd29f8b4728e9393f35cf7bf035","text":"Use CloudTrail to monitor PUT and POST calls sent to S3 and trigger a Lambda function when you identify an upload. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":false},{"id":"9d2bb40a4f2a61f4e70855a070faa693","text":"Create an S3 event trigger to execute a Lambda function when an object is created. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":true},{"id":"30a4f470d1bf4e4d01dab0088e331a6d","text":"Create an S3 bucket notification trigger to execute a Lambda function when an object is created. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":false},{"id":"a24cc963be9859fff809b1da36be8137","text":"Configure S3 to publish its event stream to an SNS topic. Subscribe a Lambda function to the SNS topic which will trigger when a file is uploaded. The function will create the thumbnail from the source image and store it in a different S3 bucket.","correct":false}]},{"id":"d4166e07-35b5-4196-b355-a04793003f88","domain":"HAFTDR","question":"You currently have a lot of IoT weather data being stored in a DynamoDB database. It stores temperature, humidity, wind speed, rainfall, dew point and air pressure. You would like to be able to take immediate action on some of that data. In this case, you want to trigger a new high or low temperature alert and then send a notification to an interested party. How can you achieve this in the most efficient way?","explanation":"Using a DynamoDB stream is the most efficient way to implement this. It allows you to trigger the lambda function only when a temperature record is created, thus saving Lambda from triggering when other records are created, such as humidity and wind speed.","links":[{"url":"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/","title":"DynamoDB Streams Use Cases and Design Patterns"}],"answers":[{"id":"295bff0a7cd575c34ecae937a4f12f8e","text":"Use a DynamoDB stream and Lambda trigger only on a new temperature reading. Send a SNS notification if a record is breached.","correct":true},{"id":"3cca31ba4defd9f00540ba540543c0af","text":"Modify your IoT devices to also log their data to Kinesis Data Firehose and trigger a Lambda function which will check for new high or low temperatures. Send a SNS notification.","correct":false},{"id":"86d964e70883ad4a58b196f134df686c","text":"Write an application to use a DynamoDB scan and select on your Sort key to determine the maximum and minimum temperatures in the table. Compare them to the existing records and send an SNS notification if they are breached. Run the application every minute.","correct":false},{"id":"bfa4fc6ae9d9c31b4764750964fc2583","text":"Use CloudWatch custom metrics to plot your temperature readings and generate an event alert if it breaches your high and low thresholds.","correct":false}]},{"id":"ec0b5238-f2d5-437c-9d06-6d0011f2ae8c","domain":"IncidentEventResponse","question":"Your organisation has dozens of AWS accounts owned and run by different teams and paying for their own usage directly in their account.  In a recent cost review it was noticed that your teams are all using on-demand instances. Your CTO wants to take advantage of any pricing benefits available to the business from AWS. Another issue that keeps arising involves authentication. It's difficult for your developers to use and maintain their logins all of their accounts, and it's also difficult for you to control what they have access to. What's the simplest solution which will solve both issues?","explanation":"AWS Single Sign-On allows you to centrally manage all of your AWS accounts managed through AWS Organizations, and it will also allow you to control access permissions based on common job functions and security requirements.","links":[{"url":"https://aws.amazon.com/single-sign-on/","title":"AWS Single Sign-On."}],"answers":[{"id":"398b5351a11512e388505258eb119be1","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Create a Billing account for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Switch to role-based authentication policies in IAM to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":false},{"id":"76f5be58299aec4cdb3dc958b35a5f6a","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Create a Billing account for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Use group-based authentication policies in IAM to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":false},{"id":"e31c64ffefb763cb85b2e44f4a6f7bb6","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Raise a Support request with Amazon in order to link the accounts for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Use LDAP to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":false},{"id":"87c78562df56f4879a6773d20c92b308","text":"Use AWS Organizations to keep your accounts linked and billing consolidated. Create a Billing account for the Organization and invite all other team accounts into the Organization in order to use Consolidated Billing.  You can then obtain volume discounts for your aggregated EC2 and RDS usage.  Use AWS Single Sign-On to allow developers to sign in to AWS accounts with their existing corporate credentials and access all of their assigned AWS accounts and applications from one place.","correct":true}]},{"id":"6313796d-1496-4bfc-b9e4-fe35e00f883b","domain":"HAFTDR","question":"Your team is excited about embarking upon their first greenfield AWS project after months of lift-and-shift migration from your old datacenter into AWS.  This will be your first true infrastructure as code project.  Your team consists of eight people who all will be making changes to infrastructure over time. You would like to use native AWS tooling for writing the infrastructure templates however you are concerned about how team member changes will actually affect the resources you have running already. You don't want to accidentally destroy important AWS resources due to a developer or engineer changing a CloudFormation property whose update property requires replacement. How can you ensure that engineers are aware of the impact of their updates before they implement them, and protect important stateful resources such as EBS volumes and RDS instances against accidental deletion?","explanation":"CloudFormation Change sets will let you submit your modified stack template, it will compare it for you and show you which stack settings and resources will change. You can then execute that change set if you are happy with the changes that will occur.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"08659d45b30150542b63689978cf172d","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they first create a CloudFormation Change Set. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":true},{"id":"1f2e9869f312d47386abc4a295c498d1","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use CloudFormation Drift Check to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":false},{"id":"3264bb68f07bc74ca58c6288d8f1074e","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use CloudFormation Modify Set to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":false},{"id":"a0fa6661560c21c0020855fcc08e5013","text":"Get the team to use AWS CloudFormation to build the infrastructure as code.  Mandate that the team include the UpdateReplacePolicy property on important resources such as RDS. Use an UpdateReplacePolicy of Retain in order to retain the old physical resource or snapshot in the AWS account, but remove it from AWS CloudFormation's scope. When your team wants to update the infrastructure templates, advise they use diff in the Linux terminal to compare their old and new templates. This will will allow them to preview the effects of their changes to see whether resources will be replaced by the CloudFormation service.","correct":false}]},{"id":"b8240606-8f74-4b39-ad43-d8374d4b275d","domain":"MonitoringLogging","question":"Your organization has been using AWS for 12 months. Currently, you store all of your custom metrics in CloudWatch. Per company policy, you must retain your metrics for 3 years before it is OK to discard or delete them. Is CloudWatch suitable?","explanation":"CloudWatch will retain metrics for 15 months, after which they will expire. If you need to keep metrics for longer you must pull them out of CloudWatch using their API and store them in a database somewhere else.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html","title":"CloudWatch Concepts"}],"answers":[{"id":"47ecaf50014e4dd77522488586ae375a","text":"Yes, CloudWatch will retain custom metrics for 3 years.","correct":false},{"id":"8faab65d1ba3ce5e0cb52e471493dd0b","text":"No, CloudWatch only retains metrics for 24 months. You will have to use the API to pull all metrics and store them somewhere else.","correct":false},{"id":"d829ad2942daa681d42ad4237f50827c","text":"No, CloudWatch only retains metrics for 15 months. You will have to use the API to pull all metrics and store them somewhere else.","correct":true},{"id":"cf00bc1c445d1daf231db593c6f998d9","text":"Yes, CloudWatch will retain custom metrics for 5 years.","correct":false}]},{"id":"bd2c5320-2840-4970-8e18-0bdd5132927b","domain":"SDLCAutomation","question":"You are using the AWS Serverless Application Model to build a serverless application on AWS. You've just completed the development of a new feature that you wish to roll out to production. Despite proper testing, you want to gradually shift customer traffic to the updated Lambda version in increments of 10% with 10 minutes between each increment until you are satisfied that it's working as expected. How can you achieve this?","explanation":"When a canary deployment preference is used, traffic will be shifted in two increments. Various available options specify the percentage of traffic that's shifted to the updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment. During traffic shifting, if any of the CloudWatch Alarms go to Alarm state, CodeDeploy will immediately flip the Alias back to old version and report a failure to CloudFormation.","links":[{"url":"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html","title":"Gradual Code Deployment"},{"url":"https://github.com/awslabs/serverless-application-model/blob/master/docs/safe_lambda_deployments.rst","title":"Safe Lambda deployments"}],"answers":[{"id":"aa6d852cdfebae63c93515f42d41ab90","text":"Configure a post-traffic hook Lambda function to run a sanity test that is invoked by CodeDeploy after traffic shifting completes.","correct":true},{"id":"a62b715c5e4b5a4687f710c050c9b7b9","text":"In the AWS SAM template, specify the Canary10Percent10Minutes Deployment Preference Type.","correct":false},{"id":"e9662a939975a537911f76c87947a375","text":"Define a DeploymentPreference of type 'Linear10PercentEvery10Minutes' in your AWS SAM template.","correct":true},{"id":"2ff7334d621d3c3d77cff05fc776c090","text":"Set up a DeploymentPreference alarm that flips, after the traffic shifting has completed, the Lambda alias back to the old version if the CloudWatch Alarm goes to the Alarm state.","correct":false}]},{"id":"50424d57-02c2-4af2-84ff-10d935637fc6","domain":"IncidentEventResponse","question":"Your company has been producing Internet-enabled Microwave Ovens for two years.  These ovens are constantly sending streaming data back to an on-premises endpoint behind which sit multiple Kafka servers ingesting this data.  The latest Microwave has sold more than expected and your Manager wants to move to Kinesis Data Streams in AWS, in order to make use of its elastic capabilities.  A small team has deployed a Proof of Concept system but are finding throughput lower than expected, they have asked for your advice on how they could put data on the streams quicker.  What information can you give to the team to improve write performance?","explanation":"You should always use the Kinesis Producer Library (KPL) when writing code for Kinesis where possible, due the Performance Benefits, Monitoring and Asynchronous performance.  You should choose any answers which include KPL as a solution.  You should also check all relevant Service Limits to ensure no throttling is occurring.  Only three of these options are shown in the question, but there are many more possibilities.  The remaining options will work, but generally will give slower performance.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-producers.html","title":"Troubleshooting Amazon Kinesis Data Streams Producers"}],"answers":[{"id":"95940bef689407d24dcd08ee0da18ca5","text":"Develop code using the SDK to put data onto the streams with the Kinesis Data Streams API.","correct":false},{"id":"8309467932d5a97196aca6a575a318af","text":"Develop code using the Kinesis Producer Library to put data onto the streams.","correct":true},{"id":"7080b30b6deaa847a5edb074ea4be9d8","text":"Use a Large Producer without the Kinesis Producer Library, but using the PutRecord operation.","correct":false},{"id":"19d55c732ada47508097e33c6e04a120","text":"Check the GetShardIterator, CreateStream and DescribeStream Service Limits.","correct":true},{"id":"c6a098ae823197ee09ed9de745ce2cc8","text":"Use a Small Producer with the Kinesis Producer Library, but using the PutRecords operation.","correct":true}]},{"id":"7bce21b9-f986-4147-b616-c58ac141d19f","domain":"PoliciesStandards","question":"You work for a global service provider, deploying critical software 24 hours a day.  You have 3 AWS Accounts; 'POC' allows for Developers to stand up new technology and try out new ideas on an adhoc basis. 'QA' allows for automated builds and testing to be carried out as part of your CI/CD Pipeline and any problems here mean that software will not get pushed into production, so it's important that any issues are resolved quickly. The final account is 'Live' which is the main Production account.  It's the most critical and requires the best response times.  You need to choose the most appropriate and cost effective Support Plan which will satisfy your support needs but also allow for the full range of AWS Trusted Advisor Checks and Recommendations.","explanation":"The important phrase in the question is that any choice must \"...allow for the full range of AWS Trusted Advisor Checks and Recommendations\", only two support plans have all the Trust Advisor checks and these are Business and Enterprise.  Anything that contains the Basic or Developer plans will not be correct.  Also, it specifies that the 'Live' account is \"critical and requires the best possible response times\", so we could use the Enterprise plan for this, but we'll only get better response time for mission critical business applications such as Microsoft, SAP and Oracle tools, which is not the software we are deploying.  Therefore taking all of this into account, along with cost, 'POC', 'QA' and 'Live' should all be allocated the Business plan.","links":null,"answers":[{"id":"e52ec269985d97d389e8e39403ec69fc","text":"The 'POC' Account should be allocated the Basic Support Plan, 'QA' the Business Support Plan and 'Live' the Enterprise Support Plan.","correct":false},{"id":"4dbc42a7817c285587307fc6102d43d2","text":"The 'POC' Account should be allocated the Developer Support Plan, 'QA' the Business Support Plan and 'Live' the Enterprise Support Plan.","correct":false},{"id":"eec4f4e8aab13e184a6c7abc550b5ded","text":"The 'POC' and 'QA' Accounts should use the Basic Support Plan, while 'Live' should use the Enterprise Support Plan.","correct":false},{"id":"b802f5ecd5a395f2c899ccdbf041d677","text":"The 'POC' Account should be allocated the Basic Support Plan and both 'QA' and 'Live' the Business Support Plan.","correct":false},{"id":"92b9da164af4d9a8f9d10be3a3964a68","text":"The 'POC', 'QA' and 'Live' Accounts should all be allocated the Business Support Plan.","correct":true}]},{"id":"4b8b72cd-5529-4633-8218-880a0767b77e","domain":"ConfigMgmtandInfraCode","question":"International Megawidgits Inc. is a manufacturing company which currently uses a DynamoDB table to store information relating to the status of a process. The Operations Team have been asked to design a solution so that whenever a message appears in the DynamoDB table, it informs another process that this has happened.  As this process only polls an SQS queue, they have decided to transfer the DynamoDB files to SQS using an AWS Step Function.  The team would like you to identify some Best Practices when designing the Step Functions, so that they don't run into any problems in the future.  Choose which of the following are recognised Step Functions Best Practices.","explanation":"All answers are Step Functions Best Practices, apart from \"Ignoring Lambda Service Exceptions\" and \"Avoiding Defining Timeouts in State Machine Definitions\".  To avoid latency, separate polling threads and open at least 100 polls per activity ARN.  Step functions have a hard 25,000 execution limit so ongoing work should be split across multiple workflow executions to prevent this. Passing large payloads (over 32KB) between states may terminate the execution so always store the data in an S3 bucket and pass the ARN instead of the raw data.","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/sfn-best-practices.html","title":"Best Practices for Step Functions"}],"answers":[{"id":"8ad70e80543575b65e907fac64b130f7","text":"Use ARNs Instead of Passing Large Payloads","correct":true},{"id":"94f823ddcf628a72b41f43bec7a134ac","text":"Ignore Lambda Service Exceptions","correct":false},{"id":"d4cbd984c27d737a4235edd5b4ef2801","text":"Avoid Reaching the History Limit","correct":true},{"id":"261651564821633c3ea93f09b194daff","text":"Avoid defining Timeouts in state machine definitions","correct":false},{"id":"bada4850863c4589dd962d5f23c1a9a1","text":"Avoid Latency When Polling for Activity Tasks","correct":true}]},{"id":"5de9216b-07fe-47be-aed7-41a4da2b44bd","domain":"HAFTDR","question":"One of your colleagues has been asked to investigate increasing the performance of the main corporate Website, for customers in the Asia Pacific Region, using a CDN.  They have decided to use CloudFront to perform this function, but have encountered problems when configuring it.  You can see that CloudFront returns an InvalidViewerCertificate error in the console whenever they attempt to to add an Alternate Domain Name.  What can you suggest to your colleague to assist them in solving the issue?","explanation":"You must ensure that the certificate you wish to associate with your Alternate Domain Name is from a trusted CA, has a valid date and is formatted correctly.  Wildcard certificates do work with Alternate Domain Names providing they match the main domain, and they also work with valid Third Party certificates.  If all of these elements are correct, it may be that there was an internal CloudFront HTTP 500 being generated at the time of configuration, which should be transient and will resolve if you try again.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/troubleshooting-distributions.html","title":"Troubleshooting Distribution Issues"}],"answers":[{"id":"2d089a04108e374436b3174f6acaa8e3","text":"The certificate relating to the Alternate Domain Name was imported from a Third Party CA and this will not work.","correct":false},{"id":"8eec3b58fb382642048a138370ae92cf","text":"Ensure the certificate is not a Wildcard certificate as these do not work with Alternate Domain Names.","correct":false},{"id":"6ef625f1455326a96638173bec379999","text":"Ensure that there is a trusted and valid certificate attached to your distribution.","correct":true},{"id":"e4f79c0e45af152d4406e7e7123c83a8","text":"There was a temporary, internal issue with CloudFront which meant it couldn't validate certificates.","correct":true}]},{"id":"82e554d2-f867-4507-974f-04b0b9020cb9","domain":"MonitoringLogging","question":"Your company has a team of Windows Software Engineers which have recently switched from developing on-premise applications, to cloud native micro-services.  The Service Desk has been inundated with questions, but they can't troubleshoot because they don't know enough about Amazon CloudWatch.  The questions they have been receiving mainly revolve around why EC2 logs don't appear in log groups and how they can monitor .NET applications.  Choose the following options which will help troubleshoot the Amazon Cloudwatch issues.","explanation":"The question suggests we are utilising a Windows development environment, so we can discount any answers which have Linux only terms such as running shell scripts.  To run Amazon CloudWatch Application Insights for .NET and SQL Server, we will need to install the SSM Agent with the correct roles, IAM policies and Resource Groups.  We also need to ensure that the CloudWatch agent is running correctly, by starting it using the amazon-cloudwatch-agent-ctl.ps1 script, and as we are assuming defaults, Cloudwatch metrics can be found under the CWAgent namespace.  There are limits for many items in Cloudwatch, utilising Custom Metrics is not one of them.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/troubleshooting-CloudWatch-Agent.html","title":"Troubleshooting the CloudWatch Agent"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.html","title":"Amazon CloudWatch Application Insights for .NET and SQL Server"}],"answers":[{"id":"aa7402fca9b873f6b49d144c5c67440e","text":"For each EC2 instance running .NET code, install the SSM Agent and attach the AmazonEC2RoleforSSM Role.  You will also need to create a Resource Group and IAM Policy.","correct":true},{"id":"ffd7b413b965db5c4e66d6dc2c8a73e8","text":"Check the aws-agent.yaml file exists and is configured correctly, and then run the following shell command; aws-cw-agent.sh start.","correct":false},{"id":"33668d47f8774c94713045bff628ba97","text":"If you are utilising CloudWatch Custom Metrics, ensure that you have not reached the default limit.","correct":false},{"id":"ff3e53ec436317efcebcc4a0bf3b52a8","text":"In the Amazon Cloudwatch console, select Metrics, AWS Namespaces and then your metrics should appear under 'CWAgent'.","correct":true},{"id":"815da42a7b90966156b2d66da1a16b9e","text":"Check the common-config.toml file exists and is configured correctly, and then run the following Powershell command; amazon-cloudwatch-agent-ctl.ps1 -m ec2 -a start.","correct":true}]},{"id":"b6a6714a-e869-4cdf-8fb0-e114ed67b133","domain":"ConfigMgmtandInfraCode","question":"Your organization currently runs all of its applications on in-house virtual machines. Your CEO likes the direction AWS are taking the cloud industry and has suggested you look into what it would take to migrate your servers to the cloud. All of your servers are built and configured automatically using Chef. Which AWS services will you transition your first batch of servers to AWS in the FASTEST possible way while maintaining a centralised configuration management?","explanation":"While AWS OpsWorks allows you to use your own custom chef cookbooks with your AWS resources, it is not as easy as simply importing your existing servers into OpsWorks. The fastest solution here is to use the AWS Server Migration Service to bring your existing Chef managed machines into EC2 and to manage them with Chef the same way you have been on your in-house system.","links":[{"url":"https://aws.amazon.com/server-migration-service/","title":"AWS Server Migration Service - Amazon Web Services"}],"answers":[{"id":"75e1b69b91db540990ec7f1b7899a81a","text":"AWS OpsWorks with EC2 VM Import/Export","correct":false},{"id":"673eb2698b26d29c462829b8f6baa851","text":"Use VM Import/Export to upload snapshots of your first batch of servers into AWS. Use OpsWorks for Chef Automate to deploy a new Chef server into your AWS account. Manually deploy the same recipes and cookbooks as you run on-prem, onto your new Chef server.","correct":false},{"id":"35b2113902504f85d4b843761552eb23","text":"Leverage the AWS Server Migration Service to migrate your instances across as AMIs into AWS.  Use  custom configuration script in the Server Migration Service console to register the instances to Chef.","correct":true},{"id":"ccdcca14f4397b834e728c31d87f72cb","text":"AWS CloudFormation and OpsWorks","correct":false}]},{"id":"276f651e-fe53-4841-8545-38c00909e9c5","domain":"PoliciesStandards","question":"After the runaway success of their first website, ACatGuru is building out its IT services to handle its tremendous growth. The CEO of ACatGuru would like to centrally manage all of its commonly deployed IT services, while still giving its employees access to deploy only the services defined by company policy allows. For example, only a developer can spin up a test stack, only a database administrator can spin up a database, and so on. There should also be restrictions on which products can be launched, with the ability to create a custom user interface for you users so they can avoid the complex AWS Management Console. Which AWS service is ideal for implementing these requirements and which features are required?","explanation":"AWS Service Catalog enables organizations to create catalogs of what can be deployed, and create custom interfaces allowing users to deploy them. They are organised as Products which are added to Portfolios to which users can be given access.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/dg/what-is-service-catalog.html","title":"What Is AWS Service Catalog? - AWS Service Catalog"}],"answers":[{"id":"b1820ee1cf68e2e65f263ff7bb207626","text":"AWS Organizations","correct":false},{"id":"3ac62db74ae690ff5ba0cdff8b04efe0","text":"A Product","correct":true},{"id":"725903c711beec9056a0f2891a402263","text":"A Project","correct":false},{"id":"96564dfa4633550e1df29d07c3dba125","text":"A Portfolio","correct":true},{"id":"132976a248c1f5f10f99b4aa7d7fe9d1","text":"AWS System Catalog","correct":false},{"id":"526775c1622cd0e7b703eb8d4a83d657","text":"AWS Service Catalog","correct":true},{"id":"b8f528bf9294fdda9450ad077dfcfc66","text":"A Placement","correct":false}]},{"id":"6e252f1f-63d8-43b5-8df5-aaa7d83874d1","domain":"HAFTDR","question":"Your application has a multi-region architecture and database reading and writing is becoming an issue. You are currently storing flat file key-pairs on a shared EFS volume across all of your application servers but it is simply too slow to handle the growth your company is experiencing. Additionally, latency of static files delivered to your customers from S3 has been noted as an issue. Which solution will be not only fast but scalable as you move forward?","explanation":"DynamoDB is the only option that supports multi-region replication and multi-master writes, and it does this using Global Tables.","links":[{"url":"https://aws.amazon.com/dynamodb/global-tables/","title":"Global Tables"}],"answers":[{"id":"d6880a6902e45e48a98990a1df3f6a3d","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use multi-region rw-replica RDS configuration to create a multi-master, cross-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false},{"id":"cdfa2716b07fec163b72bf966bede22e","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use a multi-region write replica database for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false},{"id":"e1f2bdaf532480be3c4d53afd2d88645","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use a multi-region read-replica RDS configuration to create a multi-master, cross-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":false},{"id":"cdc9c7113a0cd60f29d5d56c49e1819b","text":"Utilise Amazon CloudFront to optimise delivery of your static S3 content to your users, and use Amazon DynamoDB Global Tables to create a multi-master, multi-region data store for your application's back-end. DynamoDB streams will propagate changes between the replicas so that users will have high-performant and consistent application experience regardless of from where they access your services.","correct":true}]},{"id":"fbc8bae9-74cb-430a-aaf3-7c16cd49f9f9","domain":"MonitoringLogging","question":"You have built a serverless Node.js application which uses Lambda, S3 and a DynamoDB database. You'd like to log some simple metrics so you can possibly graph them at a later date, or analyze the logs for faults or errors. You aren't able to install the CloudWatch Logs agent into a Lambda function however. What do you do instead?","explanation":"console.log will work perfectly in Lambda and is the easiest way to log directly to CloudWatch Logs","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-logging.html","title":"AWS Lambda Function Logging in Node.js"}],"answers":[{"id":"32ad7a8632717703ab0efe26deafc538","text":"Use the CloudWatch Logs API, which will provide putLogEvents where you can upload logs to CloudWatch.","correct":false},{"id":"856b826225eea424a5e5472bb8242afa","text":"Use the console.log commands in Lambda, it will log your logs straight to CloudWatch Logs.","correct":true},{"id":"d694da5d9149fb6683cce0b6ec647e92","text":"Use an API gateway configured to log to CloudWatch Logs","correct":false},{"id":"f86fadfb6bf4f2aff4fc541fb9f962d9","text":"Use the log.console commands in Lambda, it will log your logs straight to CloudWatch Logs.","correct":false}]},{"id":"b842350d-b112-40a7-999f-1d6e4259e260","domain":"IncidentEventResponse","question":"Your company sells and supports a number of Internet-enabled printers and scanners. Every time these devices have an issue, the data is sent back to a global endpoint which ingests it and places it into a Kinesis stream for processing.  Your sales team have have been meeting all of their targets and devices are sending more and more data back into your infrastructure.  You have therefore made the decision to scale out the Kinesis stream. Recently a new Cloud Engineer has joined the Operations Team and he has been given the task of re-sharding.  The task was completed successfully, but soon after he notices the following error in the logs; \"Cannot get the shard for this ProcessTask, so duplicate KPL user records in the event of re-sharding will not be dropped during de-aggregation of Amazon Kinesis records\".  The Engineer has asked you what the error is and what they should do to resolve it.  Chose your response from the following options.","explanation":"These errors are generated from code that handles KPL messages. When the KCL consumers are restarted or the lease entries are cleared, the ListShards operation will be run which will resolve the KPL messages. None of the other resolutions listed above will resolve these messages.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/introduction.html","title":"What Is Amazon Kinesis Data Streams?"}],"answers":[{"id":"8401e9c6e54a8974f4e537555ccf7a94","text":"To resolve these errors, ensure that records use the the multi-record operation PutRecords, or are aggregated into a larger file before using the single-record operation PutRecord.","correct":false},{"id":"6bdec47e015462087aa8015af4c475c4","text":"To resolve these errors, either stop and start the KCL application or clear the lease entries from the DynamoDB lease table.","correct":true},{"id":"ccbe7f04475bbc92d4b729572830b9b7","text":"To resolve these errors, ensure the producers use multiple threads to write to the Kinesis Data Streams service at the same time.","correct":false},{"id":"0068cac99ac759e7c30c9361ca3e3fa7","text":"To resolve these errors, re-shard the Kinesis stream to the the number of shards that were originally set before the errors occurred.","correct":false}]},{"id":"7d87ad95-0cc6-4a66-8e2e-a2b02fe78f38","domain":"SDLCAutomation","question":"You receive a PermissionError from CodePipeline that complains about not being able to access your GitHub repository. What are possible reasons for that and how can you resolve this error?","explanation":"CodePipeline uses GitHub OAuth tokens and personal access tokens to access your GitHub repositories. CodePipeline doesn't automatically expire your GitHub personal access token.","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/troubleshooting.html?shortFooter=true#troubleshooting-gs2","title":"Troubleshooting CodePipeline"},{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html","title":"Managing Access Keys for IAM Users"},{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/GitHub-rotate-personal-token-CLI.html","title":"Use GitHub and the CodePipeline CLI to Create and Rotate Your GitHub Personal Access Token on a Regular Basis"}],"answers":[{"id":"61e68f6697ca35f0212a17494ee8e14d","text":"The only way to authenticate CodePipeline in GitHub is to use your AWS access token. Use your IAM users' access key ID (not the secret!) to re-register CodePipeline as an OAuth application in GitHub.","correct":false},{"id":"ea42611005235ed3043a63d37da1be8c","text":"CodePipeline uses OAuth tokens to integrate with GitHub. You might have revoked the permissions of the OAuth token for CodePipeline. Try signing in to GitHub, go to Applications under Settings, and choose Authorized OAuth Apps. If you do not see CodePipeline in the list, open the CodePipeline console, edit the pipeline, and choose Connect to GitHub to restore the authorization.","correct":true},{"id":"8b118ac224db32b1b232ec2b4f288564","text":"CodePipeline can use GitHub personal access tokens to access your GitHub repositories and retrieve the latest changes. As with all tokens, these should be stored securely and rotated or regenerated routinely. Token rotation is recommended by RFC-6819 (OAuth 2.0 Threat Model and Security Considerations). If not refreshed regularly, CodePipeline expires your GitHub personal access token after 90 days of inactivity. Regenerate a new token and run the get-pipeline command on the pipeline where you want to change the personal access token.","correct":false},{"id":"7771d289cb96cfe426d515391a62d0a5","text":"The number of OAuth tokens is limited and if CodePipeline reaches that limit, older tokens will stop working, and actions in pipelines that rely upon that token will fail. To fix this issue, try to manually configure one OAuth token as a personal access token, and then configure all pipelines in your AWS account to use that token.","correct":true}]},{"id":"781e2a28-ff7a-4712-bf0c-1ae69dec243f","domain":"SDLCAutomation","question":"You are part of a development team that has decided to compile release notes directly out of a CodeCommit repository, the version control system in use. This step is to be automated as much as possible. Standard GitFlow is used as the branching model with a fortnightly production deploy at the end of a sprint and occasional hotfixes. Select the best approach.","explanation":"Following GitFlow's standard release procedures, a release branch is merged into master. That commit on master must be tagged for easy future reference to this historical version. Both release and hotfix branches are temporary branches and would require ongoing updates of the CodeCommit trigger. Feature branches are used to develop new features for the upcoming or a distant future release and might be discarded (e.g. in case of a disappointing experiment). CodeCommit does not provide a generate release notes feature.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html","title":"Manage Triggers for an AWS CodeCommit Repository"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html#codecommit_event_type","title":"CodeCommit Events"},{"url":"https://aws.amazon.com/blogs/devops/build-serverless-aws-codecommit-workflows-using-amazon-cloudwatch-events-and-jgit/","title":"Build Serverless AWS CodeCommit Workflows using Amazon CloudWatch Events and JGit"},{"url":"https://nvie.com/posts/a-successful-git-branching-model/","title":"A successful Git branching model"},{"url":"https://docs.aws.amazon.com/codecommit/latest/APIReference/Welcome.html","title":"AWS CodeCommit API Reference"},{"url":"https://forums.aws.amazon.com/thread.jspa?messageID=756611","title":"CodeCommit Lambda triggers fire off separate events for each commit?"}],"answers":[{"id":"94ff89937241b8005e348ff0550a1947","text":"Setup up an Amazon CloudWatch Event rule to match CodeCommit repository events of type 'CodeCommit Repository State Change'. Look for 'referenceCreated' events with a 'tag' referenceType that are created when a production release is tagged after a merge into 'master'. In a Lambda function, use the CodeCommit API to retrieve that release commit message and store it in a static website hosting enabled S3 bucket.","correct":true},{"id":"daf0713b7db34409528f3e191adf166e","text":"Use the 'generate release notes' feature of CodeCommit by running the 'create-release-notes' command with the --from <datetime> (use the start of the sprint) or --previousTag <tagName> option in the AWS CLI. Create a Lambda to execute this on a regular schedule (i.e. every 2 weeks) using CloudWatch Events with a cron expression.","correct":false},{"id":"601038d6d016fa6b4637092d2a68db75","text":"Configure a trigger by choosing the 'Delete branch or tag' repository event that invokes a Lambda function when development for a sprint is finished, i.e. the last feature-* branch has been deleted. In that Lambda, retrieve the latest git merge commit message before the deletion and append it to the release notes text file stored in an S3 bucket.","correct":false},{"id":"625fdc89350c8e0422b2c105b66be63a","text":"Create a trigger for your CodeCommit repository using the 'Push to existing branch' event and apply that to any release and hotfix branch. Add an Amazon SNS topic as the target and have a Lambda listen to it. In that function, filter out specific commit type changes such as style, refactor and test that are not relevant for release notes. Store all other commit messages in a DynamoDB table and, at release time, run a query to collate the release notes.","correct":false}]},{"id":"90ad13a4-87b3-4afb-9f86-6dd9f234f8ff","domain":"MonitoringLogging","question":"Your organisation would like to implement autoscaling of your servers, so you can spin up new servers during times of high demand and remove servers during the quiet times. Your application load mainly comes from memory usage, and so you have chosen that as your scaling metric. What do you have to do next?","explanation":"Memory utilization is not a default CloudWatch metric, you will have to publish a custom metric first.","links":[{"url":"https://aws.amazon.com/autoscaling/","title":"AWS Auto Scaling"}],"answers":[{"id":"e83f40180c161cdc0807d8b69b0b85ee","text":"Configure the Auto Scaling Group with scaling policy with steps, use the default metric type 'Average Memory Utilization'","correct":false},{"id":"19f84b354da6659ed799e7df04b00825","text":"Configure the Auto Scaling Group to use the default metric type 'Average Memory Utilization'","correct":false},{"id":"228ffd56d2bbca049ca1bb76c788e09f","text":"Configure the Auto Scaling Group with a target tracking scaling policy, use the default metric type 'Average Memory Utilization'","correct":false},{"id":"e49d82811a3d3e790284aadeeb45eccd","text":"Publish a custom memory utilization metric to CloudWatch, as there isn't one by default.","correct":true}]},{"id":"b1b39162-3af1-4aa5-9cd4-4e30338d07f6","domain":"HAFTDR","question":"Your CEO wants you to start future proofing your AWS environment, so he's asked you to look into IPv6 compatibility of your existing Load Balanced EC2 stack. You make use of both Application (ALB) and Network (NLB) load balancers in your EC2-VPC. What are your findings?","explanation":"At this moment in time only the Application Load Balancer supports IPv6 in the EC2-VPC environment. Classic Load balancers only support IPv6 if you are using an EC2-Classic environment.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2017/01/announcing-internet-protocol-version-6-ipv6-support-for-elastic-load-balancing-in-amazon-virtual-private-cloud-vpc/","title":"Announcing Internet Protocol Version 6 (IPv6) support for Elastic Load Balancing in Amazon Virtual Private Cloud (VPC)"}],"answers":[{"id":"2a74b0b6126f559f800f0f5cef044526","text":"No Load Balancers in EC2 support IPv6.","correct":false},{"id":"12d596f99a6d191c9c1aaae49901a9e1","text":"Application Load balancers support IPv6, Network Load Balancers do not.","correct":true},{"id":"4aff7beb08ea7fe4fa49ab96903dfc41","text":"Application Load balancers do not support IPv6, Network Load balancers do.","correct":false},{"id":"eda9ef0a0ac30b1db3a5bf8674687603","text":"Application and Network Load Balancers both support IPv6.","correct":false}]},{"id":"abfe783d-622d-4a39-9ee5-f210c96b3a6b","domain":"ConfigMgmtandInfraCode","question":"A company has created three Docker containers which need to be deployed.  The first is a core application for the company which will need to accommodate thousands of Websocket connections every minute.  The second is the main corporate Website which is based on a Node.js application running behind an Nginx sidecar. The final container is a small departmental application written by a single developer using the React framework.  You have been asked to deploy each container on the most suitable, cost effective and reliable AWS platform from the options below.","explanation":"In order to make the decision which options are the best, we should start with the deployment that has the most constraints and that is the application using Websockets as these require many network connections and should be installed as multiple tasks across the ECS Cluster.  With this constraint dealt with, we can eliminate all options that don't include the ECS Cluster.  Putting all containers on one ECS Cluster would work technically, but wouldn't be cost effective and would mean an internal departmental application lives on the same cluster as the main core production applications.  When we also exclude installing Docker directly on EC2 as there is no redundancy, this leaves the only option as using ECS for the core application, Fargate for the Website and Elastic Beanstalk for the internal application.","links":[{"url":"https://aws.amazon.com/ecs/faqs/","title":"Amazon Elastic Container Service FAQs"},{"url":"https://aws.amazon.com/elasticbeanstalk/faqs/","title":"AWS Elastic Beanstalk FAQs"},{"url":"https://aws.amazon.com/fargate/faqs/","title":"AWS Fargate FAQs"}],"answers":[{"id":"cc5e3933e49bdce4224537871a4f765b","text":"Deploy all three applications on a managed ECS Cluster.","correct":false},{"id":"a09251d393d8cef1692aee1706d87dec","text":"Deploy the Websocket application on a managed ECS Cluster, the Corporate Website on Fargate and the Departmental application on Elastic Beanstalk.","correct":true},{"id":"37d838ae967ac13002610a1398c9ffd2","text":"Deploy all of the Docker containers in Fargate.","correct":false},{"id":"31d397d0bdcff1d57aeca0011626b830","text":"Deploy the Websocket application on a managed ECS Cluster, the corporate Website in Fargate and install Docker on a single EC2 instance to run the departmental application.","correct":false},{"id":"d8af6733a20ec3d6677a1e9d0016f47a","text":"Deploy the Websocket application and the corporate Website on a managed ECS Cluster and deploy the departmental application in Fargate.","correct":false}]},{"id":"67394e5d-b7ef-4850-b0da-9548411156f3","domain":"SDLCAutomation","question":"Vector Technologies Incorporated wants to implement a CI/CD environment for their online bill payment system. The application runs in a .NET environment on AWS Elastic Beanstalk, and the database is SQL Server. Code is stored in a GitHub repository external to AWS. Changes to the database schema are needed for most code updates, and are done with a script that is also stored on GitHub. The CI/CD process should be initiated every time new source code or a new schema update script is moved to the central repo. Which architecture will provide the capability to automatically retrieve, build, and deploy application components each time changes are made?","explanation":"Code Pipeline can be invoked every time there is a code change or new schema update script in the GitHub repository branch, both of which can be written to S3. CodePipeline can then trigger CodeBuild to use the MSBuild Windows image that has been stored in ECR to build the .NET application code. The resulting package is then placed in a build artifacts S3 folder, which is picked up by CodeDeploy to be implemented on Elastic Beanstalk. CodeDeploy can also invoke a PowerShell script to run the schema update executable. An ECS container is not needed to perform this build, and will incur additional costs. CodeBuild can work directly with the MSBuild container image in ECR. NAnt provides the capability to automate software build processes, but MSBuild is still needed to compile the .NET code. CodeDeploy won't be able to run the database schema update scripts without them being embedded in an executable.","links":[{"url":"https://aws.amazon.com/codepipeline/","title":"AWS CodePipeline"},{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild"},{"url":"https://aws.amazon.com/codedeploy/","title":"AWS CodeDeploy"},{"url":"https://aws.amazon.com/quickstart/architecture/dotnet-cicd-on-aws/","title":".NET CI/CD on the AWS Cloud"},{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html","title":"CodeDeploy AppSpec File Reference"}],"answers":[{"id":"2b48c63fdb564d11a32c45fcc0205c3f","text":"Create an MSBuild container image with the required tools for compiling .NET applications and push it to Amazon Elastic Container Registry (ECR). Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use the MSBuild container image from ECR to compile the source code. Configure CodeBuild to then construct the .NET application. Also have CodeBuild create an executable to run the schema update script. Have CodePipeline trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy invoke a Powershell script to run the schema update executable.","correct":true},{"id":"0bace657c20367e9692de64ec05c7669","text":"Write an MSBuild container image with the required tools for compiling .NET applications and push it to Amazon Elastic Container Registry (ECR). Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script and write them to S3. Have CodePipeline trigger AWS CodeBuild to create an Amazon Elastic Container Service (ECS) container from the ECR image. Use the ECS container application to retrieve the latest code updates from S3 and construct the .NET application. Have CodePipeline trigger AWS CodeDeploy to implement the .NET application on Elastic Beanstalk. Have CodeDeploy run the schema update scripts against the database.","correct":false},{"id":"3833bcf2af22e331e4ba509f55aa1236","text":"Use AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use NAnt to compile the source code and construct the .NET application. Configure CodePipeline to trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy run the schema update scripts against the database.","correct":false},{"id":"a69266d62fd4ec974f90c80873f07508","text":"Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script. Have CodePipeline trigger AWS CodeBuild to use NAnt to compile the source code and construct the .NET application. Also have CodeBuild create an executable to run the schema update script. Configure CodePipeline to trigger AWS CodeDeploy to deploy the .NET application to Elastic Beanstalk. Have CodeDeploy invoke a Powershell script to run the schema update executable.","correct":false}]},{"id":"e94bdca3-b267-4c84-8485-4e53c9319db4","domain":"ConfigMgmtandInfraCode","question":"You have an application built on docker. You would like to not only launch your docker instances at scale but load balance them as well. Your director has also enquired as to whether it is possible to build and store the containers programmatically using AWS API calls in future or if additional products are required. Which AWS services will enable you to store and run the containerized application and what do you tell the director?","explanation":"Amazon ECS and ECR are the Amazon Elastic Container Service and Registry, which will meet all of the requirements specified. It also has an API which can be used to implement your requirements.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html","title":"What is Amazon Elastic Container Service? - Amazon Elastic Container Service"}],"answers":[{"id":"361a8c7414d01c019db585e4f410d8c0","text":"Build containers and store them using AWS ECR (Elastic Container Registry). Run the containers using Amazon ECS. Automated container builds are not possible with AWS services and APIs however third-party build engines such as Jenkins and BuildKite will achieve your director's goal.","correct":false},{"id":"2786f753087512c515526c6e2f9909a3","text":"Build EC2 instances with docker using EC2 and AutoScaling and store them using AWS ECR (Elastic Container Registry). Run the containers using Autoscaling EC2. Automated container builds are not possible with AWI services and APIs however third-party build engines such as Jenkins and BuildKite will achieve your director's goal.","correct":false},{"id":"2b2965d7b7b01e772fd99e1883745cc6","text":"Build EC2 instances with docker using EC2 and AutoScaling and store them using AWS ECR (Elastic Container Registry). Run the containers on docker desktop on EC2. Yes automated container builds are possible with AWS CodeBuild pushing to ECR and using ECS APIs.","correct":false},{"id":"9d3a19960e725c6a8f4133c71337a88e","text":"Build containers and store them using AWS ECR (Elastic Container Registry). Run the containers using Amazon ECS. Yes automated container builds are possible with AWS CodeBuild pushing to ECR and using ECS APIs.","correct":true}]}]}}}}
