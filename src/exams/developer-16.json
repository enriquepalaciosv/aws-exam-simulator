{"data":{"createNewExamAttempt":{"attempt":{"id":"9ce4f397-67ce-4533-9f62-735262d54d68"},"exam":{"id":"7933d731-c22f-4d51-b0df-4c6e254265ee","title":"AWS Certified Developer - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"55fbda31-57ba-4009-a6da-09f690b40351","domain":"security","question":"You are developing an application which will use Cognito to allow authenticated Facebook users to sign-in and use your application. You would like to use Cognito to handle temporary access allowing authenticated users to access product and transaction data that your application stores in S3 and DynamoDB. Which is the best approach?","explanation":"Cognito is the recommended approach for user sign-up and sign-in for mobile applications which allow access to users with Facebook, Google or Amazon.com credentials. Identity pools enable you to grant your users temporary access to AWS services. User pools are user directories that provide sign-up and sign-in options for your app users.","links":[{"url":"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html","title":"What Is Cognito?"}],"answers":[{"id":"6e4d8be48c9bbd213381e12f94eaa9a7","text":"Configure a SAML 2 Federation to provide temporary AWS credentials to your users to allow temporary access to AWS resources","correct":false},{"id":"edac71e5f6a783c051f80c7369a90c0d","text":"Configure a User Pool to provide temporary AWS credentials to your users to allow temporary access to AWS resources","correct":false},{"id":"e83779413869ed1569c2eb035ca21ef9","text":"Configure an Identity Pool to provide temporary AWS credentials to your users to allow temporary access to AWS resources","correct":true},{"id":"67a47bfe3685e0b65597b1bb14c5426f","text":"Configure an IAM User Group to provide temporary AWS credentials to your users to allow temporary access to AWS resources","correct":false}]},{"id":"51d0eac3-d55e-4a50-aa5b-c133add86037","domain":"refactoring","question":"A content publishing organization runs its own platform, which uses DynamoDB as its data store. A bug report has come in from the content team. They say that when two editors are working on the same content they frequently overwrite each other's changes.\n\nWhat DynamoDB feature would prevent the most number of overwrite bug reports?","explanation":"Using a condition-expression we can perform a conditional update to an item. The condition must evaluate to true; otherwise, the update operation fails. We can use this feature to make sure the content of an article has not changed since it was last read, before we update it.\n\nacid-expression is incorrect because there is no such expression.\n\nDynamoDB TTL is incorrect because it is for deleting items from DynamoDB after a given duration, not creating a lock.\n\nCalling GetItem immediately before calling UpdateItem would help mitigate the issue, but still leaves a small race condition where condition-expression does not. It is, therefore, not the best solution.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ConditionExpressions.html","title":"Condition Expressions in DynamoDB"}],"answers":[{"id":"058706dd0f32f05aa73a515774a5dc2d","text":"Call GetItem immediately before calling UpdateItem to ensure the item has not changed.","correct":false},{"id":"ddb55880d9b1b331207bcb38cebd6dbf","text":"Include an acid-expression in the UpdateItem command.","correct":false},{"id":"b1d92ca8c7500f89cffdf0057251aec1","text":"Include a condition-expression in the UpdateItem command.","correct":true},{"id":"9d2d53ddaae2b905e48ea3705529a19c","text":"Apply a time-limited lock to the item while an author is editing it using a DynamoDB TTL.","correct":false}]},{"id":"08861bdf-6813-43f1-9def-4255492b4533","domain":"mon-trb","question":"Your application is using SQS to send and receive messages, your application needs to receive the messages as soon as they arrive and you need to ensure the architecture is as cost efficient as possible. Which of the following approaches will optimise the cost and performance of the application?","explanation":"In almost all cases, Amazon SQS long polling is preferable to short polling and results in higher performance and reduced cost in the majority of use cases.","links":[{"url":"https://aws.amazon.com/sqs/faqs/","title":"SQS FAQs"}],"answers":[{"id":"8757c7e091bc29e7b9225334f00531ac","text":"Reduce the total number of message queues","correct":false},{"id":"33e24a8e438593d0ed4994ce9ea68ccd","text":"Enable Long Polling","correct":true},{"id":"64dab6674843dc03406bdc90b1c5b21d","text":"Lower the message Visibility Timeout","correct":false},{"id":"9ef993bf7e61fa02b5f6ac0fc8da6b18","text":"Enable Short Polling","correct":false}]},{"id":"af4f8b9c-bfe1-44dd-803f-d12581a91f6d","domain":"development","question":"You are developing a scalable application which will run in Docker on ECS. You would like to be able to run multiple tasks on the same ECS service. How should you approach this?","explanation":"Port mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition. Dynamic port mapping with an Application Load Balancer makes it easier to run multiple tasks on the same Amazon ECS service on an Amazon ECS cluster.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/","title":"Dynamic Port Mapping for Amazon ECS"},{"url":"https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html","title":"Port Mapping for Amazon ECS"},{"url":"https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/","title":"Run Containerized Microservices with Amazon ECS and Application Load Balancer"}],"answers":[{"id":"978e984e16276c17104b4c922731f3f8","text":"Virtual Port Mapping","correct":false},{"id":"e0f10b949b1cbe40263bfe87c11a2f5d","text":"Network Load Balancer","correct":false},{"id":"f4a166b164c896d2e4f9aa43541f5c30","text":"Dynamic Port Wrapping","correct":false},{"id":"e211f72442bce02b412217a3983c847f","text":"Dynamic Port Mapping","correct":true}]},{"id":"32d5da0f-0ed7-436c-98cf-a16053badf54","domain":"development","question":"Your application runs on Lambda and you would like to enable your functions to communicate with EC2 instances in your private subnet. How can you enable this?","explanation":"In order to enable Lambda to communicate with your private VPC, you need to add VPC config information. You do not need to add a NAT or Internet gateway and it is not possible to launch a Lambda function inside your own VPC or subnet. - Please be aware that in September 2019, AWS announced that they are simplifying VPC networking for Lambda functions, with changes planned to be rolled out on a per region basis. However the exams do generally run at least 6-12 months behind any new updates or announcements, so unfortunately, you may still see exam questions referring to the old way of doing things.","links":[{"url":"https://aws.amazon.com/blogs/compute/announcing-improved-vpc-networking-for-aws-lambda-functions/","title":"Announcing improved VPC networking for AWS Lambda functions"}],"answers":[{"id":"d77b6ba1a84152571b9b01b57aa78774","text":"Add an internet gateway to your VPC","correct":false},{"id":"9b7ce49759154447fc7d7666f84adf68","text":"Launch the Lambda function in the same subnet as your EC2 instances","correct":false},{"id":"4f2ebe62076f1314a2b7ece1aae5f495","text":"Update your Lambda function with the relevant VPC config information","correct":true},{"id":"a8f43c529989f897c2749cb29caeb890","text":"Add a NAT gateway to the subnet","correct":false}]},{"id":"47e074a7-e675-4918-92bf-d9a34b82803c","domain":"security","question":"Your application needs to access content located in an S3 bucket which is residing in a different AWS account, which of the following API calls should be used to gain access?","explanation":"The STS AssumeRole API call returns a set of temporary security credentials which can be used to access AWS resources, including those in a different account","links":[{"url":"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html","title":"Providing temporary access to AWS resources"}],"answers":[{"id":"c2599de6f471b00ab3948981939f8315","text":"STS:GetFederationToken","correct":false},{"id":"5854d57d52033e05be8f9fda06330abd","text":"STS:AttachRole","correct":false},{"id":"100979100796827d7bcafe4666e4984f","text":"STS:AssumeRole","correct":true},{"id":"818a55892aa657e5ef8dae6d12ee9273","text":"IAM:AddRoleToInstanceProfile","correct":false}]},{"id":"8ee26b88-194a-4069-85a1-e28a48bcca27","domain":"security","question":"An organization has mandated that all data within its DynamoDB tables must be encrypted at rest using an AWS owned key. What must a developer do to ensure this?","explanation":"All DynamoDB tables are encrypted at rest using an AWS owned CMK by default. Non-encrypted DynamoDB tables are no longer supported in AWS. You have the option to pick an alternative AWS or Customer Managed KMS key if required.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html","title":"DynamoDB Encryption at Rest"}],"answers":[{"id":"2a13575a2661e32db0bdf9ad7b95ffee","text":"This cannot be done; DynamoDB does not support encryption at rest.","correct":false},{"id":"852abdb4878eba09a03ca6e37eb1dd16","text":"Enable DynamoDB encryption and select AWS managed CMK.","correct":false},{"id":"5744856c94411674b105aff56f20a6a3","text":"There's no need to do anything; all DynamoDB tables are encrypted at rest with an AWS owned key by default.","correct":true},{"id":"92c03c315e7b990388532214f2a73c62","text":"Enable DynamoDB encryption and select AWS owned CMK.","correct":false}]},{"id":"2988d8a0-c66e-434f-948b-acee623892fc","domain":"mon-trb","question":"You work for an electric car company that has its front-end website on EC2. Company policy dictates that you must retain a history of all EC2 API calls made on your account for security analysis and operational troubleshooting purposes. Which AWS service should you use to do this?","explanation":"CloudTrail is a web service that records AWS API calls for your AWS account and delivers log files to an Amazon S3 bucket. The recorded information includes the identity of the user, the start time of the AWS API call, the source IP address, the request parameters, and the response elements returned by the service.","links":[{"url":"https://aws.amazon.com/documentation/cloudtrail/","title":"CloudTrail Overview"}],"answers":[{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false},{"id":"a907a7338c1fb3821fbe8ed113c64b33","text":"CloudHSM","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":true},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false}]},{"id":"75298fe2-86e7-4945-b359-fa66efa511a5","domain":"deployment","question":"Your application is running on Docker in an Elastic Beanstalk. You have been asked to deploy a new version of the application code. What is the process for doing this?","explanation":"When you use the AWS Elastic Beanstalk console to deploy a new application or an application version, you'll need to upload a source bundle. Your source bundle must consist of a single ZIP file or WAR file which does not exceed 512 MB.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html","title":"Create an Application Source Bundle"}],"answers":[{"id":"cb68a9fcd34702aca58b9c8e1ca910e9","text":"Log in to the underlying EC2 instance and replace the existing Docker image with the new code","correct":false},{"id":"ef19372b909be8f395f79dce7fd549c0","text":"Use the Elastic Beanstalk console to upload and deploy the new version of your application using a zip file containing your code","correct":true},{"id":"349146001f426f05effeb7bf24d3d9e3","text":"Delete your environment and redeploy using the new code","correct":false},{"id":"f63680af566b899e62b6b0110b5677d7","text":"Log in to the EC2 instance, update the dockerfile and restart the container","correct":false}]},{"id":"7db65ca0-ae58-41f9-8817-0bc22b41d4a7","domain":"development","question":"You are running a large distributed application using a mix of EC2 instances and Lambda. Your EC2 instances are spread across multiple availability zones for resilience and are configured inside a VPC. You have just developed a new Lambda function which you are testing. However, when you try to complete the testing, your function cannot access a number of application servers which are located in the same private subnet. Which of the following could be a possible reason for this?","explanation":"To connect to a VPC, your functions execution role must have the following permissions: ec2:CreateNetworkInterface, ec2:DescribeNetworkInterfaces, ec2:DeleteNetworkInterface. These permissions are included in the AWSLambdaVPCAccessExecutionRole managed policy.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/vpc.html","title":"Configuring a Lambda Function to Access Resources in a VPC"}],"answers":[{"id":"d8a24d4f6bf371cd385b9a03e8b561f5","text":"The EC2 instances are running in a different region to the Lambda function","correct":false},{"id":"fc84432518af302b6f9d1b1157cde9eb","text":"The EC2 instances need to be in the same subnet as the Lambda function","correct":false},{"id":"518adaf29098983c5a9e4ed138960fdd","text":"Your security group does not allow connectivity from the AWS Lambda endpoint","correct":false},{"id":"3a66964403f88954ed77583314f3ae2f","text":"The function execution role does not include permission to connect to the VPC","correct":true}]},{"id":"1a8f6ca4-1edf-4cb3-8dce-63550ef898d0","domain":"security","question":"You are working on a web application which handles confidential financial data. The application runs on a few EC2 instances which are behind an Elastic Load Balancer. How can you ensure the data is encrypted end-to-end in transit between your ELB and EC2 instances?","explanation":"Terminating secure connections at the load balancer and using HTTP on the backend might be sufficient for your application. However, if you are developing an application that needs to comply with strict external regulations, you might be required to secure all network connections. First, add a secure listener to your load balancer, then configure the instances in your environment to listen on the secure port and terminate HTTPS connections.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-endtoend.html","title":"Configuring End-to-End Encryption"}],"answers":[{"id":"ed90effd6942a7cffc042b6cb610b798","text":"Perform SSL termination on the load balancer","correct":false},{"id":"9f4e11571be1c566b8e30fdd753f0802","text":"Configure the instances in your environment to listen on the secure port","correct":true},{"id":"c2bc73f05693654154a5a002cf77d3e1","text":"Configure a secure listener on your load balancer","correct":true},{"id":"cb9dd4ff2be4893bd209d207f1898600","text":"Perform SSL termination using Lambda","correct":false},{"id":"2c26367f893c5f15ae242c1965051017","text":"Terminate HTTPS connections on your EC2 instances","correct":true}]},{"id":"4ebf4856-d4c7-4ea6-85c6-602dba6571a3","domain":"development","question":"You are developing a latency-sensitive application which stores a lot of data in DynamoDB. Each item is 3.5KB in size. Which of the following DynamoDB settings would give you the greatest read throughput?","explanation":"A read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. Eventually consistent reads provide greater throughput than strongly consistent.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ProvisionedThroughput.CapacityUnits.Read","title":"DynamoDB Provisioned throughput"}],"answers":[{"id":"882e4855b7af9b2ffa948de81b75cf47","text":"Configure the table with 10 read capacity units and use high-performance reads","correct":false},{"id":"58dedf407e5eb2c525e6732d526eeca0","text":"Configure the table with 10 read capacity units and use strongly consistent reads","correct":false},{"id":"fa75717ea7d6fc295eb8d4b34d6ac4f6","text":"Configure the table with 15 read capacity units and use strongly consistent reads","correct":false},{"id":"53f0f96b15844d8d110ee8bc7174411d","text":"Configure the table with 10 read capacity units and use eventually consistent reads","correct":true},{"id":"85f208ed09607ef8653aef5841e25848","text":"Configure the table with 15 read capacity units and configure the application to use a scan operation","correct":false}]},{"id":"6e7680bf-f045-4583-b38b-b2ca3ae466ed","domain":"security","question":"An IT Auditor has started in your Security Team, they will need access to read files in S3 and DynamoDB as well as the ability to describe EC2 instances. You want to ensure that only the Auditor is granted this access and that the IAM policy you create cannot mistakenly be attached to any other user. Which IAM policy type should you use?","explanation":"When you use an inline policy, the permissions in the policy cannot be inadvertently attached to the wrong principal entity","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html","title":"Managed Policies and Inline Policies"}],"answers":[{"id":"ab18cb3ee046b7e5466057043f273700","text":"Custom Policy","correct":false},{"id":"05aaffc9a37013fcae1b36c4baffceff","text":"AWS Managed Policy","correct":false},{"id":"60a6d5742e8f4d09d2d0edb9b15fa80d","text":"Customer Managed Policy","correct":false},{"id":"449ff67db14fb547ec64ef11c4d33c40","text":"Inline Policy","correct":true}]},{"id":"22556a45-7db0-48f9-85cf-654ac74d729f","domain":"security","question":"When using the AWS REST API to upload an object to S3, which of the following request headers will ensure that your data must be encrypted using SSE?","explanation":"To request server-side encryption using the object creation REST APIs, provide the x-amz-server-side-encryption request header.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"Protecting Data Using Server-Side Encryption"}],"answers":[{"id":"f041d978ebb9256a023c1f9d263316ac","text":"amz-s3-server-side-encryption","correct":false},{"id":"6d1f5944b9a6aada7e00dc385d5373bd","text":"x-s3-server-side-encryption","correct":false},{"id":"6d38512683c3cf8052e7e47d9d12a9f6","text":"x-amz-server-side-encryption","correct":true},{"id":"63e1961675193e5c234f582f08632a28","text":"s3-amz-server-side-encryption","correct":false}]},{"id":"33a233e7-f5ba-44d5-8c29-c72d59329262","domain":"development","question":"You are planning to write some Python code which will query a DynamoDB table and display the output on your website, which of the following tools can you use to start writing your code?","explanation":"AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.","links":[{"url":"https://aws.amazon.com/cloud9/faqs/","title":"Cloud9 FAQs"}],"answers":[{"id":"07929940c38c1e9e91ecf67c01ccfa73","text":"CodeDeploy","correct":false},{"id":"fc8e90107dabb9a35c490b0d86adea06","text":"Cloud9","correct":true},{"id":"706f0c9ac9a93c06a6db5f8838d71b0c","text":"CodeStar","correct":false},{"id":"f8d1a2a80933c71d9a65e3d825976bda","text":"CodeBuild","correct":false}]},{"id":"f71363df-7941-4a84-93e1-7a5ed2ef1c08","domain":"security","question":"You have some sensitive data that you would like to encrypt. You want to be sure that once the data is encrypted, nobody but you will be able to use the encryption key to decrypt your files. Your head of security has asked you to make sure that the key used to encrypt your files is itself encrypted under another key. Which AWS technology enables this?","explanation":"When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key, and then encrypting the data key under another key.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping","title":"Envelope Encryption"}],"answers":[{"id":"8afab6f0bb19c0796620267f6a74644b","text":"Encrypt the master key with the data key","correct":false},{"id":"fdf5291377e4e56d4ee76f0328313232","text":"Use envelope encryption to encrypt the data key with another key","correct":true},{"id":"bd2c15ddb6cb6e3fee2adc749979e1e1","text":"Store the encryption keys in CloudHSM","correct":false},{"id":"834369e6ea722aa59f60696c106ba827","text":"Re-encrypt the data key with the master key","correct":false},{"id":"c5c0340c8e4f6673916050e3cbc5e2b2","text":"Store the encryption key in an encrypted S3 bucket","correct":false}]},{"id":"096903dc-fa87-479f-a185-59f1128dffc4","domain":"refactoring","question":"You are developing a new application which allows users to search for parking spots in their local area. Your application is based on Lambda and uses API Gateway to connect to third party parking lot companies to access information about parking tariffs and availability. Your application stores session data in a DynamoDB table, however you want to keep costs to a minimum and would like to automatically delete the session data from the table once the user session has expired and the data is no longer relevant. How can you most easily achieve this?","explanation":"Time To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database. TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","title":"DynamoDB TTL"}],"answers":[{"id":"ebd2e6b8cdc90d9f3f3d2f1552acbe9b","text":"Use SNS to send a notification when the session data expires, then configure API Gateway to remove the unwanted data","correct":false},{"id":"368bfaa4732707e475d4a566a7bfa0e0","text":"Use DynamoDB Streams to prune the table and remove any unwanted data","correct":false},{"id":"8a52edb7329e1e89781f22df0aa3c585","text":"Configure a TTL based on the session expiry time","correct":true},{"id":"02e9893b7f2e8c6dfa3bc682a2079313","text":"Write a Lambda function to continually poll the table to check if the session data has expired and then delete the item from the table","correct":false}]},{"id":"a3b17c55-ba03-4019-a9bd-d5c7a7eaab29","domain":"mon-trb","question":"You are running an online fitness tracker application on a number of EC2 instances behind an Elastic Load Balancer. You have noticed some anomalies with the way the application is performing lately and would like to collect the application logs from all of your application servers into one central location. Which of the following will you need to do for each instance?","explanation":"You will need the agent installed and running as well as configuring permission for the EC2 instance role to send logs to CloudWatch - i.e. permission to CreateLogGroup, CreateLogStream, PutLogEvents and DescribeLogStreams","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html","title":"CloudWatch Logs & EC2"}],"answers":[{"id":"66f294ea95cd02688d654dd101e3f370","text":"Ensure the instance role associated with your EC2 instance has permission to write logs to S3","correct":false},{"id":"e089da1319f2968b0773412e2fd71897","text":"Ensure the instance role associated with your EC2 instance has permission to write logs to CloudWatch","correct":true},{"id":"e24633db195c7650668666e7e12e5d24","text":"Ensure the instance role associated with your EC2 instance has read permission for CloudWatch","correct":false},{"id":"0370efa52057a20e18cf2cd865cc462e","text":"Ensure the CloudWatch agent is installed and running on your EC2 instance","correct":true},{"id":"47bbd1c8d4e0c0c15af8e41856e95e45","text":"Ensure the CloudWatch agent has permission to write log files on your EC2 instance","correct":false}]},{"id":"d6508f82-f5de-4013-a777-d801a0816ff6","domain":"development","question":"A developer has a requirement to trigger a Lambda function once every 24 hours. What is the best way of achieving this requirement?","explanation":"CloudWatch Events allows targets to be triggered using a Schedule Expression. A Schedule Expression can define a rate; for example, every 24 hours. Or can accept a standard cron job expression.\n\nCloudWatch Events supports many targets, including Lambda.\n\nInvoking the function from a cron job running on an EC2 instance would meet the requirements, but would require additional effort and cost; therefore, is not the best solution.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html","title":"CloudWatch Events supported targets"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html","title":"CloudWatch Events Scheduling"}],"answers":[{"id":"9ad7f45031f00ab56d5450da9aaa7432","text":"Schedule the trigger in the Lambda Runtime Scheduler.","correct":false},{"id":"478234fe13f76893bfa2f3f57911bcb3","text":"Schedule a trigger in CloudWatch Events","correct":true},{"id":"bb3413b8174c30b6418449092039dfc2","text":"Add a message into SQS that invokes the Lambda function. As part of the Lambda function's code, add a new message into SQS to re-invoke the function with a DelaySeconds equal to 24 hours.","correct":false},{"id":"49b7adae681c4743b1eac8bb6cdaca00","text":"Invoke the Lambda function from a cron job running on an EC2 instance.","correct":false}]},{"id":"88a77a58-b901-4605-a657-98cdc90dff65","domain":"deployment","question":"A developer needs to compile Java code to produce a deployment artifact. Which Amazon service can the developer use for this task?","explanation":"Amazon CodeBuild is a service that compiles source code, runs tests, and produces software packages that are ready to deploy. Amazon CodeCommit is a source control service that hosts Git-based repositories. Amazon CodeDeploy is a deployment service that automates software deployments. Amazon CodePipeline is a continuous delivery service that helps you automate your release pipelines.","links":[{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild"}],"answers":[{"id":"07929940c38c1e9e91ecf67c01ccfa73","text":"CodeDeploy","correct":false},{"id":"5f6f48261d96567b3014b43c23382021","text":"CodePipeline","correct":false},{"id":"56c20df941e069e1a35a1871f662298a","text":"CodeCommit","correct":false},{"id":"f8d1a2a80933c71d9a65e3d825976bda","text":"CodeBuild","correct":true}]},{"id":"60de4860-8791-4fd6-b3af-364209ceb5ab","domain":"mon-trb","question":"You are working on an application for an online training company which stores product data in DynamoDB. This week, the company is running a big promotion on a few courses and this is bringing lots of new traffic to your website, causing an increased number of queries to the database.  Database queries are now running much slower than usual and the Operations Team are concerned that the DynamoDB table is being throttled. Which of the following approaches would you recommend to improve read performance?","explanation":"Using DAX is the recommended approach to reducing response times for read-intensive applications, applications which read a small number of items frequently and also applications which perform repeated reads against a large set of data. Read Replicas are not a feature of DynamoDB. Configuring the application to use scans instead is not an efficient solution.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html","title":"DynamoDB DAX"}],"answers":[{"id":"c9e5713b3f811b188f8770ffd016beaf","text":"Configure the application to use scans rather than queries and run multiple scans in parallel","correct":false},{"id":"7d691dd8fcbce5a87b1211b09b47541a","text":"Add a Read Replica and point the DynamoDB API calls at the Read Replica","correct":false},{"id":"df31657d8e3247caf5b704d87a7fced3","text":"Redesign your table to use a more distinct partition key to enable the I/O load to be more evenly distributed across partitions","correct":false},{"id":"1fdd30d4d2259438830cf9536f3da218","text":"Configure a DAX cluster and point the DynamoDB API calls at the DAX cluster","correct":true}]},{"id":"9382a270-2c44-45b2-95f3-79d0cf319120","domain":"mon-trb","question":"A developer has been tasked with enabling Access Logs on the Application Load Balancer that sits in-front of their web services. As part of this task, they must configure a location to which the logs are delivered.\n\nTo what AWS service can Access Logs from an Application Load Balancer be delivered?","explanation":"S3 is the only service supported by AWS for receiving ALB access logs.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html","title":"Application Load Balancer Access Logs"}],"answers":[{"id":"a8c600be214ced26950e704d39c3ca21","text":"CloudWatch Logs","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"594025cae6dfa6b9073dc25de93ddb56","text":"Kinesis","correct":false},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true}]},{"id":"5cf3b6d4-9a0e-4602-8dab-26e687207049","domain":"development","question":"Which of the following platforms are supported in ElasticBeanstalk?","explanation":"Elastic beanstalk supports common platforms like including Tomcat, Passenger, Puma and Docker","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html","title":"Elastic Beanstalk Supported Platforms"}],"answers":[{"id":"7345f7045e4668138112c100f25517a4","text":"JBoss","correct":false},{"id":"8f72e28063c30c7468fb6af4653f4f9c","text":"Tomcat","correct":true},{"id":"c5fd214cdd0d2b3b4272e73b022ba5c2","text":"Docker","correct":true},{"id":"bf2528a296adb62d041a7519aa77f248","text":"Passenger","correct":true}]},{"id":"802be8c8-07cc-48a7-94c8-21d785d18f5a","domain":"deployment","question":"Your organization is developing a CI/CD environment to improve software delivery of your applications. It has already adopted a plan to execute the various phases of the CI/CD pipeline from continuous integration to continuous deployment. There are now discussions around restructuring the team make-up to implement a CI/CD environment. How would you recommend creating developer teams as a best practice to support this change in the long run?","explanation":"AWS recommends organizing three developer teams for implementing a CI/CD environment: an application team, an infrastructure team, and a tools team. This organization represents a set of best practices that have been developed and applied in fast-moving startups, large enterprise organizations, and in Amazon itself. The teams should be no larger than groups that two pizzas can feed, or about 10-12 people. This follows the communication rule that meaningful conversations hit limits as group sizes increase and lines of communication multiply. Hiring an external consulting firm will not be beneficial in the long run. Setting up a single team is not best practice. AWS CodePipeline is a continuous integration and continuous delivery service for fast and reliable application and infrastructure updates and not used for team structuring.","links":[{"url":"https://d0.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"b943ddf4265197477c4036cdd230b033","text":"Hire an external consulting firm to build and manage the pipeline. Provide them with the proper IAM roles to access your AWS environment.","correct":false},{"id":"3e514092970cfe2b9a3ea7331a0828cf","text":"Use CodePipeline to manage your CI/CD environment and assign team members to own different phases within your CodePipeline.","correct":false},{"id":"d163f1cc494b3cde77122207a42d9de1","text":"Set up one team to own an operate all components of the CI/CD pipeline to consolidate tasks and improve efficiency.","correct":false},{"id":"f3054a7a017a533ace8f081538d0b664","text":"Set up an application team to develop applications. Set up an infrastructure team to create and configure the infrastructure to run the applications. Set up a tools team to build and manage the CI/CD pipeline.","correct":true}]},{"id":"51ffcd4d-d314-49e3-8b95-fbbcfa102515","domain":"deployment","question":"You are responsible for developing a mobile application that allows people to store and search for reviews of their favourite local restaurants. The application stores all ratings and restaurant data in a DynamoDB table. You need to decide which DynamoDB operations you will allow your users to run on the table and you would like to keep the cost of the application as low as possible as well as provide great performance for the users. Which of the following DynamoDB operations uses the least amount of throughput?","explanation":"A query is generally more efficient than a scan, because a scan returns the whole table. An eventually consistent operation is more efficient than a strongly consistent operation, because it uses fewer read capacity units.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html","title":"Scan Vs Query"}],"answers":[{"id":"9c2019be4d041eca1a3b9205da836a1d","text":"Eventually consistent query","correct":true},{"id":"ce8155fdac81b2571251c35d8d93141a","text":"Strongly consistent scan","correct":false},{"id":"f59a2010f3a6608dcca2419d85124f94","text":"Strongly consistent query","correct":false},{"id":"272dfa82a03b2bece6b417c774500919","text":"Eventually consistent scan","correct":false}]},{"id":"4834c926-4c2e-11ea-b77f-2e728ce88125","domain":"development","question":"You want to create a continuous delivery pipeline with a build tool recommended by your Project Manager. However, you anticipate your build project to be large and complex. Which of the following AWS services will enable you to orchestrate complex pipelines?","explanation":"True to its naming, AWS CodePipeline is what you must use with the recommended build tool to make it easier to create your delivery pipelines. CodeDeploy is used for automatically deploying code, not creating pipelines. CodeBuild is for building and testing code. Although Jenkins is a build tool, it is not an AWS service.","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html","title":"What Is AWS CodePipeline?"},{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"ff02d3f07f94fff5aa3bdd045211b9e2","text":"AWS CodeBuild","correct":false},{"id":"ebf976dafc560e096501509079ba441c","text":"AWS CodePipeline","correct":true},{"id":"7be6a0f173a965818e92c1df46c6626e","text":"AWS CodeDeploy","correct":false},{"id":"2e54334c0a5ce2e3e5a5845df3ab3ada","text":"Jenkins","correct":false}]},{"id":"43e8c8e5-d0e3-4502-b7e7-8c236a6625e3","domain":"mon-trb","question":"A company wants to monitor all traffic to a network interface on their bastion host. They wish to be alerted if there are more than 10 attempts to connect to the host via SSH within a one-hour time interval. What solution can the company employ to meet this requirement?","explanation":"VPC flow logs can be sent to CloudWatch Logs. A CloudWatch metric filter and alarm can be configured to send notifications when the specified criteria are satisfied. CloudTrail is not a supported destination for VPC flow logs. Amazon Inspector cannot be used to inspect network traffic in the way specified by the requirements. It performs vulnerability assessments on the host VM. Lambda functions cannot mount EBS volumes.","links":[{"url":"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html","title":"Amazon S3 Glacier Vault Lock"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-cwl.html#flow-logs-cwl-create-flow-log","title":"Creating a Flow Log That Publishes to CloudWatch Logs"}],"answers":[{"id":"343b93587a12ff88f4e59413fbbd5d96","text":"Install the Amazon Inspector agent on the bastion host. Configure CloudWatch alerts based on Amazon Inspector findings.","correct":false},{"id":"cdab2e49cfa676ebe99e79d8f77f49b3","text":"Create a Lambda function that mounts the bastion host EBS volume and sends logs to CloudWatch logs. Create a CloudWatch metric filter for destination port 22. Create a CloudWatch Alarm trigger.","correct":false},{"id":"6e51df099a849474d4791bce4aa25bb5","text":"Configure a VPC flow log with CloudWatch Logs as the destination. Create a CloudWatch metric filter for destination port 22. Create a CloudWatch Alarm trigger.","correct":true},{"id":"ecb0bcaac946feb8b9a9c2abcaace499","text":"Create a VPC flow log for the network interface with CloudTrail as the destination. Create a Lambda function that queries the CloudTrail logs for SSH login attempts. Trigger the Lambda function every 5 minutes with a scheduled CloudWatch event.","correct":false}]},{"id":"f8566b59-119b-4a20-9402-3f0b8ab8cf73","domain":"development","question":"You are developing a serverless retail application which includes a mobile app. All your product data is stored in DynamoDB, whilst the application itself runs on Lambda. The product catalogue is updated once every 6 months, to reflect seasonal stock and price updates. Each database read is 3KB in size and the application performs around 20 reads per second. Which of the following DynamoDB settings would you recommend?","explanation":"A read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. Eventually consistent reads provide greater throughput than strongly consistent. In this case the data changes infrequently, so eventually consistent is a good option.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ProvisionedThroughput.CapacityUnits.Read","title":"DynamoDB Provisioned throughput"}],"answers":[{"id":"b35bcd1ffd25ca5d2feccdfaf102e6bf","text":"Configure the table with 20 read capacity units","correct":false},{"id":"744a67c638333d2ab1dc7063094dae67","text":"Configure the table with 10 read capacity units","correct":true},{"id":"bfd40f3271d850d341d74d81e5e0208b","text":"Configure the table to use high performance reads","correct":false},{"id":"d983271d85e1d9a838703052106e6190","text":"Use eventually consistent reads","correct":true},{"id":"e35b7f703854cc79bba03d472f38eaca","text":"Configure your application to use a query rather than a scan","correct":false},{"id":"1cff2c2aa83b7775e9cd6b16efc7c78a","text":"Use strongly consistent reads","correct":false}]},{"id":"44e023dc-183c-4d0a-8c35-a1779dcd7437","domain":"refactoring","question":"Your organization wants you to lead a development project that will perform real-time processing. The application requires the analytics and field teams to respond promptly to emerging situations based on server activity, website clicks, geo-location of devices, people, and service usage. As the development lead, what combination of services would you recommend to build out this project most efficiently and cost-effectively?","explanation":"Streaming data capture and processing is called real-time processing. The best AWS solution in this case is Amazon Kinesis. You can process data captured and stored with Kinesis sequentially and incrementally on a record-by-record basis or over sliding time windows, and use the processed data for a wide variety of analytics including correlations, aggregations, filtering, and sampling. Use AWS Lambda to process streaming data in real time. Lambda can process the data directly from Kinesis Streams, and lets you run code without provisioning or managing servers with help reduce costs. Running regular queries on Redshift is inefficient and expensive. S3 buckets will be able to store data, but won't be able to process data in real-time as efficiently as Kinesis. Amazon Aurora is a relational database solution and does not fulfill the real-time processing requirement.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html","title":"Amazon Kinesis Data Streams Terminology and Concepts"}],"answers":[{"id":"f255642584a78d5f043cde30cd580abc","text":"Use Amazon Aurora to store data in real time. Aurora will automatically replicate the data in multiple Availability Zones. Build an application on EC2 that users can call using APIs to retrieve the relevant information they need.","correct":false},{"id":"24f2f053a48d8b02ceaae4dc44a376ca","text":"Store the data on Amazon Redshift. Run queries on the Redshift cluster regularly to refresh a dashboard built on Amazon QuickSight.","correct":false},{"id":"5a50b0d5b285340be18f4a4c59ba1fd2","text":"Store all data in an S3 bucket with the correct prefixes. Develop Lambda functions for each prefix that will routinely scan and extract necessary information to another S3 bucket that will be the source for an Amazon QuickSight dashboard.","correct":false},{"id":"5d7d039072621b361ce8bde466f0efc4","text":"Use Amazon Kinesis to capture and store streaming data. Process streaming data with Lambda.","correct":true}]},{"id":"1a6ef8eb-4675-4004-ab47-3f8682a6e038","domain":"security","question":"Your EC2 instance needs to access files located in an S3 bucket, what is the best way to enable access?","explanation":"Using an IAM role associated with the EC2 instance is the recommended way, storing credentials locally is not recommended.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html","title":"IAM Roles"}],"answers":[{"id":"246491a279b8f11f71b3c083962c0e75","text":"Create a new IAM role and grant read access to S3. Store the role's credentials locally on the EC2 instance and configure your application to supply the credentials with each API request","correct":false},{"id":"f47981b92302bc40c2c9dc5f0ec40a23","text":"Create an IAM role with read access to S3 and assign the role to the EC2 instance","correct":true},{"id":"6642e43e9a2808c204a04b7c07a1f5ac","text":"Create a new IAM user and grant read access to S3. Store the user's credentials locally on the EC2 instance and configure your application to supply the credentials with each API request","correct":false},{"id":"7e00e264a60465681fb267faf13307e9","text":"Configure a bucket policy which grants read access based on the EC2 instance name","correct":false}]},{"id":"8856df48-5866-4ee3-a5a7-2033444e21eb","domain":"security","question":"You have provisioned an RDS database and then deployed your application servers using Elastic Beanstalk. You now need to connect your application servers to the database. What should you do?","explanation":"As you are connecting to a database that was not created within your Elastic Beanstalk environment, you will need to create the Security Group yourself and also provide connection string and credentials to allow your application servers to connect to the database","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html#rds-external-defaultvpc","title":"Elastic Beanstalk And RDS"}],"answers":[{"id":"50021ae41e50f1b10069ebcccab3c0ef","text":"Provide the database connection information to your application","correct":true},{"id":"24e204c62e09f215959e144cbb8611d4","text":"Configure a security group allowing access to the database and add it to your environments auto-scaling group","correct":true},{"id":"1aa1798c4c20c22832e9a949af74ada3","text":"Configure Elastic Beanstalk to install a database client on your application servers","correct":false},{"id":"26b897f7474a08158506b32e7c566323","text":"Provide the ip address of the RDS instance to Elastic Beanstalk","correct":false}]},{"id":"86524d3b-b3e8-46ca-97a2-e12d4edfabed","domain":"development","question":"Which of the following best describes Amazon ECS?","explanation":"ECS stands for Elastic Container Service: It manages running containers on your EC2 instances. It does not act as a scheduler and it is neither serverless nor software that you manage.","links":[{"url":"http://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html","title":"About Amazon ECS"}],"answers":[{"id":"8180f2f15ee5ff1c554855a0352e23bf","text":"The Elastic Container Scheduler is software that you can run and manage to orchestrate many running Docker containers.","correct":false},{"id":"a966ed9a7fc5f750acbbef6754f3ad57","text":"The Elastic Container Scheduler is a serverless system to manage running many Docker containers in a flexible and cost-effective way.","correct":false},{"id":"f0fbcb4f668b638f39ce41a6972d9a94","text":"The Elastic Container Service is software that you can run and manage to orchestrate many running Docker containers.","correct":false},{"id":"d3958d5a87a697631979b920c68a9ae2","text":"The Elastic Container Service is a service that manages running Docker containers on a group of your EC2 instances.","correct":true}]},{"id":"764c68cc-0596-485a-b5a0-4bb272444d02","domain":"deployment","question":"Which of the following services enables you to automatically build, test and release new software whenever a developer makes an update to their code?","explanation":"CodeBuild only builds your code, it won't deploy it to your environment. CloudFormation is used to deliver Infrastructure As Code. CodeCommit manages your source code. CodeDeploy can be used to deploy code, but in isolation, it cannot create an automated release process. CodePipeline automates the build, test, and can be used to deploy phases of your release process every time there is a code change, based on the release model you define.","links":[{"url":"https://aws.amazon.com/codepipeline/","title":"AWS CodePipeline"}],"answers":[{"id":"f8d1a2a80933c71d9a65e3d825976bda","text":"CodeBuild","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":false},{"id":"5f6f48261d96567b3014b43c23382021","text":"CodePipeline","correct":true},{"id":"07929940c38c1e9e91ecf67c01ccfa73","text":"CodeDeploy","correct":false},{"id":"56c20df941e069e1a35a1871f662298a","text":"CodeCommit","correct":false}]},{"id":"dc3e7895-b954-4fa8-8d4f-faffeca401d2","domain":"security","question":"Which of the following methods will allow you to *securely* upload/download your data to the S3 service? Pick all that apply.","explanation":"You can securely upload/download your data to Amazon S3 via SSL endpoints using the HTTPS protocol.","links":[{"url":"https://aws.amazon.com/s3/faqs/#security","title":"S3 Security"}],"answers":[{"id":"1019a747b087f11f97ef6a2bf46a1978","text":"HTTP endpoints using HTTPS protocol","correct":true},{"id":"d7ad40729fa333427d4d8c3032d43fdf","text":"SSL endpoints using HTTP protocol","correct":false},{"id":"937a8b8e84ad5481f1983a1842154e18","text":"HTTP endpoints using HTTP protocol","correct":false},{"id":"2b716d646634dd42d3d1ab628b210081","text":"SSL endpoints using the HTTPS protocol","correct":true}]},{"id":"b668531f-edd2-43f5-bf40-b7e68dad0d08","domain":"development","question":"A developer is working on a new green field project within an organization. The developer has been asked to recommend what technology could be used if the project is to be deployed with Elastic Beanstalk.\n\nWhich of the following platforms could the developer recommend for the project to meet its requirements?","explanation":"Elastic Beanstalk currently supports Docker, Ruby, and Go (amongst others). It does not support Perl or Swift.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html","title":"Elastic Beanstalk Supported Platforms"}],"answers":[{"id":"9916d1fc59fe22cc046a2fe1615bc764","text":"Ruby","correct":true},{"id":"0114ad06d728f1834e36fe1a39574ef4","text":"Perl","correct":false},{"id":"c5fd214cdd0d2b3b4272e73b022ba5c2","text":"Docker","correct":true},{"id":"ae832e9b5bda2699db45f3fa6aa8c556","text":"Swift","correct":false},{"id":"5f075ae3e1f9d0382bb8c4632991f96f","text":"Go","correct":true}]},{"id":"a911b79a-68c9-41f0-af02-0f57c3ccdc66","domain":"mon-trb","question":"A company's services are protected by AWS WAF. The development team would like to enable logging on the WAF to get detailed information about traffic that is analyzed by the web ACLs in order to enhance their troubleshooting efforts. Which service can the team use to collect AWS WAF logs?","explanation":"In order to enable and configure AWS WAF logs, a Kinesis Data Firehose is required for delivery of the logs to the destination.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/logging.html","title":"Logging Web ACL Traffic Information"}],"answers":[{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"33eb5c1f2566526637e791c925c4c505","text":"VPS Flow Logs","correct":false},{"id":"72e5e39b79c3d4d99d9c68a6a5e4d9f0","text":"Kinesis Data Firehose","correct":true},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false}]},{"id":"e362623f-0040-49f7-8b76-f0d7484d7ade","domain":"mon-trb","question":"You have a distributed application which is made up of a number of different Lambda functions as well as API gateway endpoints and DynamoDB tables. You have noticed that the application is running unusually slowly today. Which of the following tools would be the best choice to help identify what is going on?","explanation":"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a micro-service architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors.","links":[{"url":"https://aws.amazon.com/xray/","title":"What Is X-Ray?"}],"answers":[{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"8db8c2b0bbd0ff71d1d15bb32f69e3b8","text":"VPC Flow Logs","correct":false},{"id":"0863526e514d88cd2d00b39f8dda0920","text":"X-Ray","correct":true},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false}]},{"id":"6f11e8ea-c824-427c-9cf9-7bebec4fe4b6","domain":"development","question":"Where should the appspec.yml be stored?","explanation":"The AppSpec file (appspec.yml) must always be in the root or your application source directory otherwise the deployment will not work. The .ebextensions folder is used to set custom environment variables in Elastic Beanstalk, not CodeDeploy.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html","title":"CodeDeploy AppSpec File"}],"answers":[{"id":"0a2d3304fa88233c14e8c9ebffba0882","text":"In the .ebextentions folder","correct":false},{"id":"57f3232fcb44408176cbdb5c8b7e4b06","text":"In /opt","correct":false},{"id":"8ee84dac450c4097f3cce035b8ede57d","text":"In the config directory in your application source directory","correct":false},{"id":"b007c30133faeba98cd0fbcedbf4fa6c","text":"In the root of your application source directory","correct":true}]},{"id":"f956f10b-d615-4d75-a2a4-dd978268221b","domain":"deployment","question":"You are a developer running an application on AWS Elastic Beanstalk. You are implementing an application update and need to use a deployment policy. The requirements are to maintain full capacity, deploy five instances at once for the new version, and to terminate instances running the old version once the new instances are running successfully. How would you implement this deployment policy?","explanation":"To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances. All at Once deployment takes the instances in your environment out of service for a short time. Rolling deployment also takes a batch of servers out of service while deploying the new version in batches. Blue/Green deployments are for cases when you want to have two versions live simultaneously and be able to swap between the two versions.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html","title":"Deployment Policies and Settings"}],"answers":[{"id":"da598c0ec61581331132eb0291fa30cd","text":"Set the deployment policy as Blue/Green. Set the timeout as 900, and the batch size as 5.","correct":false},{"id":"ee833ae676b93a46ca5aafc9ce391102","text":"Set the deployment policy as Rolling. Set the batch size as 5.","correct":false},{"id":"4a909b7335924524609c65e522fe581a","text":"Set the deployment policy as All at Once. Set the batch size type as Fixed, and the batch size as 5.","correct":false},{"id":"e693d0e841ddc0314ab81115baff0563","text":"Set the deployment policy as Rolling with Additional Batch. Set the batch size type as Fixed, and the batch size as 5.","correct":true}]},{"id":"1116e527-edc2-4085-83cb-c9e079216e32","domain":"deployment","question":"You receive the following response from STS, What is happening here? \r\r <AssumeRoleWithWebIdentityResult> \r\r <SubjectFromWebIdentityToken>amzn1.account.AF6RHO7KZU5XRVQJGXK6HB56KR2A</SubjectFromWebIdentityToken> \r\r <Audience>client.5498841531868486423.1548@apps.example.com</Audience> \r\r <AssumedRoleUser> \r\r <Arn>arn:aws:sts::123456789012:assumed-role/FederatedWebIdentityRole/app1</Arn> \r\r <AssumedRoleId>AROACLKWSDQRAOEXAMPLE:app1</AssumedRoleId> \r\r </AssumedRoleUser> \r\r <Credentials> \r\r <SessionToken>AQoDYXdzEE0a8ANXXXXXXXXNO1ewxE5TijQyp+IEXAMPLE</SessionToken> \r\r <SecretAccessKey>wJalrXUtnFEMI/K7MDENG/bPxRfiCYzEXAMPLEKEY</SecretAccessKey> \r\r <Expiration>2014-10-24T23:00:23Z</Expiration> \r\r <AccessKeyId>ASgeIAIOSFODNN7EXAMPLE</AccessKeyId> \r\r </Credentials> \r\r <Provider>www.amazon.com</Provider> \r\r </AssumeRoleWithWebIdentityResult> \r\r <ResponseMetadata> \r\r <RequestId>ad4156e9-bce1-11e2-82e6-6b6efEXAMPLE</RequestId> \r\r </ResponseMetadata> \r\r </AssumeRoleWithWebIdentityResponse>","explanation":"STS AssumeRoleWithWebIdentity returns a set of temporary security credentials for users who have been authenticated in a mobile or web application with a web identity provider. Example providers include Amazon Cognito, Login with Amazon, Facebook, Google, or any OpenID Connect-compatible identity provider. See the link below for an explanation of the sample response.","links":[{"url":"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html","title":"STS AssumeRoleWithWebIdentity - Sample Response"}],"answers":[{"id":"0bbbde1281c67ded59674ac672162af8","text":"STS is returning temporary security credentials to a user who has successfully authenticated with a web identity provider.","correct":true},{"id":"f8c4ec8e47f0236f83e5f637262e037e","text":"The user with the following ARN is given access to the application: arn:aws:sts::123456789012:assumed-role/FederatedWebIdentityRole/app1","correct":false},{"id":"b5df8e29437333169d6d6dd6dfa9da80","text":"The user has requested permission to assume the following role: client.5498841531868486423.1548@apps.example.com","correct":true},{"id":"d3562fb1d308ca43930bef1cd0c73dae","text":"The web identity token that was passed could not be validated by AWS.","correct":false},{"id":"2b7d1e4142953fa9b0c10630830050ba","text":"The user is allowed to assume the following role: arn:aws:sts::123456789012:assumed-role/FederatedWebIdentityRole/app1","correct":true}]},{"id":"098e4897-54dd-493d-ae35-d28374c03576","domain":"security","question":"You application can be accessed using multiple devices, for example, laptop, tablet, iPhone or Android devices. You would like to be able to identify and track when your users access your site using different devices. Which of the following AWS technologies can enable you to do this?","explanation":"Cognito enables developers to remember the devices on which end-users sign in to their application. You can see the remembered devices and associated metadata through the console. In addition, you can build custom functionality using the notion of remembered devices. For example, with a content distribution application (e.g., video streaming), you can limit the number of devices from which an end-user can stream their content.","links":[{"url":"https://aws.amazon.com/blogs/mobile/tracking-and-remembering-devices-using-amazon-cognito-your-user-pools/","title":"Tracking and Remembering Devices Using Amazon Cognito"}],"answers":[{"id":"0d6127433a099d6824bfd30c14ad4096","text":"Create a unique user ID and associate it with the device metadata","correct":false},{"id":"5b46051451752459a684abff24663b39","text":"Store a unique session ID in ElastiCache","correct":false},{"id":"8204565cad3d39f0897a0ce85182016f","text":"Store a unique session ID in DynamoDB","correct":false},{"id":"6cbb574c9488bb59fdd64fa8f509fce8","text":"Use Cognito","correct":true},{"id":"95e34ddc7b57bc86318fc7a45f0d2e2d","text":"Use a Lambda function to store a unique device ID in DynamoDB and associate it with the user session ID","correct":false}]},{"id":"a4d334f5-b2b6-4d3c-94f4-2b39dce36264","domain":"security","question":"You work for a large pharmaceuticals company which is conducting drug trials for a number of new products. You are using SQS to handle messaging between components of a distributed application. You need to ensure that confidential data relating to your patients is encrypted, which of the following services will you use to centrally rotate the encryption keys?","explanation":"You can use a CMK to encrypt and decrypt up to 4 KB (4096 bytes) of data. Typically, you use CMKs to generate, encrypt, and decrypt the data keys that you use outside of AWS KMS to encrypt your data. This strategy is known as envelope encryption. CMKs are created in AWS KMS and never leave AWS KMS unencrypted. To use or manage your CMK, you access them through AWS KMS.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys","title":"KMS Concepts"}],"answers":[{"id":"1562bb9d2d9567740605dcb1ccab5c80","text":"SSE-S3","correct":false},{"id":"0e8433f9a404f1f3ba601c14b026d321","text":"HTTPS","correct":false},{"id":"69500631fd60fd52ad3d029cb8aa50e0","text":"AWS KMS","correct":true},{"id":"97029ab64afac0842857a7805cc7df88","text":"Client-side encryption","correct":false}]},{"id":"f7fd0d7a-5d45-4f72-aa8e-d9a65270360f","domain":"refactoring","question":"You are developing an online hotel booking application which makes an number of requests to different back end applications to get quotes for travel related add-on services. You are using API gateway handle all the API calls and you notice that the majority of requests are for the same 5 or 6 services. How can you optimize the configuration to ensure the best performance for your application?","explanation":"You can enable API caching to cache your endpoint's responses, this reduces the number of calls made to your endpoint and improves the latency of requests to your API.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"API Caching"}],"answers":[{"id":"9a86454fefc71c7430655ce2e18ac716","text":"Configure a CloudFront CDN in front of the API Gateway to cache the most frequent HTTP requests","correct":false},{"id":"b255bca6117b1096664ab7b1415fae80","text":"Add an ElastiCache cluster in front of your database to cache the most frequently accessed data","correct":false},{"id":"59a69a5bb20a3dbe9214ef93c38041f7","text":"Configure auto-scaling for the API Gateway","correct":false},{"id":"336a729744a0b4682222e3a4a1cdc750","text":"Implement API Caching to cache the endpoint's response for the most popular requests","correct":true}]},{"id":"c2d80b09-07e7-4c6c-9b1e-c8dad1c9b669","domain":"security","question":"You are performing an audit of your IAM policies. Which of the following tools will enable you to identify which specific statement in a policy results in allowing or denying access to a particular resource or action?","explanation":"With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies attached to IAM users, groups, or roles in your AWS account. You can test which actions are allowed or denied by the selected policies for specific resources.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html","title":"Testing IAM Policies with the IAM Policy Simulator"}],"answers":[{"id":"b7dd1a111a9c83018d8060d8c22f36d8","text":"IAM Access Control Simulator","correct":false},{"id":"c5dcf026a75a9a24668c13a5526e1cc1","text":"IAM Statement Simulator","correct":false},{"id":"b6632fa69795d5fabc908fe75210b177","text":"IAM Policy Simulator","correct":true},{"id":"4240f8a0b958c209f10161d60a22e254","text":"IAM Role Simulator","correct":false},{"id":"b62fc6bfda9075d9b4468c476f1d9cfe","text":"IAM Permission Simulator","correct":false}]},{"id":"3a01c37a-6939-444b-89b0-f73b1f232601","domain":"deployment","question":"You are developing a social media messaging and photo-sharing application which consists of a web front end, with persistent data stored in S3 and RDS. Which of the following instance pricing models should you choose to make running this application as cost-effective as possible?","explanation":"Reserved instances provide a significant discount compared to running instances On-Demand. You can take advantage of Spot Instances to run and scale applications such as stateless web services, image rendering, big data analytics, and massively parallel computations. Dedicated Instances are Amazon EC2 instances that run in a VPC on hardware that is dedicated to a single customer.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/when-to-use-spot-instances.html","title":"When To Use Spot Instances"},{"url":"https://aws.amazon.com/ec2/pricing/reserved-instances/","title":"When To Use Reserved Instances"},{"url":"https://aws.amazon.com/ec2/pricing/dedicated-instances/","title":"When To Use Dedicated Instances"},{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/when-to-use-spot-instances.html","title":"Cost Optimization White Paper"}],"answers":[{"id":"04be777131f511081730fe2b34662123","text":"Use dedicated instances for the web servers","correct":false},{"id":"a057af2fca16fcc4c6997976ea994e95","text":"Use Spot instances for the web servers","correct":true},{"id":"60e31915c1c4910787bf03781555aa97","text":"Use reserved instances for the database","correct":true},{"id":"86ed636cf8296c0bedacc99ec10c39f0","text":"Use dedicated instances for the database servers","correct":false},{"id":"1b65fa924bf38ff7e68d41f3495434bd","text":"Use reserved instances for the web servers","correct":false},{"id":"7e5d7958c67ef51985894e5f8b2a25a8","text":"Use Spot instances for the database","correct":false}]},{"id":"4b9c267c-f41d-4325-919e-7863e0abb6f3","domain":"development","question":"A three-tier web application is deployed using CloudFormation template. How can the CloudFormation developer ensure that the database resource is saved for backup purposes upon stack deletion?","explanation":"The DeletionPolicy attribute can be used to preserve a specific resource when its stack is deleted. The DeletionPolicy Retain option can be used to ensure AWS CloudFormation keeps the resource without deleting the resource.  The Stack Termination Protection feature enables protection against accidental deletion of an entire stack, not preservation of a specific resource. Similarly, the 'cloudformation:DeleteStack' Action applies to entire stack(s).","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html","title":"DeletionPolicy Attribute"}],"answers":[{"id":"8093dacb1a766d0f91fa60e48baa87ed","text":"Set Stack Termination Protection to Enable.","correct":false},{"id":"eee064b70f79422e8511f18b251253ef","text":"Create IAM Policy with Effect of Deny for 'cloudformation:DeleteStack' Action.","correct":false},{"id":"558cc27970e2b64a29bdc85f381b8cb9","text":"Set the DeletionPolicy to Retain in the CloudFormation template.","correct":true},{"id":"6983aeb4c0d42bb293c8ec93966aedbb","text":"Set the DeletionProtection to True in the CloudFormation template.","correct":false}]},{"id":"f83e6c6c-f206-4918-8457-d6bef8d4a555","domain":"deployment","question":"You have deployed a new version of your Lambda function, however the Application Support team have reported a number of issues with the new code. What is the easiest way to fix this?","explanation":"You can map an alias to different versions of the same function, allowing for easy roll back to a previous version","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda Versions"}],"answers":[{"id":"bb979a7d188850e77ff0bc3293db2877","text":"Roll back to a previous version by updating your PROD alias to point to the previous version of the function","correct":true},{"id":"8ab78c3f557d33ed014a6819ff45cf93","text":"Troubleshoot the issue using X-Ray, then redeploy an updated version of the function","correct":false},{"id":"80718f268b030c98e8cb509f28eebee4","text":"Roll back by restoring the original function from an EBS snapshot","correct":false},{"id":"1275e2de1d5df0a79ed92417584f94e9","text":"Delete the CloudFormation stack and redeploy using the previous version","correct":false}]},{"id":"2863369f-3c52-497f-887b-1e6d5a13e5fe","domain":"security","question":"A developer needs to share an EBS volume with a second AWS account. What actions need to be performed to accomplish this task in the most optimal way?","explanation":"It is not possible to directly share an EBS volume with another account. In order to accomplish the required task, it is required to create an EBS volume snapshot and grant permissions to that snapshot to the second AWS account. Although EBS volume snapshots are stored in S3, they are not in a user-visible bucket. Sharing a private AMI with a second account does not meet the specific requirement as defined in the question.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html","title":"Sharing an Amazon EBS Snapshot"}],"answers":[{"id":"823ae734c13e3b7ed146146637c1df42","text":"Create an EBS volume snapshot. Modify EBS snapshot permissions and add the second AWS account ID to share the snapshot. In the second AWS account, create an EBS volume from the snapshot.","correct":true},{"id":"f2cb6e39eb1e99f97c84895d0c6b0cc8","text":"Create an IAM policy granting necessary actions on the specific EBS volume. Add the second AWS account ID in the Principal element.","correct":false},{"id":"683e6c4bef943d4c2e5871fcc39e1ebd","text":"Create an AMI from the EC2 instance. Modify image permissions and add a second AWS account ID to share the AMI. Ensure 'create volume' permissions are added. In the second AWS account, create an EC2 instance using the shared AMI.","correct":false},{"id":"d477177fe8b8bb5a16854f7c5632856d","text":"Create an EBS volume snapshot. Modify S3 bucket policy granting the second AWS account access to the S3 object of the snapshot. In the second AWS account, create an EBS volume from the S3 object.","correct":false}]},{"id":"ac8156df-b2ea-447a-a070-346b3a5f7234","domain":"security","question":"You are working on an application which handles online credit card applications. It consists of a number of web and application servers running on EC2, customer reference data stored in S3 and transactional data stored in RDS. The security team have noticed that you have a lot of sensitive customer information stored in S3 and you have been asked to configure encryption at rest to protect the data. How can you do this?","explanation":"You can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket. When you use server-side encryption, Amazon S3 encrypts an object before saving it to disk in its data centers and decrypts it when you download the objects.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html","title":"S3 Default Encryption for S3 Buckets"}],"answers":[{"id":"349aac318f19b866d43ee7ad35140312","text":"Use SSL to upload the files","correct":false},{"id":"19fb0f18150f5096d8b66cea58825925","text":"Select default encryption on your S3 bucket","correct":true},{"id":"f7d3a4d76ebd5e4e87874e392c6ee6da","text":"Encrypt the files locally using the AWS Encryption SDK","correct":false},{"id":"46cf5e7bd245efaa706dc8411b60bf17","text":"Encrypt your local root disk before uploading the files","correct":false}]},{"id":"52bc2a05-9de5-46cb-ab80-feb67dadc6d9","domain":"deployment","question":"What is the maximum size of an S3 object?","explanation":"The minimum size of an object is 0 bytes (empty or 'touched' files are permitted) and the maximum size of an object is 5TB.","links":[{"url":"https://aws.amazon.com/s3/faqs/#general","title":"How Much Data Can I Store?"}],"answers":[{"id":"47a5e6e39f64ff5d5dfb25da1aa3f3f2","text":"500 GB","correct":false},{"id":"6aaa2ffc817cac34a806a149bee36350","text":"50 GB","correct":false},{"id":"951b67b6006b1e714e7d8b65b90d56b5","text":"50 TB","correct":false},{"id":"b3dd42c7956751be5d38b5c0cb2e09c0","text":"5 TB","correct":true}]},{"id":"a45d7c37-eb4b-4a39-9fb2-d5298cb40491","domain":"security","question":"You work for a large I.T. recruitment company that are launching a mobile application which will allow job seekers to apply for jobs online and attach their rsum to their application. Users will be able to log in to their account using Facebook and the application stores their contact and profile details in a DynamoDB table. Which of the following approaches would you recommend for enabling the users to gain access to view and update their data?","explanation":"With Web Identity Federation, you don't need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google. For most Web Identity Federation scenarios, we recommend that you use Amazon Cognito because it acts as an identity broker and does much of the federation work for you.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html","title":"Web Identity Federation"}],"answers":[{"id":"e0bc3be9eb85e3ef437aac25d15f5be4","text":"Allow customers to embed user credentials in settings of the mobile app","correct":false},{"id":"3b5209dda18fcdea46a196b06d17c586","text":"Configure Web Identity Federation with Cognito","correct":true},{"id":"f11ad00f139ee7fd7ecde821e3769c1a","text":"Configure cross-account access between the mobile app and DynamoDB ","correct":false},{"id":"ff1fd8b56d2c836edd2795619fa9b681","text":"Configure Web Identity Federation with ADFS","correct":false}]},{"id":"bd31be6d-dcf3-4103-b281-72d44c552186","domain":"refactoring","question":"You are working on a social media application which allows users to share BBQ recipes and photos. You would like to schedule a Lambda function to run every 10 minutes which checks for the latest posts and sends a notification including an image thumbnail to users who have previously engaged with posts from the same user. How can you configure your function to automatically run at 10 minute intervals?","explanation":"You can direct AWS Lambda to execute a function on a regular schedule using CloudWatch Events. You can specify a fixed rate - for example, execute a Lambda function every hour or 15 minutes, or you can specify a cron expression.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html","title":"Using AWS Lambda with Amazon CloudWatch Events"}],"answers":[{"id":"547b2838c15c420120f6d664b99d6dbb","text":"Use CloudWatch Events to schedule the function","correct":true},{"id":"2d428d9cc2b8ba9e3f0fb6f7b7c5d585","text":"Use AWS SWF to schedule the function","correct":false},{"id":"2255149fa723010e237589bd73a85d2d","text":"Use EC2 with cron to schedule the function","correct":false},{"id":"34d04b0eb9dd5f8f69d5c60eb65f6bc8","text":"Use Lambda with cron to schedule the function","correct":false}]},{"id":"5d9b4539-7320-47c6-82c5-4552d4b3bf29","domain":"development","question":"You are developing an application using multiple AWS services. You need to find a solution to decouple the application components, so that they can fail independently of one another. Which of the following AWS services will enable this?","explanation":"SQS is a fully managed message queuing service that enables you to decouple and scale micro-services, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware, and empowers developers to focus on differentiating work.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html","title":"SQS Developer Guide"}],"answers":[{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":false},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":false},{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":false},{"id":"ef4ba2f338cdf4b615ed280ff0b2777d","text":"SQS","correct":true}]},{"id":"0471ebc4-d106-4e3d-a3d4-0f594589ad5f","domain":"deployment","question":"How long can a message be retained in an SQS Queue?","explanation":"Messages will be retained in queues for up to 14 days.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-how-it-works.html#sqs-basic-requirements","title":"SQS - Basic Prerequisites"}],"answers":[{"id":"947d8520f04473da621f2718138f3bc6","text":"30 days","correct":false},{"id":"0e92d2ae86e7d3e8eece27b399af6ea3","text":"14 days","correct":true},{"id":"e3b481d5297f475abc283227bedbd9b9","text":"1 day","correct":false},{"id":"cb8f14fd3a41cfe1236a3c6b90077ca0","text":"7 days","correct":false}]},{"id":"2d9b36ae-20bb-4dfd-94b0-0956312b6b90","domain":"development","question":"Which of the following specifies the correct run order for lifecycle hooks for an In-Place deployment using CodeDeploy?","explanation":"The logical sequence is: ApplicationStop, BeforeInstall, AfterInstall, ApplicationStart","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order","title":"In-Place Deployment Run Order of Hooks"}],"answers":[{"id":"ab9490190c13b314076b0343b84e008c","text":"BeforeInstall, ApplicationStop, AfterInstall, ApplicationStart","correct":false},{"id":"274e4d32692fa00bae2f7aceb66b5ee3","text":"ApplicationStop, BeforeInstall, ApplicationStart, AfterInstall","correct":false},{"id":"20c91632d36d53294ddd6fa1f3ee974f","text":"ApplicationStop, BeforeInstall, AfterInstall, ApplicationStart","correct":true},{"id":"c4048c2c844fbbab9e3eca808aabd8fe","text":"BeforeInstall, AfterInstall, ApplicationStop, ApplicationStart","correct":false}]},{"id":"c73f812b-373b-4429-9a32-a3d71186c137","domain":"deployment","question":"Your application needs 100 strongly consistent reads on items that are 9KB in size every second. How many units of read capacity units should you provision?","explanation":"9KB rounds up to 12KB. 12KB/4KB=3 strongly consistent read capacity units each. 3*100=300 strongly consistent read capacity units.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html","title":"Throughput Capacity for Reads and Writes"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html","title":"DDB - Read Consistency"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CapacityUnitCalculations.html","title":"Calculating CU"}],"answers":[{"id":"3644a684f98ea8fe223c713b77189a77","text":"200","correct":false},{"id":"7ef605fc8dba5425d6965fbd4c8fbe1f","text":"150","correct":false},{"id":"9de6d14fff9806d4bcd1ef555be766cd","text":"350","correct":false},{"id":"94f6d7e04a4d452035300f18b984988c","text":"300","correct":true}]},{"id":"007a648d-faa9-4464-9816-0b646386047f","domain":"mon-trb","question":"A three-tier application consists of a presentation and application tier deployed on EC2 instances in a public VPC subnet, and a data tier hosted on an RDS databases in a private VPC subnet.  When attempting to establish a connection to the RDS database, the application times out.  What could be the source of this problem?","explanation":"A VPC Network Security Groups are a network security capability providing firewall functionality.  They control inbound and outbound traffic on EC2 or RDS instances. NSG rules applied on the RDS database must be configured to allow inbound traffic from the NSG on the EC2 instances on the database port. If the NSG's on the RDS database are not configured correctly, any connection attempting to access the RDS instance will time out as the database will be unreachable. If database credentials were incorrect, the application would not time out. A connection to the RDS instance would be established, an the database would return an error code.  The question specifically states that there is an issue establishing a connection. VPC peering is used to establish a secure connection between two VPCs, and is not suitable in this scenario. Similarly, Internet Gateway is used in public subnets to route outbound traffic to the internet and is not relevant to the situation.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html","title":"Controlling Access with Security Groups"}],"answers":[{"id":"2954caaf920be59b173a0e00624c47f0","text":"VPC Peering is not configured properly.","correct":false},{"id":"77a29c8aaf1cae7a01a6aa1960c59878","text":"Database credentials are incorrect.","correct":false},{"id":"caf6ad7f31cbd20a32d3eea156a6cffb","text":"The public subnet does not have Internet Gateway configured.","correct":false},{"id":"b7adf16356fbe2a22cd0c74e0b333b26","text":"Database NSG is not configured to allow traffic from EC2 instances.","correct":true}]},{"id":"bfec04c8-32f9-42df-b7de-1ba954709dff","domain":"refactoring","question":"Using the AWS console, you are trying to Scale DynamoDB past its pre-configured maximums. Which service can you increase by raising a ticket to AWS support?","explanation":"Provisioned throughput limits","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#default-limits-capacity-units-provisioned-throughput","title":"Capacity Units and Provisioned Throughput"}],"answers":[{"id":"3c11d73c8083e3aae0a758891afe3a0a","text":"Item Sizes","correct":false},{"id":"1ed115474e3650b86e2a64fa03839943","text":"Global Secondary Indexes","correct":false},{"id":"eb1167f3583beb42c4f2e414033c5ae6","text":"Local Secondary Indexes","correct":false},{"id":"657289d6192c6cf5d405aa8b4596bd21","text":"Provisioned throughput limits","correct":true}]},{"id":"364e3d34-4218-44f0-8a7b-40165724cfd8","domain":"development","question":"You want to deploy a docker image to ECS, which command would you use to add your image to the Elastic Container Registry?","explanation":"The docker push command will push the image to the registry","links":[{"url":"https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html","title":"Pushing an Image to ECR"}],"answers":[{"id":"6a100fcee87852662d457afaa64af06f","text":"docker push","correct":true},{"id":"8e404964b0aacdf31fe02c48913ff3bd","text":"docker pull","correct":false},{"id":"1915a0cd1649e80f5076619cfbc3f51d","text":"docker build","correct":false},{"id":"9277ffcc0c6be41f97f1aa08b1217cb0","text":"docker tag","correct":false}]},{"id":"3efc0266-4c2e-11ea-b77f-2e728ce88125","domain":"deployment","question":"Which of the following statements are true about the concept of blue/green deployment regarding development and deployment of your application?","explanation":"With blue/green deployment, you can shift traffic between two identical environments that are running different versions of your application. It allows you to easily deploy changes to your application and roll-back on changes very quickly.","links":[{"url":" https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf","title":"Blue/Green Deployments on AWS"}],"answers":[{"id":"3dae7ad4a3084f0fa3d2598654de9066","text":"The blue environment represents the current version of your application serving production traffic.","correct":false},{"id":"69da7fc6d495def048f27d73e7e55e82","text":"It allows you to shift traffic between two identical environments that are running different versions of your application.","correct":true},{"id":"72f75a1c7558a3b8e4c011917ad6105d","text":"The green environment represents the production environment.","correct":false},{"id":"545fd2a37ac85b63dad4f92c91401436","text":"The green environment represents the staging environment.","correct":false}]},{"id":"bc69cb14-0b50-4173-925a-a9c2fdb00bb5","domain":"mon-trb","question":"You have multiple applications running on a large number of EC2 instances. You need to access the application logs from a single central location, what should you do?","explanation":"You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources. CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can create CloudWatch custom metrics for your EC2 instance statistics by creating a script through the AWS Command Line Interface and then monitor that metric by pushing it to CloudWatch. However, custom metrics are only metrics or findings reported by running a script, they are not pushing log files into CloudWatch.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html","title":"What Is Amazon CloudWatch Logs?"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-custom-metrics/","title":"Custom Metrics and CloudWatch"}],"answers":[{"id":"a8c600be214ced26950e704d39c3ca21","text":"CloudWatch Logs","correct":true},{"id":"458bb7239f53c3a09ada45ce8d2eb321","text":"CloudWatch custom metrics","correct":false},{"id":"f0d36f4f7323e9b431c82349dcc3ab89","text":"CloudTrail Logs","correct":false},{"id":"208fa683260008af411c8bfe394bb8bf","text":"Write a script to send the application logs to CloudWatch. Install the script on each of your application servers","correct":false}]},{"id":"f70f0615-b415-485e-93d5-2286bc2c25cc","domain":"development","question":"What is the maximum size of an item in a DynamoDB table?","explanation":"The maximum item size in DynamoDB is 400 KB.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html","title":"Limits in Amazon DynamoDB"}],"answers":[{"id":"462e2e8040f8c2b03228a91524dd8953","text":"400 MB","correct":false},{"id":"3c0512af07779a4b40b2b3d95dd1375b","text":"40 KB","correct":false},{"id":"5bdde53de5ac658c8aeeacac334b0152","text":"40 MB","correct":false},{"id":"1310fc461ca5af7082d588ba9c2a795b","text":"400 KB","correct":true}]},{"id":"b476baa3-99dc-4d87-a37a-2d0b23a7375c","domain":"deployment","question":"What is the largest size file you can transfer to S3 using a PUT operation?","explanation":"The largest file you can transfer to S3 using a PUT operation is 5GB. ","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html","title":"Uploading Objects to S3"}],"answers":[{"id":"84fb7b98e6e50302bf6cc709c92a6192","text":"5TB","correct":false},{"id":"208b4367cf771ded0e29ff6f9282442e","text":"1GB","correct":false},{"id":"6df47862fbfbd67605dc294d3f41925a","text":"100MB","correct":false},{"id":"0be25e5b91d25f9db3b4d3dcaf2cfd1f","text":"5GB","correct":true}]},{"id":"b1a8a6e6-0eb1-4c01-8d8b-4baebf067f88","domain":"development","question":"An application uses DynamoDB table as its backend data store. Each item has size of 10KB.  The application needs to perform 100 eventually consistent read operations per second, and 50 write operations per second.  What is the provisioned RCU value required to meet these requirements?","explanation":"1 RCU is equivalent to two eventually consistent reads per second of an item up to 4KB in size.  Thus, to calculate the required RCU in this scenario we need to: 1) Round up the item size to the nearest 4KB (12KB). 2) Divide by 4KB to calculate number of read units (12/4 = 3). 3) Divide by 2 to calculate the number of eventually consistent read units per item (3/2 = 1.5). 4) Multiple by operations per second to get the total RCU required (1.5*100 = 150 RCU).","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","title":"Provisioned Mode"}],"answers":[{"id":"cee631121c2ec9232f3a2f028ad5c89b","text":"500","correct":false},{"id":"a9b7ba70783b617e9998dc4dd82eb3c5","text":"1000","correct":false},{"id":"94f6d7e04a4d452035300f18b984988c","text":"300","correct":false},{"id":"7ef605fc8dba5425d6965fbd4c8fbe1f","text":"150","correct":true}]},{"id":"427e94b9-4fba-461b-aea9-d1a0c40a150e","domain":"development","question":"A DynamoDB table is configured in provisioned throughput mode with 500 RCU and 100 WCU.  How much data can be read and written to the table each second?","explanation":"One read capacity unit is equivalent to one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. One write capacity unit is equivalent to one write per second for an item up to 1 KB in size.  Therefore, 500 RCU is equivalent to: 1) 500 RCU * 4KB = 2000 KB per second for strongly consistent read operations; 2) 500 RCU * 4KB = 2000 KB per second * 2 = 4000 KB per second for eventually consistent read operations; 3) 100 WCU * 1KB = 100 KB per second for write operations.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","title":"Provisioned Mode"}],"answers":[{"id":"c233b8f618b2e1721b960acd0f8a9c81","text":"500 KB for strongly consistent read operations, 1000 KB for eventually consistent read operations, 400 KB for write operations.","correct":false},{"id":"3d31cbf18bacabdeabf60932252ab4e0","text":"2000 KB for strongly consistent read operations, 4000 KB for eventually consistent read operations, 400 KB for write operations.","correct":false},{"id":"28e9c06ef3d9789d4e86426c1a59bbcf","text":"2000 KB for strongly consistent read operations, 4000 KB for eventually consistent read operations, 100 KB for write operations.","correct":true},{"id":"3e1674995e5d9212840ca5a62310ccf9","text":"500 KB for strongly consistent read operations, 1000 KB for eventually consistent read operations, 100 KB for write operations.","correct":false}]}]}}}}
