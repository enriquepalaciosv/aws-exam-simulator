{"data":{"createNewExamAttempt":{"attempt":{"id":"8c30730b-50df-4162-907d-6537c1abc99f"},"exam":{"id":"1f40cb41-6c3e-42ae-b51b-f1cb91afe419","title":"AWS Certified SysOps Administrator - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"89d55a3e-4442-4e1a-9700-ca84f89c2bd2","domain":"security-comp","question":"As a consultant, many of your AWS clients come to you for answers regarding security on AWS. One client is concerned about data confidentiality and security running on their EC2 instances. The client has learned that the same instance host is shared between multiple AWS customers. She has asked you if sharing the resource would make it easy to hack into her resource to obtain confidential data. Which of the below responses would ease your client's concerns?","explanation":"256-bit AES is used for encrypting the data. Ensuring the isolation of the VMs running on a hypervisor is not the purpose of AES-256. CMKs are also used for encrypting data but have no part in securing underlying hardware. IAM permissions have nothing to do with the isolation of the VMs running on a hypervisor. The shared responsibility model for infrastructure services, such as Amazon Elastic Compute Cloud (Amazon EC2) for example, specifies that AWS manages the security of the following assets: Facilities, Physical security of hardware, Network infrastructure, Virtualization infrastructure.","links":[{"url":"https://d0.awsstatic.com/whitepapers/aws-security-best-practices.pdf","title":"AWS Security Best Practices"}],"answers":[{"id":"74a2e39c263c57894c9cd11a70b72db8","text":"EC2 instances running on the same physical host are isolated from each other via IAM permissions per AWS account.","correct":false},{"id":"3341fae60e0212eff137abdf4602fa85","text":"EC2 instances running on the same physical host are isolated from each other via the hypervisor.","correct":true},{"id":"5a474b0ed68df81d57c5bddfb611f66c","text":"EC2 instances running on the same physical host are isolated from each other via Customer Master Keys (CMKs) under KMS policies that you own and manage.","correct":false},{"id":"748cb8264234b20c598b5ca25cfed5a2","text":"EC2 instances running on the same physical host are isolated from each other via 256-bit Advanced Encryption Standard (AES-256).","correct":false}]},{"id":"988e212c-6400-4c9d-8e9b-25ce5413256b","domain":"dep-prov","question":"If you use an IAM user to copy an instance-store-backed AMI, the user must have which of the following Amazon S3 permissions?","explanation":"If you use an IAM user to copy an instance-store-backed AMI, the user must have the following Amazon S3 permissions: s3:CreateBucket, S3:GetObject, S3:PutObject, s3:GetBucketAcl, s3:ListAllMyBuckets and s3:PutObjectAcl.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/creating-an-ami-instance-store.html","title":"Creating an Instance Store-Backed Linux AMI"}],"answers":[{"id":"3d48378493503b3c7c04da8400942853","text":"s3:DeleteBucket","correct":false},{"id":"28e5f48e9f12d4928897b706c15e527a","text":"s3:RestoreObject","correct":false},{"id":"d0e4bee632dc55607e492853b76d8194","text":"s3:CreateBucket","correct":true},{"id":"7002b8d843e69183ff3fcc03410db0ad","text":"s3:GetObjectAcl","correct":false},{"id":"8e0864f8129f5d2b658668b9103e83a5","text":"s3:PutObject","correct":true},{"id":"53cfd5032381d05f65cbc6d64e0004f5","text":"s3:GetObject","correct":true}]},{"id":"c4f661ee-5ea1-4e69-9440-52bea0321a6a","domain":"mon-rep","question":"You have set CloudWatch billing alarms for your instances running in eu-west-2. However, when you try to access the billing information and alarms, no information is visible. Why might this be?","explanation":"Billing and Alarm data can be accessed only from the us-east-1 region.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/free-tier-alarms.html","title":"Creating a Billing Alarm"}],"answers":[{"id":"fb7a7d16c3f39960e7afde6babd422e1","text":"Billing and Alarm data can be accessed only from the us-east-1 region.","correct":true},{"id":"b5ff6f2dfd759f7e70f27d7529e4462b","text":"You need to login as the account owner to see such information.","correct":false},{"id":"7f3a3688c3f0cddb24c07255b9d13767","text":"Billing and Alarm data can be accessed only from the us-west-1 region.","correct":false},{"id":"cd2c9fa4324b5861276dbbf7f4f593a8","text":"You need to login as the root user to see such information.","correct":false}]},{"id":"ab86f355-ca8c-4f03-bc21-bd2b6add379f","domain":"high-avail","question":"You need to deploy a DynamoDB table in the production environment for an application. The read and write traffic is low during weekdays. On weekends, the traffic becomes much higher due to the increasing number of users. The traffic pattern is very predictable and there is no unexpected spike. Which kind of capacity mode would you configure for the DynamoDB table?","explanation":"In this scenario, DynamoDB Auto Scaling should be used as the pattern is predictable. On-demand mode is more suitable for unknown workloads and development environments. Reserved capacity would cause a waste of resources. And users cannot configure a schedule to automatically adjust the capacity in DynamoDB.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html","title":"DynamoDB read/write capacity mode"}],"answers":[{"id":"b799c024f6af20489d00692976f1bfd1","text":"On demand mode without the need to configure the capacity.","correct":false},{"id":"4fdafc1c54632989128146ba919a0e07","text":"Provisioned capacity with auto scaling.","correct":true},{"id":"fe576b38a59cfeaa466623938285521e","text":"Provisioned capacity on weekdays with a schedule to automatically increase the capacity on weekends.","correct":false},{"id":"35ba3d07134765e8d2b001befc79e653","text":"Reserved capacity that is able to cover the high traffic on weekends.","correct":false}]},{"id":"6fd27cf3-e84d-4a26-875c-7695a786a998","domain":"networking","question":"Two EC2 instances in two VPCs cannot ping each other. Both instances are located in private subnets. Which of the following would help you troubleshoot the problem? Select two.","explanation":"You can only peer VPCs, not individual subnets. There must also exist a route between the peered VPCs. Subnet security policy isn't a real thing although Network ACLs could be considered as such. VPC endpoint policy controls access to S3 or DynamoDB, not to a peered VPC.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html","title":"What is VPC Peering?"}],"answers":[{"id":"e88fa7d7390ad4d45912151c4e94b106","text":"Check that a VPC endpoint policy allows ping","correct":false},{"id":"19b59f579598d8721b6507a457658f92","text":"Check that a subnet security policy allows ping","correct":false},{"id":"b60b61736af41d60dfcadf8fde5d83b6","text":"Check that a route exists between the VPCs","correct":true},{"id":"088c1791b9d25f53b479820769d14995","text":"Check that there is a peering connection between the VPCs","correct":true},{"id":"d6d0388ad6ed9fa8508ce01fce126328","text":"Check that there is a peering connection between the subnets","correct":false}]},{"id":"8c1dec5f-627d-4f25-af24-119b092ca2ef","domain":"high-avail","question":"A client asks you how they can make their current database running on AWS highly available. The client is running a MySQL RDS database in us-west-1. The client does not currently have a Multi-AZ deployment. The client wants to know what the benefits are with a Multi-AZ deployment as it would incur additional costs to their AWS bill. How would you explain the benefits to the customer?","explanation":"Amazon RDS Multi-AZ deployments provide enhanced availability and durability for databases. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby. It does not lower latencies nor does it increase read performance. It cannot tolerate the failure of a single AWS Region as failure of a Region would implicate failure of all the Availability Zones within that Region.","links":[{"url":"https://aws.amazon.com/rds/details/multi-az/","title":"Amazon RDS Multi-AZ Deployments"}],"answers":[{"id":"834345e14f96992b8d4b2e785595f697","text":"Multi-AZ tolerates the failure of a single Region. It also allows higher availability during maintenance tasks.","correct":false},{"id":"4983004598ef3be1bb46819c748a09ab","text":"Multi-AZ tolerates the failure of a single Availability Zone. It also allows higher availability during maintenance tasks.","correct":true},{"id":"aa4c2ced080ae70c33f36fe825b972ce","text":"Multi-AZ lowers latencies for application servers when they are accessing the database in multiple Availability Zones.","correct":false},{"id":"6772f4dc3173a8dd298656b65d2b6efa","text":"Multi-AZ makes it faster for application servers to access the database by reading data at a quicker rate.","correct":false}]},{"id":"fa06cd79-3e11-4410-b67d-078ff338c946","domain":"automation","question":"A FinTech company has been aggressively managing their AWS resources using CloudFormation templates. The company has started using a new AWS service that is not yet available in CloudFormation. The engineering team has been given the strict compliance requirement of making sure that all resources need to be deployed and orchestrated automatically. The engineering manager has also been instructed to ensure that the team does not over-engineer or over-complicate the solution. How can the team accomplish this?","explanation":"Given that CloudFormation may not necessarily support new AWS resources and products right away, CloudFormation custom resources will allow CloudFormation to use another resource such as AWS Lambda to create resources on its behalf and continue the processing of the template after the Lambda function has completed its execution. AppSync is not used to generate new AWS service resources and is used primarily for hosting GraphQL powered APIs. Generating an Elastic Beanstalk environment would over-complicate the solution. Using nested stacks would not solve the unsupported service issue of CloudFormation.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html","title":"AWS CloudFormation - Template Custom Resources"}],"answers":[{"id":"1c3eb64e993a1f308aa47da24cfd0143","text":"Use nested stacks in CloudFormation to generate the new AWS service resource","correct":false},{"id":"97ebd9f4402ff1438c34af60ee4997f2","text":"Generate an AppSync resource in CloudFormation that generates the new AWS service resource","correct":false},{"id":"4eb557d837b6a756dabf8724f0237688","text":"Use custom resources in CloudFormation to generate the new AWS service resource","correct":true},{"id":"19a39cb24cfb7593d023becd2b712ec3","text":"Generate an Elastic Beanstalk environment in CloudFormation that generates the new AWS service resource","correct":false}]},{"id":"7a800486-b78e-43ec-88ea-ffa673a94745","domain":"mon-rep","question":"A SysOps administrator has been asked to implement monitoring of an application using Elasticache to improve database response times.  Recently the application has begun to perform slowly.  They notice that the eviction rate is high for the Cluster.  What could you consider doing to rectify this issue?","explanation":"Scaling up and out is the recommended approach when the eviction rate is high on the cluster.  Elasticache is an in-memory service and therefore cannot use Provisioned IOPs for storage.  AWS does not provide an auto scaling service in Elasticache. Increasing the ConnectionOverhead value would reduce the amount of memory available for storing cache items, so is unlikely to improve the eviction rate.","links":[{"url":"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/CacheMetrics.WhichShouldIMonitor.html#metrics-evictions","title":"Which Metrics Should I Monitor?"}],"answers":[{"id":"0dedcafa7f1671fda48c2d4515da1fcb","text":"Enable Autoscaling on the Elasticache Cluster","correct":false},{"id":"c8e144f5e160266566a48649def7cb74","text":"Redeploy the Elasticache cluster to use Provisioned IOPs storage","correct":false},{"id":"ea661bc3f4ee7300ed230572ef878635","text":"Increase the cluster size or add more nodes to the cluster","correct":true},{"id":"85688e53214b00837ac8ae9bb22e9d81","text":"Increase the ConnectionOverhead value of the cluster","correct":false}]},{"id":"414ac705-4dc4-4261-9523-5439c70ee19e","domain":"data-man","question":"You are running a relational database on a provisioned IOPS volume that is set to handle a moderate amount of traffic during the month. You know to expect a 10x spike in traffic during the final three days of each month due to month-end processing. How would you architect your storage environment to meet this expected increase in demand?","explanation":"With Elastic Volumes, you can increase volume size, adjust performance, or change the volume type while the volume is in use. You can continue to use your application while the change takes effect. Using CloudWatch alarms to automate these workflows is best practice. A reserved instance would not help in scaling to meet increased demand. Multi-AZ is for failover/disaster recovery purposes. Attaching multiple volumes is not best practice as it would increase cost and waste storage space during periods of low demand.","links":[{"url":"https://aws.amazon.com/blogs/aws/amazon-ebs-update-new-elastic-volumes-change-everything/","title":"Amazon EBS Update â€“ New Elastic Volumes Change Everything"}],"answers":[{"id":"d49d9044d5ba871697b4be060e80701e","text":"Enable multi-AZ for your RDS instance to account for the increased demand and spread the load between at least two instances.","correct":false},{"id":"afe5d1141b5bb2e9227d1d7d8adeb777","text":"Attach multiple Provisioned IOPS volumes for your relational database in order to meet the expected increase in demand.","correct":false},{"id":"870c3d2a2d0fd0b784490ec327392d9e","text":"Purchase an AZ-specific reserved instance for your relational database in order to gain the benefit of reserved capacity.","correct":false},{"id":"8df9d9e9da10ed671385be6f7b1a0ffc","text":"Use a CloudWatch alarm to watch for a volume that is running at or near its IOPS limit. Initiate a workflow and approval process that could provision additional IOPS or change the type of the volume.","correct":true}]},{"id":"a8d6d4f3-6877-4e89-9e7c-c6f83b83e1de","domain":"automation","question":"A small business has a monolithic application utilizing several EC2 instances. The technology consultant has suggested to the engineering team of the business that the application is containerized to improve the deployment and scaling processes of the team. The engineering team processes both critical and non-critical workloads within the application. Given the size of the application and the number of transactions being processed by the system, the CFO has instructed the engineering team to ensure that the setup must be as cost-effective as possible without introducing the risk of data loss for critical workloads. How can the engineering team accomplish this?","explanation":"On-demand EC2 instances should be used for critical workloads and spot instances can be used for non-critical workloads. ECR (Elastic Container Registry) is not used to run container processes. Instead, it is used to store container images. For containerization requirements, ECS (Elastic Container Service) is used to manage container workloads. Step Functions is used for orchestrating different processes and can not be used by itself to manage the workloads.","links":[{"url":"https://aws.amazon.com/blogs/compute/running-high-scale-web-on-spot-instances/","title":"Running High-Scale Web Applications on Spot Instances"}],"answers":[{"id":"1d3682fdb0beec13b13a37502cfb3108","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use Step Functions and on-demand EC2 instances for non-critical workloads.","correct":false},{"id":"ddf2c2e29f2118061f6a160054593338","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use ECR and spot EC2 instances for non-critical workloads.","correct":false},{"id":"b8864524b96a467a8f1023b5a2c9f003","text":"Use Step Functions and on-demand EC2 instances for the critical workloads. Use Step Functions and spot EC2 instances for non-critical workloads.","correct":false},{"id":"47e42a6e665f0a913236e5580553e3ad","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use ECS and spot EC2 instances for non-critical workloads.","correct":true}]},{"id":"7d7ac6a5-ba31-4418-9d4c-6da76657dbf2","domain":"networking","question":"You are designing a network with a bastion host (jump box) for security. Your network admins will SSH in to the bastion host and then on to other EC2 instances in a private subnet. You need your bastion host to be highly available. How should you build this environment?","explanation":"There has been is a much discussion about resilient Bastion design. The ELB does not add much value in this situation. Although you can get around it the ELB session timeouts will cause an SSH session to become disconnected if idle.  The answer with two AZs is a trap of the type you will see on the exam.  While 2x AZs would be ideal, the WHOLE answer must be correct, not just part of it. You should never use an ELB IP address for business as it is ephemeral and may change at any time.  DNS convectional round-robin will achieve the resiliency needed, as would an R53 Failover policy. The answer with two subnets does not exclude 2x AZs even though it does not stipulate it. Another design option you might see is EC2 Auto-Recovery or an Autoscaling group of Max=1 & Min=1 so that if the Bastion Host fails it is recreated automatically. ","links":[{"url":"https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html","title":"Bastion Hosts on AWS - Architecture"},{"url":"https://en.wikipedia.org/wiki/Round-robin_DNS","title":"DNS convectional round-robin"},{"url":"hhttps://aws.amazon.com/blogs/aws/new-auto-recovery-for-amazon-ec2/","title":"Auto Recovery for Amazon EC2"}],"answers":[{"id":"d5cfd03aad43f4cb7329ceb07c3b288d","text":"Create 2 Bastion EC2 instances in the same subnet. Create a DNS entry in Route53 which uses Round Robin DNS and points to each instance. Tell your SysAdmins to connect using the new DNS entry.","correct":false},{"id":"cb402e295d67552ab9dfcffabcfb0dcb","text":"Create 1 Bastion EC2 instance in a private subnet. Connect to this EC2 instance using a site to site VPN. Configure your router to automatically reconnect if the VPN is dropped.","correct":false},{"id":"01a3b5d266f7455e0d0cfc25070de365","text":"Create 2 Bastion EC2 instances in separate availability zones. Place these instances behind an elastic load balancer, and ask your SysAdmins to connect to the ELB's public IP Address.","correct":false},{"id":"c87a02a4d83b0d5310563e4b700e303a","text":"Create 2 Bastion EC2 instances in different subnets. Create a DNS entry in Route53 which uses Round Robin DNS and points to each instance. Tell your SysAdmins to connect using the new DNS entry.","correct":true}]},{"id":"e2dd07ca-6d80-4c5b-87ce-95bd37a03b28","domain":"dep-prov","question":"You are helping to design a multi-instance storage solution for volume data. Your CTO believes utilizing Amazon EBS volumes is the best solution but has come to you for advice. What would you suggest?","explanation":"Amazon EBS volumes can only be attached to one EC2 instance at a time. If you need multiple EC2 instances accessing volume data at the same time, consider using Amazon EFS as a file system. Taking a snapshot and attaching new volumes from the snapshot to every instance would be an administrative burden. S3 is not used for volume data.","links":[{"url":"https://d0.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf","title":"AWS Storage Services Overview"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html","title":"Amazon EFS: How It Works"}],"answers":[{"id":"b7e0cfad15004b2f664c81878bfd1091","text":"Use Amazon EBS volumes. Take a snapshot of the volume. Create a new volume from the snapshot and attach to all instances.","correct":false},{"id":"e417bdbb8845ac556e877cff73411a3a","text":"Use Amazon EFS. Mount EFS file system on instances within a VPC.","correct":true},{"id":"43fe6d9ea40b98964da1097e42fb0c54","text":"Use Amazon EBS volumes. Attach the same EBS volume to all instances.","correct":false},{"id":"03c2ae52e51575a97a45797cd779317b","text":"Use Amazon S3. Attach an IAM role to all instances granting access to the S3 bucket hosting the data.","correct":false}]},{"id":"cde5021e-5b37-406b-b25a-1bdb489d3b24","domain":"security-comp","question":"Following a recent security event, a SysOps administrator has been asked to provide details of source IP addresses of requests to a website which is hosted on EC2 instances behind an Application Load Balancer. Where can the administrator find these details?","explanation":"Application Load Balancer (ALB) Access Logs record details of client connections which include the client's IP address and port.  CloudTrail logs AWS API calls so will not provide client IP addresses, neither will CloudWatch Custom Metrics which are for logging performance metrics.  EC2 instances will log the ALB's internal IP address as the source unless specifically configured to record the true source IP by using the x-forwarded-for header, which is not enabled by default.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html#access-log-entry-format","title":"ALB Access Log Entries"}],"answers":[{"id":"8480ea5c787566c9a8bd2a608c725a06","text":"ALB Access Logs","correct":true},{"id":"6a219c87573826721cb51b987140a267","text":"CloudTrail Event Trail","correct":false},{"id":"9a19403fef543b4899ec570ad1a29ca6","text":"CloudWatch Custom Metrics","correct":false},{"id":"43bc48b76d9918101fb772b4cfc58235","text":"/var/log/httpd/","correct":false}]},{"id":"b4e4d4f8-b9af-47da-9f90-2b63cab25ddf","domain":"automation","question":"As a SysOps Administrator you are managing your company's infrastructure as code. You have a number of CloudFormation templates that automate the provisioning of AWS resources for disaster recovery purposes. Your CISO have asked you for additional insights into the changes that teams are making to the CloudFormation templates in order to see when templates are updated with what changes. How would you build a solution that fulfills the CISO's ask?","explanation":"Change sets allow you to preview how proposed changes to a stack might impact your running resources. For example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. Config and Lambda would be complicated to configure and unnecessary as you would be able to directly do this using change sets. Amazon Inspector is used for EC2 instances.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"00c32ce29ae8b3d7291d321ea5a8c6ba","text":"Run Amazon Inspector report periodically to identify changes made to a CloudFormation stack. Forward these reports to your CISO.","correct":false},{"id":"44fab4e192c83f7e36b2143702b9958e","text":"Configure an AWS Config rule to detect changes to a CloudFormation stack. Send an SNS notification to the CISO for any changes.","correct":false},{"id":"a4e164938d62b8c500a3c3bc4680f546","text":"Create a change set by submitting changes against the stack you want to update.","correct":true},{"id":"a374362c79930669cc8b737ca45f03cb","text":"Create a Lambda function that parses through CloudWatch logs for any changes made to a CloudFormation stack. Ensure CloudFormation has a role assigned that sends logs to CloudWatch.","correct":false}]},{"id":"739bcd50-ad59-4cbe-a911-17cff575b80d","domain":"security-comp","question":"You have been asked by your company's CISO to create and manage an S3 bucket with highly confidential company information. Only two business-critical individuals within the company should have read and write access to the bucket. All other personnel should not have access. How would you go about ensuring the security and confidentiality of the bucket in the most efficient manner?","explanation":"Bucket policies provide centralized access control to buckets and objects based on a variety of conditions, including Amazon S3 operations, requestors, resources, and aspects of the request (for example, IP address). The policies are expressed in the access policy language and enable centralized management of permissions. The permissions attached to a bucket apply to all of the objects in that bucket. AWS Config would only report on if buckets are in compliance to any rules set. An explicitly deny for all IAM users would work but it isn't the most efficient solution. Bucket access control lists apply to bucket objects, not the bucket itself.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html","title":"Bucket Policies"}],"answers":[{"id":"48f34a3e8e8a95fcbe3abee586e0f775","text":"Create a bucket access control list to explicitly grant access to the two individuals.","correct":false},{"id":"ebf041183040a34a7220734ce5497478","text":"Create an IAM policy for all non-business-critical individuals that explicitly denies access to the bucket.","correct":false},{"id":"fdadf86a8030fc5cbfd2cdb0b16b95f6","text":"Create a bucket policy explicitly granting access to the two principals.","correct":true},{"id":"06899090182266f944ef7ff8a34750ed","text":"Create an AWS Config rule that denies access to the S3 bucket except for the business-critical users. Set an alarm whenever someone attempts to access the bucket.","correct":false}]},{"id":"999d3279-74be-4cb4-b1a3-52088c96503d","domain":"security-comp","question":"Your company's security policy requires you to keep a record of all changes to IAM permissions. You need to record who made a change and when. Which service could you use to record this data?","explanation":"CloudTrail is the service to use as it records user API activity on your account and allows you to access information about this activity. Systems Manager is an automation tool, not a monitoring service. CloudWatch is used for metrics, collecting logs and setting alarms. Trusted Advisor inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps.","links":[{"url":"https://aws.amazon.com/cloudtrail/","title":"CloudTrail"}],"answers":[{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":true},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"8e75b153e61c22a8ea4e14aadc7cb4ee","text":"Systems Manager","correct":false}]},{"id":"b84511ce-eca6-4a78-8b45-e76b4a0f37af","domain":"data-man","question":"An organization is moving its existing data lake from on-premises SAN storage to AWS.  You have 30TB of data to move. Your new cloud footprint includes an AWS Direct Connect link of 200Mbps, which is used for the mission critical link between your offices and the new systems in Production in AWS. Which of the below options would you recommend for the fastest transfer to AWS S3?","explanation":"With such a large amount of data to transfer it makes sense to use the AWS Snowball service.  Additionally flooding the Direct Connect connection with such a lot of traffic for several weeks would be a high risk to Production.  Snowball will provide a reliable and quick way to move the data. AWS DataSync is another possible candidate but it will place a heavy load on the Direct Connect link.","links":[{"url":"https://aws.amazon.com/snowball/faqs/#when-to-use","title":"AWS Snowball FAQs"}],"answers":[{"id":"64bf8ac94115dbfb0847b75ceda67d5a","text":"AWS S3 Accelerated Transfer","correct":false},{"id":"56169776bdeba6f7384a8ad473b0fc72","text":"AWS Data Sync","correct":false},{"id":"c2d838ea2b9043be2161255da328dc60","text":"AWS Snowball","correct":true},{"id":"0d9da8d932c0312c10d819af40de0ba8","text":"AWS Snowball Edge","correct":false}]},{"id":"b2d3b949-889b-4bbd-8ec9-c65b764c47c3","domain":"mon-rep","question":"You are a SysOps Administrator monitoring a web app that lets users upload high-quality images and use them online. Each image requires resizing and encoding. The images are placed in an Amazon SQS queue for processing by an EC2 instance. It processes the images and then publishes the processed images where they can be viewed by users. When you monitor the EC2 instance you see that the CPU utilization is consistently at 90% and that image processing time is being delayed. The team is looking for a cost-effective solution. What would you recommend?","explanation":"You can use an Auto Scaling group to manage EC2 instances for the purpose of processing messages from an SQS queue. Set a custom metric to send to Amazon CloudWatch that measures the number of messages in the queue per EC2 instance in the Auto Scaling group, and then set a target tracking policy that configures your Auto Scaling group to scale based on the custom metric and a set target value. CloudWatch alarms invoke the scaling policy. Increasing the size of the instance may work but is not a cost-effective solution since Auto Scaling gives you the option to scale down during low demand. Kinesis Data Streams are best suited for real-time data processing, and they have a size limit of 1MB which would be too low for high-quality images. Migrating the data to DynamoDB would not be a viable, let alone cost-effective, solution.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html","title":"Scaling Based on Amazon SQS"}],"answers":[{"id":"3a781b7e075f07b31a51c98b18d84a2e","text":"Increase the size of the instance and ensure that it is compute-optimized to boost it's capacity to process the images.","correct":false},{"id":"40fe2365fa7bae167e628c9ef29bd6ca","text":"Move the images into Kinesis Data Streams where you'll be able to process the data in real time.","correct":false},{"id":"52bcbdff61b39eafa67d9496dc77ee09","text":"Migrate the image data into DynamoDB. Attach a role to the instance to be able to access the data from DynamoDB and process the images.","correct":false},{"id":"c1c44e18b3c0f81b8ec026e9e1ab5b38","text":"Place the instance in an Auto Scaling group. Use CloudWatch metrics to scale out the Auto Scaling group depending on the size of the SQS queue.","correct":true}]},{"id":"eab3e224-a279-4865-a746-4595551b4cfb","domain":"security-comp","question":"As a systems administrator, it's your job to grant IAM access to your entire development team as your company transitions to AWS. What's the best strategy in doing this?","explanation":"Instead of defining permissions for individual IAM users, it's usually more convenient to create groups that relate to job functions (administrators, developers, accounting, etc.). Next, define the relevant permissions for each group. Finally, assign IAM users to those groups. All the users in an IAM group inherit the permissions assigned to the group.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#use-groups-for-permissions","title":"Using Groups to Assign IAM Permissions"}],"answers":[{"id":"58d601b0c64e419e78ca3f11a589b34a","text":"Create groups based on the relevant permissions for that job function and assign each user to the appropriate group.","correct":true},{"id":"5ed32311b773a2bfca008d3086a7bd62","text":"Use Active Directory and copy its permissions.","correct":false},{"id":"f070e391c12dea8466645d67cc18ef12","text":"Use the default access provided by your identity provider.","correct":false},{"id":"d07d419ef9a8d5dfbaa087cd534554de","text":"Create IAM access specific to each user's needs.","correct":false}]},{"id":"9a336e95-d0f5-4b79-904b-f0618496cc2f","domain":"networking","question":"You have created a new VPC with the CIDR block of 10.0.0.0/16. You create 2 subnets: 10.0.1.0/24 and 10.0.2.0/24. 10.0.1.0 will be a public subnet and 10.0.2.0 will be a private subnet. You deploy a NAT gateway with the name i-7c1507ab into 10.0.1.0 and assign it a public IP address. You now need to update your route table to complete the setup. Which of the following is the correct route table listing?","explanation":"Destination: 0.0.0.0/0 Target:i-7c1507ab is correct.","links":[{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.html#WorkWithRouteTables","title":"Working With route Tables"}],"answers":[{"id":"c4362c9a78985f2334fe591db6c45c50","text":"Destination: 10.0.0.0/16 Target:i-7c1507ab","correct":false},{"id":"7540f0e829de47c0b695dae438a647d0","text":"Destination: 10.0.2.0/24 Target:i-7c1507ab","correct":false},{"id":"eee0734b451befed6d35d25ecd8b7e00","text":"Destination:10.0.1.0/16 Target:i-7c1507ab","correct":false},{"id":"5b62777103d2e471ec99428f0453f132","text":"Destination: 0.0.0.0/0 Target:i-7c1507ab","correct":true}]},{"id":"f3ec51e7-232c-4d1d-9d28-d0dcc089732","domain":"networking","question":"A prescription drug company runs its in-house developed fulfillment application on EC2 with multiple instances behind an ELB Network Load Balancer in a single AWS region. They'd like to be able to failover to a different AWS region if issues arise with the application in the primary region. Security policy requires that both the primary and failover EC2 instances run in private subnets. Which architecture will provide the most reliable solution for failover between regions?","explanation":"The monitored resources of Route 53 health checks must have public IP addresses, so standard Route 53 health checks won't work for the private subnet instances in this use case. You can deploy a Lambda function in the private subnet where the instances reside, and use it to check an application's health. You can then use CloudWatch Events and alarms to inform Route 53 of the need for a failover to another region using a failover routing policy. The Route 53 API doesn't support invoking a routing policy. Route 53 Resolver supports hybrid clouds, not cross-region health check resolution.","links":[{"url":"https://aws.amazon.com/route53/","title":"Amazon Route 53"},{"url":"https://aws.amazon.com/blogs/networking-and-content-delivery/performing-route-53-health-checks-on-private-resources-in-a-vpc-with-aws-lambda-and-amazon-cloudwatch/","title":"Performing Route 53 health checks on private resources in a VPC with AWS Lambda and Amazon CloudWatch"}],"answers":[{"id":"9436bd62455fac582d39528d0fbfaa03","text":"Deploy Amazon CloudWatch Events to invoke an AWS Lambda function at regular intervals to check the health of the primary application. Trigger a CloudWatch alarm when the Lambda function returns an unhealthy status. Attach the alarm to an Amazon Route 53 failover routing policy that re-routes the traffic to the load balancer of the failover instances.","correct":true},{"id":"d9bc7673b4bf5a5ed631d877bb205d21","text":"Schedule an AWS Lambda function to check the health of the primary application. Call the Amazon Route 53 API from the Lambda function to invoke a failover routing policy that re-routes the traffic to the load balancer of the failover instances.","correct":false},{"id":"6a65fb4fb046619eeae442309bbf8d7e","text":"Implement an Amazon Route 53 health check to monitor the primary application. Deploy a failover routing policy in both regions and use Route 53 Resolver to re-route traffic to the load balancer in the failover region when an issue is discovered.","correct":false},{"id":"beb25447af410abf3aaa61057b471b4c","text":"Implement an Amazon Route 53 health check to monitor the primary application. Use a failover routing policy to re-route the traffic to the load balancer of the failover instances when an issue is discovered.","correct":false}]},{"id":"bc639267-4b1b-4a5e-820e-27495782af83","domain":"data-man","question":"You have a mobile gaming application that uses an RDS MySQL database. The allocated storage of the database instance is 100GiB. As users are adopting rapidly, the available database storage is also rapidly decreasing, and will soon lead to performance issues if additional storage capacity is not added soon. Which of the following statements is correct in terms of addressing this database storage issue?","explanation":"You can manually scale up the storage. There is no outage during scaling and the performance of the server is not degraded. When the storage is modified, the change can be applied immediately. You can also enable auto scaling to automatically adjust the storage.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html","title":"Working with storage for Amazon RDS DB instances"}],"answers":[{"id":"528fd2301cd5c616abc137beeb7755e9","text":"The database status is \"degrading\" when you scale up the storage.","correct":false},{"id":"bea16cc246ddc8340245b09063aeb7e8","text":"The database has an outage for a short period when the storage is being modified.","correct":false},{"id":"65998a26d3cbe450d273182cc29db11d","text":"You have to wait until the next scheduled maintenance window to apply the change.","correct":false},{"id":"7a4336887970da0724cffecb782ab21f","text":"You can enable the RDS Storage Autoscaling feature to avoid manually scaling up the database storage.","correct":true}]},{"id":"8f2d3d61-092d-429c-a109-69ab00fa4065","domain":"mon-rep","question":"AWS Cost Management encompasses a number of services to help you to organize, control and optimize your AWS costs and usage.  Which of the following Cost Management related tools gives you the ability to set alerts when costs or usage are exceeded?","explanation":"The correct answer is AWS Budgets.  AWS Cost Explorer lets you visualize, understand, and manage your AWS costs and usage over time. AWS Cost & Usage Report lists AWS usage for each service category used by an account and its IAM users and finally, Reserved Instance Reporting provides a number of RI-specific cost management solutions to help you better understand and manage RI Utilization and Coverage.","links":[{"url":"https://aws.amazon.com/aws-cost-management/aws-budgets/","title":"AWS Budgets"}],"answers":[{"id":"824fd559c917b4ae56f36787b886eb81","text":"AWS Cost & Usage Report","correct":false},{"id":"eef79d956328d5e4ec426d448cc53c74","text":"Reserved Instance Reporting","correct":false},{"id":"e32a801c8e0beab6abb9361e937365be","text":"AWS Budgets","correct":true},{"id":"c7f176d72688fd87853e31b84159d541","text":"AWS Cost Explorer","correct":false}]},{"id":"983c6fbc-20eb-4389-a91f-491d1f1e230b","domain":"networking","question":"You have a simple VPC with a single public subnet and a security group that allows access from source 0.0.0.0/0. Although you have both an Internet Gateway and Elastic IP specified for your instance, you are still unable to reach the instance via SSH. What have you forgotten to do?","explanation":"For the outside world to be able to communicate with your instance, you must allow inbound traffic on both the Security Group and the Route Table.","links":[{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.html#AddRemoveRoutes","title":"Adding and Removing Routes from a Route Table"}],"answers":[{"id":"cb7394a526c29652c88dc22d58ad386a","text":"You have failed to associate the Internet Gateway with the custom Route Table.","correct":true},{"id":"93b61a7d33d3f54dee0f2c7dfd033254","text":"You haven't associated the Internet Gateway with the Security Group.","correct":false},{"id":"1e8881ac02fb21089d364fb6f88436a7","text":"You have forgotten to associate the Elastic IP with the private IP.","correct":false},{"id":"9c4da4b8220f1bcf509bc4cd3ccd943f","text":"You forgot to associate the Security Group with the Route Table.","correct":false}]},{"id":"05d71be4-026e-433e-bd8b-eb4a3929ba63","domain":"automation","question":"A development team wants to use the latest Windows AMI whenever they launch an EC2 instance. Which service will allow them to query the AWS-managed Parameter Store namespace to retrieve the newest AMI for their CloudFormation template?","explanation":"AWS publish the latest AMI IDs for Operating Systems in AWS-managed parameters in the Parameter Store.  By using a Custom Resource in Lambda you can retrieve the relevant AMI ID and return it to the CloudFormation service, that way ensuring that your templates always use the newest AMI.","links":[{"url":"https://aws.amazon.com/blogs/mt/query-for-the-latest-windows-ami-using-systems-manager-parameter-store/","title":"Select AMI using Systems Manager Parameter Store"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html","title":"AWS Lambda-backed Custom Resources"}],"answers":[{"id":"2751cfe1530d4333f0bdac2d7b7c21bd","text":"CloudFormation using AWS Systems Manager Parameter Store","correct":true},{"id":"7eb8f6238570dc713a360eae3029648f","text":"CloudFormation Mappings","correct":false},{"id":"dc0efa07b1be89f7cfd1ab666df2f949","text":"CloudFormation Custom Resource using Lambda","correct":true},{"id":"8c19fb5ff9d451c3f315e96ca8563b84","text":"CloudFormation Linked Parameters","correct":false},{"id":"cdf3a2f6faa3abf891b952dde17eb469","text":"CloudFormation Template Transformation","correct":false}]},{"id":"281dba1d-32b8-4e19-a324-72e004b2f7c6","domain":"mon-rep","question":"The Jet Engine Division of Consolidated Aerospace Corporation would like to centralize monitoring of both their on-premises systems and their AWS servers in the AWS cloud. On-premises operating systems include Red Hat, Debian, AIX, and Windows. EC2 operating systems used include Red Hat and Windows. Which architecture will provide the most robust monitoring and alerting solution?","explanation":"The CloudWatch agent is supported on Red Hat, Debian, and Windows operating systems for writing to CloudWatch Logs for both on-premises and EC2 systems. The agent is not supported for AIX, so writing those log messages to an EC2 instance for forwarding to CloudWatch Logs will work. Configuring CloudWatch metric filters will result in numerical metrics for graphing or alarming. CloudWatch alarms perform actions based on CloudWatch metrics, so setting up the metric filters needs to happen first","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html","title":"What Is Amazon CloudWatch Logs?"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html","title":"Searching and Filtering Log Data"},{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html","title":"Installing the CloudWatch Agent on On-Premises Servers"}],"answers":[{"id":"1247cb867a2d6b15f0e4e149aee44e02","text":"Install the CloudWatch agent on the Red Hat, Debian, and Windows systems, both on-premises and EC2. Write AIX logs to an EC2 instance which sends messages to CloudWatch Logs. Set up CloudWatch metric filters to alarm on potential issues.","correct":true},{"id":"4a3ae3aa58ce592572b629601964f7bf","text":"Install the CloudWatch agent on the EC2 systems. Write on-premises logs to an EC2 instance which sends log messages to CloudWatch Logs. Set up CloudWatch metric filters to alarm on potential issues.","correct":false},{"id":"150fdd86d9a864feb197592ccedbeee3","text":"Install the CloudWatch agent on the Red Hat and Windows systems, both on-premises and EC2. Write Debian and AIX logs to an EC2 instance which sends messages to CloudWatch Logs. Configure CloudWatch alarms to alert on error messages in the logs.","correct":false},{"id":"b506e93e4197e20db1193673763fa6b9","text":"Install the CloudWatch agent on the EC2 systems. Write on-premises logs to an EC2 instance which sends log messages to CloudWatch Logs. Configure CloudWatch alarms to alert on error messages in the logs.","correct":false}]},{"id":"8dade21b-c268-4ccd-af0a-18c01cd2c638","domain":"automation","question":"You check the last bill of your AWS account and find that the storage of EBS snapshots charges a lot. A large number of EBS snapshots are very old and can be deleted. You want to keep 10 snapshots for an EBS volume and old snapshots are deleted automatically. The strategy should also help you to create a snapshot every 24 hours. Which is the best way of implementing this strategy?","explanation":"The Amazon EBS Snapshot Lifecycle can automate the creation, retention, and deletion of EBS snapshots. You only need to configure a lifecycle policy in the Lifecycle Manager. You do not need to configure a Lambda Function or a Cron job in an EC2 instance to implement the same policy.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html","title":"Automating the Amazon EBS Snapshot Lifecycle"}],"answers":[{"id":"630f293528bb790c603b2f77b266a56d","text":"Use a Cron job that runs in an T2.micro EC2 instance. The job creates a new snapshot and deletes the old one every day.","correct":false},{"id":"ac159fa50438475f44d25f96b6a2bea4","text":"Configure a Lambda Function that runs every 24 hours to create a snapshot and delete the old snapshot.","correct":false},{"id":"e711537d471732823f103b6f63ab96c2","text":"Configure a Cloudwatch Event rule to execute every 24 hours. The target is a Lambda Function that creates a new snapshot and deletes the old one.","correct":false},{"id":"6e23ea86e4cf4e8929aa5b846a7e5ad4","text":"Create a Snapshot Lifecycle Policy to automatically create new snapshots and delete old snapshots.","correct":true}]},{"id":"0b73a5a1-e9ff-4bf8-8f53-0fb5aa540566","domain":"high-avail","question":"You run a bespoke security application on AWS and have a very limited (but highly valuable) number of customers over the globe. Your application is extremely sensitive and you limit who can access this application. The application sits on a fleet of EC2 webservers in an autoscaling group across multiple availability zones behind an elastic load balancer. Your end customers are investments banks and this application helps to keep one of their extremely sensitive database servers secure. The financial regulations state that these databases should not be internet facing and if they need access to specific internet resources, these resources must be whitelisted on the banks firewalls by a single fixed IP address only. In order to have a single fixed IP address to give to your customers in order to connect to your application, what choice of Elastic Load Balancer should you make to meet this strict security requirement?","explanation":"Only the Network Load Balancer supports the use of a single fixed IP address. The other load balancer offerings do not provide this","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html","title":"Network Load Balancer"}],"answers":[{"id":"e0f10b949b1cbe40263bfe87c11a2f5d","text":"Network Load Balancer","correct":true},{"id":"37e242ab2d525505933bbdb47d50d2b9","text":"Route53","correct":false},{"id":"b3b5475001f327d331389c6f07ff7c3a","text":"Application Load Balancer","correct":false},{"id":"b4ec634f996fd486030f44e2c5fab630","text":"Classic Load Balancer","correct":false}]},{"id":"2b08815b-ca6c-44ed-969b-8cc0dfc22faf","domain":"data-man","question":"A large real estate broker wants to provide a mobile device dashboard of home sales trends by geographic area for its agents. Dashboard metrics will be updated three times each day based on recent activity. The backend application will run on Linux servers in the company data center. Minimizing operational costs is a priority, so they've decided to store the application's data on AWS. Which storage solution will be the most cost effective?","explanation":"A Storage Gateway Volume Gateway in cached mode uses S3 as it's primary storage and caches frequently accessed data locally. A Volume Gateway in cached mode will be more cost effective than one in stored mode because only 20% of the storage capacity needs to be purchased for the on-premises Storage Gateway server in cached mode, whereas the full storage capacity needs to be purchased for a stored mode gateway. Amazon EFS Standard storage is currently $0.30 per GB per month, whereas Storage Gateway Volume Gateway pricing is currently $0.023 per GB per month. EBS volumes can only be accessed by EC2 instances, not remotely as iSCSI devices.","links":[{"url":"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html","title":"How AWS Storage Gateway Works (Architecture)"}],"answers":[{"id":"f79405543a29779912fb22c1d8fd858f","text":"Deploy an Amazon Elastic File System file system and mount it via NFS","correct":false},{"id":"6179b86bded76a4fa17bb52009d596e7","text":"Use an AWS Storage Gateway Volume Gateway in stored mode. Mount the volumes as iSCSI devices on the servers","correct":false},{"id":"eca78458c820df850fbb09ee72a3cabe","text":"Implement an AWS Storage Gateway Volume Gateway in cached mode. Mount the volumes as iSCSI devices on the servers","correct":true},{"id":"1510097be2cd1ce079dfc261617a8176","text":"Store the data on Amazon Elastic Block Store volumes. Mount the volumes as iSCSI devices through a VPC endpoint","correct":false}]},{"id":"7e182684-0217-472e-a62a-53d34cbd972c","domain":"dep-prov","question":"You have been asked to decouple an application by utilising SQS.  The application dictates that messages on the queue can be delivered more than once, but must be delivered in the order that they have arrived, and also must allow for efficient, repeated polling of the queue.  Which of the following options are most suitable?","explanation":"This question has two parts which need to be considered, the type of queue and the type of polling.  The question states that messages, \"can be delivered more than once\" but, \"must be delivered in the order that they have arrived\", which means that it can only be a FIFO queue as it is the only SQS type which will deliver messages in order, regardless of how many times the message is delivered.  The question also states that the queue, \"must allow for efficient polling\" and in this case long polling is the most efficient and cost effective option in situations where the queue will be polled constantly.  The correct answer is therefore to configure a FIFO SQS queue with long polling enabled.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html","title":"Amazon SQS FIFO (First-In-First-Out) Queues"},{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html","title":"Amazon SQS Long Polling"}],"answers":[{"id":"8ced375cf136690dd622d3bdc98e16c2","text":"Configure a standard SQS queue and use default polling","correct":false},{"id":"a078e24ae55cd5b1c779ffdfdfd8fcf9","text":"Configure a FIFO SQS queue and enable long polling","correct":true},{"id":"13aac3121a54296c93468234033a5608","text":"Configure a standard SQS queue and use long polling","correct":false},{"id":"2a55c515f67b00a75c46c6d27e2cc2ed","text":"Configure a FIFO SQS queue and enable short polling","correct":false}]},{"id":"1a45301ef-de9d-42bb-842d-8c1a42220a08","domain":"mon-rep","question":"There is increasing demand of your application running on EC2, and you need to monitor available memory space to ensure you can scale with demand. How would you monitor this on AWS?","explanation":"Memory and disk space utilization is a custom metric that is CloudWatch does NOT collect natively. Users must install the CloudWatch agent to collect metrics for memory and disk space. Neither the EC2 Dashboard nor the CloudWatch Dashboard natively provides the ability to monitor memory. You much install the CloudWatch Agent on your instances. AWS would not provide this report as these are specific to your EC2 instances.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html & https://aws.amazon.com/blogs/aws/amazon-cloudwatch-user-defined-metrics/","title":"CloudWatch Custom Metrics"}],"answers":[{"id":"fcf91ec5e036123e3b515882a4d079eb","text":"Create a Support Case to AWS and request a report on available memory space on your instances.","correct":false},{"id":"25084caeeca98bb0ed5317e16eb25f5b","text":"Utilize the CloudWatch Dashboard to view memory and disk metrics that are available by default.","correct":false},{"id":"f5537acc0c2715be1415c64741ce5ed7","text":"Check the EC2 Dashboard to monitor instance metric details.","correct":false},{"id":"4a31b453359b5547f03d47279651a6c7","text":"Install the CloudWatch Agent on your instance to monitor memory metrics.","correct":true}]},{"id":"7afdb34a-f5da-4e8f-ab9c-e059038d650e","domain":"networking","question":"You have just launched an EC2 instance in the public subnet of a newly-created VPC, but you forgot to assign a public IP address during creation. How might you make your instance reachable from the outside world?","explanation":"For an instance to be reachable from the internet, your VPC must have an Internet Gateway and your instance must have a Public or Elastic IP. Public IP addresses can be created only at the time of instance creation. You cannot \"go back\" and create one.  In order for a subnet to be described as a \"Public subnet\" it must already have a route to the Internet","links":[{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Internet_Gateway.html","title":"Internet Gateways"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html","title":"About Elastic IP Addresses"},{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html","title":"About Public subnet"}],"answers":[{"id":"93d71e191e2b0ea46c66ef9f7de5f064","text":"Go back and create a Public IP address. Associate it with your Internet Gateway.","correct":false},{"id":"568466971ef09a401e2f77e242156994","text":"Create an Internet gateway and an Elastic IP address. Associate the Elastic IP with the EC2 instance.","correct":false},{"id":"0fa79a82264c55eff18e206e030f5c16","text":"Create an Elastic IP address for your instance. Associate the Elastic IP with the EC2 instance.","correct":true},{"id":"e9ce11f00035e2973045eabe5deb1a36","text":"Create an Internet Gateway and associate it with the private IP address of your instance with it.","correct":false}]},{"id":"8cf25d39-5ee7-44ee-8b5c-418f472c6b03","domain":"dep-prov","question":"A business needs to migrate a suite of PHP applications to Amazon Web Services. Which service provides the simplest migration path with the lowest ongoing maintenance overhead?","explanation":"AWS Elastic Beanstalk allows us to deploy an existing PHP application into managed infrastructure with very little technical overhead or refactoring of code.","links":[{"url":"https://docs.aws.amazon.com/en_pv/elasticbeanstalk/latest/dg/create_deploy_PHP_eb.html","title":"Creating and Deploying PHP Applications on AWS Elastic Beanstalk"}],"answers":[{"id":"b9a69ca1bdc628b8e6761d1d772b7fa3","text":"AWS Code 9","correct":false},{"id":"0ce7192826b79135a6c788bfa0e5d085","text":"Custom AMI builds and Autoscaling","correct":false},{"id":"c8257f188608b250ed868721eba35a1d","text":"AWS EKS","correct":false},{"id":"bcf6eb183b7da148701bcc059a34675f","text":"AWS Elastic Beanstalk","correct":true}]},{"id":"9efe0b3f-2e6a-4918-81b2-c1a827654892","domain":"networking","question":"Which of the below statements about subnets and CIDR blocks as part of a VPC in AWS is correct?","explanation":"The maximum size for a subnet is /16 so this is a correct answer. Default behaviour is that every time you create a subnet it is added to the main route table - allowing other subnets using the main route table to route to it. As for the incorrect statements - the minimum size of a subnet is /28 (16 addresses) therefore /30 is incorrect, subnets are local to a single AZ and cannot span multiple AZs (although VPCs in which they live can). Every VPC will need an IPv4 CIDR allocated - even if the intent is just to use IPv6","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html","title":"VPC Subnets"}],"answers":[{"id":"badb9d21322bff4ed8278d92ebc7174b","text":"The maximum size for a subnet is /16 with 65,536 addresses","correct":true},{"id":"32df7fa7ee27b28e38ab223822a82204","text":"The minimum size for a subnet is /30 with 4 addresses","correct":false},{"id":"70426e4ee2fa35fde1533f02d72886a1","text":"A subnet can span multiple AZs","correct":false},{"id":"ed1e31d6b69887380df735258ef1bd1a","text":"A subnet that will only be used for IPv6 doesn't need an IPv4 CIDR associated ","correct":false},{"id":"22b96919d7cd98b93972df376d49d356","text":"By default, all subnets you create can route to each other","correct":true}]},{"id":"4c7fb750-0ec0-4c9f-8dc6-378122edf97a","domain":"automation","question":"The engineering team of a digital marketing company has a lot of AWS Lambda functions directly created and managed using the AWS Console. The CTO has mandated that the code and the deployments are managed using templates and the code is stored in a code repository to enable proper version control processes. How can the team achieve this?","explanation":"SAM templates and CloudFormation templates can be used to manage the Lambda function code. Out of all the options, only CodeCommit can be used directly as a managed service for a code repository. S3 buckets are not used directly as a code repository.","links":[{"url":"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html","title":"What is SAM"}],"answers":[{"id":"a85a26cd33fe99adb584452fa780dce2","text":"Use CloudFormation templates to manage the lambda function code. Use S3 buckets for the code repository.","correct":false},{"id":"c5fe89ee64d944e98f26b2db3fa7cea2","text":"Use SAM templates to manage the lambda function code. Use CodeCommit for the code repository.","correct":true},{"id":"b670d764967a3aa65a1424279e37291c","text":"Use SAM templates to manage the lambda function code. Use S3 buckets for the code repository.","correct":false},{"id":"f33ec453646a2074aa930a61f466fc1a","text":"Use CloudFormation templates to manage the lambda function code. Use ECR for the code repository.","correct":false}]},{"id":"f0c47538-7997-40fc-9c82-27eea818fac2","domain":"security-comp","question":"A company has started running its e-commerce application in container workloads in AWS. The e-commerce application is running its web tier in Amazon ECS and the database tier in RDS all inside a VPC. Under the AWS shared responsibility model, which activities is AWS NOT responsible for?","explanation":"AWS takes care of the underlying software for managed services. For services such as EC2, the instance hypervisor and underlying hardware are managed and maintained by AWS. It is the customer's responsibility to monitor and manage the memory utilization of the containers in services such as ECS and EKS.","links":[{"url":"https://aws.amazon.com/compliance/shared-responsibility-model/","title":"Shared Responsibility Model"}],"answers":[{"id":"1dfb8644f2cce2dca687a152b45b7d54","text":"Patching the database instance software of RDS","correct":false},{"id":"b7a8057898c923210f320a3bcccbcbca","text":"Maintaining the underlying hardware infrastructure of the instances used by ECS","correct":false},{"id":"ac7ca331a57c32d9975616bf91327c82","text":"Monitoring and managing the memory utilization of the containers","correct":true},{"id":"9abfc73643ea9090379aae3cd89400d1","text":"Patching the instance hypervisor","correct":false}]},{"id":"c7e5ebf3-7eae-45bf-a02b-28bc06a6d575","domain":"mon-rep","question":"You have several CloudWatch Log Groups and Lambda Functions send logs to them. You need to use a tool to quickly search and analyze the log data in the log streams. The tool should automatically discover information in the Lambda logs such as the timestamp, max memory used and execution duration. It should also help you to perform the query using simple and pre-built query languages. Which tool is the best one for you to choose?","explanation":"CloudWatch Logs Insights is the most suitable tool to perform pre-build queries on CloudWatch Logs. Users do not need to transfer or transform the log streams. The log fields contained in the Lambda logs are automatically discovered. AWS Athena only performs queries on S3 objects. For Amazon ElasticSearch or Kinesis stream, extra configuration steps are required.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html","title":"Analyze log data with CloudWatch Logs Insights"}],"answers":[{"id":"01551b4fd4b50ed9ceb2be1d0e338932","text":"Use AWS Athena to run queries on the log streams. The query language of Athena is based on SQL.","correct":false},{"id":"2c4abdd6514dcf68f6bace3f1a769e4f","text":"Stream the log data to an Amazon Kinesis stream and perform real time queries or analysis in the stream.","correct":false},{"id":"79930c2280cb2398212ed1465b34b81e","text":"Export the log data to an Amazon ElasticSearch Service. Use ElasticSearch to perform queries or analysis.","correct":false},{"id":"7edc1b3e500b915d821d4db1db01280c","text":"Use CloudWatch Logs Insights to select the log groups and perform queries.","correct":true}]},{"id":"3b299de7-5eed-4f52-b332-e58a1788d451","domain":"networking","question":"You plan to use a placement group for separating important systems running on a small number of EC2 instances. For compliance reasons, every instance must be in a separate rack. Which of the following placement groups are the most suitable?","explanation":"Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks. Spread placement groups provide access to distinct racks, and are therefore suitable for mixing instance types or launching instances over time. Partition cluster groups are suitable for separating groups of instances on distinct hardware. Cluster placement groups are designed to place hardware as close as possible to increase performance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#concepts-placement-groups","title":"Placement Groups"}],"answers":[{"id":"ea65fadc86756ddcd18793374fe31349","text":"A Cluster Placement Group","correct":false},{"id":"e0f55d640e28ee8139799619632e4a68","text":"A Partition Placement Group","correct":false},{"id":"5b108266549d80fa8e65d875a00a8213","text":"A Spread Placement Group","correct":true},{"id":"93a08fc1ff6bbf8d8ea03e58b442f724","text":"A Dedicated Instance Placement Group","correct":false}]},{"id":"d17b79ff-5e85-4d51-826c-1d7d89d1977e","domain":"data-man","question":"What is the most secure way to ensure the long-term safety of objects you store in S3?","explanation":"By default, all requests to your S3 buckets require your AWS account credentials. However, if you enable Versioning with MFA delete, *two* forms of authentication are required to permanently delete an object.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html#MultiFactorAuthenticationDelete","title":"MFA Delete"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html","title":"Using Versioning"}],"answers":[{"id":"b2b33a739c212ba3d20147f3d3fac36b","text":"None of these options.","correct":false},{"id":"253efe32850426ce32c509612536c798","text":"Enable versioning on the bucket.","correct":false},{"id":"7e4fe344631393a9e197345b8ccd4aa9","text":"Encrypt the contents of the bucket.","correct":false},{"id":"b24a2a262afe3872a9020f786ccf9a6a","text":"Enable both versioning and MFA delete on the bucket.","correct":true}]},{"id":"762031fb-db4e-4e09-bab4-09bf0fc50472","domain":"high-avail","question":"You run a very popular fashion blog and during a major event your wordpress site struggles immensely with the amount of traffic that you are receiving. The wordpress site sits across a fleet of EC2 instances in an autoscaling group which scales based on CPU utilization. You notice from your CloudWatch metrics that your webservers appear fine, however your back end Aurora database is running at 100% CPU Utilization. What can you do to alleviate the situation?","explanation":"Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html","title":"Aurora Read Replicas"}],"answers":[{"id":"98a15d02b2c6ead5dff3036e99269acd","text":"Place the Aurora database in to a the same Autoscaling group as the EC2 instances and configure the launch configuration to deploy new Aurora instances whenever a node capacity reaches 80% for 5 minutes","correct":false},{"id":"f663aa878eb8286dbadf97aa0f95be56","text":"Add additional Aurora Read Replica Nodes and update your connection string on the webservers to point to the new nodes to spread the load","correct":true},{"id":"e38c819297b357216e3ddc63cecd22ec","text":"Migrate from Aurora to MySQL RDS instance with multi-AZ turned on to better handle the load","correct":false},{"id":"10239931e046b099c14736b5b9c422d0","text":" Place the Aurora database in to a separate Autoscaling group and configure the launch configuration to deploy new Aurora instances whenever a node capacity reaches 80% for 5 minutes","correct":false}]},{"id":"a7e0b35e-a0a9-43fb-86a8-4677970f57ae","domain":"mon-rep","question":"Your Auto-Scaling group is configured to launch a new EC2 instance whenever it detects an unhealthy instance in your Auto-Scaling group. However, you wish to be notified when this happens. Which of the following AWS services would you join with Auto-Scaling to achieve this?","explanation":"When you use Auto Scaling to scale your applications automatically, it is useful to know when Auto Scaling is launching or terminating the EC2 instances in your Auto Scaling group. Amazon SNS coordinates and manages the delivery or sending of notifications to subscribing clients or endpoints. You can configure Auto Scaling to send an SNS notification whenever your Auto Scaling group scales.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/latest/userguide/ASGettingNotifications.html","title":"SNS Notifications With Auto Scaling"}],"answers":[{"id":"f62772d94b939126ee608465cf5e0881","text":"SWF","correct":false},{"id":"ef4ba2f338cdf4b615ed280ff0b2777d","text":"SQS","correct":false},{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":true},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false}]},{"id":"32db26ef-70e3-4541-b3e5-7cde3ad73c9e","domain":"dep-prov","question":"In order to enable encryption at rest using EC2 and Elastic Block Store,  you must ________.","explanation":"To enable encryption, you must specify encryption when creating the EBS volume.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html","title":"About EBS Encryption"}],"answers":[{"id":"64237cc9809266ebc9fb12b6cede9aa8","text":"Configure encryption using X.509 certificates","correct":false},{"id":"aaed5dee4871b8256093659e8bbeba4a","text":"Configure encryption using the appropriate Operating Systems file system","correct":false},{"id":"4bd8d581e477097d1396901aab8b3cf4","text":"Mount the EBS volume in to S3 and then encrypt the bucket using a bucket policy","correct":false},{"id":"d8e355c7726e173ad29951d3461865d6","text":"Configure encryption when creating the EBS volume","correct":true}]},{"id":"6eb6dee0-9456-492e-afc7-b7c860b04c92","domain":"mon-rep","question":"You have a fleet of EC2 webservers behind an application load balancer. Your web application had some down time which involved some 5XX errors during a very important time in your business 1 week ago. Although you maintain application logs on individual EC2 instances, you do not store these logs anywhere central and unfortunately the EC2 instances that experienced the downtime have since been terminated. How could you review this log data?","explanation":"Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. These logs are encrypted and stored in S3.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html","title":"Application Load Balancer Access Logs"}],"answers":[{"id":"d53665e5cde1c8f661130ca2882d789a","text":"Open the AWS artifact service. Create a new artifact job and point the AWS artifact agents at the terminated EC2 instances. Download the metrics and review in CloudWatch.","correct":false},{"id":"49774faac3030c4425f2aa345cb89dc0","text":"If access logs is turned on for your application load balancer you could review this data by reviewing the logs in S3.","correct":true},{"id":"c13c66278433ebb606e21e9c9f5250fd","text":"Create a new AWS inspector job to pull the snapshots of the EC2 instances from S3 and run a report in conjuction with AWS Athena.","correct":false},{"id":"ae59c599efa75d438daecb43cc08ad41","text":"Use AWS X-ray to restore the logs from the terminated EC2 instances","correct":false}]},{"id":"1de9eb98-4b61-4178-b30f-68d5d6422439","domain":"mon-rep","question":"You need to monitor application-specific events every 10 seconds. How can you configure this?","explanation":"you need to configure a custom metric to handle application specific events and if you want to monitor at 10 second intervals, you need to use high-resolution metrics. Detailed monitoring reports metrics at 1 minute intervals.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"CloudWatch Custom Metrics"}],"answers":[{"id":"886f04b8880657a2d07a12c6355639a7","text":"Configure the application to send notifications using SNS every 10 seconds","correct":false},{"id":"8625460160031630c52a466b7a91f16b","text":"configure a high-resolution custom metric in CloudWatch","correct":true},{"id":"da902bf148db5983868bf3383162183a","text":"Select high-resolution metrics in CloudWatch","correct":false},{"id":"f3e15ffbefd8415dda26321a2912dca1","text":"Select detailed monitoring in CloudWatch","correct":false}]},{"id":"8b30bb8d-7142-4b59-8d3c-4d5033f31a8f","domain":"mon-rep","question":"You are working for a company which is migrating all of its data into S3, the migration is underway but your Security Architect is concerned that not all buckets are secure and wants you identify all buckets which allow public read or write. Which service can you use to find out?","explanation":"AWS Config allows gives you a view of the configuration of your AWS infrastructure and compares it for compliance against rules you can define","links":[{"url":"https://aws.amazon.com/blogs/aws/aws-config-update-new-managed-rules-to-secure-s3-buckets/","title":"AWS Config Rules to Secure S3"}],"answers":[{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":true},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"7c90c8f2a24f3a1a28525f19fb2c75ab","text":"AWS Inspector","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":false}]},{"id":"95096463-bf44-4a55-a938-134e17096ace","domain":"security-comp","question":"Instance 'A' and instance 'B' are running in two different subnets 'A' and 'B' of a VPC. Instance 'A' is not able to ping instance 'B'. Which of the following is a possible reason for this failure?","explanation":"The NACL on subnet B does not allow outbound ICMP traffic; and the security group attached to instance B does not allow inbound ICMP traffic.","links":[{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html#ACLs","title":"Network ACL Basics"}],"answers":[{"id":"b683f2644731441134b6c0db51d38f6e","text":"The policy linked to the IAM role on instance 'A' is not configured correctly; and the NACL on subnet 'B' does not allow outbound ICMP traffic.","correct":false},{"id":"48173a79d09d626e62e08c080f3dafcb","text":"The route table of subnet 'A' has no target route to subnet 'B'; and the security group attached to instance B does not allow inbound ICMP traffic.","correct":false},{"id":"788e0ab5d1b2c372b5d80c26c17c6a71","text":"The NACL on subnet B does not allow outbound ICMP traffic; and the security group attached to instance B does not allow inbound ICMP traffic.","correct":true},{"id":"87877573324ed4201e723046eb1d8409","text":"The security group attached to instance 'B' does not allow inbound ICMP traffic; the policy linked to the IAM role on instance 'A' is not configured correctly.","correct":false}]},{"id":"a9593b23-49e7-4831-b16d-b7618fd07bfd","domain":"mon-rep","question":"Which of the following ELB response Codes indicates a normal, successful response from the registered instances.","explanation":"A HTTPCode_Backend_2XX  indicates a normal, successful response from the registered instances.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html#loadbalancing-metrics-clb","title":"CloudWatch Metrics for Your Classic Load Balancer"}],"answers":[{"id":"e12e4cbddc5e0433d4f8b642c591b631","text":"HTTPCode_Backend_3XX","correct":false},{"id":"ed7ec39cbf617481ed14efc52061f350","text":"HTTPCode_Backend_5XX","correct":false},{"id":"e7415e6c2943791d842013f7aba6c120","text":"HTTPCode_Backend_4XX","correct":false},{"id":"4cbb9bc9e8892ec2b03ce9300089bbae","text":"HTTPCode_Backend_2XX","correct":true}]},{"id":"46d872a7-bf30-4880-9cbd-3a63c50fb75e","domain":"mon-rep","question":"You need to enable a CloudWatch alarm to alert you if an EC2 instance which holds a key customers database goes over 100% CPU Utilization for more than two minutes. Which service should you use?","explanation":"Detailed Monitoring collects data at 1 minute intervals, whereas Basic or Standard Monitoring is every 5 minutes. Artifact allows you to check which industry and regulatory compliance standards AWS adheres to.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html","title":"CloudWatch Detailed Monitoring"}],"answers":[{"id":"af64135daff929ae7e26207cfae5e24a","text":"CloudWatch Standard Monitoring","correct":false},{"id":"96778f7d823daef4c612d60aa5bf2312","text":" CloudWatch Detailed Monitoring","correct":true},{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":false},{"id":"3d3e141cae28c035547bafe32dea1423","text":"CloudTrail Expedite","correct":false}]},{"id":"dc1b5bc1-79d0-49ab-9c2f-e0dea66f0361","domain":"dep-prov","question":"You are helping to migrate a customer from their on-premise data center to AWS. The customer has over 1,000 users in their Active Directory service and wants to be able to using their existing on-premise directory to quickly and easily log into AWS. The customer wants to be able to continue using Microsoft Active Directory with AWS. How would you configure this set up for the customer?","explanation":"AD Connector helps connect your on-premises Microsoft Active Directory to the AWS cloud. AD Connector is designed to give you an easy way to establish a trust relationship between your Active Directory and AWS. With AD Connector, you can streamline identity management by sourcing and managing all your user identities from Active Directory. It also enables you to reuse your existing Active Directory security policies such as password expiration, password history, and account lockout policies. Also, your users will no longer need to remember yet another user name and password combination. SimpleAD does not connect existing on-premises AD to AWS. SimpleAD is a Microsoft Active Directory compatible directory from AWS Directory Service and supports common features of an active directory. AWS Directory Service for Microsoft AD is an AWS managed service that is hosted on the AWS cloud, it does not connect your AD with AWS. Creating IAM users, groups, and roles would not be feasible with 1,000 users and is not best practice.","links":[{"url":"https://docs.aws.amazon.com/quickstart/latest/active-directory-ds/architecture.html","title":"Deployment Scenarios and Architecture"}],"answers":[{"id":"63140d266692d56af00a75ac1e583ff5","text":"Use AWS Directory Service for Microsoft AD to connect the on-premise Microsoft Active Directory to AWS.","correct":false},{"id":"3d15e64aa60139df5f0cd74532823f6f","text":"Create IAM users, groups, and roles based on the current on-premise users to mirror their permissions on AWS.","correct":false},{"id":"2b121dea9ea29a83bc210acdacd096e4","text":"Use AD Connector to connect the on-premise Microsoft Active Directory to AWS.","correct":true},{"id":"59dc56b1ea5b335ecde98e0658573ae3","text":"Use SimpleAD to connect the on-premise Microsoft Active Directory to AWS.","correct":false}]},{"id":"daea596c-77ce-4763-bc26-c7eabbeb3fed","domain":"automation","question":"The CTO of an e-commerce company has mandated the use of Infrastructure as Code (IaC) services and tools to manage the application resources and processes. The engineering team has divided the resources into two groups: application resources and network (VPC) resources. The engineering team lead has been instructed to transform the configuration of these resources into templates that can easily be configured to prepare different environments. How can the engineering team lead accomplish this?","explanation":"CloudFormation can be used for the IaC requirements in provisioning and managing AWS resources including VPC resources and other managed services. Out of all the options, only OpsWorks can be used as a Configuration Management service for the management application level processes and workloads inside EC2 instances. OpsWorks is not used for provisioning network resources. Amazon EKS is for managing and orchestrating containers using Kubernetes.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/welcome.html","title":"OpsWorks"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-vpc.html","title":"EC2 VPC"}],"answers":[{"id":"9328d9954617430931d24f7812f5d613","text":"Use Amazon EKS for the network resources. Use OpsWorks for the application resources.","correct":false},{"id":"17ad6545a81ec8297ada8e103fe040f9","text":"Use OpsWorks for the network resources. Use CloudFormation for the application resources.","correct":false},{"id":"450822349ee2cf9d667ee39de6b39f39","text":"Use CloudFormation for the network resources. Use AppSync for the application resources.","correct":false},{"id":"1148de578466f53ad53ec6613254366b","text":"Use CloudFormation for the network resources. Use OpsWorks for the application resources.","correct":true}]},{"id":"e05ee44b-cd10-4658-9853-ff5cea9c9d32","domain":"automation","question":"The company has started experiencing deployment issues due to the increasing complexity of the application and the lack of a structured testing and release process. The DevOps team of the company plans to set up a continuous integration pipeline in AWS to improve the stability of the releases and through the enforcement of the use of automated tests. The Head of DevOps has been instructed to use managed services as much as possible to reduce the maintenance overhead of the continuous integration pipeline. How can the DevOps team accomplish this?","explanation":"CodeBuild and CodePipeline are managed services that can be used to easily build a continuous integration pipeline. CodeBuild can run the tests and CodePipeline can manage the pipeline steps for the CI testing and deployment pipeline. Given that managed services are preferred, running Jenkins in an EC2 instance is not the priority option. AppSync is for building GraphQL powered APIs and is not used for continuous integration pipeline requirements.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html","title":"Use CodePipeline with CodeBuild to Test Code and Run Builds"}],"answers":[{"id":"8adba880488851800b50283a6de55215","text":"Use Jenkins in an EC2 instance and CodePipeline for the continuous integration pipeline.","correct":false},{"id":"ce1176f044958de744d567d7ac7d0534","text":"Use Jenkins in an EC2 instance and AWS Step Functions for the continuous integration pipeline.","correct":false},{"id":"6574dc863e2d0855e66841be7aee54e7","text":"Use CodeBuild and CodePipeline for the continuous integration pipeline.","correct":true},{"id":"4c32bcc37cbaab42d77738b8b6875d19","text":"Use AppSync and CodeBuild for the continuous integration pipeline.","correct":false}]},{"id":"fa8987c8-41e8-43a5-b9d3-1446dce8e0ca","domain":"security-comp","question":"Your Security Architect would like to perform Penetration Testing on your entire AWS environment. They would like to schedule this as soon as possible, how should you approach this?","explanation":"Penetration Testing is allowed with prior approval from AWS","links":[{"url":"https://aws.amazon.com/security/penetration-testing/","title":"Pen Testing in AWS"}],"answers":[{"id":"7be6630643492a9ae03b1911fb54573a","text":"This will depend on the service and the type of test you want to perform - some will require permission","correct":true},{"id":"f61573848ecce2f46433c0841355646d","text":"You need to request approval before you can perform Penetration Testing in AWS","correct":false},{"id":"530b142692a4e704a1565e00789ad493","text":"You are free to perform Penetration Testing on your own environment any time you like","correct":false},{"id":"ddf28d1bf64dae2b0a151350c25ca26e","text":"There is no need to perform Penetration Testing because AWS automatically runs regular tests on your behalf and provides any security recommendations in the Trusted Advisor console","correct":false},{"id":"2e72d2d8a44bb5d14dd29d0fd1e97d65","text":"Penetration Testing is not supported at any time as it can be disruptive in a multi-tenant environment","correct":false}]},{"id":"a6f0b148-f08b-4a0e-bf13-791fe0af285e","domain":"security-comp","question":"You have a new manager who would like to introduce automated security assessments to allow you to test all of your applications running on EC2. Which AWS tool do you recommend?","explanation":"Inspector allows you to perform automated vulnerability assessments on applications running on EC2, Trusted Advisor can help you reduce cost, increase performance and improve security by optimizing your AWS environment, AWS Shield provides DDOS protection, Systems Manager is an Operational Management tool","links":[{"url":"https://aws.amazon.com/inspector/faqs/","title":"Inspector"}],"answers":[{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"9deb03cd21d41a691cdc24bfaab2820c","text":"Inspector","correct":true},{"id":"637d82e8a7206e87344161109cf7112d","text":"AWS Shield","correct":false},{"id":"8e75b153e61c22a8ea4e14aadc7cb4ee","text":"Systems Manager","correct":false}]},{"id":"463d990a-0af1-4e11-ad8a-c379444c02fd","domain":"automation","question":"A government agency currently runs all of their AWS workloads in a single region. Services utilized include S3, EC2, ECS, RDS, CloudFront, and Route 53 in multiple accounts. Their new business continuity plan calls for readiness in a separate AWS region in case disaster recovery is needed. Which approach will provide them with the most efficient way to manage their primary and business continuity environments?","explanation":"By default, CloudFormation templates are tied to a single account and a single region. However, CloudFormation allows for the creation of a custom resource that can launch a nested stack in another account or region with the appropriate IAM roles setup. The CloudFormation parameter and mappings sections don't provide cross-region functionality. Creating templates in each account would also work, but will require significant maintenance to keep resource definitions in-sync across all the templates.","links":[{"url":"https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/","title":"Multiple-account, multiple-Region AWS CloudFormation"}],"answers":[{"id":"b3128e59899200446191df0bbc118793","text":"Create AWS CloudFormation templates in each account that provision stacks with resources into the primary and business continuity regions using the CloudFormation 'Region' parameter","correct":false},{"id":"12eb00153c980c0e9e1fec8d03035f78","text":"Create AWS CloudFormation templates in each account and each region that provision stacks with the resources for their specific accounts/regions","correct":false},{"id":"fabf8fa8f7205c00448ae603a1d1af90","text":"Create AWS CloudFormation templates in a single account and configure the CloudFormation mappings section for the appropriate primary and business continuity regions","correct":false},{"id":"21d658a249d3a95d1de0860519bd585b","text":"Create an AWS CloudFormation template in a single account that defines custom resources to launch nested stacks into the other accounts and regions","correct":true}]},{"id":"57f94d74-595a-4aad-8cee-0e7ed18ebf82","domain":"data-man","question":"A legacy application uses an Elastic IP address (EIP) to bind a known IP address to an autoscaling workload.  What do you need to check before you can migrate another five applications with the same architecture?","explanation":"In this case the only service limit that we are concerned about is the one regarding Elastic IP addresses. By default an AWS account is allowed to use five Elastic IP addresses. In order to consume and more than that requires creating a support request to raise the limit.","links":[{"url":"https://docs.aws.amazon.com/en_pv/AmazonS3/latest/dev/transfer-acceleration.html","title":"Amazon S3 Transfer Acceleration"}],"answers":[{"id":"ad4836155de3dde58d4b9870ebd2a0dd","text":"Check your Service limits and ensure your AWS account limit is above five","correct":false},{"id":"12148935a909c7f3eafaab603ccb0b9e","text":"Check your Service limits and ensure your EC2 Launch limit is above five","correct":false},{"id":"97827048dd4d750fb2e1a46197c67079","text":"Check your Service limits and ensure your Elastic IP address account limit is above five","correct":true},{"id":"468189efd2eeb2d9cca37313a09189f6","text":"Check your Service limits and ensure your autoscaling group limit is above five","correct":false}]},{"id":"4871ce12-9da0-4e28-b1e4-373560dbdffa","domain":"automation","question":"A telecommunications company sends out monthly bills to their customers. Usage is accumulated during the month by nightly batch jobs that process call details. The company is in the process of migrating the billing system to AWS to reduce costs. What approach will provide them with the most cost effective solution for the compute portion of their nightly batch runs?","explanation":"AWS Batch provides allocation strategies to consider capacity and throughput in addition to cost when provisioning instances for jobs. This is a newer feature that provides more flexibility than the previous scheme that chose an instance that was the best fit based on vCPU, memory, and GPU requirements. Creating a pool of EC2 Reserved Instances might result in unused capacity if workload requirements change. Lambda is not currently available as a compute resource for AWS Batch.","links":[{"url":"https://aws.amazon.com/batch/","title":"AWS Batch"},{"url":"https://aws.amazon.com/blogs/compute/optimizing-for-cost-availability-and-throughput-by-selecting-your-aws-batch-allocation-strategy/","title":"Optimizing for cost, availability and throughput by selecting your AWS Batch allocation strategy"}],"answers":[{"id":"3220afaa7c6fd31e7a8d35ce1e2df1fa","text":"Configure AWS Batch to choose an instance type for each job based on vCPU, memory, and GPU requirements at the lowest cost.","correct":false},{"id":"96ef18a4d93470acb7dbd558eb666ca3","text":"Specify AWS Lambda as the compute resource for AWS Batch. Invoke the appropriate Lambda functions for each job.","correct":false},{"id":"0bb4d60773589240e55a5a506ee84275","text":"Use AWS Batch allocation strategies to define capacity, throughput, and cost priorities for instance type provisioning.","correct":true},{"id":"8633cd86b6633324660fa073362c2f98","text":"Schedule jobs with AWS Batch into a pool of EC2 Reserved Instances that contains enough servers for the minimum number of jobs that will be run on any one night. Use an Auto Scaling Group to provision Spot Instances to handle any additional demand.","correct":false}]},{"id":"3d005524-36c5-4851-8500-665f29e4c0b7","domain":"networking","question":"A team of engineers has set up a VPC with EC2 web instances in the public subnets and the RDS instances, EC2 background worker instances and the NAT Gateways in the private subnets. The team has noticed that the background worker instances do not have internet connectivity and the issue might be due to the VPC setup. How can the team resolve this issue?","explanation":"NAT gateways need to be in the public subnet in order for the resources in the private subnet to have internet connectivity. Usually, the default route for a private subnet points to the NAT gateway. If the NAT gateway is in a private subnet, then the traffic cannot reach the internet and the resources in the private subnet won't have internet connectivity.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html","title":"VPC Scenario"}],"answers":[{"id":"73cbf7879ad8bbb9f58a4e7bb9a999c9","text":"Turn off the NAT Gateways in the private subnets. Create new NAT Gateways in the public subnets.","correct":true},{"id":"85f3551b459e74e7bf48aa77dbe7a729","text":"Create a new VPC with only private subnets. Transfer the background worker instances to the new VPC. Set up a peering connection to the new VPC from the original VPC.","correct":false},{"id":"210235cf61b065b37cf583e72630e77c","text":"Create a new VPC with only private subnets. Transfer the background worker instances and the NAT Gateways to the new VPC.","correct":false},{"id":"4fd2e84c5c3112b4c0f92f51aed239f1","text":"Set up new NAT Gateways in the private subnets until the background worker instances start getting internet connectivity.","correct":false}]},{"id":"594aa1e8-45f7-4cc9-a4d9-0241868d6e47","domain":"high-avail","question":"Your company has an application running on Amazon EC2 instances behind an Elastic Load Balancer (ELB). The instances are in an Auto Scaling group that spans multiple Availability Zones. You are a SysOps Administrator and as your check the health of your Auto Scaling group you notice that instances that are failing the Load Balancer health checks are not being replace. How would you remedy this situation?","explanation":"If you attached a load balancer or target group to your Auto Scaling group, you can configure the group to mark an instance as unhealthy when Elastic Load Balancing reports it as OutOfService. If you configure your Auto Scaling group to use the Elastic Load Balancing health checks, Amazon EC2 Auto Scaling determines the health status of the instances by checking both the EC2 status checks and the Elastic Load Balancing health checks. After an instance has been marked unhealthy because of an Amazon EC2 or Elastic Load Balancing health check, it is almost immediately scheduled for replacement. Re-creating an Auto Scaling group or launching new instance types would not solve the issue. HTTPS port 443 will also have no impact.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Checks for Auto Scaling Instances"}],"answers":[{"id":"0cc2395369502a58ef4d3168644555f6","text":"Save the AMIs of the instances and terminate the existing instances. Re-launch the AMIs with new instances under the latest generation instance type (i.e. m4 to m5).","correct":false},{"id":"d28a2bf653d5f98af3ed710e025e87a1","text":"Configure the Auto Scaling group to determine health status using ELB health checks.","correct":true},{"id":"c4cb9c892c467fcccbff0e8640bd4d5b","text":"Delete the Auto Scaling group and re-create a new one using the same configuration.","correct":false},{"id":"a26b5f88aff4c163eb6d9cad25ac904e","text":"Ensure that HTTPS port 443 is open on all the security groups attached to these instances.","correct":false}]},{"id":"z43ur61l-age6-mxal-w90h-oxgkf0r1rqdm","domain":"data-man","question":"You've been tasked with storing petabytes of non-critical data. It might never be accessed and you'll have 24 hours to get it back if you do need it. What is the most cost-effective storage solution in this scenario?","explanation":"Glacier is the most cost-effective option here.","links":[{"url":"https://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html","title":"About Glacier"}],"answers":[{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":true},{"id":"248c1b0bcb74394beb4a08030c4a6847","text":"EBS","correct":false},{"id":"9d4c23c2bbecbcde969cff9721fd18f5","text":"S3-IA","correct":false},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":false}]},{"id":"70431b97-0c8f-4053-8f08-39a853c40574","domain":"mon-rep","question":"An insurance company has a serverless application setup utilizing API Gateway, AWS Lambda, and DynamoDB for its web application. The engineering manager of the company has instructed the team to identify, track, and detect all potential bottlenecks related to POST method calls being performed by the AWS Lambda functions. How can the team accomplish this?","explanation":"By modifying the code to use the X-Ray snippets, potential bottlenecks accessing different resources and endpoints can easily be detected. CloudTrail is used for auditing API call logs. CloudWatch is used for monitoring resource usage and metrics. X-Ray is a distributed tracing system. Using AppSync instead of the API Gateway would not solve the monitoring and tracing requirements.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html","title":"AWS X-Ray"}],"answers":[{"id":"79b2cfe0b560a65754739eed236044d0","text":"Use AWS CloudTrail","correct":false},{"id":"ec9e1adbee82bfcb915a1773a27a9a43","text":"Use AppSync instead of API Gateway","correct":false},{"id":"ade3d51642e3463e7659ec0f6b480094","text":"Use AWS CloudWatch","correct":false},{"id":"ac6d6d3d9e1a43f18cb5cf78b181951b","text":"Use AWS X-Ray","correct":true}]},{"id":"216ba36a-8f55-47f4-80c8-44ac20c3a739","domain":"security-comp","question":"A new project is beginning in your company. The project utilizes two S3 buckets. All of the project members already have IAM user accounts provisioned. New members might join the project later on and some might leave before it's finished. What's the easiest way to configure access to the S3 bucket?","explanation":"All of the proposed solutions are technically feasible. However, you should grant the permissions on the whole group rather than for every individual user. If additional users require the same access, you can give it to them by adding them to the group. When a user no longer needs access to a resource, you can remove them from the group.","links":[{"url":"https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf","title":"AWS Security Best Practices"}],"answers":[{"id":"9b0876d4f2d9fd237877d5748604315b","text":"Create two IAM policies, each allowing access to a bucket. Attach the policies to all the IAM users.","correct":false},{"id":"cde3f9182db55502f071cdb552e75a7c","text":"Create two bucket policies, one for each bucket. Allow access for all the IAM users in the bucket policies.","correct":false},{"id":"606624dc111236ee567ddebb534beb0c","text":"Create two IAM policies, each allowing access to a bucket. Attach the policies to a group where you add all the IAM users.","correct":true},{"id":"80920c27da15505dd506eeabd067fc3f","text":"Create an IAM role which can access the buckets. Create inline policies to allow each IAM user to assume the role.","correct":false}]},{"id":"c6a8e29b-70b4-4a72-b587-8ed388d71004","domain":"security-comp","question":"You are an administrator with full admin access to S3. There are several S3 buckets within your organization that need to comply with a policy that requires all objects to be encrypted in-transit. What data encryption mechanism would you apply to fulfill this requirement?","explanation":"Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, use a master key you store within your application. Server-side encryption is encrypting data at rest. SSE-S3, SSE-KMS, and SSE-C are methods of server-side encryption and would not fulfill a data in-transit encryption policy.","links":[{"url":"https://docs.aws.amazon.com/en_pv/AmazonS3/latest/dev/UsingClientSideEncryption.html","title":"Protecting Data Using Client-Side Encryption"}],"answers":[{"id":"7a36a5c2c5cf1986b38d1310f69352a6","text":"Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3).","correct":false},{"id":"163a4655e01a9cbc21d84cff0d1b33a1","text":"Use Server-Side Encryption with Customer-Provided Keys (SSE-C).","correct":false},{"id":"6d50168a818719b2b2c1e9cd4f07f38a","text":"Use Client-Side Encryption.","correct":true},{"id":"034ffe67b63ae2e68b278955fa9e740c","text":"Use Server-Side Encryption with Keys Stored in AWS KMS (SSE-KMS).","correct":false}]},{"id":"1db48074-bc12-4a56-92ce-865f2766c080","domain":"dep-prov","question":"You are running an application where you need to bring your own licenses for your AWS EC2 instances. To maintain the license compliance, the instances should be consistently deployed to the same physical servers over time. And the number of sockets and physical cores used in EC2 need to be well controlled. Which instance purchasing option would you choose?","explanation":"In this scenario, it is required that the instances are always deployed to the same physical hosts. Dedicated hosts should be chozen and this option also supports Bring Your Own License (BYOL). Dedicated instances are dedicated to a single customer, however it is not guaranteed that they are deployed to the same physical servers. Both scheduled reserved and reserved instances are incorrect because they are not dedicated to the same machines.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html","title":"Dedicated hosts"}],"answers":[{"id":"46c8b5c0c6ebe7840383ec6b058d0394","text":"Dedicated instances","correct":false},{"id":"bcc129e8d9496a05a331c903d4c5bcb0","text":"Reserved instances","correct":false},{"id":"9d6afa7b92408e486a11a7f9b3179f8f","text":"Dedicated hosts","correct":true},{"id":"82595be34ecee78fa9be27b7d108b98b","text":"Scheduled reserved instances","correct":false}]},{"id":"4c1c93f7-a7c8-4c21-be81-6719e4c149e6","domain":"dep-prov","question":"A company has an existing static blog site hosted on top of Amazon S3 that has been running for weeks. The development team has been instructed to upgrade the blog site to serve dynamic content. The development manager has mandated that managed services and serverless architecture patterns must be used as much as possible. How can the development team accomplish this?","explanation":"For a serverless architecture, the API Gateway, Lambda, and DynamoDB combo would allow a user to prepare serverless API endpoint that allows custom logic to be performed inside a Lambda function. The DynamoDB table(s) would contain the data being processed by the Lambda function.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html","title":"API Gateway - Getting Started with Lambda Integration"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-on-demand-https-example-configure-event-source_1.html","title":"Lambda - Configure Event Source"}],"answers":[{"id":"3de8f0c485401d6141e7f0aca4b8c68e","text":"Use API Gateway, Lambda, and DynamoDB for the API serving the dynamic content.","correct":true},{"id":"5780728868d4030a2f6f36eedfd3f2d5","text":"Use API Gateway, CodeBuild, and RDS for the API serving the dynamic content.","correct":false},{"id":"12c194830010e504e8e7e72650cb6952","text":"Use API Gateway, CodeBuild, and DynamoDB for the API serving the dynamic content.","correct":false},{"id":"ab2bbd6f5fb42ed01a72bc28be974191","text":"Use API Gateway, SQS, and RDS for the API serving the dynamic content.","correct":false}]},{"id":"2a9f2907-f109-459f-99da-63a648bcfb8a","domain":"dep-prov","question":"You are a SysOps Administrator at a fast growing start up that has scripted most of their infrastructure. You have a fleet of EC2 instances behind an elastic load balancer. When a new instance is launched, it performs a number of system updates before automatically copying the website's code from an S3 bucket. Due to the number of steps taken when launching a new instance, it can sometimes take up to 5 minutes for the new instance to be a fully functioning web server. This length of time is now causing a problem, as the Elastic Load Balancer reports the new instance to be unhealthy and your autoscaling group then deletes it before it can become live. What should you do to prevent this from happening again?","explanation":"","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html#health-check-configuration","title":"Health Check Configuration"}],"answers":[{"id":"9f6498b7a4b36388e649e7fc57f3efd3","text":"Reduce the number of automated steps so that the instance provisions faster and becomes healthy faster.","correct":false},{"id":"53342f9f4014b177b4680206ea1e699d","text":"Get rid of autoscaling and simply add new instances manually when you need them.","correct":false},{"id":"ffe5a0819729e3a4da24204b5a6486a0","text":"Adjust the health check on your elastic load balancer so that an instance is considered healthy within 10 seconds of it serving HTTP traffic.","correct":true},{"id":"29f0d143ae42d3a09f8168087f049ef5","text":"Pre-warm the elastic load balancer so that it can handle more requests faster.","correct":false}]}]}}}}
