{"data":{"createNewExamAttempt":{"attempt":{"id":"e8ba3284-cd0c-4aac-ae02-75800f3ccc49"},"exam":{"id":"98e3ed1c-9fae-4b03-8ee6-17de7fb46408","title":"AWS Certified SysOps Administrator - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"7432af70-890b-411b-abbb-6b0b403aa3f5","domain":"mon-rep","question":"You're an ops manager and you want one of your AWS accounts to receive the price breaks associated with Consolidated Billing for Organizations. What needs to happen for you to be included in the Organization and receive the discounted pricing?","explanation":"You can invite existing AWS accounts to join your organization. When you start this process, AWS Organizations sends an invitation to the account owner, who then decides whether to accept or decline the invitation. You can use the AWS Organizations console to initiate and manage invitations that you send to other accounts. You can send an invitation to another account only from the master account of your organization.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html","title":"Inviting an AWS Account to Join Your Organization"}],"answers":[{"id":"8cacf7f83f0c9a58c2e264de726c764f","text":"You must ask AWS to associate your account with the Master account.","correct":false},{"id":"5f46574bd37f7ef74f5a95c1debbad29","text":"The Master account must ask AWS to associate your account with theirs.","correct":false},{"id":"8a358ee9ae18997a0c91f0541fad6f77","text":"You must request to be added to consolidated billing.","correct":false},{"id":"3b0f522229b67ca4ea9eb21840d96e41","text":"The Master account must send you an invitation to join your account with theirs and you must accept.","correct":true}]},{"id":"7707b89f-e00f-474f-82ae-d360914577c2","domain":"dep-prov","question":"You are developing a Python Flask application that will need to scale with increasing users. The application will leverage a MySQL database to store user credentials. You also require that the application be easy to update by deploying new versions regularly with little to no maintenance. Which of the following AWS services would be the best choice?","explanation":"With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. The other choices would not fulfill all the requirements of scalability, no maintenance, and regular updates.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html","title":"What is Elastic Beanstalk?"}],"answers":[{"id":"913da08ad261e37a0432a2f5fd467840","text":"AWS CodeDeploy.","correct":false},{"id":"2a031c2217eafaf3506b5a726dc4664d","text":"Amazon EC2 with Amazon RDS.","correct":false},{"id":"24622bea15b6a709cb6b20ebbc63e9d9","text":"AWS CodePipeline with Amazon DynamoDB.","correct":false},{"id":"57302b10ca32edec882a5c270bb3f84d","text":"Amazon Elastic Beanstalk.","correct":true}]},{"id":"44323aa9-db88-4c8e-8594-5af3edab91b8","domain":"networking","question":"You're consulting for a consumer electronics company that markets its products globally. A new customer-facing application will be deployed in seven AWS Regions worldwide. Business logic will be handled by microservices deployed on EC2 instances in each region. The data layer will be hosted on Amazon Aurora in a single AWS Region. Which architecture will provide the highest performing solution for end users?","explanation":"A Route 53 latency routing policy will send requests to the destination with the lowest latency, generally resulting in the best performance. A Route 53 geolocation routing policy will probably not provide better performance than a latency routing policy. Even though targets may be physically closer, they may involve more network hops. Geolocation policies are generally used to serve localized content. Application Load Balancers work well for microservice architectures since targets can be registered as a specific port on an EC2 instance. CloudFront path patterns are for routing different file types, not for distinguishing origins in different regions.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy"},{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html","title":"What Is an Application Load Balancer?"}],"answers":[{"id":"562b2939dfd8d3940e82593f75d73725","text":"Implement an Amazon Route 53 record set for the application with a geolocation routing policy. Use an ELB Network Load Balancer in front of the EC2 instances in each region","correct":false},{"id":"9a5a8c536cd4c0209770d8e9b9897abd","text":"Create an Amazon Route 53 record set for the application with a latency routing policy. Deploy an ELB Application Load Balancer in front of the EC2 instances in each region","correct":true},{"id":"915fca3a9a0bf0eaf51de3fe684ef9e0","text":"Configure an Amazon Route 53 record set for the application with a geolocation routing policy. Implement an ELB Application Load Balancer in front of the EC2 instances in each region","correct":false},{"id":"8f8597399845c09e57ce08b6791b3ab0","text":"Deploy the EC2 instances behind an ELB Network Load Balancer in each region and set each one up as an Amazon CloudFront origin. Create path patterns to route all requests to the load balancer in the desired region","correct":false}]},{"id":"082f8b00-13da-4a51-918c-f976a16580ca","domain":"high-avail","question":"You have a web application that queries ElastiCache to cache your database queries. You are using Memached with ElastiCache and you use CloudWatch metrics to monitor your memcached performance. You notice that two metrics, Evictions (The number of non-expired items the cache evicted to allow space for new writes) and GetMisses (The number of get requests the cache has received where the key requested was not found), are getting very high. What should you do to scale your environment further?","explanation":"You should increase the number of nodes in your Memcached cluster or increase the size of each node in your cluster.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CacheMetrics.Memcached.html","title":"Metrics for Memcached"}],"answers":[{"id":"ba0c29a489d86a1d6386e6b65cbd93d3","text":"Use CloudFront as an alternative caching engine.","correct":false},{"id":"3678711f6b0323c2c64b95f85b586d6d","text":"Migrate from Memcached to Redis.","correct":false},{"id":"fdc0f2a5336b08f741567e4b5c2cc2a0","text":"Decrease the number of nodes in your memcached cluster or decrease the size of each node in your cluster.","correct":false},{"id":"e97e7e506a3503958146ba4d62e89c26","text":"Increase the number of nodes in your Memcached cluster or increase the size of each node in your cluster.","correct":true}]},{"id":"a9593b23-49e7-4831-b16d-b7618fd07bfd","domain":"mon-rep","question":"Which of the following ELB response Codes indicates a normal, successful response from the registered instances.","explanation":"A HTTPCode_Backend_2XX  indicates a normal, successful response from the registered instances.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html#loadbalancing-metrics-clb","title":"CloudWatch Metrics for Your Classic Load Balancer"}],"answers":[{"id":"e12e4cbddc5e0433d4f8b642c591b631","text":"HTTPCode_Backend_3XX","correct":false},{"id":"e7415e6c2943791d842013f7aba6c120","text":"HTTPCode_Backend_4XX","correct":false},{"id":"ed7ec39cbf617481ed14efc52061f350","text":"HTTPCode_Backend_5XX","correct":false},{"id":"4cbb9bc9e8892ec2b03ce9300089bbae","text":"HTTPCode_Backend_2XX","correct":true}]},{"id":"23e299c8-e80d-4333-b726-4f8ef1e89cfc","domain":"security-comp","question":"You want to restrict who can access a specific bucket that the development team use to store artifacts from their development pipeline. What kind of in-line policy will you need to use to insure this access","explanation":"Resource based policies are inline policies that restrict access to a specific resource, a good example of this is an S3 bucket policy.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html","title":"Policies and Permissions"}],"answers":[{"id":"8e2ddf5878aac8b5d22a6acab856040d","text":"Access Policy","correct":false},{"id":"2d53d9afa60c8a03ddae8baa0cb72fb2","text":"Resource-based Policy","correct":true},{"id":"eeec60437e6675cbd890b5f482628cfc","text":"Identity-based Policy","correct":false},{"id":"33b02b9157e2a10758df1edb0fa38865","text":"Service Control Policy","correct":false}]},{"id":"723a6cb2-65a0-454f-b391-8647401fa54d","domain":"networking","question":"A photo sharing application is growing in popularity.  The application uses S3 to store photographs. Your boss has asked you how you can improve the upload and download times for your end users?","explanation":"S3 upload and download times can be improved by enabling Transfer Acceleration. This leverages Points of Presents (PoPs) in the CloudFront network to provide connection points closer to users, thereby improving transfer speeds.  Disabling Object Versioning or enabling Intelligent Tiering would not affect upload or transfer speeds.  Lastly, BitTorrent support is for publicly-available files and is designed to increase availability of files and reduce S3 costs but does not improve upload and download times from the S3 service itself.","links":[{"url":"https://docs.aws.amazon.com/en_pv/AmazonS3/latest/dev/transfer-acceleration.html","title":"Amazon S3 Transfer Acceleration"}],"answers":[{"id":"792c81205244e2382feec0be4a8a8716","text":"Enable S3 BitTorrent support","correct":false},{"id":"3992a5af710f81ade0226fff00fedf2d","text":"Enable S3 Intelligent Tiering","correct":false},{"id":"5c6679012efaeb73c2123036ba676303","text":"Enable S3 Transfer Acceleration on your buckets","correct":true},{"id":"26b2f3dfb7078bf49a2aafd952d45a13","text":"Disable S3 Object Versioning","correct":false}]},{"id":"9ad79f24-9def-4d1e-9419-2037fdeda2cf","domain":"data-man","question":"In your company, in order to meet compliance requirements, production files in all S3 buckets need to be replicated into S3 buckets in a different region. These files already have a prefix of PROD. Other files without this prefix should not be copied. Which of the following is the fastest and most cost-efficient way to achieve this requirement?","explanation":"Users can easily configure Cross-Region Replication to copy S3 objects to a bucket in another region. In the replication rule, the source can be the entire bucket, a prefix or tags. The solution of using the EC2 instance is not cost-efficient. The Lambda function should only copy files with the PROD prefix. The \"aws s3 cp\" command cannot replicate new or updated files to the target.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html","title":"S3 Cross-Region Replication"}],"answers":[{"id":"b4d89a58bd92a685500e925adf4d68d6","text":"Create a T2.micro EC2 instance. Set up a cron job to sync production files to the target S3 buckets every 10 minutes.","correct":false},{"id":"e0c7b585219f6c9ec9213169d6c29f23","text":"Use AWS CLI command \"aws s3 cp s3://sourceBucket/PROD/* s3://targetBucket/PROD/* --recursive\" to replicate production files.","correct":false},{"id":"db95dac10149c21840a7dd4c3e6694b2","text":"Configure a Cross-Region Replication rule in S3 buckets. Only replicate the objects with the prefix of PROD to the target S3 buckets in another region.","correct":true},{"id":"d786d6ad7df12da04c1096d758233df6","text":"Use a Lambda function to copy all objects in S3 buckets to another region. Then delete the files that do not have the PROD prefix.","correct":false}]},{"id":"694ba690-219c-47cc-aca7-8e52a305016a","domain":"dep-prov","question":"Your 16TB gp2 EBS volume is experiencing I/O issues causing performance issues for the application. The Application Support team asks you if you can fix this without downtime, what do you recommend?","explanation":"At 16TB, the gp2 volume is already allocated the maximum 16,000 IOPS, in order to increase the I/O bandwidth, you will need to change to a Provisioned IOPS (io1) type volume type. You can modify the volume type without detaching the volume.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html","title":"EBS Volume Types"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modify-volume.html","title":"Modifying EBS Volumes"}],"answers":[{"id":"8e79d84c35c355fc300b424999da7727","text":"Migrate the data to S3","correct":false},{"id":"7fcf791ac787fb7fb7622591d6bfea93","text":"Modify the volume type to io1","correct":true},{"id":"3453d4afff7ffbc49232d037fd2b6819","text":"Detach the volume and re-attach an io1 volume","correct":false},{"id":"77a3dce9a889c55f548615174dca7769","text":"Increase the size of the volume which will increase the IOPS capability","correct":false}]},{"id":"e05ee44b-cd10-4658-9853-ff5cea9c9d32","domain":"automation","question":"The company has started experiencing deployment issues due to the increasing complexity of the application and the lack of a structured testing and release process. The DevOps team of the company plans to set up a continuous integration pipeline in AWS to improve the stability of the releases and through the enforcement of the use of automated tests. The Head of DevOps has been instructed to use managed services as much as possible to reduce the maintenance overhead of the continuous integration pipeline. How can the DevOps team accomplish this?","explanation":"CodeBuild and CodePipeline are managed services that can be used to easily build a continuous integration pipeline. CodeBuild can run the tests and CodePipeline can manage the pipeline steps for the CI testing and deployment pipeline. Given that managed services are preferred, running Jenkins in an EC2 instance is not the priority option. AppSync is for building GraphQL powered APIs and is not used for continuous integration pipeline requirements.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html","title":"Use CodePipeline with CodeBuild to Test Code and Run Builds"}],"answers":[{"id":"4c32bcc37cbaab42d77738b8b6875d19","text":"Use AppSync and CodeBuild for the continuous integration pipeline.","correct":false},{"id":"ce1176f044958de744d567d7ac7d0534","text":"Use Jenkins in an EC2 instance and AWS Step Functions for the continuous integration pipeline.","correct":false},{"id":"8adba880488851800b50283a6de55215","text":"Use Jenkins in an EC2 instance and CodePipeline for the continuous integration pipeline.","correct":false},{"id":"6574dc863e2d0855e66841be7aee54e7","text":"Use CodeBuild and CodePipeline for the continuous integration pipeline.","correct":true}]},{"id":"c4f661ee-5ea1-4e69-9440-52bea0321a6a","domain":"mon-rep","question":"You have set CloudWatch billing alarms for your instances running in eu-west-2. However, when you try to access the billing information and alarms, no information is visible. Why might this be?","explanation":"Billing and Alarm data can be accessed only from the us-east-1 region.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/free-tier-alarms.html","title":"Creating a Billing Alarm"}],"answers":[{"id":"cd2c9fa4324b5861276dbbf7f4f593a8","text":"You need to login as the root user to see such information.","correct":false},{"id":"7f3a3688c3f0cddb24c07255b9d13767","text":"Billing and Alarm data can be accessed only from the us-west-1 region.","correct":false},{"id":"fb7a7d16c3f39960e7afde6babd422e1","text":"Billing and Alarm data can be accessed only from the us-east-1 region.","correct":true},{"id":"b5ff6f2dfd759f7e70f27d7529e4462b","text":"You need to login as the account owner to see such information.","correct":false}]},{"id":"1ccc6579-f892-435f-a85c-5b976ac4f5cc","domain":"networking","question":"You have a stateless web application running on a several EC2 instances behind a Classic Load Balancer. As a SysOps Administrator you check the CPU utilization in CloudWatch for the instances and see that only one of the instances is running at 90% CPU utilization. The other EC2 instances are running at 10% CPU utilization. How would you troubleshoot this issue?","explanation":"With sticky sessions, the load balancer binds a user's session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance. Disabling the sticky sessions would evenly distribute the load. Cross-zone load balancing and connection draining would not solve the issue - Cross-zone load balancing distributes requests evenly across instances in all Availability Zones while connection draining stops sending requests to instances that are de-registering or unhealthy. Changing the instance sizes would not solve the issue.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html","title":"Configure Sticky Sessions for Your Classic Load Balancer"}],"answers":[{"id":"a7088c71d4d9d933ef7db7bc61e57957","text":"Enable cross-zone load balancing to evenly distribute the load across all EC2 instances.","correct":false},{"id":"3d06f0cd6554a14e6822464e6f4a1697","text":"Decrease the connection draining duration to allow the load to move from the instance with high CPU utilization sooner.","correct":false},{"id":"b0e61186c84d6eb0c683ca1c20263258","text":"Decrease the instance size of the instances at 10% CPU utilization while increasing the instance size of the instance at 90% CPU utilization. Add additional instances if the load is still unbalanced.","correct":false},{"id":"b98d3b1c376f2c2a52d9e06913946d0e","text":"Disable sticky sessions on the Classic Load Balancer. Check CloudWatch to see if the issue is resolved.","correct":true}]},{"id":"f0c663f6-c133-4064-93f1-5a68ec604f86","domain":"networking","question":"A company is developing a software product on AWS. The product requires some dependencies on an external software application developed by another company. In order for the product to run properly, it must connect with the external software that has a configured AWS PrivateLink to run some tasks. Both companies want the connection between the product and the external application to be secured privately and not over the open Internet. How would you configure this connection?","explanation":"An interface VPC endpoint is required to use AWS PrivateLink. In this case, since the external software application has configured a PrivateLink, connecting the interface VPC endpoint to the PrivateLink will provide private connectivity. A VPN connection and Direct Connect are best suited for connectivity between AWS and an on-premise data center. A VPC endpoint is more appropriate in this case. Cross account access would not apply in this case.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html#what-is-privatelink","title":"What Is Amazon VPC?"}],"answers":[{"id":"82d6bd6a8ad9b046abc832a10aab46ec","text":"Configure a Direct Connect connection between your software product and the external software application's VPC. Data traversed over this connection will be private.","correct":false},{"id":"396ed1524965dd42f15a5584a5df2111","text":"Set up a VPN connection between a gateway endpoint on your VPC and a customer gateway in the external company's VPC. Encrypt the IPSec tunnel to ensure private connectivity.","correct":false},{"id":"125e90bd89dd9a655f80754756185ade","text":"Put the product within a VPC and configure a VPC endpoint for the external application. Use the elastic network interface in the subnet with a private IP address.","correct":true},{"id":"a295a681bdc68fccdefa0245a0f3cb22","text":"Create a IAM role for the product. Enable cross account access for the product to communicate with the external software application to run its dependencies.","correct":false}]},{"id":"2d62b529-330e-4757-817c-853f19d31841","domain":"security-comp","question":"Regions are designed with at least two, often more, Availability Zones. What is the purpose of these availability zones?","explanation":"Regions consist of at least two Availability Zones this is with availability in mind, and to ensure fault isolation.","links":[{"url":"https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf?refid=em_","title":"AWS Security Best Practices"}],"answers":[{"id":"504bc5d3c03b93a69a8343b7b1cd2c29","text":"To provide different levels of network security.","correct":false},{"id":"0d3e0fbd7b193b148b931c20c2a23388","text":"To provide a low latency connection for end users.","correct":false},{"id":"9d85908f6c7d0a68f913f8c8843b4980","text":"To allow for load balancing.","correct":false},{"id":"2dbf99411ef147bbbd86e4426a961691","text":"To provide fault isolation.","correct":true},{"id":"2110ad4d6cecb9bdbf10d127442c9361","text":"To allow for HA (high availability) design.","correct":true}]},{"id":"c9f0e61e-627b-421e-8a57-0d04983c25c4","domain":"security-comp","question":"You are a consultant working for a company who has recently completed their migration from an on-premise data centre to AWS. Most of the migration has been for EC2 instances, which have been sized to match the specifications they were originally using on-premise (CPU, memory, etc.). They have setup a Business Support plan with AWS. The technical manager is unhappy with the high costs, and wants to find ways to reduce them. What would the simplest way be to find ways to reduce their AWS costs in the short-term?","explanation":"This is a very common scenario for businesses migrating to the cloud, and discovering the operational expenditure (OPEX) costs of AWS. AWS Trusted Advisor has a lot of simple and effective recommendations for Cost Optimization. Some may not be applicable in your case, but it is a very easy way to find potential options for reducing your costs. These features are unlocked with the AWS Support Plans of Business or Enterprise. CloudWatch metrics can be very useful for right-sizing your instances (aligning the needs of the application workload with the instance specifications), but CloudWatch will also automate this to an extent as well by finding under-utilized instances. Reducing your support plan is usually an unwise move for production workloads, despite their cost, as they can be very important when outages are experienced. Advocating for a transition to PaaS and SaaS is definitely a strategic cost-saving measure, but it forms part of a long term business strategy, since it involves significant resources, both in time and money","links":[{"url":"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/","title":"AWS Trusted Advisor"},{"url":"https://d1.awsstatic.com/whitepapers/architecture/AWS-Cost-Optimization-Pillar.pdf","title":"AWS Whitepaper - Cost Optimization Pillar"}],"answers":[{"id":"55af961ef2471aec130c45657098b110","text":"Investigate recommendations from AWS Trusted Advisor's automated checks","correct":true},{"id":"39788d9f10797f3b8c876dbe95022c0d","text":"Advocate a transition to more cost-effective PaaS and SaaS solutions","correct":false},{"id":"b4e67747b2a23a55a53554b964d1ab9b","text":"Reduce the expensive AWS Support Plan to lower costs","correct":false},{"id":"52107942e028b953519341a78f6815ca","text":"Inspect the CloudWatch metrics to better right-size the instances","correct":false}]},{"id":"cf072a65-cda4-492e-b42a-7ee7305bb9b6","domain":"high-avail","question":"You have a web application with the front end hosted on EC2 and the database hosted on RDS in a single Availability Zone. You notice that when backups are taken from your RDS instance, your applications performance is severely degraded. Your boss asks you to fix the issue. What should you do?","explanation":"You should create a multi-AZ RDS instance and migrate your DB to it. This way, when the backups are taken, they will be taken from the secondary -- not the primary.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html","title":"Multi-AZ RDS"}],"answers":[{"id":"ebcb098cff009244307167ae9489e2b1","text":"Create a multi-AZ RDS instance and migrate your DB to it. This way, when the backups are taken, they will be taken from the secondary -- not the primary.","correct":true},{"id":"82ca3fd61d44001c088391d1a6af1442","text":"Upgrade your RDS instance to an instance that has better disk IO. This way, the IO suspension from the back up will be \"equaled out\" by the increase in the new IO from the upgraded instance.","correct":false},{"id":"531a46711a70d61a60f10cf29f31171b","text":"Move your RDS instance to an in-house SQL server that has Netbackup installed.","correct":false},{"id":"ff7e77cc465d1a0285e8ea4c236bee2c","text":"Turn off backups for RDS. This will fix the performance issue immediately.","correct":false}]},{"id":"0a54c671-954d-4e52-8a46-83eadcf029cd","domain":"mon-rep","question":"Which of the following are valid alarm statuses in CloudWatch?","explanation":"The three alarm statuses are OK, INSUFFICIENT_DATA and ALARM.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html","title":"About CloudWatch Alarms"}],"answers":[{"id":"9de6d0a670ae5a0dee31a6318aa00e8d","text":"ALARM","correct":true},{"id":"f5d0aa0db6ffc40d938f1412b89d946c","text":"INSUFFICIENT_DATA","correct":true},{"id":"320f86f60f25459ba5550e000b2c3929","text":"ALERT","correct":false},{"id":"e0aa021e21dddbd6d8cecec71e9cf564","text":"OK","correct":true}]},{"id":"cde5021e-5b37-406b-b25a-1bdb489d3b24","domain":"security-comp","question":"Following a recent security event, a SysOps administrator has been asked to provide details of source IP addresses of requests to a website which is hosted on EC2 instances behind an Application Load Balancer. Where can the administrator find these details?","explanation":"Application Load Balancer (ALB) Access Logs record details of client connections which include the client's IP address and port.  CloudTrail logs AWS API calls so will not provide client IP addresses, neither will CloudWatch Custom Metrics which are for logging performance metrics.  EC2 instances will log the ALB's internal IP address as the source unless specifically configured to record the true source IP by using the x-forwarded-for header, which is not enabled by default.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html#access-log-entry-format","title":"ALB Access Log Entries"}],"answers":[{"id":"9a19403fef543b4899ec570ad1a29ca6","text":"CloudWatch Custom Metrics","correct":false},{"id":"43bc48b76d9918101fb772b4cfc58235","text":"/var/log/httpd/","correct":false},{"id":"8480ea5c787566c9a8bd2a608c725a06","text":"ALB Access Logs","correct":true},{"id":"6a219c87573826721cb51b987140a267","text":"CloudTrail Event Trail","correct":false}]},{"id":"557a8cbf-ffb7-492a-b24a-7662b95c6269","domain":"automation","question":"A startup is planning to migrate their existing on-premise application to AWS. The company is already using Chef as the configuration management tool to manage their on-premise application and is looking to continue using Chef to manage their AWS resources. In addition to this, the team is looking to use a managed service to reduce the overhead of managing its PostgreSQL databases. How can the team accomplish this?","explanation":"OpsWorks is a managed service for Chef and Puppet and can easily be used to deploy and manage resources utilizing the Chef recipes already prepared. CloudFormation uses AWS's own engine to process and convert JSON or YAML templates to AWS resources. For requirements involving Chef and Puppet, OpsWorks is the primary (and only) managed service. For managed database service requirements, Aurora, RDS, and DynamoDB can be used as the managed database.","links":[{"url":"https://aws.amazon.com/opsworks/","title":"OpsWorks"}],"answers":[{"id":"ce66fcb736d2db75172e1c56777a36e1","text":"Use CloudFormation for the configuration management requirements. Use Aurora for the database management requirements.","correct":false},{"id":"33b2bf9a830d2dddb0570706a774902f","text":"Use OpsWorks for the configuration management requirements. Use RDS for the database management requirements.","correct":true},{"id":"5b18a2364c2c69356af48e23e5347e81","text":"Use Elastic Beanstalk for the configuration management requirements. Use DynamoDB for the database management requirements.","correct":false},{"id":"378e25243c4a87f77553aed4ddbb2808","text":"Use ECS for the configuration management requirements. Use Athena for the database management requirements.","correct":false}]},{"id":"a5022f74-8a68-451a-8291-b3df925d05ac","domain":"security-comp","question":"You start your new job at CISO at a company which specialises in creating underground tunnels using electric boring/tunnelling machines. The company would like to revolutionise the transport industry by creating underground tunnels across major metropolitan cities at a fraction of the cost that is currently available. Your job is quite boring, however to add some excitement to your work youâ€™ve been asked to automatically identify any S3 buckets that are public. Which AWS service can you use to quickly determine this?","explanation":"AWS Config includes rules that allow you to identify buckets that have public read or write enabled.","links":[{"url":"https://aws.amazon.com/blogs/aws/aws-config-update-new-managed-rules-to-secure-s3-buckets/","title":"AWS Config and S3"}],"answers":[{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":false},{"id":"7c90c8f2a24f3a1a28525f19fb2c75ab","text":"AWS Inspector","correct":false},{"id":"5714e9332e476d05d9a1763a1b10be50","text":"AWS CloudWatch","correct":false},{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":true}]},{"id":"88d422b1-765f-4919-b83d-91b1b862b26f","domain":"networking","question":"You need a load balancer with support for SSL offloading, cross-zone load balancing and Path-Based Routing. What load balancer should you choose?","explanation":"Path Load balancer isn't a real thing. Application Load balancer, Network Load Balancer and Classic Load Balancer all support SSL offloading and cross-zone load balancing. However, only Application Load Balancer supports Path-Based Routing.","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/","title":"Elastic Load Balancing features"}],"answers":[{"id":"b3b5475001f327d331389c6f07ff7c3a","text":"Application Load Balancer","correct":true},{"id":"b4ec634f996fd486030f44e2c5fab630","text":"Classic Load Balancer","correct":false},{"id":"e0f10b949b1cbe40263bfe87c11a2f5d","text":"Network Load Balancer","correct":false},{"id":"682b66281be2437eb8d29a051355963d","text":"Path Load balancer","correct":false}]},{"id":"05d71be4-026e-433e-bd8b-eb4a3929ba63","domain":"automation","question":"A development team wants to use the latest Windows AMI whenever they launch an EC2 instance. Which service will allow them to query the AWS-managed Parameter Store namespace to retrieve the newest AMI for their CloudFormation template?","explanation":"AWS publish the latest AMI IDs for Operating Systems in AWS-managed parameters in the Parameter Store.  By using a Custom Resource in Lambda you can retrieve the relevant AMI ID and return it to the CloudFormation service, that way ensuring that your templates always use the newest AMI.","links":[{"url":"https://aws.amazon.com/blogs/mt/query-for-the-latest-windows-ami-using-systems-manager-parameter-store/","title":"Select AMI using Systems Manager Parameter Store"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html","title":"AWS Lambda-backed Custom Resources"}],"answers":[{"id":"7eb8f6238570dc713a360eae3029648f","text":"CloudFormation Mappings","correct":false},{"id":"cdf3a2f6faa3abf891b952dde17eb469","text":"CloudFormation Template Transformation","correct":false},{"id":"dc0efa07b1be89f7cfd1ab666df2f949","text":"CloudFormation Custom Resource using Lambda","correct":true},{"id":"2751cfe1530d4333f0bdac2d7b7c21bd","text":"CloudFormation using AWS Systems Manager Parameter Store","correct":true},{"id":"8c19fb5ff9d451c3f315e96ca8563b84","text":"CloudFormation Linked Parameters","correct":false}]},{"id":"f5213e28-c41d-4552-810a-7aa8f5ba2a1c","domain":"networking","question":"A Developer is unable to connect to an EC2 instance in a VPC. A SysOps Administrator investigates the connectivity issue. Going through a troubleshooting checklist, which conditions should be checked? Select two.","explanation":"Internet gateways allow all traffic and that cannot be configured. EC2 instances always have a private IP but a public IP is required for internet access. Security groups are stateful so only an allow rule for incoming traffic is required.","links":[{"url":"","title":""}],"answers":[{"id":"50be1cda181093d2df7e72aed5f80ab8","text":"The instance has a public IP address.","correct":true},{"id":"f9b2c3489abcbefefb1b289a51e33ccb","text":"The internet gateway associated with the VPC allows incoming traffic.","correct":false},{"id":"578d4a14aed3e21944a0f081460e32af","text":"The security group has an allow rule for outgoing traffic.","correct":false},{"id":"692e30da816ea07b3744e393cbbd28cd","text":"The instance has a private IP address.","correct":false},{"id":"8b476944980fce87668c22d0c40470e9","text":"The internet gateway associated with the VPC allows outgoing traffic.","correct":false},{"id":"08eefee7811ea91e42e46ba287ff5d6a","text":"The security group has an allow rule for incoming traffic.","correct":true}]},{"id":"20fba999-ed77-473c-b47d-4edf8458a905","domain":"data-man","question":"You are consulting for an AWS project and your customer needs help in identifying the best AWS storage service. Your customer needs to host dynamic websites that use server-side scripting. Which storage service would you recommend?","explanation":"Although Amazon S3 is ideal for static content websites, dynamic websites that depend on database interaction or use server-side scripting should be hosted on Amazon EC2 or Amazon EFS. Amazon Glacier is best suited for encrypted archival storage with infrequent read access. S3 is best suited for static web site hosting. RDS is best suited for storing structured, rapidly changing data.","links":[{"url":"https://d0.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf","title":"AWS Storage Services Overview"}],"answers":[{"id":"270fcb785810d0206945029bb05f4e97","text":"Amazon S3","correct":false},{"id":"aec460fb5e400d1c59014baf941c91d1","text":"Amazon Glacier","correct":false},{"id":"f7b96044a16becafecad63df1725e9c8","text":"Amazon EFS","correct":true},{"id":"35686630aa3baddc18c904374e570233","text":"Amazon RDS","correct":false}]},{"id":"eb2317bb-3bd4-4593-b217-e0610e79e106","domain":"mon-rep","question":"A critical application which runs on an EC2 instance behind an ELB is experiencing occasional outages. Associated with the outages are Windows event log entries. How can you detect these events and alert your team?","explanation":"CloudWatch Logs let you to stream logs from your EC2 instances to the CloudWatch service. A log filter on a Log Group allows you to detect occurrences of a key word or phrase. SNS can then deliver alerts for these alarms. ELB logs would not contain the keyword/phrase, nor would CloudTrail or the Autoscaling activity history.","links":[{"url":"https://docs.aws.amazon.com/en_pv/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html","title":"What Is Amazon CloudWatch Logs?"},{"url":"https://docs.aws.amazon.com/en_pv/AmazonCloudWatch/latest/logs/CountOccurrencesExample.html","title":"Example: Count Occurrences of a Term"}],"answers":[{"id":"d0520b3777e34e62b94d0046d78ec3b3","text":"Use CloudTrail logs and message the team using SNS","correct":false},{"id":"a325e8469dc87698c1ba13863e512af5","text":"Use Autoscaling activity logs and message the team using SNS","correct":false},{"id":"33e9387fac100300d7d2d03d2561f1f4","text":"Use ELB logs and a filter to alarm on the event then message the team using SNS","correct":false},{"id":"cff2a2162045687c91304445129e482d","text":"Use CloudWatch logs with a log filter to alarm on an occurrence of the event then message the team using SNS","correct":true}]},{"id":"a6f0b148-f08b-4a0e-bf13-791fe0af285e","domain":"security-comp","question":"You have a new manager who would like to introduce automated security assessments to allow you to test all of your applications running on EC2. Which AWS tool do you recommend?","explanation":"Inspector allows you to perform automated vulnerability assessments on applications running on EC2, Trusted Advisor can help you reduce cost, increase performance and improve security by optimizing your AWS environment, AWS Shield provides DDOS protection, Systems Manager is an Operational Management tool","links":[{"url":"https://aws.amazon.com/inspector/faqs/","title":"Inspector"}],"answers":[{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"8e75b153e61c22a8ea4e14aadc7cb4ee","text":"Systems Manager","correct":false},{"id":"9deb03cd21d41a691cdc24bfaab2820c","text":"Inspector","correct":true},{"id":"637d82e8a7206e87344161109cf7112d","text":"AWS Shield","correct":false}]},{"id":"74489755-3525-47c8-89c0-c840d59c5f54","domain":"dep-prov","question":"It is not possible to share an AMI across AWS Regions.","explanation":"You can copy an Amazon Machine Image (AMI) within or to another AWS region using the AWS Management Console, the AWS command line tools or SDKs, or the Amazon EC2 API. Copying a source AMI results in an identical but distinct target AMI with its own unique identifier.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html#copy-amis-across-regions","title":"Cross-Region AMI Copy"}],"answers":[{"id":"129cfa3cabe8c0069adacf27b64e1022","text":"True, but only if you have sufficient permissions.","correct":false},{"id":"f827cf462f62848df37c5e1e94a4da74","text":"True","correct":false},{"id":"f8320b26d30ab433c5a54546d21f414c","text":"False","correct":false},{"id":"2a861232a02567f87d2cdbd622fbc89d","text":"False. AMIs must be copied to another region. One cannot simply share them.","correct":true}]},{"id":"ac509f33-b7fd-483a-a610-b5744bc89943","domain":"automation","question":"An organization wants to understand when the infrastructure it is deploying from CloudFormation has been manually changed by a rogue developer in the team.  Which three services would help to detect changes and determine who performed the change?","explanation":"AWS CloudFormation Drift detection allows for you to identify changes over time of resources, when compared to the Cloudformation stack which created them.  The AWS Config managed rule 'cloudformation-stack-drift-detection-check' allows for a drift check to be initiated.  Any stacks with drift can be flagged as a non-compliant stack.  AWS Config can also provide you with a timeline of modified resources, along with the associated CloudTrail event that caused the configuration to change.. including which IAM role or user performed the change.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2018/11/aws-cloudformation-now-supports-drift-detection/","title":"AWS CloudFormation Now Supports Drift Detection"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/11/aws-config-launches-a-new-aws-config-rule-to-support-aws-cloudformation-stack-drift-detection/","title":"AWS Config Launches a New AWS Config Rule to Support AWS CloudFormation Stack Drift Detection"}],"answers":[{"id":"b00e2e3d5e092e8527268afe49e5a5e2","text":"AWS CloudFormation Change Sets","correct":false},{"id":"26ca9b29af36f84c577a85a80a58a381","text":"AWS CloudTrail","correct":true},{"id":"3c4dbfbe3fe821153b16f5a7b3e98a96","text":"AWS CloudFormation Drift","correct":true},{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":true},{"id":"28408acf54ab04fe847fd24957e528d9","text":"AWS CloudWatch Logs","correct":false},{"id":"7c1c0a0eb09dcbcd9acf3ade0b16cb91","text":"AWS Guard Duty","correct":false}]},{"id":"a8410bf2-3c51-49cc-8e44-ca3a1241eff2","domain":"dep-prov","question":"A developer has a monolithic Python application which is being migrated and refactored to use a microservice architecture. The developer has decided to use multiple AWS Lambda functions to run the Python code. During the preparation of the Lambda functions, the developer has noticed that the code requires several third party dependencies which are not part of the standard library. What needs to be done to ensure that the Lambda function code referencing the third party libraries can be executed properly?","explanation":"In order for a Lambda function to run scripts that involve third party libraries, a deployment package needs to be generated in a Lambda-like environment and uploaded to the Lambda or an S3 bucket. This is automatically done by tools like the SAM CLI. Third party libraries are not stored in EFS, ECR, or AppSync.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html","title":"Lambda Python - How to Create a Deployment Package"}],"answers":[{"id":"b41396a5bc20297186c72b266cf8be9e","text":"Use AppSync to manage and store the third party libraries.","correct":false},{"id":"de93e97f2545dd02641eecd1a42fba8d","text":"Store the third party libraries in EFS and link the Lambda function to the EFS store.","correct":false},{"id":"d3ae725bd59d297fd27e71f3db582c01","text":"Store the third party libraries in ECR and link the Lambda function to the ECR repo.","correct":false},{"id":"e45dd8b567270f7faf25f4111f41fa9e","text":"Generate a deployment package containing the code and the third party libraries.","correct":true}]},{"id":"2a16f1cd-530d-4e30-ad08-d89afae34484","domain":"mon-rep","question":"You create a new DynamoDB table with the provisioned read and write capacity units set to 5. The auto scaling feature is enabled for both read and write. And the target utilization is set as 70%. After monitoring the table for some time, you notice that there are two CloudWatch alarms related to the table. The description of one alarm is \"ConsumedWriteCapacityUnits < 150 for 15 datapoints within 15 minutes\". Which action do you need to take to address the alarm?","explanation":"DynamoDB manages the throughput capacities automatically with the auto scaling feature. The alarms are used for the feature and no action is required. There is no need to disable the feature or modify the provisioned capacities.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html","title":"Manage throughput capacity automatically with DynamoDB Auto Scaling"}],"answers":[{"id":"859412aa0d0666ab4c2b8d20707c8f41","text":"They are fake alarms. You can manually delete them from the console.","correct":false},{"id":"3a5208c005673a46313dce67b9e2dd57","text":"You should disable the auto scaling feature for the table.","correct":false},{"id":"2131aeffdfb01948cc4943432a680327","text":"You need to adjust the provisioned read and write capacities to a higher value such as 10.","correct":false},{"id":"252cd5486e4ae9685450c3e9ad208b05","text":"No action is required as the alarms are used for auto scaling for the DynamoDB table.","correct":true}]},{"id":"7b724c4c-726d-4da7-9021-59e459427ecd","domain":"networking","question":"Placement Groups can either be of the type 'Cluster', 'Spread', or 'Partition'.  Choose options from below which are only specific to Spread Placement Groups.","explanation":"There is only one answer that is specific to Spread Placement Groups.  Whilst some of these answers are correct for either Cluster Placement Groups only, or for both Cluster and Spread Placement Groups, the question stated that only options specific to Spread Placement Groups should be chosen, which would rule out two options as they are true for both Spread & Cluster type placement groups.  The Logical grouping of instances within a single Availability Zone is only true of Cluster Placement Groups and is also incorrect.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html","title":"Placement Groups"}],"answers":[{"id":"0156cb85707daf221315fb25ac1bb505","text":"An instance can be launched in one placement group at a time and cannot span multiple placement groups.","correct":false},{"id":"5e91fe627c2bd0613378a96fab7f8bc3","text":"Spread placement groups require a name that is unique within your AWS account for the region","correct":false},{"id":"2ce403ef492eb181bff27c45219186ad","text":"A spread placement group is a logical grouping of instances within a single Availability Zone","correct":false},{"id":"bb229a2ce0bd114f2e002f94923618d2","text":"A spread placement group is a group of instances that are each placed on distinct underlying hardware","correct":true}]},{"id":"c6a8e29b-70b4-4a72-b587-8ed388d71004","domain":"security-comp","question":"You are an administrator with full admin access to S3. There are several S3 buckets within your organization that need to comply with a policy that requires all objects to be encrypted in-transit. What data encryption mechanism would you apply to fulfill this requirement?","explanation":"Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, use a master key you store within your application. Server-side encryption is encrypting data at rest. SSE-S3, SSE-KMS, and SSE-C are methods of server-side encryption and would not fulfill a data in-transit encryption policy.","links":[{"url":"https://docs.aws.amazon.com/en_pv/AmazonS3/latest/dev/UsingClientSideEncryption.html","title":"Protecting Data Using Client-Side Encryption"}],"answers":[{"id":"7a36a5c2c5cf1986b38d1310f69352a6","text":"Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3).","correct":false},{"id":"034ffe67b63ae2e68b278955fa9e740c","text":"Use Server-Side Encryption with Keys Stored in AWS KMS (SSE-KMS).","correct":false},{"id":"6d50168a818719b2b2c1e9cd4f07f38a","text":"Use Client-Side Encryption.","correct":true},{"id":"163a4655e01a9cbc21d84cff0d1b33a1","text":"Use Server-Side Encryption with Customer-Provided Keys (SSE-C).","correct":false}]},{"id":"f988ca16-ff44-497e-aae5-876954a55a31","domain":"security-comp","question":"You are creating a fleet of EC2 instances that will be inside an autoscaling group. These EC2 instances will need to write a custom metric to CloudWatch and will need the appropriate permissions with which to do this. What is the most secure way to enable this?","explanation":"you should create an IAM role with CloudWatch permissions and modify the autoscaling launch configuration to use EC2 instances that have been assigned the new role.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/using-service-linked-roles.html#create-service-linked-role","title":"IAM: Creating a Service-Linked Role"}],"answers":[{"id":"29ed526af182672c01bd543082494019","text":"Create an IAM role with CloudWatch permissions and assign this to RDS. The existing EC2 instances will automatically be able to report to CloudWatch via RDS.","correct":false},{"id":"5a06900e462d7f086d273697c7a6abdc","text":"Create a unique user in IAM with CloudWatch permissions and store these credentials in GitHub. Have the EC2 instances pull these credentials when they need to log to CloudWatch.","correct":false},{"id":"abefd699f3f65202335f75e9e67df3a9","text":"Create an IAM role with CloudWatch permissions and modify the autoscaling launch configuration to use EC2 instances that have been assigned the new role.","correct":true},{"id":"5764c31d8e5f450a679416b744d2d853","text":"Create a unique user in IAM with CloudWatch permissions and modify the autoscaling group to include a boot strap script that passes the EC2 instance that users credentials.","correct":false}]},{"id":"095bed19-081d-4865-acff-d6f9bc29eb7c","domain":"automation","question":"Which of the following is the only required component of a CloudFormation template?","explanation":"As the primary purpose of CloudFormation is to create a collection of related AWS resources, the Resources section is the only required section of a CloudFormation template.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"CloudFormation Template Anatomy"}],"answers":[{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":false},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":true},{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false},{"id":"7df96b18c230f90ada0a9e2307226338","text":"Templates","correct":false}]},{"id":"08eb44ba-042a-4ced-b833-888d2943bb07","domain":"dep-prov","question":"You are experiencing issues with an Application Load Balancer you have recently set up. You have not changed any of the default logging settings. In this situation, which of the following monitoring tools is the most appropriate to help with troubleshooting client requests?","explanation":"You can use Amazon CloudWatch to retrieve statistics about data points for your load balancers and targets as an ordered set of time-series data, known as metrics. You can use these metrics to verify that your system is performing as expected and they are enabled by default. Access Logs capture detailed information about requests sent to your load balancer but are disabled by default. You can use request tracing to track HTTP requests but this functionality requires that access logging be enabled. You can use AWS CloudTrail to capture detailed information about the calls made to the Elastic Load Balancing API and store them as log files in Amazon S3. You can use these CloudTrail logs to determine which calls were made, the source IP address where the call came from, who made the call, when the call was made, and so on. CloudTrail logs are not appropriate for troubleshooting client requests.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html","title":"Monitor Your Application Load Balancers"}],"answers":[{"id":"6a292fdb897093b64ef80b39e7db0a4a","text":"Request tracing","correct":false},{"id":"50ed91980adb1dac23689554eb719277","text":"CloudWatch metrics","correct":true},{"id":"8c6ded942a243b91e65d037ab4e21f7d","text":"CloudTrail logs","correct":false},{"id":"6c5c81f47915de5f03d2577e8fae1c34","text":"Access logs","correct":false}]},{"id":"b2d3b949-889b-4bbd-8ec9-c65b764c47c3","domain":"mon-rep","question":"You are a SysOps Administrator monitoring a web app that lets users upload high-quality images and use them online. Each image requires resizing and encoding. The images are placed in an Amazon SQS queue for processing by an EC2 instance. It processes the images and then publishes the processed images where they can be viewed by users. When you monitor the EC2 instance you see that the CPU utilization is consistently at 90% and that image processing time is being delayed. The team is looking for a cost-effective solution. What would you recommend?","explanation":"You can use an Auto Scaling group to manage EC2 instances for the purpose of processing messages from an SQS queue. Set a custom metric to send to Amazon CloudWatch that measures the number of messages in the queue per EC2 instance in the Auto Scaling group, and then set a target tracking policy that configures your Auto Scaling group to scale based on the custom metric and a set target value. CloudWatch alarms invoke the scaling policy. Increasing the size of the instance may work but is not a cost-effective solution since Auto Scaling gives you the option to scale down during low demand. Kinesis Data Streams are best suited for real-time data processing, and they have a size limit of 1MB which would be too low for high-quality images. Migrating the data to DynamoDB would not be a viable, let alone cost-effective, solution.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html","title":"Scaling Based on Amazon SQS"}],"answers":[{"id":"40fe2365fa7bae167e628c9ef29bd6ca","text":"Move the images into Kinesis Data Streams where you'll be able to process the data in real time.","correct":false},{"id":"c1c44e18b3c0f81b8ec026e9e1ab5b38","text":"Place the instance in an Auto Scaling group. Use CloudWatch metrics to scale out the Auto Scaling group depending on the size of the SQS queue.","correct":true},{"id":"3a781b7e075f07b31a51c98b18d84a2e","text":"Increase the size of the instance and ensure that it is compute-optimized to boost it's capacity to process the images.","correct":false},{"id":"52bcbdff61b39eafa67d9496dc77ee09","text":"Migrate the image data into DynamoDB. Attach a role to the instance to be able to access the data from DynamoDB and process the images.","correct":false}]},{"id":"5b71bc80-b7ef-4fde-86f0-1a73d1d63e4c","domain":"high-avail","question":"You are an AWS administrator and have set up an Elastic Load Balancer inside a VPC. The ELB spans several Availability Zones. The ELB sits in front of a web application running on Amazon EC2. You notice that incoming traffic is not being evenly distributed across the AZs. How would you solve this issue?","explanation":"Traffic not evenly distributed across the instances in multiple AZs means the traffic is going to only specific EC2 instances. This happens when either the instances which are not receiving the traffic are unhealthy, or the instances that are receiving the traffic are holding on to the session. Since there is no mention of unhealthy instances, disabling sticky sessions on the ELB is the best answer. Increasing the number of subnets and/or instances will not solve the problem as users will remain stuck to the original instance. Increasing the frequency of health checks will have no impact to force even distribution of traffic.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html","title":"Configure Sticky Sessions for Your Classic Load Balancer"}],"answers":[{"id":"d532ac3048a69a5902a79b51bc219bd3","text":"Increase the number of EC2 instances behind the ELB.","correct":false},{"id":"b7116d2a43d8462c83e8b93fda085c71","text":"Disable sticky sessions on the ELB.","correct":true},{"id":"c0d9e44a3e92d6d91864058257e9922c","text":"Add additional subnets within your ELB and configure your ELB to span the new subnets.","correct":false},{"id":"e23eea1cecbc63f7423de00e65f29614","text":"Increase the frequency of the health checks to the EC2 instances running your application.","correct":false}]},{"id":"f8a73a53-7c3b-4203-abda-2a0df772b8c0","domain":"data-man","question":"Your company's on-premises CRM application captures all lead tracking activities performed by the account team. Leadership would like to leverage machine learning to increase the lead-to-customer conversion ratio. They direct you to create ML models on Amazon SageMaker to predict which lead activities should be applied to specific customers based on previous successes. How will you architect the solution to get the CRM data to SageMaker in the most operationally efficient and cost effective way?","explanation":"AWS Storage Gateway File Gateway provides the capability to process hybrid cloud workflows through a file interface that seamlessly stores files as objects on S3 for AWS services such as machine learning, big data analytics, and serverless functions. Storage Gateway Volume Gateway stores data on premises and writes EBS Snapshots to S3 for backup. The EBS Snapshots would not be readable by SageMaker. Maintaining a script to write full files will require less effort than maintaining a program that parses CRM files and writes individual records to a Kinesis stream. Using a Snowball device will add time to the process due to shipping.","links":[{"url":"https://aws.amazon.com/storagegateway/?nc=sn&loc=0&whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc","title":"AWS Storage Gateway"},{"url":"https://aws.amazon.com/storagegateway/faqs/?nc=sn&loc=6","title":"What Can I Do With File Gateway?"}],"answers":[{"id":"038b6b51cd73fe4e7ebdf85ad0cf26d9","text":"Use an AWS SDK to programmatically write CRM activity records to an Amazon Kinesis Data Firehose stream. Ingest the data into SageMaker from S3.","correct":false},{"id":"e5d1933264a00d252115c375f7202d99","text":"Write the files to an AWS Snowball device. Ship the Snowball device to AWS each week. Ingest the data into SageMaker from S3.","correct":false},{"id":"2d79bea0e750fdc444dd765b28cd49a4","text":"Install an AWS Storage Gateway File Gateway. Use a script to periodically write the CRM files to the File Gateway file system. Ingest the data into SageMaker from S3.","correct":true},{"id":"d5351d8c29a3e0729ea2f1502a512b6f","text":"Implement an AWS Storage Gateway Volume Gateway in stored mode. Use a script to periodically write the CRM files to the Volume Gateway volume. Ingest the data into SageMaker from S3.","correct":false}]},{"id":"cb3a3e4a-87a1-4810-a732-ab1818b84b1d","domain":"data-man","question":"Your company's on-premises CRM software uses file storage. The marketing department would like to add a new social media module which will require a large increase in storage capacity. There is no capital budget to purchase additional storage servers, so the decision is made to place the new module's data on Amazon Elastic File System (EFS). A leader of the application team raises concerns about the latency that could be incurred with a single application accessing data both on-premises and in the cloud. How would you architect the solution to minimize the possibility of poor application performance?","explanation":"Either AWS Direct Connect or AWS VPN is required to connect on-premises servers to Amazon EFS. Bursting Throughput mode is the default mode for EFS file systems, scaling up as the size of a filesystem grows. Provisioned Throughput mode can provide higher throughput for applications with requirements greater than those of Bursting Throughput mode. Max I/O Performance mode will only benefit highly parallelized applications, which CRM generally is not.","links":[{"url":"https://aws.amazon.com/efs/","title":"Amazon Elastic File System"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/performance.html#throughput-modes","title":"Amazon EFS Performance"}],"answers":[{"id":"6ebf148cfe4cebe47a593afb84737882","text":"Connect the CRM application to EFS through an AWS Service Endpoint. Deploy the EFS file system in both Provisioned Throughput and Max I/O Performance modes","correct":false},{"id":"9d857d0961d3644ba26dec63c2d911e2","text":"Have the CRM application connect to the AWS cloud over an AWS Direct Connect. Configure the EFS file system in Provisioned Throughput mode","correct":true},{"id":"ed115154d20878d7d538a3557109c6e2","text":"Connect the CRM application to the AWS cloud over an AWS Direct Connect. Configure the EFS file system in Max I/O Performance mode","correct":false},{"id":"7b828048a4ab8b3c896ce413de929519","text":"Have the CRM application connect to the AWS cloud over an AWS VPN. Implement the EFS file system in both Bursting Throughput and Max I/O Performance modes","correct":false}]},{"id":"3xre6hrv-j02a-kj8k-nkyn-5951wwipzpzd","domain":"automation","question":"Which service can you use to enable configuration management using Chef or Puppet?","explanation":"OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Config allows you to record and evaluate configuration but doesn't use Chef or Puppet, Systems Manager is an operational insights tool and Athena is used to run SQL queries on data held in S3.","links":[{"url":"https://aws.amazon.com/opsworks/","title":"OpsWorks"}],"answers":[{"id":"8e75b153e61c22a8ea4e14aadc7cb4ee","text":"Systems Manager","correct":false},{"id":"582ca45acfd3e21caca8b786c1413850","text":"Athena","correct":false},{"id":"c42aaccedc51aac929c8ae313066f320","text":"OpsWorks","correct":true},{"id":"fa535ffb25e1fd20341652f9be21e06e","text":"Config","correct":false}]},{"id":"6fd27cf3-e84d-4a26-875c-7695a786a998","domain":"networking","question":"Two EC2 instances in two VPCs cannot ping each other. Both instances are located in private subnets. Which of the following would help you troubleshoot the problem? Select two.","explanation":"You can only peer VPCs, not individual subnets. There must also exist a route between the peered VPCs. Subnet security policy isn't a real thing although Network ACLs could be considered as such. VPC endpoint policy controls access to S3 or DynamoDB, not to a peered VPC.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html","title":"What is VPC Peering?"}],"answers":[{"id":"b60b61736af41d60dfcadf8fde5d83b6","text":"Check that a route exists between the VPCs","correct":true},{"id":"19b59f579598d8721b6507a457658f92","text":"Check that a subnet security policy allows ping","correct":false},{"id":"e88fa7d7390ad4d45912151c4e94b106","text":"Check that a VPC endpoint policy allows ping","correct":false},{"id":"d6d0388ad6ed9fa8508ce01fce126328","text":"Check that there is a peering connection between the subnets","correct":false},{"id":"088c1791b9d25f53b479820769d14995","text":"Check that there is a peering connection between the VPCs","correct":true}]},{"id":"a7e11457-61a0-4212-b057-2853f6bf0c73","domain":"dep-prov","question":"You run a digital marketing company and many of your clients are bloggers and small businesses. The majority of your customers either use Wordpress or Joomla and would like these sites to be deployed on AWS so they can then go in a manage them independently. You want to be able to deploy these instances as quickly as possible for new customers as well as having them patched for OS security patches and application patches. What is the quickest way to achieve this?","explanation":"An Amazon Machine Image (AMI) provides all the information required to launch an instance, for example an Operating System with your applications, security settings and patches applied. A custom AMI can be created from an existing EC2 instance which you have configured according to the specification you require.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html","title":"Amazon Machine Images (AMIs)"}],"answers":[{"id":"c5382eebf8fd57d5e2b5ef1d687dafa8","text":"Create a lambda function which will run at scheduled times. Program this lambda function to provision EC2 instances with the required software on it automatically","correct":false},{"id":"d50d598e74c83ecddc82ee3e3fc4a350","text":"Hire an intern who will create the EC2 instances manually, install wordpress/joomla as well as Apache and MySQL and then apply all necessary security updates","correct":false},{"id":"75454744a1bfe749b0f7c048f5c8374b","text":" Use a bootstrap script to install MySQL/Apache/PHP and then to download, unzip and install Wordpress/Joomla in an automated pattern","correct":false},{"id":"7f84a9c50026f2d984ee3517ddaee6ed","text":"Create a gold template AMI with Wordpress/Joomla and MySQL/Apache/PHP already installed. Use a bootstrap script to apply any necessary updates","correct":true}]},{"id":"0ac7251e-689e-4cd6-bfb9-992628fb3e3c","domain":"mon-rep","question":"There has been a major outage of S3 in US-East-1 where many of your companyâ€™s AWS assets are. Your boss wants to know what effect this will have on your organization. What dashboard can you use to help diagnose how this will affect your individual organisation?","explanation":"AWS Personal Health Dashboard provides alerts and remediation guidance when AWS is experiencing events that may impact you. Inspector is an automated security assessment service. AWS X-Ray helps developers analyze and debug production, distributed applications.","links":[{"url":"https://aws.amazon.com/premiumsupport/phd/","title":"AWS Personal Health Dashboard"}],"answers":[{"id":"cf4db9f312542c8284a6ccdefcd98544","text":"Personal Health Dashboard","correct":true},{"id":"3473fa31769f9b170662878d3f67fc8c","text":"AWS Inspector Dashboard","correct":false},{"id":"6bd8c280d2d212f5f6338714620001a4","text":"AWS Service Dashboard","correct":false},{"id":"0863526e514d88cd2d00b39f8dda0920","text":"X-Ray","correct":false}]},{"id":"1a45301ef-de9d-42bb-842d-8c1a42220a08","domain":"mon-rep","question":"There is increasing demand of your application running on EC2, and you need to monitor available memory space to ensure you can scale with demand. How would you monitor this on AWS?","explanation":"Memory and disk space utilization is a custom metric that is CloudWatch does NOT collect natively. Users must install the CloudWatch agent to collect metrics for memory and disk space. Neither the EC2 Dashboard nor the CloudWatch Dashboard natively provides the ability to monitor memory. You much install the CloudWatch Agent on your instances. AWS would not provide this report as these are specific to your EC2 instances.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html & https://aws.amazon.com/blogs/aws/amazon-cloudwatch-user-defined-metrics/","title":"CloudWatch Custom Metrics"}],"answers":[{"id":"fcf91ec5e036123e3b515882a4d079eb","text":"Create a Support Case to AWS and request a report on available memory space on your instances.","correct":false},{"id":"4a31b453359b5547f03d47279651a6c7","text":"Install the CloudWatch Agent on your instance to monitor memory metrics.","correct":true},{"id":"f5537acc0c2715be1415c64741ce5ed7","text":"Check the EC2 Dashboard to monitor instance metric details.","correct":false},{"id":"25084caeeca98bb0ed5317e16eb25f5b","text":"Utilize the CloudWatch Dashboard to view memory and disk metrics that are available by default.","correct":false}]},{"id":"5ba4b098-ba54-49c0-9bd4-84461f4b7f77","domain":"security-comp","question":"An IT company has several AWS accounts that are part of an AWS Organization. The root account and all linked accounts have configured Service Control Policies (SCPs) to help restrict access to unneeded resources and manage governance for the Organization. If a user in one of the linked accounts wants to enable VPC Flow Logs to monitor IP traffic coming in/out of their VPC, what policy conditions would allow the user to do so?","explanation":"An SCP policy at the root account cascades down to all linked accounts under it. Therefore the child account only has permissions permitted by the parent above it. In this case, the IAM User would only be able to enable VPC Flow Logs if the SCP for the root account and linked account, and the IAM policy attached to the user ALL allow access to enabling VPC Flow Logs. Remember, an explicit deny always trumps an explicit allow.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html","title":"Policy Evaluation Logic"}],"answers":[{"id":"cfe34960caff96755d23848fae9707b1","text":"The SCP for the root account permits actions with the default AWSFullAccess policy. The SCP for the linked account does not permit enabling VPC Flow Logs. The IAM User policy allows the user to enable VPC Flow Logs.","correct":false},{"id":"c692b032c5024821ef739e00f1f7605e","text":"The SCP for the root account and the SCP for the linked account do not permit enabling VPC Flow Logs. The IAM User policy explicitly allows the user to enable VPC Flow Logs.","correct":false},{"id":"6b8c1e7d84f5dbdb40a33a7c3b65cb35","text":"The SCP for the root account and the SCP for the linked account permit enabling VPC Flow Logs. The IAM User policy allows the user to enable VPC Flow Logs.","correct":true},{"id":"0cf5189bcdacb3696d3bda3e43512f31","text":"The SCP for the root account does not allow enabling VPC Flow Logs. The SCP for the linked account does not permit enabling VPC Flow Logs. The IAM User policy allows the user to enable VPC Flow Logs.","correct":false}]},{"id":"88de3a5c-b9ce-46eb-928f-f0f909bfe5dd","domain":"networking","question":"You own the registered domain name cloudcanines.com and are trying configure Route53 to map the DNS name to the DNS name of your Elastic Load Balancer. Which Route53 record can you use to achieve this?","explanation":"cloudcanines.com is a Zone Apex and you can't create a CNAME record at the zone apex. Instead you need to use an alias record to map the Zone Apex to the Elastic Load Balancer","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html","title":"Route53 Record sets"}],"answers":[{"id":"effdb9ce6c5d44df31b89d7069c8e0fb","text":"Alias","correct":true},{"id":"adc4bfdb0829dae99e3699393e3fbaa4","text":"CNAME","correct":false},{"id":"0b98720dcb2cc6fd60358a45dfbc5b87","text":"MX","correct":false},{"id":"de46eab399f3ea0bbf1912c1d14a1544","text":"Zone Apex","correct":false}]},{"id":"e105d8a7-6333-48d8-9610-7c2f9ad73991","domain":"mon-rep","question":"There has been a steady rise in costs with your AWS bill, and the security team has noticed that there has been an increase in the number of requests, even though the number of IAM users has decreased due to employee turnover and down-sizing. The CISO has tasked you with identifying whether recent requests to the AWS account's environment were made with temporary security credentials for a role or federated user. How would you go about identifying this?","explanation":"A trail enables CloudTrail to deliver log files to an Amazon S3 bucket. By default, when you create a trail in the console, the trail applies to all AWS Regions. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs. The userIdentity element contains details about the type of IAM identity that made the request, and which credentials were used. If temporary credentials were used, the element shows how the credentials were obtained. Identify Federation is used for access to an account. Config will not log API activity. Lambda cannot scrape an entire account; it needs to access an S3 bucket of CloudTrail logs to take any action.","links":[{"url":"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-user-identity.html","title":"CloudTrail userIdentity Element"},{"url":"https://docs.aws.amazon.com/config/latest/developerguide/log-api-calls.html","title":"Logging AWS Config API Calls with AWS CloudTrail"}],"answers":[{"id":"97ddb7778b0f9f5decefb8013df4d710","text":"Set up Identity Federation with SAML. Create IAM roles and create policies for employees to assume with the correct permissions. Log all requests using the Credential report.","correct":false},{"id":"4ffef50c6af5fe0e395fb58ca8319f05","text":"Record all ongoing events in the AWS account using AWS Config. Create a CloudWatch alarm to send an SNS topic when an identity under AssumeRole makes a request to the account.","correct":false},{"id":"41e73582d973a777c1838270973c1a5a","text":"Create a Lambda function that is triggered daily that scrapes the account for requests from any assumed roles. Have the Lambda function revoke access if any requests are non-compliant.","correct":false},{"id":"73e4bf9add9c239534c52f8a03fb95fb","text":"Create a CloudTrail log to deliver files to an Amazon S3 bucket. Use Amazon Athena to query the logs to search for the userIdentity element.","correct":true}]},{"id":"dff22820-ec03-46d8-aa62-cc85ddd8fe19","domain":"dep-prov","question":"An old on-premises application is migrated to AWS EC2. The EC2 instances are managed by an Auto Scaling Group. You need to create a Load Balancer to route the traffic to the ASG. The SSL traffic should be terminated at the Load Balancer and you want to configure the Load Balancer to route client requests based on the value of a custom HTTP header. Which type of the Load Balancer should you choose?","explanation":"All types of Elastic Load Balancers support SSL offloading. However only the Application Load Balancer supports the HTTP header-based routing as it operates at the application layer and HTTP/HTTPS is a layer 7 protocol.","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/#compare","title":"Elastic Load Balancing features"}],"answers":[{"id":"b3b5475001f327d331389c6f07ff7c3a","text":"Application Load Balancer","correct":true},{"id":"2493298314dcf1d9722b3269375198cd","text":"HTTP Load Balancer","correct":false},{"id":"e0f10b949b1cbe40263bfe87c11a2f5d","text":"Network Load Balancer","correct":false},{"id":"b4ec634f996fd486030f44e2c5fab630","text":"Classic Load Balancer","correct":false}]},{"id":"5e40fe16-8d0b-485b-8a2b-c387b1421d8b","domain":"data-man","question":"A DynamoDB table is created to store data for a project. The table is configured with the default encryption type. As part of the disaster recovery plan, the DynamoDB table needs to have backups for at least 30 days. In the event of incorrect data being accidentally written to the table, you need to be able to restore to a given date and time when the data was correct. Which method should you use to configure the backups?","explanation":"You can enable Point-in-time Recovery so that DynamoDB maintains continuous backups of the table for the last 35 days. By default, this feature is turned off. CloudWatch Event rule is not as efficient as Point-in-time Recovery and you cannot restore to a given time with on-demand backups. And there is no Lifecycle Policy in Systems Manager. Lifecycle Policy is an EC2 feature for EBS snapshots instead of DynamoDB backups.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery_Howitworks.html","title":"DynamoDB Point-in-time Recovery"}],"answers":[{"id":"90b7a4b9354bec0cc794a38060532c70","text":"Enable the Point-in-time Recovery for the table. It provides continuous backups of your DynamoDB table. When enabled it maintains incremental backups of your table for the last 35 days until you explicitly turn it off.","correct":true},{"id":"e2b30e30db99c37dd774db01c430cb6f","text":"Set up a CloudWatch Event rule that is triggered every one hour. When the rule is triggered, create an on-demand backup for the DynamoDB table.","correct":false},{"id":"6e496dbe5059cd5960f002dbdc2650c9","text":"By default, DynamoDB tables have continuous backups for 90 days. Within the time, the table can be restored at any given date and time.","correct":false},{"id":"7c280b0f0f681085429067cdab9bb76e","text":"Use Systems Manager Lifecycle Policy to automatically create backups for the table. The table can be restored to any time within 30 days.","correct":false}]},{"id":"8b30bb8d-7142-4b59-8d3c-4d5033f31a8f","domain":"mon-rep","question":"You are working for a company which is migrating all of its data into S3, the migration is underway but your Security Architect is concerned that not all buckets are secure and wants you identify all buckets which allow public read or write. Which service can you use to find out?","explanation":"AWS Config allows gives you a view of the configuration of your AWS infrastructure and compares it for compliance against rules you can define","links":[{"url":"https://aws.amazon.com/blogs/aws/aws-config-update-new-managed-rules-to-secure-s3-buckets/","title":"AWS Config Rules to Secure S3"}],"answers":[{"id":"7c90c8f2a24f3a1a28525f19fb2c75ab","text":"AWS Inspector","correct":false},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":false},{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":true}]},{"id":"3f043cde-54b2-42c7-9a19-7153fe5b6e95","domain":"high-avail","question":"Your customer has asked about cost-savings opportunities with AWS. They've noted that their EC2 instances are on most, if not all, the time but metrics show that aggregate CPU utilization is low. Demand for their application is also unpredictable. They want to cut costs around their EC2 fleet. Which of the below suggestions would you recommend to your customer to maximize savings?","explanation":"AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. AWS Auto Scaling makes scaling simple with recommendations that allow you to optimize performance, costs, or balance between them. With auto scaling you can scale-out Amazon EC2 instances seamlessly and automatically when demand increases, shed unneeded Amazon EC2 instances automatically and save money when demand subsides, and scale dynamically based on your Amazon CloudWatch metrics, or predictably according to a schedule that you define. Purchasing reserved instances, although cheaper than on-demand, would not necessarily cut costs. Since demand is unpredictable you may be purchasing a commitment that you may not use. There is no indication that the application will be running for at least a year. This is true even after right-sizing. Storing snapshots of EBS in S3 is indeed cheaper than storing EBS volumes but that does not address the issue of the EC2 instances themselves.","links":[{"url":"https://aws.amazon.com/autoscaling/","title":"AWS Auto Scaling"}],"answers":[{"id":"99040729979583a2eb85e3502b484989","text":"Purchase convertible reserved instances for your EC2 fleet. They will experience up to 66% savings compared to on-demand costs and will have the option to change their instance types if the application needs change.","correct":false},{"id":"76a2f03f07c2ade7d896075a501c6bc5","text":"Utilize auto scaling groups for the EC2 fleet. Set up a scaling policy that will launch EC2 instances when CUP utilization is above a threshold, and release instances when CPU utilization is below a threshold.","correct":true},{"id":"a8dd2caf5c16147c7b1b6bd321b6558d","text":"Take snapshots of the EBS volumes attached to the EC2 instances and store them in S3. Delete the EBS volumes as storing in S3 is a cheaper alternative than EBS storage costs.","correct":false},{"id":"89a84b7404990525469a3025ab3b5116","text":"Decrease the instance sizes for those instances with low CPU utilization. Purchase standard reserved instances after right-sizing the instances.","correct":false}]},{"id":"4680fe6f-7f9c-4af7-b8da-5cc0933ebe57","domain":"dep-prov","question":"A company is migrating several workloads using different languages from their on-premise setup to AWS. The migration team of the company has been given a short period of time to perform the migration for the first phase of the project. How can the team accomplish this?","explanation":"Given the time constraints and the need to support multiple languages (and potentially multiple frameworks and environments), the use of containers will solve the problem. Out of all the options, only Elastic Beanstalk will support the preparation of environments with Docker container processes running.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/single-container-docker.html","title":"Elastic Beanstalk - Single Container Docker"}],"answers":[{"id":"677dbd1b57d7ca7a67cb5212ddcf07f0","text":"Wrap the application code and processes inside Docker containers. Deploy the containers to ECR.","correct":false},{"id":"91a3239475f30c6a9593a60c67f617da","text":"Wrap the application code and processes inside Docker containers. Deploy the containers behind a Kinesis Stream.","correct":false},{"id":"2bbb835baf49b3cc461d4cde5adab575","text":"Wrap the application code and processes inside Docker containers. Deploy the containers to Elastic Beanstalk environments.","correct":true},{"id":"9144ac49f4f40c6c26bdebe22d465fed","text":"Wrap the application code and processes inside Docker containers. Deploy the containers behind an AppSync API.","correct":false}]},{"id":"60a05f6f-23b7-4f0d-b2bf-0e29c276f8ce","domain":"automation","question":"You are working on an application that has multiple phases: development, staging, and production. Each phase runs on Amazon EC2 with an Amazon EBS volume. You use an Application Load Balancer to manage the application's traffic and CloudWatch metrics to collect metric data for each phase. You need to find an efficient way to manage the services, and modify the settings of each phase of your application. What is the most effective way of doing this?","explanation":"You can use a single page to view and manage your resources using AWS Resource Groups. Check your resources for each stage of your application by opening the resource group. View the consolidated information on your resource group page. To modify a specific resource, choose the resource's links on your resource group page to access the service console that has the settings that you need. Without Resource Groups, you would have to access multiple consoles but this is unncessary. OpsWorks is configuration management service that provides managed instances of Chef and Puppet. Service Catalog is to create and manage catalogs of IT services that are approved for use on AWS for compliance.","links":[{"url":"https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html","title":"What Is AWS Resource Groups?"}],"answers":[{"id":"8b78b4f2fe4a52d464dc6d4e66bc684d","text":"Open multiple consoles to check the status of your services and to apply the necessary modifications to each phase's settings.","correct":false},{"id":"2dbb34771a820b660e255fc85e30bb10","text":"Create portfolios of products related to each phase of the application. Group the products to easily manage and update the services within a portfolio using version control.","correct":false},{"id":"1ed632c93b70e6d21a83ffdc04373d13","text":"Create a custom console in AWS Resource Groups that organizes and consolidates information based on criteria specified in tags, or the resources for each phase of your application.","correct":true},{"id":"47d6536a1b47257e2ed3d370744d7291","text":"Use AWS OpsWorks to automate operational tasks across your AWS resources, view operational data for monitoring and troubleshooting, and take action on your groups of resources.","correct":false}]},{"id":"b44c9f0b-805c-4de9-8ae7-71a6c2885b76","domain":"mon-rep","question":"Which of the following can you use to monitor API usage in AWS?","explanation":"CloudTrail logs all API calls within your account, CloudWatch monitors performance metrics, RunCommand is a Systems Manager feature which lets you run a command simultaneously on multiple instances, Trusted Advisor makes security, performance and cost optimization recommendations.","links":[{"url":"https://aws.amazon.com/cloudtrail/faqs/","title":"CloudTrail FAQs"}],"answers":[{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"64d231d79e9f7640a4572f7ae75aa226","text":"RunCommand","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":true}]},{"id":"b4e4d4f8-b9af-47da-9f90-2b63cab25ddf","domain":"automation","question":"As a SysOps Administrator you are managing your company's infrastructure as code. You have a number of CloudFormation templates that automate the provisioning of AWS resources for disaster recovery purposes. Your CISO have asked you for additional insights into the changes that teams are making to the CloudFormation templates in order to see when templates are updated with what changes. How would you build a solution that fulfills the CISO's ask?","explanation":"Change sets allow you to preview how proposed changes to a stack might impact your running resources. For example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. Config and Lambda would be complicated to configure and unnecessary as you would be able to directly do this using change sets. Amazon Inspector is used for EC2 instances.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"00c32ce29ae8b3d7291d321ea5a8c6ba","text":"Run Amazon Inspector report periodically to identify changes made to a CloudFormation stack. Forward these reports to your CISO.","correct":false},{"id":"a4e164938d62b8c500a3c3bc4680f546","text":"Create a change set by submitting changes against the stack you want to update.","correct":true},{"id":"a374362c79930669cc8b737ca45f03cb","text":"Create a Lambda function that parses through CloudWatch logs for any changes made to a CloudFormation stack. Ensure CloudFormation has a role assigned that sends logs to CloudWatch.","correct":false},{"id":"44fab4e192c83f7e36b2143702b9958e","text":"Configure an AWS Config rule to detect changes to a CloudFormation stack. Send an SNS notification to the CISO for any changes.","correct":false}]},{"id":"4c7fb750-0ec0-4c9f-8dc6-378122edf97a","domain":"automation","question":"The engineering team of a digital marketing company has a lot of AWS Lambda functions directly created and managed using the AWS Console. The CTO has mandated that the code and the deployments are managed using templates and the code is stored in a code repository to enable proper version control processes. How can the team achieve this?","explanation":"SAM templates and CloudFormation templates can be used to manage the Lambda function code. Out of all the options, only CodeCommit can be used directly as a managed service for a code repository. S3 buckets are not used directly as a code repository.","links":[{"url":"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html","title":"What is SAM"}],"answers":[{"id":"a85a26cd33fe99adb584452fa780dce2","text":"Use CloudFormation templates to manage the lambda function code. Use S3 buckets for the code repository.","correct":false},{"id":"f33ec453646a2074aa930a61f466fc1a","text":"Use CloudFormation templates to manage the lambda function code. Use ECR for the code repository.","correct":false},{"id":"b670d764967a3aa65a1424279e37291c","text":"Use SAM templates to manage the lambda function code. Use S3 buckets for the code repository.","correct":false},{"id":"c5fe89ee64d944e98f26b2db3fa7cea2","text":"Use SAM templates to manage the lambda function code. Use CodeCommit for the code repository.","correct":true}]},{"id":"fa546ba1-3d10-4f56-8cca-03f69c7bb141","domain":"mon-rep","question":"You have a task to maintain a CloudFormation template for a new application. The template contains resources for an AutoScaling Group, a Launch Configuration and a Classic Load Balancer. To monitor the application running status, you need to create a new CloudWatch Log Group. The Launch Configuration user data in the template is modified to install the CloudWatch Agent and configuration files. Which resource should you also create in the CloudFormation template for the CloudWatch Log Group to work?","explanation":"The CloudFormation stack should create a CloudWatch Log Group resource. A Log Group is a group of log streams. The correct resource type is AWS::Logs::LogGroup. Other resources types such as AWS::CloudWatch::Dashboard, AWS::Logs::Destination or AWS::Logs::LogStream are unnecessary or invalid.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-loggroup.html","title":"Create CloudWatch log groups in CloudFormation"}],"answers":[{"id":"753205d9356936ff0505312dd82f15ed","text":"A resource with the resource type as AWS::Logs::LogStream.","correct":false},{"id":"b763f5ea4415de324920f7c7c639e4ac","text":"An AWS::Logs::Destination resource where the logs are sent to.","correct":false},{"id":"546e068e392aafa083829c464e843ed2","text":"A CloudWatch dashboard with the resource type AWS::CloudWatch::Dashboard.","correct":false},{"id":"5d8b7a94aecd66edf0bbbac7a39b9010","text":"A resource with the resource type of AWS::Logs::LogGroup.","correct":true}]},{"id":"8c1dec5f-627d-4f25-af24-119b092ca2ef","domain":"high-avail","question":"A client asks you how they can make their current database running on AWS highly available. The client is running a MySQL RDS database in us-west-1. The client does not currently have a Multi-AZ deployment. The client wants to know what the benefits are with a Multi-AZ deployment as it would incur additional costs to their AWS bill. How would you explain the benefits to the customer?","explanation":"Amazon RDS Multi-AZ deployments provide enhanced availability and durability for databases. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby. It does not lower latencies nor does it increase read performance. It cannot tolerate the failure of a single AWS Region as failure of a Region would implicate failure of all the Availability Zones within that Region.","links":[{"url":"https://aws.amazon.com/rds/details/multi-az/","title":"Amazon RDS Multi-AZ Deployments"}],"answers":[{"id":"4983004598ef3be1bb46819c748a09ab","text":"Multi-AZ tolerates the failure of a single Availability Zone. It also allows higher availability during maintenance tasks.","correct":true},{"id":"834345e14f96992b8d4b2e785595f697","text":"Multi-AZ tolerates the failure of a single Region. It also allows higher availability during maintenance tasks.","correct":false},{"id":"aa4c2ced080ae70c33f36fe825b972ce","text":"Multi-AZ lowers latencies for application servers when they are accessing the database in multiple Availability Zones.","correct":false},{"id":"6772f4dc3173a8dd298656b65d2b6efa","text":"Multi-AZ makes it faster for application servers to access the database by reading data at a quicker rate.","correct":false}]},{"id":"a56dcc4d-97b3-43f7-94b7-5daeeebefe20","domain":"security-comp","question":"What type of policies can grant permissions to IAM users? Select two.","explanation":"IAM policies and S3 bucket policies can grant IAM users permissions. SCPs, VPC endpoint policies and permission boundaries can be used to limit those permissions with allow and deny rules but they won't grant the permissions themselves.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html","title":"Policy Evaluation Logic"}],"answers":[{"id":"4e1f839391e849a6065024e60ace4c38","text":"IAM policy","correct":true},{"id":"98850da8d6a8c95f98551976f45f5b51","text":"AWS Organizations service control policy (SCP)","correct":false},{"id":"701ae44da9c1b1b7f53753b480acd1b8","text":"Permissions boundary","correct":false},{"id":"326da76e6202b801f1b6ea68e76c422c","text":"VPC endpoint policy","correct":false},{"id":"e87c75c0e57d922b4c9578483c661f61","text":"S3 bucket policy","correct":true}]},{"id":"0e17eb91-9745-4365-8b54-5ebb2c6ffeb5","domain":"data-man","question":"A company wants to create a disaster recovery account involving creating snapshots of RDS, EC2 instances and EFS.  There are additional business and regulatory backup compliance requirements such as backups must be kept for three years but then must be destroyed.  Your manager wants to know how you could go about taking scheduled snapshots and deleting them once the retention period is expired with the lowest cost and operational overhead.","explanation":"AWS Backup is a centralised place to create backups of your EBS, RDS, and EFS resources.  There is no additional cost for setting up backup plans and retention policies, and this is a managed service so it's a perfect option to present to your manager.","links":[{"url":"https://aws.amazon.com/backup/","title":"AWS Backup"}],"answers":[{"id":"8be1adb291f4f253ef46691652254d1f","text":"Browse the AWS Marketplace and purchase a backup tool which can run in your AWS account and perform the backup for you.","correct":false},{"id":"fc24b51d19eedb53ab68e9e1569c9458","text":"Use AWS Backup to create a vault and a Backup Plan to take backups on a schedule and automatically delete them once expired.","correct":true},{"id":"52b5e3e90161fce097eca53313efd955","text":"Use AWS Data Recovery Manager to create a storage vault and automated backup and retention rules.","correct":false},{"id":"d75279ac650753a76148e6ac6c1b380e","text":"Your team should write some Lambda functions which are triggered by CloudWatch Events on a cron expression.  Create another lambda to delete snapshots once they are expired.","correct":false}]},{"id":"cd2751c9-f211-4b02-8892-c8eead5bb7c5","domain":"security-comp","question":"You are a SysOps Administrator for a large online media outlet. You are concerned about complex large-scale DDoS attacks, which could overwhelm your website using CloudFront. The company accepts it may need to pay more for superior protection, as keeping the website up is a priority. What would the most efficient solution be to implement?","explanation":"AWS Shield Advanced includes a number of additional features to mitigate more sophisticated and large scale attacks, including tailored monitoring, special support from the AWS DDoS Response Team, and cost protections in case a DDoS attack impacts your billing. There is an additional monthly fee, plus data transfer charges for this service. AWS Shield Standard will cope with most attacks, but may not be suitable where DDoS attacks are a particular concern. Engaging an AWS Partner Network member may be useful in designing your solution and in certain aspects of response, but may not be as effective on it's own without AWS Shield Advanced. Auto-scaling your instances can be useful in mitigating extra load, including as part of prevent DDoS attacks from taking your site offline, but it is not the most cost-effective option","links":[{"url":"https://aws.amazon.com/shield/faqs/","title":"AWS Shield FAQs"},{"url":"https://aws.amazon.com/shield/pricing/","title":"AWS Shield Pricing"}],"answers":[{"id":"8e55e18aaeeb5818e6eb59fb4d5cb61f","text":"Ensure AWS Shield Standard is in place, providing DDoS mitigation at CloudFront","correct":false},{"id":"900cd54a466f343487a9f7dd05e32e3e","text":"Engage an AWS Partner Network member specializing in DDoS mitigation","correct":false},{"id":"05c22cc94d0c8b789de829fc50471fc7","text":"Implement AWS Shield Advanced, including it's enhanced protections at additional cost","correct":true},{"id":"2cb88e85a444425855a2f96006c40e82","text":"Setup auto-scaling for your backend instances to meet capacity demands","correct":false}]},{"id":"df47b876-0fc7-439f-87b6-01f53249e980","domain":"data-man","question":"A company has a popular web app that frequently reads and writes customer data in a DynamoDB table. Your team is developing a new application and it needs to capture the data about these updates to the table, and provide some near real-time usage metrics for the web app. How should the application get the required data?","explanation":"The DynamoDB stream contains the information about changes to items in a DynamoDB table. After the stream is enabled, the information is stored in a log for up to 24 hours. The application can get the data from streams endpoints such as \"streams.dynamodb.<region>.amazonaws.com\". The application cannot fetch the streaming data from an S3 bucket. And the CloudTrail service does not capture the data events of DynamoDB tables.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html","title":"Capturing table activity with DynamoDB Streams"}],"answers":[{"id":"e2e524ab2e2071f38fd5e1e1c74daf56","text":"Enable item level streams for the DynamoDB table and configure a S3 bucket to save the streaming data. The application can get the sequence of modifications from the S3 bucket.","correct":false},{"id":"a71c3c217d4196fc1179a0ca0207cb90","text":"Enable DynamoDB Streams for the DynamoDB table so that a time-ordered sequence of modifications in the table is captured. Get the streaming data from the DynamoDB Streams endpoints.","correct":true},{"id":"fadc7afe691bf8186ca9574e2cbef16e","text":"Enable the object level logging for the DynamoDB table and save the activity events in a S3 bucket. The application can get the events from the S3 bucket and generate real-time metrics.","correct":false},{"id":"b39b6791ccfc53f69a9e0c05b74a7bf1","text":"Configure a CloudTrail that includes data events for DynamoDB tables. The application can read the streaming data from the trail and generate near real-time usage metrics.","correct":false}]},{"id":"8dade21b-c268-4ccd-af0a-18c01cd2c638","domain":"automation","question":"You check the last bill of your AWS account and find that the storage of EBS snapshots charges a lot. A large number of EBS snapshots are very old and can be deleted. You want to keep 10 snapshots for an EBS volume and old snapshots are deleted automatically. The strategy should also help you to create a snapshot every 24 hours. Which is the best way of implementing this strategy?","explanation":"The Amazon EBS Snapshot Lifecycle can automate the creation, retention, and deletion of EBS snapshots. You only need to configure a lifecycle policy in the Lifecycle Manager. You do not need to configure a Lambda Function or a Cron job in an EC2 instance to implement the same policy.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html","title":"Automating the Amazon EBS Snapshot Lifecycle"}],"answers":[{"id":"6e23ea86e4cf4e8929aa5b846a7e5ad4","text":"Create a Snapshot Lifecycle Policy to automatically create new snapshots and delete old snapshots.","correct":true},{"id":"630f293528bb790c603b2f77b266a56d","text":"Use a Cron job that runs in an T2.micro EC2 instance. The job creates a new snapshot and deletes the old one every day.","correct":false},{"id":"e711537d471732823f103b6f63ab96c2","text":"Configure a Cloudwatch Event rule to execute every 24 hours. The target is a Lambda Function that creates a new snapshot and deletes the old one.","correct":false},{"id":"ac159fa50438475f44d25f96b6a2bea4","text":"Configure a Lambda Function that runs every 24 hours to create a snapshot and delete the old snapshot.","correct":false}]},{"id":"55e3410c-cf36-4a46-948a-95e2440003fd","domain":"mon-rep","question":"You are running an Application Load Balancer (ALB) in front for a fleet of web servers running on EC2. These servers are in a public subnet. Your customers connect to the ELB domain name to access web servers using HTTP. You want to know your customers' IP addresses to gain metrics into where your customers are located. This information will be helpful for improving your application based on the location of your customers. How would you collect log data for your ELB?","explanation":"The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server. If you were using TCP protocol (rather than HTTP), no additional configuration would be needed. CloudTrail is not appropriate as it only shows data regarding API requests sent within your AWS account. CloudWatch and Lambda would be an administrative burden and are not necessary.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html","title":"HTTP Headers and Classic Load Balancers"}],"answers":[{"id":"ee23873239420b48da4e1d7e54294f34","text":"Enable CloudWatch logs for your application and push the logs to a custom CloudWatch metric. Use Lambda to parse through the log files to search and extract the client IP addresses into a DynamoDB table.","correct":false},{"id":"db95fed268eaf12b5b9c069f8256be2f","text":"Enable CloudTrail on your ELB and push the logs to an S3 bucket. Search the logs using Athena or download them as a CSV to identify the IP addresses of your customers.","correct":false},{"id":"0fcaa257260dc2e705852935fcc43979","text":"Modify the application code to pull the client IP into the X-Forwarded-For header so the web servers can parse the information.","correct":true},{"id":"c790427146d378ddfcfb7c4e0821a00c","text":"No additional configuration is needed. The proxy protocol will pass the client IP automatically, and you can check the ELB logs to find this information.","correct":false}]},{"id":"d5760f8d-25af-48b0-8e78-5d8d3d1e91cc","domain":"dep-prov","question":"You have a DynamoDB table in region ap-southeast-2 that stores usersâ€™ subscription data. As more and more users come from Europe, you want to configure a replica table that has the same name and schema in region eu-central-1. When the application writes data to a replica table, DynamoDB should automatically propagate the write to the other one. How should you implement this?","explanation":"Global Table in DynamoDB is a feature that configures DynamoDB as a fully-managed, multi-region and multi-master database. The global table consists of one replica table per Region and DynamoDB automatically keeps the table data in sync between replicas.","links":[{"url":"https://docs.aws.amazon.com/en_pv/amazondynamodb/latest/developerguide/globaltables_HowItWorks.html","title":"DynamoDB Global Tables"}],"answers":[{"id":"4de5416858aeddf52eafd42cce3ef87f","text":"Configure a read replica in region eu-central-1.","correct":false},{"id":"dafd6112e9fb08d950b54c7836f9805a","text":"Configure automatic backups for the table in another region.","correct":false},{"id":"849e48fd8762258d9c7afa5c95203db0","text":"Configure a global table in region eu-central-1.","correct":true},{"id":"82b856354099b33a5dbb0b6e1a12b264","text":"Create a new table in region eu-central-1 with the same name and schema. Sync the original table with the new table.","correct":false}]}]}}}}
