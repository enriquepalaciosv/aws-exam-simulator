{"data":{"createNewExamAttempt":{"attempt":{"id":"499e903c-cb10-4603-8da8-6a3f8f2c01dc"},"exam":{"id":"40827eb5-e963-4ec8-bf19-9a818e62ce8b","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"6d93e859-e1a9-468f-9a05-61a2dbc2be9c","domain":"awscsapro-domain5","question":"You manage a group of EC2 instances that host a critical business application.  You are concerned about the stability of the underlying hardware and want to reduce the risk of a single hardware failure impacting multiple nodes.  Regarding Placement Groups, which of the following would be the best course of action in this case?","explanation":"Spread Placement Groups ensure your instances are each placed on separate underlying hardware so this reduces the risk of a single hardware failure taking down multiple instances.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-spread","title":"Placement Groups - Amazon Elastic Compute Cloud"}],"answers":[{"id":"e6d129a06320300aaf95634a6691b7cb","text":"You would the AWS Console to move the existing instances into a clustered placement group.","correct":false},{"id":"3bde2d7fff7ead39c4035ab0600275f1","text":"You would use the AWS CLI to move the existing instances into a diversified placement group.","correct":false},{"id":"e346d1667489b5726c0367eff4ea4a34","text":"You would use the AWS CLI to move the existing instances into a spread placement group.","correct":true},{"id":"b6153af57a0b8805fc28d93a2859fb9e","text":"You cannot move existing instances into a new placement group.  You would create AMIs from the existing instances and redeploy them into a clustered placement group.","correct":false},{"id":"c8b809a2fef3f21146774b82e8f03f12","text":"You would move the instances onto a Dedicated Host.","correct":false}]},{"id":"dd8b46c7-d1d5-4326-a092-927b9333fd2a","domain":"awscsapro-domain5","question":"You are helping a company transition their website assets over to AWS.  The project is nearing completion with one major portion left.  They want to be able to direct traffic to specific regional EC2 web servers based on which country the end user is located.  At present, the domain name they use is registered with a third-party registrar.  What can they do?","explanation":"You can use Route 53 if the domain is registered under a third-party registrar.  When using Geolocation routing policies in Route 53, you always want to specify a default option in case the country cannot be identified.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy - Amazon Route 53"}],"answers":[{"id":"381d1621ab4c93aca8cc780f05e98c50","text":"Initiate a domain transfer request with the current registrar.  Once the request goes through, create a public hosted zone in Route 53.  Create SRV records for each regional EC2 instance using a Geolocation routing policy.  Create an alias record for the top-level domain and link that to the SRV records.","correct":false},{"id":"724dab26998c86ab7ddc7faa063285b8","text":"You cannot use Route 53 routing policies unless AWS is the registrar of record for your domain.  A workaround could be to configure your own top-level DNS server using BIND.  Ensure the NS and SOA records point to this instances.  Create A-type records pointing to the IP addresses of the regional EC2 web servers.  Dynamically redirect requests using customized BIND rules and a third-party IP geolocation database.","correct":false},{"id":"81efc0b140e618c378c9e9bc59dd4ca8","text":"Create a public hosted zone for the domain in Route 53.  Update the DNS entries in the registrars database to use AWS DNS Servers as defined in the NS record on Route 53.  Create A-type records for all EC2 instances. Configure CNAME records for the main FQDN that point to regional A records using a Geolocation routing policy.  Create another CNAME record as a default route.","correct":true},{"id":"294ce2854fbe727bd4f4917543d45bec","text":"Create a private hosted zone for the domain in Route 53.  Update the DNS record entries in the registrars database to use AWS DNS Servers.  Once the DNS changes are fully propagated across the internet and the TTL has expired, convert the private hosted zone to a public hosted zone.  Create A-type records for all the regional EC2 instances and configure a Geo-proximity policy for each record, ensuring the bias across all records sums to 100.","correct":false}]},{"id":"9fc0785a-d5cb-47e3-bc2f-829b5a36ba26","domain":"awscsapro-domain3","question":"You work for a Genomics company which has decided to migrate its DNA Sequencing application to the AWS Cloud. The application is containerized. Currently, container image A works on genomics data residing on an on-premises file server, validating the data and updating the metadata in a local database. When it is done, engineers manually trigger 100 or more instances of container image B that process this data in parallel by reading the metadata, creating output files. When all these container instances have done their job, engineers manually trigger container image C that validates the results, cleans up and sends notifications.\nThe CTO has decided to use S3 for storing the input and output data files. She has also mandated that the parallel processing phase should run on a fleet of Spot EC2 instances to reduce compute costs. She also wants to automate the workflow, so that engineers do not have to manually trigger the next set of actions. The requirement is to minimize administrative overhead and custom development for the migration.\nAs the AWS Architect, which of the following approaches should you recommend?","explanation":"AWS ECS does not natively provide workflow management. In an ECS service definition file, you cannot specify a sequence of tasks with execution dependencies such that one will be run only after the previous one completes. Hence, the two ECS choices are ruled out.\nDistraction warning - Fargate does not allow you to specify Spot instances as it is serverless in nature (it absolves you from specifying server details). This effectively creates a distraction - when the candidate rules out ECS Fargate due to this reason, they may be relieved to see the ECS EC2 choice and jump to a conclusion because it is relatively easy to remember that EC2 launch type actually lets you select Spot instances. However, this distraction is designed to take focus away from the fact that neither of these two choices is correct. Both of the choices require service definition files to set up execution workflows. Task instances mentioned in an ECS service definition file are executed in parallel - ECS does not control the sequence of tasks.\nAWS SWF does not let you specify Spot instances either. Also, SWF is usually used in cases where human intervention is needed in the workflow.\nThis leaves AWS Batch as the correct answer. AWS Batch is indeed the most suitable AWS service for this scenario as it meets all requirements.","links":[{"url":"https://docs.aws.amazon.com/batch/latest/userguide/create-compute-environment.html","title":"How to create a compute environment for AWS Batch"},{"url":"https://docs.aws.amazon.com/batch/latest/userguide/example_array_job.html","title":"Example AWS Batch Array Job Workflow"},{"url":"https://aws.amazon.com/ec2/spot/containers-for-less/get-started/","title":"How to run ECS clusters in EC2 Spot Instances"}],"answers":[{"id":"757ddde350053553e44844d066c91386","text":"Use AWS ECS with Fargate Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"e46ada36d33a9e5b23aa37ee94c4c5d6","text":"Use AWS ECS with EC2 Launch Type to run the container images, configuring the cluster to use Spot Instances and setting up the workflow in the service definition JSON file so that it runs Task C only after Task B is completed and it runs Task B only after Task A is completed","correct":false},{"id":"ceb4c03a526e8ddb01ada7a40bb60001","text":"Use AWS Batch, setting up an array job with 100 or more copies preceded by pre-requisite and follow-up jobs where the workflow is controlled by dependencies between jobs. Also, use Spot as the Provisioning Model for compute environment","correct":true},{"id":"49755d6c34da495b8c91964f52946d29","text":"Use AWS SWF workers and deciders to manage the workflow. Configure the workers to use EC2 Spot Instances","correct":false}]},{"id":"ffae5615-188b-4023-aae2-71270158730a","domain":"awscsapro-domain5","question":"Several teams are using an AWS VPC at the same time. The VPC has three subnets (subnet-1a, subnet-1b, subnet-1c) in three availability zones (eu-west-1a, eu-west-1b, eu-west-1c) respectively. As there are more and more AWS resources created, there is a shortage of available IP addresses in subnet-1a and subnet-1b. The subnet subnet-1c in availability zone eu-west-1c still has plenty of IP addresses. You use a CloudFormation template to create an Auto Scaling group (ASG) and an application load balancer for the ASG. You enable three availability zones for the load balancer and the Auto Scaling group also spans all the three subnets. The CloudFormation stack usually fails to launch because there are not enough IP addresses. High availability is not required for your project since it is a proof of concept. Which of the following methods is the easiest one to resolve your problem?","explanation":"The easiest way is to enable only the eu-west-1c availability zone for ELB and launch ASG instances in subnet-1c. Only the IP addresses from eu-west-1c are required to create the resources. For a VPC, the existing CIDR cannot be modified and you also cannot add another CIDR IP range to an existing subnet. For an application load balancer, there is no IP balancing feature and IP addresses from a subnet cannot be reserved by other subnets.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-subnets.html","title":"Availability Zones for Your Application Load Balancer"}],"answers":[{"id":"9cb12513be8578fb017ebb7d1b302f9a","text":"Enable the IP balancing feature in the application load balancer so that the IP addresses are equally distributed among subnets. When elastic load balancer is created, available IP addresses in one subnet can be reserved by other subnets.","correct":false},{"id":"b8c040b89e07e5150d4a8a3899d41659","text":"Add another IPv4 CIDR to the VPC which should have at least 256 IP addresses. Add another IP CIDR block to subnet-1a and subnet-1b to increase the available IP addresses.","correct":false},{"id":"1f29e88227ce0771f9e43093bd53583a","text":"Add more IP addresses by extending the CIDR range for the VPC. Create a new subnet in each availability zone and reserve at least 128 IP addresses in a subnet. Modify the CloudFormation template to use the new subnets for the Auto Scaling group and ELB.","correct":false},{"id":"94887cfd8a06ae986d5721a940c3c52b","text":"Modify your CloudFormation template to enable only the availability zone eu-west-1c for the application load balancer and launch the Auto Scaling group in subnet-1c which belongs to eu-west-1c.","correct":true}]},{"id":"f43ec458-0ff5-4633-a57b-6bf82f60bd14","domain":"awscsapro-domain5","question":"You have a target group in an elastic load balancer (ELB) and its target type is \"instance\". You attach an Auto Scaling group (ASG) in the target group. All the instances pass the health check and have a healthy state in the target group. Due to a new requirement, the ELB target group needs to forward the incoming traffic to an IP address that belongs to an on-premise server. The ASG is no longer needed. There is already a VPN connection between the on-premise server and AWS VPC. How would you configure the target in the ELB target group?","explanation":"The target type of existing target groups cannot be changed from \"instance\" to \"IP\". Because of this, users have to create a new target group and set the target type to be \"IP\". After that, the on-premise IP address can be registered as a target. A domain name cannot be registered as a target in the target group. You also do not need to create a new elastic load balancer since you only need a new target group to register the IP address.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#target-type","title":"Target type in ELB target group"}],"answers":[{"id":"e0b619a421d68626ffb82b1a6e1d22d5","text":"Remove the Auto Scaling group from the target group and modify the target type to be \"IP\". Attach the IP address to the target and set up the IP address and port in the health check configurations.","correct":false},{"id":"49646fca04901301d145a2a814a7e481","text":"In the elastic load balancer, create a new target group with an \"IP\" target type. Register the on-premise IP address as its target. Monitor if the target becomes healthy after some time. Remove the old target group.","correct":true},{"id":"99f63c1cf5bcfbcb188328714abb8ed3","text":"Register a record set in AWS Route 53 to forward a domain name to the on-premise IP address. Modify the target group to register the domain name as its target. Remove the previous Auto Scaling group from the target group.","correct":false},{"id":"265de6fdcab5323b156774dcb949d309","text":"Create a new network load balancer with a new listener and target group. Configure the target type to be \"IP\" in the target group and attach the on-premise IP address to it. Set up the health check using the HTTP protocol.","correct":false}]},{"id":"8d0d69dd-35f2-468a-883b-18ad1135b564","domain":"awscsapro-domain2","question":"You are providing a security administrator with some on-the-job training regarding IAM, roles and policies.  The security administrator asks just what the policy evaluation logic is when a request is made?  For example, when a user tries to use the AWS Management Console, what is the process that AWS goes through to determine if that request is allowed?","explanation":"Knowing this logic flow can help troubleshoot security issues.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html","title":"Policy Evaluation Logic - AWS Identity and Access Management"}],"answers":[{"id":"e13db2078457fb9b19c98ad9ae06563c","text":"Upon receiving the request, AWS will first evaluate the permissions of the principal and the request.  Next, any STS assumed role policies are evaluate.  Then, specific user or role permission boundaries are evaluated against the request and the principal.  Finally, any Organizational boundaries (SCPs) are evaluated against the request.  If there are no explicit deny actions, the request is allowed.","correct":false},{"id":"f3087f569306369e9cfa2b1f6806acbc","text":"The AWS service receives the request.  AWS first authenticates the principal.  Next, AWS determines which policy to apply to the request.  Then, AWS evaluates the policy types and arranges an order of evaluation. Finally, AWS then processes the policies against the request context to determine if it is allowed.","correct":true},{"id":"eb4b98badf50f00af2105b3132c620af","text":"First AWS evaluates organizational boundaries of the request in context of the principal.  Next, any STS assumed role policies are evaluated.  Then, permission boundaries are evaluated against the principals request.  Finally, AWS then issues a decision of allow if there were no explicit deny.","correct":false},{"id":"5afa15bfdd702c70736460429675418c","text":"The AWS service receives the request from the principal.  AWS first evaluates the authority of the principal in context of the service.  Next, any identity-based or resource-based policies are evaluated.  Then, explicit Action statements are evaluated in context of the request.  Finally, AWS issues a decision on whether to allow or deny the request.","correct":false}]},{"id":"580790f0-3491-4b4c-a9b8-42b36a787cf5","domain":"awscsapro-domain2","question":"A new project needs a simple, scalable Amazon Elastic File System (EFS) to be used by several Amazon EC2 instances located in different availability zones. The EFS file system has a mount target in each availability zone within a customized VPC. You have already attached a security group to EC2 instances that allows all outbound traffic. In the meantime, how would you configure the EFS volume to allow the ingress traffic from these EC2 instances?","explanation":"EC2 instances connect to the Amazon EFS file system through the mount targets using the Network File System (NFS) port. The mount targets use a security group to control the access from EC2 instances. It should have an inbound rule to allow the NFS port (TCP:2049). The source of the security group would be the EC2 security group rather than the VPC CIDR. Either IAM role or network ACL cannot be used to control inbound access to an EFS system.","links":[{"url":"https://docs.aws.amazon.com/efs/latest/ug/network-access.html","title":"Security Groups for Amazon EC2 Instances and EFS Mount Targets"}],"answers":[{"id":"070a2bfc6b40165751997ed4d5d4e295","text":"Attach a security group to the mount targets in all availability zones. Allow the NFS port 2049 for its inbound rule. Identify the EC2 security group name as the source in the rule of the security group.","correct":true},{"id":"b346d4a177db41a1cfb51b80ae95708a","text":"Add an inbound rule in the network ACL to allow the ingress traffic from the NFS TCP port 2049. The source of the rule should be 0.0.0.0/0. Apply the network ACL in all the subnets where the EFS mount targets exist.","correct":false},{"id":"0fb49dc49facb19b17db23bdff392a1d","text":"Configure a new security group to allow inbound traffic from the VPC CIDR IP range such as 10.10.0.0/16. Associate the security group with the EFS file system directly to allow the ingress traffic from EC2 instances in the VPC.","correct":false},{"id":"10fa29df072c2421e53a66048a92cbe3","text":"Make sure the attached IAM role in EC2 instances has the \"elasticfilesystem:*\" permission. The ingress traffic in the EFS volume is automatically allowed if the IAM role has enough permissions to interact with Elastic File Systems.","correct":false}]},{"id":"bc9f1b35-bf56-4bf1-b563-9bd2d864e4bc","domain":"awscsapro-domain2","question":"A global digital automotive marketplace is using Lambda@Edge function with CloudFront to redirect incoming HTTP traffic to custom origins based on matching custom headers or client IP addresses with a list of redirection rules. The Lambda@Edge function reads these rules from a file, rules.json, which it fetches from an S3 bucket. The file changes every day because several teams in the company uses the file for different purposes, including but not limited to, (a) the security team uses the file to honeypot potential malicious traffic (b) the engineering team uses the file to do A-B testing on new features, (c) the product team experiments with new mobile platforms by redirecting traffic from a specific kind of mobile device to a specific set of server farms, etc.. As a result, the file can be as big as 200 KB. Recently, the response time of the website has degraded. On investigation, you have found that this Lambda@Edge function is taking too long to fetch the rules.json file from the S3 bucket. The existing CI-CD pipeline deploys the file to a versioning-enabled S3 bucket when any change is committed to source control. Any change in rules.json must reflect within 1 hour at all Cloudfront Edge locations. Select two options from the ones below that will not work in improving the latency of fetching this file?","explanation":"A key to answering this question is to not miss the fact that it asks which two of the answers will not help. AWS SA-P exam can occasionally frame the question with a not. Also, knowledge of how Lambda@Edge functions work with CloudFront is important for the exam.\nThere will be no improvement in the fetching time if we reconfigure the S3 bucket as a static website. In fact, doing so might add a layer of redirection during routing.\nLambda@Edge does not guarantee the persistence of global variables in memory between invocations. While it might be possible to use global variables for a short time as cache, provided the code does not make any assumptions about the guarantee of persistence, it is a bad idea to solely depend on Lambda@Edge memory between invocations. AWS does not guarantee using the same container instance for any number of requests, though it will try to re-use a warmed up instance for the same function invocation landing on the same edge node. If it is re-using the same container instance from the one used by the last Lambda@Edge function, the global variable trick will work. However, as the option clearly says that such usage is guaranteed (which is false and will not work), it is one of the answer choices to select in this case.","links":[{"url":"https://aws.amazon.com/blogs/networking-and-content-delivery/leveraging-external-data-in-lambdaedge/","title":"Leveraging external data in Lambda@Edge"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-limits.html#limits-lambda-at-edge","title":"Limits on Lambda@Edge"}],"answers":[{"id":"ffa2541e9e07a612a02728b1135014f9","text":"Include the rules.json file in the Lambda@Edge deployment package. Change the CI-CD pipeline to deploy a new Lambda@Edge version every time the file changes. Change the Lambda@Edge function code to read the file locally instead of reading it from S3. This will improve the latency of fetching the file.","correct":false},{"id":"058638ecef0dd62ee83782f5cbbba63b","text":"Reconfigure the S3 bucket as a static website. Use the website endpoint to download the file instead of directly accessing the bucket from the Lambda@Edge function. This will cause HTTP GET requests to be cached by S3, thus improving the latency of fetching the file","correct":true},{"id":"5a14a6d0367c2a6daf2d4faccf1fbdb3","text":"Change the Lambda@Edge code to save the contents of the rules.json file in a global variable so that it is cached in Lambda@Edge memory, with a TTL of 55 minutes, persisted between invocations. Lambda@Edge guarantees persistence of variables in memory between invocations.","correct":true},{"id":"fd6d6c32ad84f6d5856477cd8f27d361","text":"Define a separate cache behaviour for *.json in your Cloudfront web distribution, setting the origin as the S3 bucket. Change the Lambda@Edge function code to use the Cloudfront download URL instead of downloading the file directly from S3. This way, the file will be cached by Cloudfront avoiding expensive round trip time to S3 each time. Set the Cloudfront TTL to 45 minutes.","correct":false}]},{"id":"05e085a9-4de3-46fe-9470-10c7f2faba57","domain":"awscsapro-domain5","question":"You are consulting with a client who is in the process of migrating over to AWS.  Their current on-prem Linux servers use RAID1 to provide redundancy.  One of the big benefits they are looking forward to with moving to AWS is the ability to create snapshots of EBS volumes without downtime.  Right now, they intend on migrating the servers over to AWS and retaining the same disk configuration.  What is your advice for them?","explanation":"Because RAID is based upon multiple volumes being in sync, taking snapshots of an individual volume that's part of a active and mounted RAID array would not create a proper backup.  You must first unmount the RAID volume and then create the snapshots of the component volumes.  This of course means any data on the RAID volume would be unavailable.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html","title":"RAID Configuration on Linux - Amazon Elastic Compute Cloud"}],"answers":[{"id":"21a6a5218cf9778e0184eed7897c54ce","text":"Consider using RAID6 rather than RAID1 on AWS for performance reasons.","correct":false},{"id":"bee3a756d6bfddce4e9917e171a4b0e2","text":"Consider using RAID0 when on AWS for performance reasons.","correct":false},{"id":"99a9d29ef15a0ced996c1510ff6d8f6a","text":"EC2 does not support RAID configurations.","correct":false},{"id":"ead9938f5d4fc4d2df30763406b6a8e5","text":"Consider using RAID10 when on AWS because it offers the best of both RAID0 and RAID1.","correct":false},{"id":"73b207bc7a947de1eed26bc058b4b67b","text":"If snapshots without downtime are the priority, do not use RAID.","correct":true}]},{"id":"b533b3c1-222f-4f33-99da-2c828e98ff91","domain":"awscsapro-domain5","question":"You have run out of root disk space on your Windows EC2 instance.  What is the most efficient way to solve this?","explanation":"We can easily increase the size of an EBS from the console or the CLI (using modify-volume) but then we also need to allow the OS to expand the resized volume so we can use it.  For Windows Server, we could use Disk Manager.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/expand-ebs-root-volume-windows/","title":"Expand the EBS Root Volume of Your EC2 Windows Instance"}],"answers":[{"id":"38862a719074689f75df6a20a42f7df7","text":"Use AWS System Manager Run service to remotely execute a PowerShell script using AWS Tools for PowerShell to expand the volume using the ModifyInstance command.","correct":false},{"id":"229b013eff2d5f53df7b9c3a60bd2418","text":"From the AWS Console, select Modify Volume for the EBS volume.  Enter the new size and confirm the change.  Connect to your Windows instance and use Disk Manager to extend the newly resized volume.","correct":true},{"id":"346ac05b4353d34c630eb6d233f8a35d","text":"Compress all files on the root volume using the built-in zip utility.  Modern versions of Windows will automatically unzip the files when they are accessed.","correct":false},{"id":"b893490015da5047b17b1220d43f4a1c","text":"From the AWS CLI, use the \"modify-instance\" command for EC2 to resize the volume to a larger size.  Using RDP, connect to the Windows instances and use Disk Manager to expand the volume.","correct":false}]},{"id":"6b6689f4-b150-482a-aa96-eab1674cb232","domain":"awscsapro-domain5","question":"Quality Auto Parts, Inc. has installed IoT sensors across all of their manufacturing lines. The devices send data to both AWS IoT Core and Amazon Kinesis Data Streams. Kinesis Data Streams triggers a Lambda function to format the data, and then forwards it to AWS IoT Analytics to perform monitoring and time-series analyses, and to take actions based on business processes. After an equipment failure on one of the manufacturing lines causes tens of thousands of dollars in revenue losses, it's determined that alarms for a specific piece of equipment where received seventy-five seconds after the issue originated, and that automated corrective action within a few seconds of the problem could have avoided the financial losses altogether. What changes should be made to the architecture to improve the latency of device alerts?","explanation":"AWS IoT Analytics is useful for understanding long-term device performance, performing business reporting, and identifying predictive fleet maintenance needs, but common latencies run from seconds to minutes. If you need to analyze IoT data in real-time for device monitoring, use Kinesis Data Analytics, which provides latencies in the millisecond to seconds range. A Lambda function can be used as the destination for Kinesis Data Analytics to perform corrective actions. IoT Core rules can write messages to a Kinesis stream, but not directly to Kinesis Data Analytics. Having a Lambda function perform anomaly detection will work, but will require more logic to be written for query setup and execution than using a specialized service like Kinesis Data Analytics. With Amazon CloudWatch Alarms, an alarm will watch a single metric over a period time, but will not provide the capabilities of SQL to detect complex anomaly conditions.","links":[{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/aws-reference-architecture-time-series-processing.pdf?did=wp_card&trk=wp_card","title":"Processing IoT Time Series Data on AWS"},{"url":"https://aws.amazon.com/iot-analytics/faq/","title":"AWS IoT Analytics FAQs"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/05/introducing-real-time-iot-device-monitoring-with-kinesis-data-analytics/","title":"Introducing Real-Time IoT Device Monitoring with Kinesis Data Analytics"}],"answers":[{"id":"522af3cd7d520d3e94e97c02d19c0672","text":"Create an AWS IoT Core rule to write the message to Amazon CloudWatch Alarms to detect anomalies in the data. Invoke another AWS Lambda function from CloudWatch Alarms to perform device corrective action when needed.","correct":false},{"id":"fe8ff20697982ca413f81ea14472e603","text":"Add Amazon Kinesis Data Analytics as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":true},{"id":"7e2eb8f3a96a390aab88e66000821c26","text":"Create an AWS IoT Core rule to write the message to Amazon Kinesis Data Analytics to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":false},{"id":"71ade679994c0c1e6e55b3853194e4c5","text":"Add another AWS Lambda function as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Have the Lambda function write the anomalies to Amazon DynamoDB and perform device corrective action when needed.","correct":false}]},{"id":"1080e29c-045b-4714-8f41-6be669f865a5","domain":"awscsapro-domain4","question":"Your company stores financial transactions in a write-heavy MySQL 5.6 RDS database instance. The innovation team is developing a simulation algorithm. They have made a point-in-time copy of the production database for running tests. Each such test runs for a few minutes and changes the data. They need to run thousands of such tests and compare the results, where each test is run on the same test data. Therefore, after each test is completed, they want the test database back in its original state with the initial data, so that they can run the next test quickly. Currently, they use a DB Snapshot that contains the initial state. They restore the database after every test run. The restoration process takes a couple of hours, slowing them down. They want to automate the process of running tests and ideally run these tests every few minutes to be more productive. Suggest a suitable mechanism for quickly achieving this with least effort and lowest cost.","explanation":"This question tests the knowledge of Aurora Backtracking feature, which is the best answer. This question also tests the knowledge of EBS and RDS MySQL. An EBS restore will not necessarily be any quicker than a DB Snapshot restore. Also, spinning up thousands of instances of RDS will firstly be met with a soft-limit increasing resistance by AWS (the current per account per region limit is 40), not to mention the drastic cost implications of so many instances running. Also, MySQLDump is not necessarily a whole lot quicker, not like Aurora backtracking anyway.\nIt is important to remember the fundamental difference between backtracking and point-in-time restore from DB Snapshots. Backtracking rewinds the same database. Point-in-time restoration from snapshots creates a new database. Hence, the latter is more time-consuming. Backtracking is much faster, but it is only available in Aurora, that too, only for certain database engines and versions as of 2019. Remember that if you turn on backtracking, you are paying for the extra storage, as Aurora must keep change records in order to be able to backtrack. However, that extra cost for one instance is not much compared to running hundreds or thousands of RDS instances. Hence, the Aurora solution is also the most cost-effective.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html","title":"Backtracking an Aurora DB Cluster"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RDSMySQL.Import.html","title":"Migrating an RDS MySQL Snapshot to Aurora"}],"answers":[{"id":"093d4a2bd5748fd50abe9ea0a1adc3b7","text":"Spin up as many RDS MySQL Instances from the initial snapshot as required to run all the tests in parallel. That way, the innovation team can save time and not have to wait for one test to complete to run the next","correct":false},{"id":"b29796f55566af78fb926244627009f8","text":"Migrate the initial RDS MySQL Snapshot to an AWS Aurora DB Cluster. Turn on backtracking while creating the test Aurora MySQL DB Cluster. After each test is completed, backtrack to the point in time before the test.","correct":true},{"id":"9d29133ab1fe457147ed07251f102d5b","text":"Use MySQLDump to make a dump of the database in its initial state. Instead of restoring the database from its RDS DB Snapshot each time, restore it from the dump. This will make the process much quicker","correct":false},{"id":"8ff1b66ec30e68f3b0af77ca834fa8b4","text":"Restore the database in its initial state on an EC2 instance with Provisioned IOPS EBS disks. Take an EBS Snapshot. After each test, restore an EBS Volume from the EBS Snapshot. EBS Snapshots are much quicker to restore. Run the test on the EC2 Instance with the restored EBS Volume","correct":false}]},{"id":"e50e5c98-f9f8-48fd-80fc-6a8741d17482","domain":"awscsapro-domain1","question":"You have just completed setup of Direct Connect from your data center to VPCs in us-east-1.  You would also like to leverage that Direct Connect for communication between your data center and other VPCs in other regions.  What is the simplest way to do this?","explanation":"You can use an AWS Direct Connect gateway to connect your AWS Direct Connect connection over a private virtual interface to one or more VPCs in your account that are located in the same or different regions. You associate a Direct Connect gateway with the virtual private gateway for the VPC, and then create a private virtual interface for your AWS Direct Connect connection to the Direct Connect gateway. You can attach multiple private virtual interfaces to your Direct Connect gateway.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html","title":"Direct Connect Gateways - AWS Direct Connect"}],"answers":[{"id":"d9271b66eb35f235c0b61fd11f3af41d","text":"Associate the Direct Connect gateway with the virtual private gateway for the VPC, and then create a private virtual interface for your Direct Connect connection to the Direct Connect gateway.","correct":true},{"id":"38573789b2722bdd38529167996196d1","text":"You cannot use a Direct Connect gateway in one region to reach another region.  You must order a second Direct Connect link from a Partner in the desired regions.","correct":false},{"id":"15955693a61b06cc43a38a2ac5a2ede9","text":"Setup a Transitive VPC and configure virtual private gateways between the us-east-1 VPC and other VPCs in other regions.","correct":false},{"id":"17077e5e04411e1e4e4b8ab017ae6c18","text":"Add a new route to the VPC peer in the desired region in the Customer Gateway.  Ensure that BGP routing is enable to propagate the route.","correct":false}]},{"id":"24937858-d37e-4f9b-b195-d87c42b3f1ca","domain":"awscsapro-domain2","question":"You are designing a workflow that will handle very confidential healthcare information.  You are designing a loosely coupled system comprised of different services.  One service handles a decryption activity using a CMK stored in AWS KMS.  To meet very strict audit requirements, you must demonstrate that you are following the Principle of Least Privilege dynamically--meaning that processes should only have the minimal amount of access and only precisely when they need it.  Given this requirement and AWS limitations, what method is the most efficient to secure the Decryption service?","explanation":"Grants in KMS are useful for dynamically and programmatically allowing a process the ability to use the key then revoking after the need is over.  This is more efficient than manipulating IAM roles or policies.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/grants.html","title":"Using Grants - AWS Key Management Service"}],"answers":[{"id":"45b8475be8fa894d6c7145b5536d21fe","text":"Use a grant constraint to deny access to the key except for the service account that is running the workflow processes.  Enable CloudTrail alerts if any other role attempts to access the CMK.","correct":false},{"id":"6c5638772104a82518657e9bfbc0970d","text":"In the step right before the Decryption step, programmatically apply a grant to the CMK that allows the service access to the CMK key.  In the step immediately after the decryption, explicitly revoke the grant.","correct":true},{"id":"2202679b0d915deca81caf1d431477a4","text":"Create a IAM key policy that explicitly allows access to the CMK and assign that to a role.  Assign the role to the process that is executing the Decryption service.  At the end of the day, programmatically revoke that role until the start of the next day.","correct":false},{"id":"b5f054e3c6369a2c4a9256e55359f9ff","text":"The current AWS platform services are not well suited for implementing Principle of Least Privilege in a dynamic manner.  Consider a different design that makes use of a more monolithic architecture rather than services.","correct":false},{"id":"3cc7443088dcd1d7935eaf578a49d078","text":"Create an IAM key policy that explicitly denies access to the Decryption operation of the CMK.  Assign that policy to a role that is then assigned to the process executing the Decryption service.  Use a Lambda function to programmatically remove and add the IAM policy to the role as needed by the decryption process.","correct":false}]},{"id":"9564dd14-763c-45c7-8546-a8bbe687a55e","domain":"awscsapro-domain2","question":"You are helping a company design a new data analysis system.  The company captures data in the form of small JSON files from thousands of pollution sensors across the country every 10 minutes.  Presently, they use some BASH scripts to load data into an aging IBM DB2 database but they would like to upgrade to a more scalable cloud-based option.  A key requirement is the ability to replay the time-series data to create visualizations so they have to keep and query lots of detailed data. They intend to keep their current visualization tools which are compatible with any database that provides JDBC drivers.  Which of the following architectures IS NOT suited to help them Ingest and analyze the sensor data?","explanation":"For EMR, the task nodes do not store any HDFS data.  Task node storage is considered ephemeral so this would not be a good choice.","links":[{"url":"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html","title":"Understanding Master, Core, and Task Nodes - Amazon EMR"}],"answers":[{"id":"c552ecc7ce7256e713857b27429e7753","text":"Load the files into S3 and use AWS Athena as the database layer.","correct":false},{"id":"98678aaeaf75f088f0d57ca7b321f688","text":"Use Lambda to read the sensor files and write them to a DynamoDB table.  Use a third-party JDBC driver to provide query access.","correct":false},{"id":"23ad2d64a285f603e5ca0a29c6a444e7","text":"Spin up an EMR cluster.  Ingest the data into HDFS partitions on the EMR task nodes using Flume.  Use Hive to provide JDBC access.","correct":true},{"id":"0439c9dae7a15237bd7e1bfdca41bbd2","text":"Spin up a Redshift database.  Use Kinesis Firehose to load the data directly into Redshift.","correct":false},{"id":"9ee2b80a7b38c15f156422085185bf91","text":"Deploy Amazon Aurora for MySQL and download the MySQL JDBC drivers. Use AWS Glue to Ingest the raw sensor files into Aurora.","correct":false}]},{"id":"35fc536d-d968-472d-84d5-a7ae5d343564","domain":"awscsapro-domain1","question":"You work for a genetics company that has extremely large datasets stored in S3. You need to minimize storage costs, while maintaining mandated restore times that depend on the age of the data. Data 30-59 days old must be available immediately, and data ≥ 60 days old must be available within 12 hours. Which of the following options below should you consider?","explanation":"You should use S3 - IA for the data that needs to be accessed immediately, and you should use Glacier for the data that must be recovered within 12 hours. S3 - RRS and 1Zone-IA would not be suitable solution for irreplaceable data or data that required immediate access (reduced Durability or Availability), and CloudFront is a CDN service, not a storage solution.  The use of absolute words like 'Must' is an important clue as it will eliminate options where the case may not be possible such as with OneZone-IA.","links":[{"url":"https://aws.amazon.com/s3/faqs/#sia","title":"S3 - Infrequent Access"},{"url":"https://aws.amazon.com/s3/faqs/#glacier","title":"About Glacier"}],"answers":[{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":true},{"id":"31e831ec49678aed7f467f791d1f8704","text":"S3 - RRS","correct":false},{"id":"4340570ba672bfa48cd45e3f026c01d1","text":"S3 - IA","correct":true},{"id":"e9a5105fa288ef2b71c037e42d665d91","text":"S3 - OneZone-IA","correct":false},{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false}]},{"id":"49107f33-5b31-4d7e-a2cb-95f3ce8a2d75","domain":"awscsapro-domain1","question":"Your customer has setup AWS Organizations to help manage a collection of AWS Accounts.  They are running into a problem though and need your help.  They have created accounts for each business unit and applied SCPs to those OUs. However, they notice that root accounts in in those sub-accounts can still change root access keys and disable MFA.  How do you instruct your customer?","explanation":"Service Control Policies can control many aspects but they cannot restrict root account actions of changing root access keys or disabling MFA.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"94a2f948d3d2f9c317a6ebb1f5a24ea5","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the specific sub-accounts.","correct":false},{"id":"194cc2d07b5c378b62b1e090f0aea956","text":"You can add an explicit Deny for \"arn:aws:iam:<account>:user/root\" in the SCP for the entire OU in the root account.","correct":false},{"id":"77df34553819fdc2e31fb79762948993","text":"You can establish a trust with the top-level account and use the \"organizations:ServicePrinciple\" condition key to restrict root access at the sub-account level.","correct":false},{"id":"3d722952f024bcec9174a311c17dcc14","text":"You can not use SCPs to restrict root account activities of changing the root password or managing MFA settings.","correct":true}]},{"id":"12556935-2b08-4f73-b7f6-6b82bd7fa1a0","domain":"awscsapro-domain1","question":"Your company has recently acquired another business unit and is in the process of integrating it into the corporate structure.  Like your company, the acquisition's IT assets are fully hosted on AWS.  They have a mix of EC2 instances, RDS instances and Lambda-based applications but these will become end-of-life as the new business unit transitions to your company's standard applications over the next year.  Fortunately, the CIDR blocks of the respective VPCs do not overlap.  If the goal is to integrate the new network into your current hub-and-spoke network architecture to provide full access to each other's resource, what can you do that will require the least amount of disruption and management?","explanation":"VPC Peering provides a way to connect VPCs so they can communicate with each other.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/peering/invalid-peering-configurations.html","title":"Unsupported VPC Peering Configurations - Amazon Virtual Private Cloud"}],"answers":[{"id":"8e438cf633d07a0d70f2d21978e083bb","text":"Configure a VPC Gateway Endpoint for EC2, RDS and Lambda services.  Configure route tables in your existing VPCs to use the endpoints to communicate with the new VPCs.","correct":false},{"id":"b4cd9cd07c83868a626224de0d92c70a","text":"Initiate a Transitive Peering request from each new VPC to your hub VPC.  Configure routes in the hub to direct traffic to and from the new VPCs.","correct":false},{"id":"6088d94d774b3a3e9be7fc71331181d1","text":"setup a VPN connection via Internet Gateway between each new and existing VPC to create a mesh network.  Update route tables to direct traffic to the appropriate VPC.","correct":false},{"id":"d8d7beaa505f84c757a93d64f70a0ae7","text":"Initiate a VPC peering request from each of your spoke VPCs to each new VPC.  Configure route tables in each spoke VPC and new VPC to route traffic to the respective VPC.","correct":false},{"id":"0d3332dc947d3085b33697562a70c9f5","text":"Initiate a VPC peering request between your hub VPC and all VPCs from the new business.  Setup routes in the hub to direct traffic to and from the new VPCs.","correct":true}]},{"id":"34351bd0-7925-4246-bb61-c64bbf4d5baf","domain":"awscsapro-domain4","question":"An application in your company that requires extremely high disk IO is running on m3.2xlarge EC2 instances with Provisioned IOPS SSD EBS Volumes. The EC2 instances have been EBS-optimized to provide up to 8000 IOPS. During a period of heavy usage, the EBS volume on an instance failed, and the volume was completely non-functional. The AWS Operations Team restored the volume from the latest snapshot as quickly as possible, re-attached it to the affected instance and put the instance back into production. However, the performance of the restored volume was found to be extremely poor right after it went live, during which period the latency of I/O operations was significantly high. Thousands of incoming requests timed out during this phase of poor performance.\nYou are the AWS Architect. The CTO wants to know why this happened and how the poor performance from a freshly restored EBS Volume can be prevented in the future. Which answer best reflects the reason and mitigation strategy?","explanation":"Data gap cannot be the reason for high disk I/O latency. Whether the data being requested is on the disk or not cannot be responsible for the extended period of high disk I/O latency, as all operating systems index the contents in some way. They do not scan the whole disk to conclude that something is missing. Hence, the choice that suggests data gap as the reason is eliminated.\nEBS Optimization works straight away after a freshly restored volume is attached to an EBS optimized instance. Hence, the choice that suggests that EBS Optimization takes some time to kick in is eliminated.\nThere is nothing called set-up-cache command. The option that suggests that there is an inbuilt caching mechanism that needs to be activated is completely fictional, and is eliminated.\nThe only correct option is the one that correctly states that every new block read from a freshly restored EBS Volume must first be downloaded from S3. This is because EBS Snapshots are saved in S3. Remember that EBS Snapshots are incremental in nature. Every time a new snapshot is taken, only the data that changed is written to that particular snapshot. Internally, it maintains the pointers to older data that was written to S3 as part of previous snapshots. These blocks of data continue to reside on S3 even after an EBS Volume is restored, and is read the first time they are accessed. Linux utilities like dd or fio can be used after restoring an EBS Volume to read the whole volume first to get rid of this latency problem when the instance is put back in production.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-initialize.html","title":"Initializing Amazon EBS Volumes"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html","title":"Amazon EBS–Optimized Instances"}],"answers":[{"id":"ae32d192c3829287819a74ded72e0da7","text":"The latest snapshot did not have the most current data. It only had the data from the last time a snapshot was taken. The requests timed out because of this data gap. To mitigate this, increase the frequency of taking EBS snapshots.","correct":false},{"id":"51e87b2dbdfcb476b026779380119b06","text":"A freshly restored EBS Volume cannot utilize EBS Optimization Instances straight away, as the network traffic and EBS traffic traverse the same 10-gigabit network interface. Only after the entire volume is scanned by an asynchronous process, EBS Optimization kicks in. This increases the I/O latency until the volume is ready to utilize EBS Optimization. To fix this, update the restoration process to wait and run random I/O tests on a freshly restored EBS Volume. Put the instance back to production only after the desired I/O levels are reached.","correct":false},{"id":"1a47200401a925a2ad1df3d3286e26dc","text":"When a data block is accessed for the first time on a freshly restored EBS Volume, EBS has to download the block from S3 first. This increases the I/O latency until all blocks are accessed at least once. To fix this, update the restoration process to run tools to read the entire volume before putting the instance back to production.","correct":true},{"id":"d3a1c3b2669127dabe2eaf2c490fcd30","text":"A freshly restored EBS Volume needs pre-warming to activate the inbuilt caching mechanism. To fix this, update the restoration process to run the set-up-cache command on the freshly restored EBS Volume first before the instance is put back in production. Also, include random I/O tests to ensure that desired I/O levels are reached before putting the instance back to production.","correct":false}]},{"id":"f679d23d-14d4-4021-9749-481bbe11046c","domain":"awscsapro-domain3","question":"A telecommunications company has decided to migrate their entire application portfolio to AWS. They host their customer database and billing application on IBM mainframes. IBM AIX servers running WebSphere provide an API layer into the mainframes. Customer-facing online applications are hosted on Linux systems. The customer service backend application resides on Oracle Solaris and makes use of gigabytes of persistent information. Their ERP and CRM systems also run on Solaris boxes. Telecom switches send call records to Linux-based applications, and their employee productivity suite runs on Windows. They need to complete the project in twelve months to satisfy budgetary constraints. Which migration strategy will provide them with the most resilient, scalable, and operationally efficient cloud environment within the project time frame?","explanation":"Since the company has twelve months to complete the project, they can plan for a highly cloud-centric migration. Refactoring the mainframe billing application to EC2 and the customer database to Aurora will require significant cost and effort, but will result in significant intermediate to long-term business value for most companies. A number of AWS Partner Network (APN) solutions are available to assist with this. The WebSphere layer can be replaced by API Gateway with HTTP, REST, or WebSocket APIs that call modules on the EC2 instances. Refactoring the customer-facing online apps to Lambda serverless and Step Functions will provide high operational efficiency. Performing a replatform of the Solaris customer service application to EC2 with Auto Scaling will achieve elasticity to avoid the excess capacity inefficiencies that were most-likely present in the on-premises environment. Many robust ERP, CRM, and employee productivity SaaS solutions exist and should be leveraged rather than trying to manage these applications with in-house staff. The call record processing Linux system can simply be rehosted to EC2. Repurchasing mainframe capacity from a third party provider only extends the rigidness of making mainframe changes whenever new business requirements arise. A Lambda/Step Functions solution will provide all the functionality needed for the online apps, and will be more economical than Elastic Beanstalk. Refactoring the customer service application to Lambda presents issues with processing the gigabytes of persistent information, so replatforming to EC2 is a better choice.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud"},{"url":"https://aws.amazon.com/blogs/apn/automated-refactoring-of-a-new-york-times-mainframe-to-aws-with-modern-systems/","title":"Automated Refactoring of a New York Times Mainframe to AWS with Modern Systems"},{"url":"https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/","title":"Build a Serverless Web Application"}],"answers":[{"id":"57d6d888b4cee16e22852910fa09cdb8","text":"Refactor the mainframe applications onto Amazon EC2 Linux instances, and migrate the customer database to Amazon Aurora. Replace the API layer with Amazon API Gateway. Rehost the customer-facing online applications to Amazon Elastic Beanstalk. Refactor the customer service application to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Repurchase SaaS solutions for the ERP and CRM systems. Rehost the call record processing applications and the employee productivity suite onto EC2.","correct":false},{"id":"c875a6eb6d74f098c971af4a638f288f","text":"Repurchase mainframe capacity from a third party provider and run the customer database and billing application there. Replatform the API layer onto EC2 Linux instances. Refactor the customer-facing online applications to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the customer service, ERP, and CRM applications to EC2 Linux instances with Auto Scaling. Rehost the call record processing applications onto EC2, and repurchase SaaS applications for the employee productivity suite.","correct":false},{"id":"5a404284de876ce2a054dcd916ed80ec","text":"Refactor the mainframe applications onto Amazon EC2 Linux instances, and migrate the customer database to Amazon Aurora. Replace the API layer with Amazon API Gateway. Refactor the customer-facing online applications to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the customer service applications to EC2 Linux with Auto Scaling. Repurchase SaaS solutions for the ERP and CRM systems. Rehost the call record processing applications onto EC2, and repurchase SaaS applications for the employee productivity suite.","correct":true},{"id":"290da60b3bdd11f28e175fa86972386a","text":"Repurchase mainframe capacity from a third party provider and run the customer database and billing application there. Replace the API layer with Amazon API Gateway. Rehost the customer-facing online applications to Amazon Elastic Beanstalk. Refactor the customer service application to serverless on AWS Lambda, and orchestrate workflows with AWS Step Functions. Replatform the ERP and CRM applications to EC2 Linux instances with Auto Scaling. Rehost the call record processing applications, and repurchase SaaS applications for the employee productivity suite.","correct":false}]},{"id":"cb24982e-2c2d-43d4-872f-2dabbdb7e367","domain":"awscsapro-domain2","question":"You are helping an IT Operations group transition into AWS.  They will be created several instances based off the latest Amazon Linux 2 AMI.  They are unsure of the best way to enable secure connections for all members of the group and certainly do not want to share credentials. Which of the following methods would you recommend?","explanation":"Of the provided options, the only one that upholds AWS best practices for providing secure access to EC2 instances is to use AWS Session Manager.  ","links":[{"url":"https://aws.amazon.com/blogs/aws/new-session-manager/","title":"New – AWS Systems Manager Session Manager for Shell Access to EC2 Instances  | AWS News Blog"}],"answers":[{"id":"93ecb84b9fc90ec0bc4db84968ab5ebf","text":"Allow administrators to update the SSH key of the instance in the AWS console each time they need access to a system.","correct":false},{"id":"e36f501af80c8f03e08321d565cc900e","text":"Share the single private SSH key with each administrator in the group.","correct":false},{"id":"c594a26a8a9141cdd5615a0761fb2438","text":"Create a bastion host and use it like a jump-box.  Paste each administrators private key into the known_hosts file on the bastion host.","correct":false},{"id":"cce617457cd8701c595d236b7fa4ed7c","text":"Allow each administrator to create their own SSH keypair and assign them all to the SSH Key for the instance upon each launch.","correct":false},{"id":"b1ab01927f47a067694506df9d5249e3","text":"Configure IAM role access for AWS Systems Manager Session Manager.","correct":true}]},{"id":"9382645e-ce3e-4063-b95e-1a72d0b644d1","domain":"awscsapro-domain2","question":"You have built an amazing new machine learning algorithm that you believe would be of benefit to many paying business customers.  You want to expose it as a REST API to your customers and offer three different consumption levels: Silver, Gold and Platinum.  The backend is completely serverless using Lambda functions. What is the most efficient and least cost way to make your API available for paying customers with a per-request pricing model?","explanation":"Since 2016, AWS has allowed developers to monetize their APIs in AWS Marketplace using API Gateway.  The developer must first create a Developer Portal to provide a method for customers to register for access and then associate the assigned Product Code, received when the developer registers the API in the Marketplace, to the desired usage plan within API Gateway.  AWS then handles accounting for the usage and billing.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/sell-api-as-saas-on-aws-marketplace.html","title":"Sell Your API Gateway APIs through AWS Marketplace - Amazon API Gateway"},{"url":"https://aws.amazon.com/blogs/compute/monetize-your-apis-in-aws-marketplace-using-api-gateway/","title":"Monetize your APIs in AWS Marketplace using API Gateway | AWS Compute Blog"}],"answers":[{"id":"ea9b822cf5c0fc7fc226c7130821d810","text":"Deploy your API using API Gateway using the \"managed-service\" mode.  Use AWS Batch to export usage logs to S3.  Use AWS Glue to aggregate and transform the raw logs into daily usage and save in DynamoDB.  Build a Payment Gateway using the AWS SDK to read the DynamoDB billing table and prepare invoices for customers.  Use SES to email invoices to customers.","correct":false},{"id":"54bb1f265f3f353d8a065327131b21af","text":"Deploy your API to an S3 bucket using the Static Hosting feature.  Enable \"requester pays\" for the bucket to handle billing.  Create a serverless customer portal that will allow customers to register for API access and dynamically create an IAM role for them using Lambda.","correct":false},{"id":"93741211ae6c981d6c73be4eac40c975","text":"Port your Lambda functions over to a Docker container and deploy using EKS.  Setup metered usage for each customer you expect to subscribe and deploy unique API keys to those customers.  Use CloudTrail to generate usage data for the API containers and import into RedShift for aggregation and processing.  Use the Amazon Pay API to issue invoices to customers based on monthly queries of the RedShift data.","correct":false},{"id":"bca3af4da0c0286bf50f5dde56d2a017","text":"Setup your own API Gateway Serverless Developer Portal to create API keys for subscribers.  Register as a seller with AWS Marketplace and specify the usage plans and developer portal.  Submit a product load form with a dimension named \"apigateway\" of the \"requests\" type.  Create a metering IAM role to allow metrics to be sent to AWS Marketplace.  Associate your provided Product Code with the corresponding usage plan.","correct":true},{"id":"9ade23b86bae0ad5bcbc882074feedb2","text":"Use API Gateway to configure a usage plan for the production stage of the API.  Register as a seller with AWS Marketplace and define three different levels of service and pricing. Assign the respective product code to the proper usage plan in the API Gateway console.","correct":false}]},{"id":"c01346c8-d230-4b52-b53d-78cdbfbc7794","domain":"awscsapro-domain4","question":"You work for an Insurance Company as an IT Architect. The development team for a microservices-based claim-processing system has created containerized applications to run on ECS. They have spun up a Fargate ECS cluster in their development VPC inside a private subnet. The containerized application uses awslogs driver to send logs to Cloudwatch. The ECS task definition files use private ECR images that are pulled down to ensure that the latest image is running always. The cluster is having connectivity problems as it cannot seem to connect with ECR to pull the latest images and neither can it connect with Cloudwatch to log. The development team has approached you to help troubleshoot the issue. What is a possible reason for this and what is the best way to fix it?","explanation":"ECS Fargate clusters can be deployed in a private subnet. Hence, we can safely eliminate the choice that says that ECS Fargate clusters must be deployed in a public subnet only.\nECS Fargate clusters do not need the user to control the ECS Agent Version on the nodes, as Fargate is serverless by design. Hence, we can safely eliminate the choice that deals with ECS Agent Version.\nThis leaves two options. One proposes using NAT Gateway. The other proposes using ECS Interface VPC Endpoint. Both are working solutions. However, one of them makes a false claim - it states that ECS Fargate clusters connect to ECR or Cloudwatch only over the internet. That is not true, as it can connect either using a public or a private network. Hence, the only fully correct choice is the one that uses ECS Interface VPC Endpoint","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/vpc-endpoints.html","title":"Amazon ECS Interface VPC Endpoints (AWS PrivateLink)"},{"url":"https://aws.amazon.com/blogs/compute/setting-up-aws-privatelink-for-amazon-ecs-and-amazon-ecr/","title":"Setting up AWS PrivateLink for Amazon ECR"}],"answers":[{"id":"43901acf8308ae39f9cf7006afa72751","text":"ECS Fargate clusters can connect to ECR (to pull down latest private images) or Cloudwatch (to log) using either private or public network. Hence, if it is deployed in a private subnet, deploy an ECS Interface VPC Endpoint in the same subnet for connecting to internal services. Add appropriate routes to the Routing Table","correct":true},{"id":"f54cb63446f49ab92455024f17ae67b8","text":"ECS Fargate clusters must be deployed in a public subnet so that it can use the Internet Gateway to communicate with ECR or Cloudwatch. To fix this, redeploy the cluster in a public subnet","correct":false},{"id":"d73a8a48aaef0ddbe725f06264c3c6a3","text":"ECS Fargate clusters can connect to ECR (to pull down latest private images) or Cloudwatch (to log) only over the internet. Hence, if it is deployed in a private subnet, it needs a route to a NAT Gateway which must be connected to an Internet Gateway. To fix this issue, deploy a NAT Gateway in a public subnet of the VPC and add appropriate routes to the Routing Table","correct":false},{"id":"d103c9eb11e007e2a9410cfb2e1bc1ba","text":"The version of the ECS agent may be too old. To fix this, upgrade the ECS Agent version in the cluster nodes to be compatible with connectivity requirements","correct":false}]},{"id":"694c2138-4b6e-466d-b5b2-7a34cc0a3360","domain":"awscsapro-domain1","question":"AWS Cost Management encompasses a number of services to help you to organize, control and optimize your AWS costs and usage.  Which of the following Cost Management related tools gives you the ability to set alerts when costs or usage are exceeded?","explanation":"The correct answer is AWS Budgets.  AWS Cost Explorer lets you visualize, understand, and manage your AWS costs and usage over time. AWS Cost & Usage Report lists AWS usage for each service category used by an account and its IAM users and finally, Reserved Instance Reporting provides a number of RI-specific cost management solutions to help you better understand and manage RI Utilization and Coverage.","links":[{"url":"https://aws.amazon.com/aws-cost-management/aws-budgets/","title":"AWS Budgets"}],"answers":[{"id":"c7f176d72688fd87853e31b84159d541","text":"AWS Cost Explorer","correct":false},{"id":"824fd559c917b4ae56f36787b886eb81","text":"AWS Cost & Usage Report","correct":false},{"id":"eef79d956328d5e4ec426d448cc53c74","text":"Reserved Instance Reporting","correct":false},{"id":"e32a801c8e0beab6abb9361e937365be","text":"AWS Budgets","correct":true}]},{"id":"a4d41d3b-abbe-4121-8e1d-5567b1ec7294","domain":"awscsapro-domain5","question":"You are an AWS administrator and you need to maintain multiple Amazon Linux EC2 instances. You can SSH to the instances with a .pem key file created by another colleague. However, as the colleague will leave the company shortly, the SSH key pair needs to be changed to a new one created by you. After the change, users should be able to access the instances only with the new .pem key file. The old key should not work. How would you get the key pair replaced properly?","explanation":"You do not need to launch new instances as you can simply paste the public key content in the \".ssh/authorized_keys\" file to enable the new key pair. You cannot directly change the key through AWS Management Console by clicking the \"change SSH key\" button. You are also not allowed to change the SSH key when stopping and starting instances. Users can only select an SSH key pair when they launch a new instance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#replacing-key-pair","title":"Adding or Replacing a Key Pair for Your Instance"}],"answers":[{"id":"be5d2e07f838b7752b235431feaae361","text":"The SSH key pair cannot be changed after EC2 instances are launched. Take AMIs or EBS snapshots from existing instances, terminate the instances, launch new ones with the AMIs/snapshots and select another SSH key pair.","correct":false},{"id":"b216d43c3e1e71b67af45a138184b181","text":"Create a new SSH key pair, get the public key information from it, paste it in the \".ssh/authorized_keys\" file in the Amazon Linux EC2 instances and remove the old public key.","correct":true},{"id":"4fa9ce5fdb3a2215ed8798d2e734fd42","text":"In AWS Management Console, create a new key pair and download the .pem file at a safe place. Select the Amazon EC2 instances and click \"change SSH key\" to get the key replaced. The EC2 instances are automatically restarted after the change.","correct":false},{"id":"521ec8da4f5fa8bee591740d44a8427e","text":"Create a new key pair locally or through AWS EC2 service. Take AMIs of the instances as backups. Stop the instances and choose the new key pair when restarting the instances. Notify the team to start using the private key for SSH connections.","correct":false}]},{"id":"375e7161-43df-4d2f-adab-75cc6166a453","domain":"awscsapro-domain5","question":"You build a CloudFormation stack for a new project. The CloudFormation template includes an AWS::EC2::Volume resource that specifies an Amazon Elastic Block Store (Amazon EBS) volume. The EBS volume is mounted in an EC2 instance and contains some important customer data and logs. However, when the CloudFormation stack is deleted, the EBS volume is deleted as well and the data is lost. You want to create a snapshot of the volume when the resource is deleted by CloudFormation. What is the easiest method for you to take?","explanation":"The easiest method is using the DeletePolicy attribute in the CloudFormation template. The \"Snapshot\" value ensures that a snapshot is created before the CloudFormation stack is deleted. The \"Retain\" value is incorrect as it keeps the volume rather than creates a snapshot. The EBS lifecycle manager can create daily snapshot however it is not required in the question. When the CloudWatch Event rule is triggered, the EBS volume may already be deleted and no snapshot can be taken. Besides, the CloudFormation deletion cannot be suspended by a Lambda function.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html","title":"DeletionPolicy Attribute"}],"answers":[{"id":"692ea271f5ada1d555a5889be933267e","text":"Modify the CloudFormation template to create an EBS snapshot strategy in EBS lifecycle manager which creates a daily snapshot as backup and also another snapshot when the EBS volume’s CloudFormation stack is being deleted.","correct":false},{"id":"0719057e0f4aca49eb47cd94c333e599","text":"Add a DeletePolicy attribute in the CloudFormation template and specify \"Snapshot\" to have AWS CloudFormation create a snapshot for the EBS volume before deleting the resource.","correct":true},{"id":"061af6db30e4b65a1d544d4e64546085","text":"Modify the CloudFormation template by adding a DeletePolicy variable for the AWS::EC2::Volume resource. Specify the value of \"Retain\" to automatically create a snapshot of the EBS volume before the stack is deleted.","correct":false},{"id":"555e7ef93e63402dd6a074edd5b8a16d","text":"Create a CloudWatch Event rule that checks the CloudFormation delete-stack event. Trigger a Lambda function that pauses the CloudFormation stack deletion, creates the EBS snapshot of the volume and resumes the stack deletion after the snapshot is created successfully.","correct":false}]},{"id":"07f91ae7-094b-48a9-8924-a4d142cbbcb6","domain":"awscsapro-domain5","question":"On your last Security Penetration Test Audit, the auditors noticed that you were not effectively protecting against SQL injection attacks.  Even though you don't have any resources that are vulnerable to that type of attack, your Chief Information Security Officer insists you do something.  Your organization consists of approximately 30 AWS accounts.  Which steps will allow you to most efficiently protect against SQL injection attacks?","explanation":"Firewall Manager is a very effective way of managing WAF rules across many WAF instances and accounts.  It does require that the accounts be linked as an AWS Organization.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"9a5fa8e9a1a9be9149138c6307abde19","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service.  Use Firewall Manager to create an ACL rule to deny requests that contain SQL code.  Apply the ACL to WAF instances across all organizational accounts.","correct":true},{"id":"e072f443d93c569cce94eb4946a912af","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service and use Consolidated Billing. Subscribe to AWS Shield Advanced to automatically enable SQL injection protection across all sub-accounts.","correct":false},{"id":"2238e9ae7489214f767fa479d013cd23","text":"Create a custom NACL filter using Lambda@Edge to check requests for SQL code.  Use OpsWorks to apply the NACL across all public subnets across the organization. ","correct":false},{"id":"3927b32bfb85cc040d2b7dcb21015fc5","text":"Use AWS WAF to create an ACL that denies requests that include SQL code.  Assign the ACL to Firewall Manager instances in each account using AWS OpsWorks.","correct":false},{"id":"ee9c067e3dea2fa4cd2e3968abaf86de","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations.  Use CloudFormation to implement request restrictions for SQL code on the CloudFront distributions across all accounts.  Setup a CloudWatch event to notify administrators if requests with SQL code are seen.","correct":false}]},{"id":"6c770228-3fa1-4020-94a8-119c67a7e2d1","domain":"awscsapro-domain2","question":"You are volunteering with a local STEM (Science, Technology, Engineering and Math) program for youth.  You have decided that you'd like to help them learn about AWS by spinning up their very own WordPress site.  Given that the youth have no experience with AWS and the program, you want to choose the easiest way for students to spin up a simple webserver.  Which AWS technologies would you choose?","explanation":"AWS Lightsail is designed to be a very easy entry-level experience for those just starting out with virtual private servers.  A WordPress site can be deployed with literally a single click and does not require AWS Console access or knowledge of EC2 or VPCs.","links":[{"url":"https://aws.amazon.com/lightsail/","title":"Lightsail"}],"answers":[{"id":"d85c80578ad0849a611c1056b63e385c","text":"VPC","correct":false},{"id":"b21eea42e76007ac061cf37a5a41037d","text":"Lightsail","correct":true},{"id":"c8f63ecaff5e983a2441126a241c4cfa","text":"ECS","correct":false},{"id":"81b22456f78954c460ce2f531b5e048f","text":"EC2","correct":false},{"id":"cab27dc53edb571cac663ce2e16450dc","text":"AWS Marketplace","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":false}]},{"id":"5d35c6d3-3eaf-49d0-b64e-611d74d40af0","domain":"awscsapro-domain2","question":"You need to design a new CloudFormation template for several security groups. The security groups are required in different environments such as QA, Dev and Production. The allowed CIDR range in the security group ingress rule depends on environments. For example, the allowed inbound address range is 10.0.0.0/16 for non-production and 10.1.0.0/16 for production. You prefer to maintain a single template for all the environments. What is the best method for you to choose?","explanation":"CloudFormation has an optional Conditions section that contains statements to determine whether or not entities should be created or configured. Then you can use the \"Fn::If\" function to check the condition and return different values. You do not need to use Jenkins to pre-process the template and there are no \"Fn::Case\" and \"Fn::Switch\" intrinsic functions.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html","title":"CloudFormation conditions"}],"answers":[{"id":"c4a393b1cce09df81069eb93f2d38054","text":"Use the environment type as an input parameter and create a condition based on the parameter. In the AWS::EC2::SecurityGroupIngress resource, use Fn::If to check the condition and return related source CIDR range.","correct":true},{"id":"d2661ed9bcd0f8b132885660604a665a","text":"Create a Jenkins pipeline with an environment variable. Depending on the variable value, modify the template script to use the correct CIDR IP range. Deploy the CloudFormation stack with the updated template.","correct":false},{"id":"7afa8c2f0fa2ef8966917c74044516dd","text":"CloudFormation does not provide functions for conditions. Use Terraform instead. Create a variable file to manage different CIDR IP ranges. Pass in the environment variable value and use \"Terraform apply\" to deploy all the security group resources at one time.","correct":false},{"id":"d273c3216496e7d41cb97094f83d3e33","text":"Create an environment variable and pass it to the CloudFormation template. In the security group resources, use Fn::Case and Fn::Switch to check the variable value and return different CIDR IP range.","correct":false}]},{"id":"91006a08-8658-479c-9f96-4f9cd9c770c6","domain":"awscsapro-domain2","question":"Your customer is a commercial real-estate company who owns parking lots in all major cities in the world. They are building a website on AWS that will be globally accessed by international travellers. The website will provide near-real-time information on available parking lot spaces for all cities. You need a back-end comprising a multi-region multi-master data storage solution, with writes happening in all regions. Writes must be replicated between regions in near real-time so that different regional website compute instances can query the database instance in its own region and retrieve the data pertaining to all other regions. In addition, all users should be able to access the website using the same domain name, but they need to be routed to the Elastic Load Balancer that responds most quickly to their request (which should be the one closest to them most of the time). The website must also serve locally cached static content and have protection against malicious attacks. Which AWS-based architecture should you choose?","explanation":"The only AWS managed database service that offers multi-region multi-master is DynamoDB Global Tables. AWS Aurora does offer multi-master clusters, but all nodes of an Aurora multi-master must be in the same region. Aurora also offers cross-region replication, but the nodes, in that case, are Read Replicas, they are not Masters (in the database world, a master is an instance that accepts write requests). Similarly, AWS RDS does not currently have a multi-master multi-region solution to leverage. Thus, the only choice that can satisfy the multi-region multi-master nature of the requirement is DynamoDB Global Tables with DynamoDB Streams enabled for Replication. Additionally, the question tests knowledge in other areas. However, just focusing on the above fact about the database tier is enough to eliminate all incorrect answers.\nRoute 53 latency-based multi-region routing is the correct Routing solution here. Multi-value answering records results in randomly picking one, which is not the requirement here (in this scenario, the region must be picked based on latency, not randomly). Similarly, geolocation-based routing honours country boundaries, not latency. If we use geolocation-based routing, an end-user in country A will always go to the AWS Region in country A, but that may not be the one with least latency, especially if the user is close to an international boundary, and the AWS data centre happens to be on the other end of his country.\nLambda@Edge can be used with Cloudfront to re-write origins for requests. This is an important usage of Lambda@Edge. Similarly, WAF can be activated with a Cloudfront web distribution for protection against malicious traffic.","links":[{"url":"https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/","title":"How to use Amazon DynamoDB global tables to power multiregion architectures"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html","title":"Search for - all DB instances in a multi-master cluster must be in the same AWS Region"},{"url":"https://aws.amazon.com/blogs/aws/latency-based-multi-region-routing-now-available-for-aws/","title":"Multi-Region Latency Based Routing now Available for AWS"}],"answers":[{"id":"32667dba126e63f9c8c6adb288419974","text":"For the database, use DynamoDB Global Tables, with instances in each AWS Region configured as master, and replication enabled using DynamoDB Streams. For routing, use Route 53 multi-region latency based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":true},{"id":"25c4e1f274a66d3e2b5d67c75702ef4b","text":"In AWS, all cross-region replication solutions treat secondary instances as read-only, so multiple masters are not possible in a managed multi-region database solution. Use Cassandra on EC2 instances in different regions that are peered to each other and configure Cassandra in multi-master mode. For routing, use Route 53 multi-region latency based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":false},{"id":"c74e4146c5eb59b1a892f52ac51bfbd7","text":"For the database, use Amazon Aurora Cross-Region Replication, with instances in each AWS Region configured as master. For routing, use Route 53 geolocation-based routing. Use Cloudfront with Lambda@Edge to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF on Cloudfront web distribution.","correct":false},{"id":"9faf11728364108db71381c81e091523","text":"For the database, use Amazon RDS with Cross Region Replication, with instances in each AWS Region configured as master. For routing, use Route 53 multi-region multi-value answering based routing. Use Cloudfront to serve static content from a cross-region-replicated set of S3 buckets, and dynamic content from the closest origin. Activate WAF with Cloudfront geo restriction feature.","correct":false}]},{"id":"8aa313fe-cd0f-4899-a2f4-e8f2fd64c245","domain":"awscsapro-domain4","question":"Your business depends on AWS S3 to host three kinds of files - images, documents and compressed installation packages. These files are accessed and downloaded by end-users from all US regions and west EU, though the compressed installation packages are downloaded rarely as users tend to access the service from their browsers instead of installing anything on their machines. Each installation package bundles several images and documents, and also includes binaries that are downloaded from a 3rd party service while creating the package files.\nThe images and documents range from a few KBs to a few hundred KBs in size and they are mostly static in nature. However, the compressed installation package files are generated every few hours because of changes done by the 3rd party service to their binaries, and some of them are as large as a few hundred GB-s. The installation package files can be regenerated from the images and documents fairly quickly if required. It is important to be able to retrieve older versions of the images and documents.\nWhich of the following storage solutions is the most cost-effective approach to design the storage for these files?","explanation":"The areas tested by this question are:\\n1. Versioning cannot be enabled at the object level. It is a bucket-level feature. This rules out the choice where we have a single bucket and selectively turn on versioning on for some objects only.\\n2. If you enable Versioning for a bucket containing large objects that are frequently created/uploaded, it will result in higher storage cost as all the previous versions will result in storage volume growing quickly because of frequent writes. In the given scenario, the compressed installation package files are large and also frequently generated (every few hours). There is no requirement to version them, as they can be quickly generated on-demand. Hence, putting them in a bucket that has Versioning enabled is not a good cost-effective solution. This rules out two choices - one where we have a single versioned bucket, the other where we enable versioning for both buckets.\\n3. Note that all options except one correctly identify the storage class requirements - the compressed installation package files should be stored as One-Zone IA because durability is not a prime requirement for these files (simply because they can be regenerated on-demand easily). They are rarely downloaded, hence IA is the correct class. Combined with low durability, One Zone IA is the most cost-effective solution. Only one option uses the incorrect storage tier for these files - note that IA is more expensive than One-Zone IA, and the question is about cost-effectiveness.\nHence, the only correct answer is the one that addresses both Versioning and Storage Class requirements correctly.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html","title":"Documentation on Object Versioning"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-howtoset","title":"Setting the Storage Class of an Object"}],"answers":[{"id":"9e88398b12bdf0dd2fdc12a19f4962a1","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for Bucket A only. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":true},{"id":"fd1d330daf4d05b4fa4c888a0584130f","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the bucket. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to One-Zone Infrequent Access while uploading compressed installation package files","correct":false},{"id":"2e4e12f367286b12f14ae74b1fd4e350","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the image and document objects only, but not for the compressed installation package files. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to Infrequent Access while uploading compressed installation package files","correct":false},{"id":"90f6b37d063a0158682b034d578bf29b","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for both the buckets. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":false}]},{"id":"9ff9e447-acaa-4dae-9bbf-08db786af693","domain":"awscsapro-domain2","question":"You are a AWS Solutions Architect, and your team is working on a new Java based project. After the application is deployed in an EC2 instance in the development AWS account, an encrypted EBS snapshot is created. You need to deploy the application in the production AWS account using the EBS snapshot. For security purposes, a different customer managed key (CMK) in key management service (KMS) is required to encrypt the EBS volume in the new EC2 instance. What is the best method to achieve this requirement?","explanation":"In order to share an encrypted EBS snapshot between AWS accounts, you must also share the customer managed key (CMK) used to encrypt the snapshot. You can then copy the snapshot to a new one encrypted with another encryption key. You cannot directly copy an EBS volume between accounts. An EBS snapshot encrypted by the default AWS key is not allowed to be shared with another account.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html","title":"Sharing an Amazon EBS Snapshot"}],"answers":[{"id":"5751e42f86f4695e7309214d370c4d2b","text":"Share the snapshot and the encryption key to the production AWS account. Copy the snapshot to a new snapshot in the production account and encrypt it using another CMK in KMS. Create an AMI using the snapshot and launch a new EC2 instance.","correct":true},{"id":"941319ca17d0d8915ed822083e1dc2d4","text":"Re-encrypt the EBS snapshot with the default EBS encryption key in the development account. Share both the snapshot and the default EBS key to the production account. In the production account, create a volume from the snapshot and attach the volume to a new EC2 instance.","correct":false},{"id":"16c8a2b39dcb6723939653ed8deb9f90","text":"Share the CMK with the production account. In the development account, create a volume from the EBS snapshot. Copy the volume to the production account and encrypt it with another customer managed key. Launch a new EC2 instance and attach the new EBS volume to it.","correct":false},{"id":"536a3b4a4f859146660d3b6dc2036650","text":"Create an AMI from the EBS snapshot and copy the AMI from the development account to the production account. Launch an Auto Scaling group with the AMI and encrypt it with a new customer managed key.","correct":false}]},{"id":"4c49e888-8f76-4b15-b267-7f6ec35579ca","domain":"awscsapro-domain5","question":"A client has asked you to review their system architecture in advance of a compliance audit.  Their production environment is setup in a single AWS account that can only be accessed through a monitored and audited bastion host. Their EC2 Linux instances currently use AWS-encrypted EBS volumes and the web server instances sit in a private subnet behind an ALB that terminates TLS using a certificate from ACM. All their web servers share a single Security Group, and their application and data layer servers similarly share one Security Group each. Their S3 objects are stored with SSE-S3.  The auditors will require all data to be encrypted at rest and will expect the system to secure against the possibility that TLS certificates might be stolen by would-be spoofers.  How would you help this client pass their audit in a cost effective way? ","explanation":"All the measures they have taken with Certificate Manager, S3 encryption and the EBS volumes meet the audit requirements.  There is no need for LUKS, CloudHSM or client-side encryption.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html","title":"Amazon EBS Encryption - Amazon Elastic Compute Cloud"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"Protecting Data Using Server-Side Encryption with Amazon S3-Managed  Encryption Keys (SSE-S3) - Amazon Simple Storage Service"}],"answers":[{"id":"92194f6603feb3f83a46b00bda37de5a","text":"Deploy CloudHSM and migrate the TLS keys to that service.","correct":false},{"id":"0c31fb48e65443ba5bfa312a7dcc117c","text":"Encrypt the S3 objects with OpenPGP locally before re-uploading them to S3.","correct":false},{"id":"ab9ad7bfd57b97954a6f861d872c6137","text":"Continue to use the ACM for the TLS certificate.","correct":true},{"id":"93113c2b6ca9be67acbd3561eef56481","text":"Reconfigure the EC2 EBS volumes to use LUKS OS-Level encryption.","correct":false},{"id":"d9447af4853ab8736e49349138cac8fb","text":"Make no changes to the EBS volumes.","correct":true},{"id":"bdddd09d0832e16504afd5c88136cf7e","text":"Leave the S3 objects alone.","correct":true}]},{"id":"872cd65b-287b-4fdb-b59f-f07f7bff707f","domain":"awscsapro-domain5","question":"Your client is a small engineering firm which has decided to migrate their engineering CAD files to the cloud.  They currently have an on-prem SAN with 30TB of CAD files and growing at about 1TB a month as they take on new projects.  Their engineering workstations are Windows-based and mount the SAN via SMB shares.  Propose a design solution that will make the best use of AWS services, be easy to manage and reduce costs where possible. ","explanation":"At present, EFS doesn't support Windows-based clients.  Storage Gateway-File Gateway does support SMB mount points.  The other options introduce additional unneeded costs.","links":[{"url":"https://aws.amazon.com/storagegateway/faqs/","title":"AWS Storage Gateway FAQs - Amazon Web Services"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/limits.html","title":"Amazon EFS Limits - Amazon Elastic File System"}],"answers":[{"id":"7cad520a624f4c3b174014f339f732df","text":"Use AWS CLI to sync the CAD files to S3.  Use EC2 and EBS to create an SMB file server.  Configure the CAD workstations to mount the EC2 instances.  Setup Direct Connect to ensure performance is acceptable.","correct":false},{"id":"f52e177dc6a594ed2c0852c91b6133d3","text":"Order a Snowmobile to migrate the bulk of the data.  Setup S3 buckets on AWS to store the data.  Use AWS WorkDocs to mount the S3 buckets from the engineering workstations.","correct":false},{"id":"340da8cc24330ac5b143b526c880e4f7","text":"Order a Snowball appliance to migrate the bulk of the data.  Setup an EFS share on AWS and configure the CAD workstations to mount via SMB.  ","correct":false},{"id":"9c4821e0d9178e80636a5d4c7d0c6441","text":"Setup Storage Gateway-File Gateway and configure the CAD workstations to mount as iSCSI.  Use a Snowball appliance to sync data daily to S3 buckets at AWS.","correct":false},{"id":"c15496feac5aa7ce58d0c5d4813a5a29","text":"Use AWS CLI to sync the CAD files to S3.  Setup Storage Gateway-File Gateway locally and configure the CAD workstations to mount as SMB.","correct":true}]},{"id":"338ce579-a236-4282-9670-8da3b0baf2e9","domain":"awscsapro-domain3","question":"You are helping a Retail client migrate some of their assets over to AWS.  Presently, they are in the process of moving their Enterprise Data Warehouse.  They are planning to re-host their very large Oracle data warehouse on EC2 in a high availability configuration across AZs.  They presently have several Scala scripts that process some detailed Point of Sale data that is collected each day.  The scripts perform some aggregation on the data and import the aggregate into their Oracle database.  They want to move this process to AWS as well.  Which option would be the most cost-effective way for them to do this?","explanation":"AWS Glue is a fully managed extract, translate and loading service and is compatible with Scala.  EMR could do this but represents more overhead than necessary.  Lambda is not compatible with Scala and migrating to Redshift does not bring anything in this case if the customer wants to retain their Oracle database.","links":[{"url":"https://aws.amazon.com/glue/faqs/","title":"AWS Glue Features - Amazon Web Services"}],"answers":[{"id":"a445a1a877009cd7c31858687a818116","text":"Import your Scala scripts into AWS SCT for processing.","correct":false},{"id":"4f1ff8b853c3ba363bdd2bda53538ab4","text":"Migrate from Oracle to Redshift and use Kinesis Firehose.","correct":false},{"id":"04578ae8419780f9dc441d01fe11582d","text":"Create Lambda functions using your Scala scripts.","correct":false},{"id":"1a4de6676c8c078310e08aad71d9dce6","text":"Migrate the processing to AWS EMR.","correct":false},{"id":"8b01d948d5ad2f4b1c8e817c2d98e7c2","text":"Migrate the processing to AWS Glue.","correct":true}]},{"id":"4e6b1423-41e3-4d39-93b1-c5c47705477b","domain":"awscsapro-domain2","question":"Given an IP CIDR block of 56.23.0.0/24 assigned to a VPC and the single subnet within that VPC for that whole range, how many usable addresses will you have?","explanation":"For VPCs and subnets, you can use IP addresses that are in RFC1918 or not.  If you choose addresses not in the RFC1918 ranges, you will not be able to route traffic to the internet directly with those addresses.  You would have to use a NAT.  For a /24 netmask, you can expect 251 usable addresses because of the 5 reserved addresses.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html","title":"IP Addressing in Your VPC - Amazon Virtual Private Cloud"}],"answers":[{"id":"c52f1bd66cc19d05628bd8bf27af3ad6","text":"254","correct":false},{"id":"6aa49d907a3637314f53838d83286e5d","text":"Zero.  You must use a private IP range as defined in RFC1918 for VPCs.","correct":false},{"id":"19f3cd308f1455b3fa09a282e0d496f4","text":"251","correct":true},{"id":"3873a2be62d17c29ac441293ab21e143","text":"You cannot assign the entire CIDR range of a VPC to a single subnet.","correct":false},{"id":"f7efa4f864ae9b88d43527f4b14f750f","text":"4096","correct":false}]},{"id":"374fde7d-232a-4cfe-b5d6-7755d564c6ca","domain":"awscsapro-domain1","question":"You are consulting for a company that has decided to partially migrate some resources to AWS from their two data centers (DC1 and DC2).  Their first order of business is to design a robust, redundant and cost-effective network connection between their data centers and AWS.  They already have redundant links between DC1 and DC2.  Which of the following architectures provides the highest availability at the least cost?","explanation":"A common and cost effective way to provide a redundant link to AWS with Direct Connect is a VPN connection.  In the event that the Direct Connect path fails at DC1, your on-prem router can redirect traffic over the VPN at DC2 via the DC1-DC2 link.  Having dual Direct Connect links is definitely redundant but more expensive than a VPN.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/","title":"Multiple Data Center HA Network Connectivity – AWS Answers"}],"answers":[{"id":"77c9fb7459e1b19f6f655d63556016e8","text":"Configure a Direct Connect connection from DC1 to a Virtual Private Gateway on AWS.  Setup a VPN connection from DC2 to a Virtual Private Gateway on AWS.  Configure a dynamic route across DC1 and DC2 for both paths with a route priority favoring the Direct Connect path to AWS.","correct":true},{"id":"abd8b38f393b6a0d0b42f68dca5ec24d","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure BGP to dynamically route traffic across the nearest Direct Connect link.","correct":false},{"id":"425adb8435a7c170eef3698fa729f5ea","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure a default route in both DC1 and DC2 to route traffic to the local Direct Connect link.","correct":false},{"id":"bcac2f1d84c5f367ede3342f0adda492","text":"Ensure that DC1 and DC2 have separate ISPs.  Setup VPN connections from DC1 and DC2 to a Virtual Private Gateway on AWS.  Create static routes at each DC to use the local VPN to AWS.  Use CloudTrail to monitor traffic on the Virtual Private Gateway and trigger a script to update the static route if one of the VPN connections goes down.","correct":false}]},{"id":"1eb605a0-e0bc-4666-9fe9-aa249901bcb5","domain":"awscsapro-domain3","question":"Your company currently runs SharePoint as it's internal collaboration platform. It's hosted in the corporate data center on VMware ESXi virtual machines. To reduce costs, IT leadership has decided not to renew its VMware license agreement for the coming year. They've also decided on an AWS cloud-first approach going forward, and have ordered an AWS Direct Connect for connectivity back to the corporate network. On-premises Active Directory handles SharePoint authentication, and will continue to do so in the future. You've been tasked with determining the best way to deliver SharePoint to the company's users after the VMware agreement expires. How will you architect the solution in a cost effective and operationally efficient way?","explanation":"Deploying SharePoint Web Front Ends in separate Availability Zones behind a Network Load Balancer, with SharePoint App Servers in those same subnets, provides a highly reliable solution. RDS SQL Server supports Always On Availability Groups. Since RDS is a managed service, operational efficiency is achieved. Amazon Workspaces also provides managed service benefits for remote desktops, and gives the company the opportunity to have users use lower cost hardware. It can all be authenticated through an AWS Managed AD trust relationship with the on-premises Active Directory. The Managed AD managed service provides better operational efficiency than creating Domain Controllers on EC2. Introducing VMware Cloud on AWS for the database layer results in more networking complexity, and is not necessary since RDS supports Always On clusters. Remote Desktop Gateways will require higher cost end-user hardware.","links":[{"url":"https://d1.awsstatic.com/VMwareCloudonAWS/SharePoint-Hybrid_Reference-Architecture.pdf?did=wp_card&trk=wp_card","title":"SharePoint Reference Architecture - AWS and VMware Cloud on AWS"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html","title":"Multi-AZ Deployments for Microsoft SQL Server"},{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_microsoft_ad.html","title":"AWS Managed Microsoft AD"}],"answers":[{"id":"140b5814920d29ba818858f73c97577b","text":"Implement a Network Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers, SQL Server instances, and Active Directory Domain Controllers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Configure the SQL Server instances as Always On clusters. Join the Domain Controllers to the on-premises AD forest. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":false},{"id":"c2b436befa546d5de6bf07d1d3eb3766","text":"Configure an Application Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Run SQL Server Always On clusters on VMware Cloud on AWS. Use AWS Directory Service AD Connector for authentication from the on-premises Active Directory. Implement Remote Desktop Gateways in each subnet to provide connectivity for Windows desktops.","correct":false},{"id":"56ace4242f33e5150f0a40b3627f9568","text":"Use an Application Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers and SQL Server instances on EC2 instances in the same subnets as the SharePoint Web Front Ends. Configure the SQL Server instances as Always On clusters. Use AWS Directory Service AD Connector for authentication from the on-premises Active Directory. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":false},{"id":"1ff58ba530d63513506d1a8a58cb91b9","text":"Deploy a Network Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Run an Amazon RDS SQL Server Always On cluster. Use AWS Directory Service Managed AD for authentication in a trust relationship with the on-premises Active Directory. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":true}]},{"id":"3b286cd3-ec67-4bc4-8f37-e19e6f629198","domain":"awscsapro-domain2","question":"You are the solution architect for a research paper monetization company that makes large PDF Research papers available for download from an S3 bucket. The S3 bucket is configured as a static website. A Route53 CNAME record points the custom website domain to the website endpoint of the S3-hosted static website. As demand for downloads has increased throughout the world, the architecture board has decided to use a Cloudfront web distribution that fetches content from the website endpoint of the static website hosted on S3. The Route 53 CNAME record will be modified to point at the Cloudfront distribution URL.\nFor security, it is required that all request from client browsers use HTTPS. Additionally, the system must block anyone from accessing the S3-hosted static website directly other than the Cloudfront distribution. Which approach meets the above requirements?","explanation":"The key to answering this question correctly is to note the fact that the origin is a website and not just a plain S3 bucket - note the usage of the phrase website endpoint in the question. While setting up such an origin, one cannot just pick the S3 bucket as the origin, or use OAI. Hence, the two choices that rely on picking the S3 bucket as the origin and using OAI to restrict access are incorrect.\nIn the given scenario, the Cloudfront web distribution is being configured to use the website endpoint of the static website as the origin. A big difference between these two scenarios is - if you use an S3 bucket as the origin, Cloudfront uses the REST API interface of S3 to communicate with the origin. If you use the website endpoint as the origin, Cloudfront uses the website URL as the origin. These endpoints have different behaviours - see the link titled Key Differences Between the Amazon Website and the REST API Endpoint. S3 REST API is more versatile, allowing the client to pass richer information like AWS Identity, thereby allowing the exchange of information that makes OAI possible. That is the reason why OAI cannot be used when Cloudfront is using the website endpoint where only GET and HEAD requests are allowed on objects.\nTherefore, in this scenario, OAI cannot be used. Instead, we have to use a custom header that only Cloudfront can inject into the Origin-bound HTTP request. The bucket policy of the S3 bucket hosting the static website can then check for the existence of said header. The assumption here is that if any browser ever directly uses the website URL of the S3-hosted static website (which is of the format examplestaticwebsitebucket.s3-website-us-east-1.amazonaws.com), their request will not contain this header, and hence will be rejected by the bucket policy.\nAlso, S3-hosted static websites do not support HTTPS. Therefore, Origin Protocol Policy, in this case, cannot be set to HTTPS Only. We can only set Viewer Protocol Policy. Only the browser to Cloudfront half will be HTTPS. The Cloudfront to Origin half cannot be HTTPS in this case","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteEndpoints.html#WebsiteRestEndpointDiff","title":"Key Differences Between the Amazon Website and the REST API Endpoint"},{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html","title":"Values That You Specify When You Create or Update a Distribution"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/","title":"How do I use CloudFront to serve a static website hosted on Amazon S3?"}],"answers":[{"id":"3a34f270648088ee645d11ba0aa6420d","text":"While setting up the Cloudfront Web Distribution, select the S3 bucket as the origin. Select Restrict Bucket Access to Yes, and create a new Origin Access Identity (OAI) that will prevent anyone else other than the Cloudfront web distribution to access the S3 bucket. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS.","correct":false},{"id":"2e8883471b2e107d4883062beed92ea6","text":"While setting up the Cloudfront Web Distribution, use the website endpoint of the S3-hosted static website as the Origin Domain Name. Also, set up Origin Custom Header. Then specify a header like Referrer, with its value set to some secret value. Set the bucket policy of the S3 bucket to allow s3 GetObject on the condition that the HTTP request includes the custom Referrer header. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS. Additionally, set the value of Origin Protocol Policy to HTTPS Only.","correct":false},{"id":"99836ac4dd080c1897cc5b3fc4d05bf7","text":"While setting up the Cloudfront Web Distribution, select the S3 bucket as the origin. Select Restrict Bucket Access to Yes, and create a new Origin Access Identity (OAI) that will prevent anyone else other than the Cloudfront web distribution to access the S3 bucket. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS. Additionally, set the value of Origin Protocol Policy to HTTPS Only.","correct":false},{"id":"bf689f9ae9dcc4c34420f34e012811a4","text":"While setting up the Cloudfront Web Distribution, use the website endpoint of the S3-hosted static website as the Origin Domain Name. Also, set up Origin Custom Header. Then specify a header like Referrer, with its value set to some secret value. Set the bucket policy of the S3 bucket to allow s3 GetObject on the condition that the HTTP request includes the custom Referrer header. In the Cloudfront web distribution, set the value of the property Viewer Protocol Policy to HTTPS Only, or Redirect HTTP to HTTPS.","correct":true}]},{"id":"5af539b7-b132-4a3a-bc80-406c620e7325","domain":"awscsapro-domain1","question":"A food service business has begun an initiative to migrate all applications and data to the AWS cloud. Governance needs to be established before any migrations can occur. Business units such as sales, marketing, and product management have fluctuating infrastructure capacity and security requirements, while other business units like finance, operations, and human resources have more static demand. Security policies and compliance needs vary by project group within each business units. Each business unit is responsible for it's own cost center, and the finance group would like cost reporting to be as streamlined as possible. Which AWS account structure will best satisfy the company's governance needs?","explanation":"Leveraging AWS Organizations to manage an account structure with a core Organizational Unit and Organizational Units for each business unit provides flexibility for future organizational changes. Creating an account for each project group facilitates security policy differences within business units, and limits the exposure of a single security event. Managing differing security requirements by project group in a single account will require more governance maintenance. Creating billing, shared services, and log archive accounts in multiple Organizational Units will result in duplication of services, and can be done at the core level.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-laying-the-foundation/introduction.html","title":"Laying the Foundation: Setting Up Your Environment for Cost Optimization"},{"url":"https://aws.amazon.com/solutions/aws-landing-zone/?did=sl_card&trk=sl_card","title":"AWS Landing Zone"}],"answers":[{"id":"03705913700b8d76205d4203c58dc5e1","text":"Use AWS Organizations with a single Organizational Unit to consolidate costs. Create a billing account, a shared services account, and a log archive account in the Organizational Unit. Create individual accounts for each business unit. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs","correct":false},{"id":"bd400ff0d22480599228a0442d2bb8d4","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Create an Organizational Unit for each business unit that contains accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":true},{"id":"a8f7d8fbb7c6c3a1a14c91577dff42e1","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Place business units with similar security requirements in shared Organizational Units. Create accounts for each business unit in the shared Organizational Units. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"a66d8391267460b5800c5c3d07921767","text":"Use AWS Organizations to create Organizational Units for each business unit. Create a billing account, a shared services account, and a log archive account in each Organizational Unit. Create accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false}]},{"id":"2c204ae8-6bba-49a1-b8f6-1aa4330c3d8c","domain":"awscsapro-domain5","question":"You are helping an IT organization meet some security audit requirements imposed on them by a prospective customer.  The customer wants to ensure their vendors uphold the same security practices as they do before they can become authorized vendors.  The organization's assets consist of around 50 EC2 instances all within a single private VPC.  The VPC is only accessible via an OpenVPN connection to an OpenVPN server hosted on an EC2 instance in the VPC.  The customer's audit requirements disallow any direct exposure to the public internet.  Additionally, prospective vendors must demonstrate that they have a proactive method in place to ensure OS-level vulnerability are remediated as soon as possible.  Which of the following AWS services will fulfill this requirement?","explanation":"AWS Macie is a service that attempts to detect confidential data rather than OS vulnerabilities.  Since there is no public internet access for the VPC, services like GuardDuty and Shield have limited usefulness. They help protect against external threats versus any OS-level needs.  AWS Artifact is simply a document repository and has no monitoring functions.  Only AWS Inspector will proactively monitor instances using a database of known vulnerabilities and suggest patches.","links":[{"url":"https://aws.amazon.com/inspector/faqs/","title":"FAQs - Amazon Inspector - Amazon Web Services (AWS)"},{"url":"https://aws.amazon.com/macie/","title":"Amazon Macie | Discover, classify, and protect sensitive data | Amazon Web  Services (AWS)"}],"answers":[{"id":"7ed3972608faa6c3dfc6fda7f151889c","text":"Enable AWS GuardDuty to monitor and remediate threats to my instances.","correct":false},{"id":"81133f0650fa1ca2fbe1b920a6c67cc9","text":"Employ Amazon Inspector to periodically assess applications for vulnerabilities or deviations from best practices.","correct":true},{"id":"6a353f53758a3fd632209b5286a01086","text":"Enable AWS Shield to protect my instances from unauthorized access.","correct":false},{"id":"45d5e166ed185c1f7516650c714423dd","text":"Enable AWS Artifact to periodically scan my instances and prepare a report for the auditors.","correct":false},{"id":"178912f5fbd90ab710621756a2ba18ff","text":"Employ AWS Macie to periodically assess my instances for vulnerabilities and proactively correct gaps.","correct":false}]},{"id":"722221be-beb9-4a2b-8ae1-dff52b80125c","domain":"awscsapro-domain3","question":"You work for a technology company with two leased data centres (one on the east coast and one on the west coast) and one owned on-premises data centre. Management has decided to move the two leased data centres to the AWS cloud - one to us-east-1 and the other to us-west-1. The on-premises data centre will still continue running workloads which are not ready to move to the cloud.\nThis on-premises data centre must be always connected to the VPC-s in us-east-1 and us-west-1 for (a) the continuous replication of several databases and (b) the need to access some data residing on the on-premises data centre from applications running in both the AWS regions. The peak bandwidth required for these connections is (a) 500 Mbps between us-east-1 and on-premises, and (b) 8 Gbps between us-west-1 and on-premises. The applications would still be able to function at lower bandwidth, but the experience will be poor, which is not desirable. Both these connections must be Highly Available with 99.999% uptime. The connectivity solution must be cost-effective as well.\nAs the AWS Architect, what connectivity solution would you propose, so that all Bandwidth, HA and cost-effectiveness requirements are met?","explanation":"We can eliminate the VPC Peering solution immediately, as VPC Peering is for connecting two VPC-s on AWS. VPC Peering cannot be used to connect an AWS VPC with an on-premises network.\nOut of the remaining choices, the one that proposes connecting to us-east-1 using VPN and us-west-1 using Direct Connect comes very close to fulfilling all requirements. It suffers from two problems, however. One - the peak bandwidth requirement for us-east-1 is 500 Mbps. A VPN connection cannot be expected to provide 500 Mbps most of the time, as the true bandwidth someone can get from a VPN connection depends on a lot of factors including internet traffic it is sharing the route with. Secondly, if we are paying for a Direct Connect connection for the other region anyway, why not just use that one for this region too? Now, there is something called Direct Connect Gateways that makes it possible to share multiple AWS Regions using the same Direct Connect connection. The knowledge of Direct Connect Gateways is important for the AWS SA-P exam. Hence, this question tests this knowledge. The correct answer is the only one that uses Direct Connect Gateway.\nThe other choice that uses two separate Direct COnnect connections (one for each region) is not cost-effective, especially because since 2017, Direct Connect Gateways make it possible to connect to multiple AWS Regions using the same Direct Connect connection.\nRegarding HA, it is always a good practice to set up a VPN connection as a back-up for Direct Connect. The only requirement to do this is that the back-up VPN connection must also use the same Virtual Private Gateway on the AWS VPC side, otherwise traffic cannot fail over easily.\nNote about Direct Connect Gateways - they not only allow a customer to connect to two AWS Regions using a single Direct Connect connection, they also let the connected Regions communicate with each other! (This is why the VPC CIDR-s in us-east-1 and us-west-1 in the correct answer have to be non-overlapping.) There may be questions testing this aspect as well. Before Direct Connect Gateways existed, VPC Peering would be the only way for Inter-Region VPC Access. There is also another solution now - Transit Gateway, but this was announced late 2018. Usually, topics do not start appearing on the exam unless they have been more than 6 months in GA. Expect Transit Gateways to start appearing in questions now as well!","links":[{"url":"https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/","title":"AWS Direct Connect Gateway for Inter-Region VPC Access"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/","title":"Configuring VPN as back up of Direct Connect"},{"url":"https://aws.amazon.com/directconnect/sla/","title":"Direct Connect SLA puts uptime target at 99.9%. Therefore, if we need more than that, we should set up VPN as back up"}],"answers":[{"id":"85b6b49a01a28a90d71ef3c20ca9da8d","text":"Connect the on-premises data centre and us-east-1 using redundant site-to-site VPN connections as its bandwidth requirements do not require a costly Direct Connect connection. The redundant VPN connections must use different customer gateways and will provide an HA solution for that region. Connect the on-premises data centre with us-west-1 using a 10 Gbps Direct Connect circuit. Set up a back-up VPN connection for this region such that it uses the same Virtual Private Gateway as the Direct Connect circuit","correct":false},{"id":"3ea4a9645ac22c0b2c6c427764a7ae01","text":"Set up a Direct Connect Gateway. Associate the Virtual Private Gateways from both the us-east-1 and us-west-1 VPC-s with this Direct Connect Gateway. Then set up a single 10 Gbps Direct Connect connection between the on-premises data centre and the Direct Connect Gateway, using a Private Virtual Interface. Ensure that the VPC CIDR-s in the two AWS Regions are non-overlapping. To increase HA, set up separate back-up VPN connections between the on-premises data centre and each of the two AWS Regions","correct":true},{"id":"608dc6c035921ef09c640ebb70de9ddd","text":"Use VPC Peering to connect the on-premises network with both the us-east-1 and us-west-1 VPC-s independently. Bandwidth provided by VPC Peering is virtually unlimited, limited only by the instance sizes used. Also, VPC peering connections are fault-tolerant and scalable, so no back-up connectivity is needed","correct":false},{"id":"f0671c30ae58d58c0d64162640e69527","text":"Use two Direct Connect connections - an 1 Gbps one between the on-premises data centre and us-east-1, and a 10 Gbps one between the on-premises data centre and us-west-1. For each Direct Connect connection, set up a back-up VPN connection that must use the same Virtual Private Gateway as the Direct Connect circuit","correct":false}]},{"id":"d750b73a-f283-4da4-97c8-4686049e9ccf","domain":"awscsapro-domain4","question":"The marketing department at your company has been using a homegrown approach to content management for the company's customer facing website. Recent content publishing issues have resulted in a directive to move to Drupal for all content management going forward. The IT budget is already stretched, and there is no capital available to purchase new servers, so the Drupal solution will be implemented on AWS. The current on-premises content management system runs on six Linux servers, each with four CPUs and sixteen gigabytes of memory. There are also two MySQL database servers, each with four CPUs and thirty-two gigabytes of memory. Spikes in volume are common during the fourth quarter of the year with increases up to fifty percent during these months. Which architecture will provide the most cost effective solution?","explanation":"Since the company has deployed six on-premises content management servers to handle seasonal spikes in volume up to fifty percent, four servers will be needed during most of the year. So we should purchase four EC2 Reserved Instances, and have Auto Scaling add up to an additional two when needed. Since content management systems are mostly read-intensive, we can save on the sizing and cost of our Aurora nodes, and improve performance by implementing larger ElastiCache nodes. The rich feature set of Redis is not needed for this use case, so we'll go with Memcached for simplicity.","links":[{"url":"https://github.com/aws-samples/aws-refarch-drupal?did=wp_card&trk=wp_card","title":"Running Drupal on AWS"},{"url":"https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/","title":"Amazon EC2 Reserved Instances Pricing"},{"url":"https://aws.amazon.com/rds/aurora/pricing/","title":"Amazon Aurora Pricing"},{"url":"https://aws.amazon.com/elasticache/pricing/","title":"Amazon ElastiCache Pricing"}],"answers":[{"id":"d67c7bb266f4dcdc91559f6f046d4f01","text":"Purchase three EC2 m5.xlarge Reserved Instances on a 3-year term. Configure Auto Scaling with Drupal AMI launch configurations. Implement one Amazon Aurora db.r5.large instance and one Amazon ElastiCache Redis r5.2xlarge instance.","correct":false},{"id":"61a0c4b4a4069b1f4343231156850d8a","text":"Purchase four EC2 m5.xlarge Reserved Instances on a 3-year term. Configure Auto Scaling with Drupal AMI launch configurations. Implement one Amazon Aurora db.r5.xlarge instance and one Amazon ElastiCache Redis r5.xlarge instances.","correct":false},{"id":"7adefaeb634686fb343da16d89ddd8fc","text":"Purchase four EC2 m5.xlarge Reserved Instances on a 3-year term. Configure Auto Scaling with Drupal AMI launch configurations. Implement one Amazon Aurora db.r5.large instance and one Amazon ElastiCache Memcached r5.2xlarge instance.","correct":true},{"id":"ae881ce32027d7c1d77feaa29c31af97","text":"Purchase six EC2 m5.xlarge Reserved Instances on a 3-year term. Configure Auto Scaling with Drupal AMI launch configurations. Implement one Amazon Aurora db.r5.xlarge instance and one Amazon ElastiCache Memcached r5.xlarge instance.","correct":false}]},{"id":"777fd0a9-391f-4072-b147-a64a2016f5a1","domain":"awscsapro-domain2","question":"You are working with a company to design a DR strategy for the data layer of their news website.  The site serves customers globally so regional diversity is required.  The RTO is defined as 4 hours and RPO have been defined as 5 minutes. Which of the following provide the most cost-effective DR strategy for this client?","explanation":"While Multi-AZ RDS may be a best practice, the question only stipulates regional resilience.  So, we are looking for options that create regional diversity and fall within our RPO and RTO.  Those options would be cross-region bucket replication and cross-region RDS replicas.  The RPO given means that we must not loose anything more than 5 minutes of data, so any sort of backup that is less frequent than every 5 minutes is eliminated.","links":[{"url":"https://aws.amazon.com/blogs/aws/cross-region-snapshot-copy-for-amazon-rds/","title":"Cross-Region Snapshot Copy for Amazon RDS | AWS News Blog"},{"url":"https://aws.amazon.com/blogs/aws/new-whitepaper-use-aws-for-disaster-recovery/","title":"New Whitepaper: Using AWS for Disaster Recovery | AWS News Blog"}],"answers":[{"id":"7c7f5169f7bfea8c0aa5d79d8f1f1565","text":"Configure RDS Read Replicas to use cross-region replication from the primary to a backup region.","correct":true},{"id":"cfe51da2c91348146ba1bb989b4c2225","text":"Configure RDS to perform daily backups then copy those to another region.","correct":false},{"id":"aa26654aaaf6239fe7acd7dc4e952d5a","text":"Write a script to create a manual RDS snapshot and transfer it to another region.  Use AWS Batch to run the script every three hours.","correct":false},{"id":"035a09840d025b52ad7c808976c94da2","text":"setup cross-region replication for S3 buckets.","correct":true},{"id":"0890c4152307ffd34ebeb6ea7c500814","text":"Write a script to export the RDS database to S3 every hour then use cross-regional replication to stage the exports in a backup region.","correct":false},{"id":"b3810d00767bfbb6b85fe44c1c3d2dd1","text":"Configure RDS to use multi-AZ and automatically fail over in the event of a problem.","correct":false}]},{"id":"a2a38759-7505-4ece-bd74-965f9d589a08","domain":"awscsapro-domain4","question":"You have been asked to help a company with optimizing cost on AWS.  You notice in reviewing documentation that they have constructed a transit network to link around 30 VPCs in different regions.  When you review traffic logs, most of the traffic is across regions.  Given this information, what might you recommend to reduce costs?","explanation":"By smartly consolidating resources into fewer regions and AZs, you are able to reduce or potentially eliminate data transfer and thus lower your overall costs.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-global-transit-network/","title":"AWS Global Transit Network – AWS Answers"}],"answers":[{"id":"9eba5c209cd53cc1ac849178ebf5fa05","text":"Implement a host-based router in the Transit VPC to intelligently route traffic based on least cost.","correct":false},{"id":"94085d779db3a238c9dabf1d52624a3c","text":"Consolidate resources into as few regions and AZs as necessary.","correct":true},{"id":"d053c2a500bfc32b54da5688d6f230a4","text":"Implement NAT rules to compensate for overlapping networks and permit more direct routes.","correct":false},{"id":"0e3b0d145e1202cb34b4947f2f6ed8ef","text":"Create VPC Gateway Endpoints within each of the 30 VPCs and add the necessary prefix lists.","correct":false}]},{"id":"3f7fa126-1155-4aa3-802d-e9eeb75f5e5a","domain":"awscsapro-domain3","question":"You work for a Clothing Retailer and have just been informed the company is planning a huge promotional sale in the coming weeks.  You are very concerned about the performance of your eCommerce site because you have reached capacity in your data center.  Just normal day-to-day traffic pushes your web servers to their limit.  Even your on-prem load balancer is maxed out, mostly because that's where you terminate SSL and use sticky sessions.  You have evaluated various options including buying new hardware but there just isn't enough time.  Your company is a current AWS customer with a nice large Direct Connect pipe between your data center and AWS.  You already use Route 53 to manage your public domains.  You currently use VMware to run your on-prem web servers and sadly, the decision was made long ago to move the eCommerce site over to AWS last.  Your eCommerce site can scale easily by just adding VMs, but you just don't have the capacity.  Given this scenario, what is the best choice that would leverage as much of your current infrastructure as possible but also allow the landscape to scale in a cost-effective manner?","explanation":"A Target Group for an ALB can contain instances or IP addresses.  In this case, we can define the private IP addresses of our on-prem web servers along side the private IP addresses of any EC2 instances we spin up.  The caveat is that we can only use private IP addresses when defining a target group in this way.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancing-via-ip-address-to-aws-on-premises-resources/","title":"New – Application Load Balancing via IP Address to AWS & On-Premises  Resources | AWS News Blog"}],"answers":[{"id":"6d3db4c52e96931f925f17fe8e9fd50f","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using public IP addresses of your on-prem web servers and additional EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"4df24111c113846bfe0505ad0c84d9a3","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define two target groups:  one containing the public IP addresses of your on-prem load balancer and one including an auto scaling group of additional EC2 instances created from the imported AMI.  Assign both target groups to the ALB using the same listener port.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"77592781918fa63474b5efbd5cc9555f","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an NLB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the NLB as an alias record.","correct":false},{"id":"3e47f65e4524f53faba23e6995b592f5","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":true}]},{"id":"2c688b4f-f267-472d-a68f-db7c9070bfae","domain":"awscsapro-domain5","question":"An application has a UI automation test suite based on Selenium and the testing scripts are stored in a GitHub repository. The UI tests need a username and password to login to the application for the testing. You check the test scripts and find that the credentials are saved in the GitHub repository using plain text. This may bring in some potential security issues. You suggest saving the username and password in a secure, highly available and trackable place. Which of the following methods is the easiest one?","explanation":"AWS Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. The credentials can be stored as ciphertext. The service is highly scalable, available, and durable. It also integrates with CloudTrail so the usage is easy to track. DynamoDB, DocumentDB and S3 are not designed to store parameters. These services need more configurations and are not as simple as AWS Parameter Store.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html","title":"AWS Systems Manager Parameter Store"}],"answers":[{"id":"33442a701fc2a058eea89266ea439ee4","text":"Create a table in DynamoDB that has a primary key for username and a sort key for password. Select the encryption type as \"KMS - AWS managed CMK\". Enable the on-demand read/write capacity mode.","correct":false},{"id":"d540b786961136abf5dfd186e84fcca0","text":"Save the username and password in AWS Parameter Store. Select the SecureString type to encrypt the data with the default key in Key Management Service (KMS). Modify the scripts to fetch the parameter values through AWS SDK.","correct":true},{"id":"26e06a6fc6a107b0dd5aac356ad3908d","text":"Edit a JSON file to store the username and password and upload the file to an S3 bucket. Encrypt the S3 bucket with SSE-S3. Modify the S3 bucket policy to only allow the testing machines to get the file.","correct":false},{"id":"fed8d54a1ce26b883744ac61a13e5ac9","text":"Configure an Amazon DocumentDB cluster with the d5.r5.large instance class. Create a schema for username and password. Enable Auto Scaling for the cluster and set up the default number of instances to be 3.","correct":false}]},{"id":"54d12a9a-149b-42a7-8491-300583d5c2b8","domain":"awscsapro-domain5","question":"A client is trying to setup a new VPC from scratch.  They are not able to reach the Amazon Linux web server instance launched in their VPC from their on-prem network using a web browser.  You have verified the internet gateway is attached and the main route table is configured to route 0.0.0.0/0 to the internet gateway properly.  The instance also is being assigned a public IP address.  Which of the following would be another potential cause of the problem?","explanation":"For an HTTP connection to be successful, you need to allow port 80 inbound and allow the ephemeral ports outbound.  Additionally, it is possible that the subnet is not associate with the route table containing the default route to the internet.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html","title":"Troubleshooting Connecting to Your Instance - Amazon Elastic Compute Cloud"}],"answers":[{"id":"25d3393c550b8dbc2179367832573598","text":"The outbound network ACL allows port 80 and 22 only.","correct":true},{"id":"c7ca1b6a8fe855bda71123163488960b","text":"The customer has disabled the ec2-user account on the Amazon Linux instance.","correct":false},{"id":"01b7463243eb231b840fcd4b737e044b","text":"The default route to the internet gateway is incorrect.","correct":false},{"id":"2bde109ce87f4a4f513679f31116184d","text":"The instance does not have an elastic IP address assigned. ","correct":false},{"id":"32cdd854c9d71059eac396dd1249830e","text":"The subnet of the instance is not associated with the main route table.","correct":true},{"id":"c9f886542d1dabe99bc64dd39c5e1615","text":"The inbound security group allows port 80 and 22 only.","correct":false},{"id":"d0f5e76f9fc753305b11c4c3a11e97ef","text":"The IAM role assigned to the LAMP instances does not have any policies assigned.","correct":false}]},{"id":"2eee6f1c-96d7-4d2b-821f-4ce8acaf3de3","domain":"awscsapro-domain5","question":"You've deployed a mobile app for a dance competition television show's viewers to vote on performances. The app's backend leverages Amazon API Gateway, AWS Lambda, and Amazon RDS Oracle, with voting activity going from devices directly to API Gateway. In the middle of the broadcast, you begin receiving errors in CloudWatch indicating that the database connection pool has been exhausted. You also see log entries in CloudWatch with a 429 status code. After the show concludes, ratings for the app indicate a very poor user experience, with multiple retries needed to cast a vote. What would be the best way to increase the scalability of the app going forward?","explanation":"Placing Kineses between API Gateway and Lambda decouples the architecture, making use of an intermediary service to buffer incoming requests. The 429 status code indicates a Lambda concurrency throttling error, which you can resolve by controlling the Kinesis batch size per batch delivery. Database sharding will increase scalability, but will still have an upper limit of capacity. Increasing available Lambda memory will have no effect. Inserting a Lambda traffic manager doesn't address the database scalability issues, nor does increasing the regional Lambda concurrency limit. Modifying RDS DB Parameter Group values will require a database restart to take effect, which won't be feasible during live voting activity.","links":[{"url":"https://aws.amazon.com/blogs/architecture/how-to-design-your-serverless-apps-for-massive-scale/","title":"How to Design Your Serverless Apps for Massive Scale"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/scaling.html","title":"AWS Lambda Function Scaling"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html","title":"Using AWS Lambda with Amazon Kinesis"}],"answers":[{"id":"8f962935878f87ebc9523dc54f505886","text":"Scale the database horizontally by creating additional instances and use sharding to distribute the data across them. Provide the Lambda function with a mapping of the sharding scheme in DynamoDB. Increase the amount of memory available to the Lambda function during execution","correct":false},{"id":"2f66b519925956d6a91d0026056904d9","text":"Have API Gateway route requests to a new Lambda function that manages traffic and retries for the voting logic Lambda function. Request that the regional function concurrency limit be increased based on volume projections","correct":false},{"id":"6b6e6fb8b54db42df4d343376ccd8c60","text":"Insert Amazon Kinesis between API Gateway and Lambda, and configure Kinesis as an event source for Lambda. Set the number of records to be read from a Kinesis shard to an optimal value based on volume projections","correct":true},{"id":"a4051078cf0289e689e79fe70eab1f14","text":"Create a separate Lambda function to increase the maximum DB connection value in the RDS DB Parameter Group when a CloudWatch Metrics DB connection threshold is exceeded. Invoke Lambda functions with an 'event' invocation type to retry failed events automatically","correct":false}]},{"id":"de88bc69-44a8-4a12-b28f-0a5e86db3939","domain":"awscsapro-domain1","question":"You have been entrusted to act as the interim AWS Administrator following the departure of the erstwhile Administrator in your company. You notice that there are several existing roles called role-engineer, role-manager, role-qa, role-dba, role-data-scientist, etc. When a new person joins the company, the new IAM user simply assumes the right role while using AWS - this allows central management of permissions and eliminates the need to manage permissions on a per-user basis.\nA new QA hire joins the company a few days later. You create an IAM User for her. You attach a Policy to the new IAM User that allows Action STS AssumeRole on any Resource. However, when this employee logs in the same day and tries to switch roles to role-qa, she is denied and is unable to assume the role-qa Role.\nWhat could be one reason why this is happening and how can it be best fixed?","explanation":"In order to allow an IAM User to successfully assume an IAM Role, two things must happen. First, the Policy attached to the User must allow the action STS AssumeRole. This is already true according to the question. Second, the Trust Policy of the Role itself must allow the User in question to assume the Role. This second condition can be met if we specify the arn of the User in the Principal element of the Trust Policy. In general, this question can be answered if the candidate is familiar with the concept of Principal in a Role, see link - A Principal within an Amazon IAM Role specifies the user (IAM user, federated user, or assumed-role user), AWS account, AWS service, or other principal entity that is allowed or denied to assume or impersonate that Role. Trust Policy is different than the Policy permissions - think of Policy Permissions as [what can be accessed] and Trust Policy as [who can access].\nTrust Policy cannot belong to an IAM User, hence the choice that claims the problem to be an unmodified User Trust Policy is incorrect. IAM changes are instantly effective, so the choice that points at the need of a time delay is also incorrect. Among the other two choices, the knowledge needed to pick the right one is an awareness of the Principal element.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html","title":"AWS JSON Policy Elements - Principal"},{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html","title":"IAM Roles"}],"answers":[{"id":"b3bf261fca3a734ad312d3ac0e5d0589","text":"You have not modified the Trust Policy of the IAM User to trust the Role role-qa. To fix this, add a Condition to the IAM Policy attached to the new user that filters on the role and specify the arn of role-qa","correct":false},{"id":"765f20e64b35dbc08c2bc319bcbe7e1a","text":"Sufficient time has not passed since you made the changes. It takes up to 12 hours to propagate IAM role changes. To fix this, ask her to try again the next day.","correct":false},{"id":"e6dacaf19a289e1f73855c5e904b21fb","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Principal element of the Trust Policy of the Role","correct":true},{"id":"dbb48c05e238c18bdb9c17ee265e387b","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Condition element of the Trust Policy of the Role","correct":false}]},{"id":"a7c939f1-277e-469f-a209-9b290e8136c9","domain":"awscsapro-domain5","question":"Your company has contracted with a third-party Security Consulting company to perform some risk assessments on existing AWS resources.  As part of a routine list of activities, they inform you that they will be launching a simulated attack on one of your EC2 instances.  After the Security Group performed all their activities, they issue their report.  In their report, they claim that they were successful at taking the EC2 instance offline because it stopped responding soon after the simulated attack began.  However, you're quite certain that machine did not go offline and have the logs prove it.  What might explain the Security company's experience?","explanation":"AWS Shield and other counter-measure technologies work to protect all AWS customers from DDoS attacks.  Unless AWS was aware of the test time and expected duration, its likely the traffic was blocked as suspicious.  AWS Firewall Manager is used to manage WAF ACLs and not dynamically blacklist IPs.  Similarly, VPC Flow Logs cannot automatically implement NACL changes as described here. Despite being a permitted service, traffic suspected of being malicious will still be blocked","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/penetration-testing/","title":"Submit a Penetration Testing Request"}],"answers":[{"id":"e3facacbe52b6423f9cf2e700d8e0b81","text":"The Security Company's traffic was seen as a threat and blocked dynamically by AWS.  AWS must grant permission before any penetration testing is done.","correct":true},{"id":"f3f963b71e307f8c28109631df115418","text":"The EC2 instance is using an ENI and the Security Company temporarily exceeded the throughput limit resulting in a throttling of their connection.","correct":false},{"id":"82a05cb45adab6d248655e827de16c6f","text":"AWS Firewall Manager is dynamically adding a blacklist entry for the Security Company's testing machine because it sees the traffic as a threat.","correct":false},{"id":"7d4826f179b8dc854c9cfb6e43678373","text":"The VPC Flow Logs record the spike in suspicious traffic and implement an update to the inbound NACL to block the remote IP address.","correct":false}]},{"id":"663fbd6a-87bd-4fa6-a0ea-428ba2de5b51","domain":"awscsapro-domain5","question":"You manage a relatively complex landscape across multiple AZs.  You notice that the incoming requests vary mostly depending on the time of day but also there is a more unpredictable component resulting in smaller spikes and valleys for your resources.  Fortunately, you manage this landscape via OpsWorks Stacks.  What options, if any, are available to you as part of the OpsWorks featureset.","explanation":"OpsWorks Stacks offers three types of scaling: 24/7 for instances that remain on all the time; time-based for instances that can be scheduled for a certain time of day and on certain days of the week; and load-based scaling which will add instances based on metrics.  All this can be configured from within the OpsWorks Stack console.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/best-practices-autoscale.html","title":"Best Practices: Optimizing the Number of Application Servers - AWS OpsWorks"}],"answers":[{"id":"216c997091da6e24174ad1b83d0be8b9","text":"You would define a baseline level of resources within the OpsWorks Stack Console to cover the average load.  But for the periodic load, that requires a scheduled auto-scaling policy.  Similarly, for the volatile spikes, you must use a stepped auto-scaling policy defined in an auto scaling group. ","correct":false},{"id":"75ab4de4ea42c1971b0ee09ae04ca591","text":"You would define a baseline level of resources and configure them for 24/7 instances.  Then you could define a time-based instances to cover certain times of day.  Finally, you could cover the volatile spikes with a load-based instances.  All this can be done within OpsWorks Stacks.","correct":true},{"id":"3622d494dceb973760a46dea038d1dc2","text":"If you need the ability to dynamically scale, you will need to use OpsWorks for Chef Automate.  OpsWorks Stacks does not support scaling.","correct":false},{"id":"b7ff5b06f51facca179494cb2bb00e55","text":"You can enabled CloudFormation Anticipated Scaling that uses past CloudWatch metrics and machine learning to automatically design a scaling policy optimized for the incoming request patterns.","correct":false}]},{"id":"73708d6f-e6cb-4b8f-90d9-723a2961496e","domain":"awscsapro-domain2","question":"Your team is architecting an application for an insurance company.  The application will use a series of machine learning methods encapsulated in an API call to evaluate claims submitted by customers.  Whenever possible, the claim is approved automatically but in some cases were the ML API is unable to determine approval, the claim is routed to a human for evaluation.  Given this scenario, which of the following architectures would most aligned with current AWS best practices?","explanation":"Formerly, AWS recommended SWF for human-involved workflows.  Now AWS recommends Step Functions be used as it requires less programmatic work to build workflows and is more tightly integrated into other AWS services.","links":[{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html","title":"Starting a State Machine Execution in Response to Amazon S3 Events - AWS  Step Functions"}],"answers":[{"id":"fa669ed2f0666420f19ad9c8836509f6","text":"Take in the claims into an SQS queue.  Create a Lambda function to poll the SQS queue, fetch the claim and submit the API call.  Use another Lambda function to evaluate the API results and if the claim is not approved, place the claim in a dead letter queue.  Train a person to periodically log into the SQS console and read the dead letter queue for review.","correct":false},{"id":"5298caaea3a55734efa8c62e637d0d40","text":"Use Kinesis to take in the claims and save them on S3 using Firehose.  Use Sagemaker to analyze the claims on S3 as a training set and devise a decider function.  Save the approved claims to another S3 bucket setup with an Event to trigger an SES message to a reviewer.","correct":false},{"id":"837fa3e7c6b347eddfa7fefd2a092017","text":"Create a workflow using Simple Workflow Service and an EC2 fleet to host worker and decider programs.  Create worker programs for each processing step and ML API call.  Create decider program to receive the output of the API and decide if the claim is approved.  For unapproved claims, create a worker program use the WorkMail SDK to place the unapproved claim into a mailbox to be reviewed by a human.","correct":false},{"id":"dfc86c259de4881886ffdac5b8106777","text":"Create a State Machine using Step Functions and a Lambda function for calling the API.  Intake the claims into an S3 bucket configured with a CloudWatch Event.  Trigger the Step Function from the CloudWatch Event.  Create an Activity Task after the API check to email an unapproved claim to a human.","correct":true}]},{"id":"e8bba7f5-4c0d-42dd-ad7a-74f042ce3dd9","domain":"awscsapro-domain3","question":"Due to a dispute with their co-location hosting company, your client is forced to move some applications as soon as possible to AWS.  The main application uses IBM DB2 for the data store layer and a Java process on AIX which interacts via JMS with IBM MQ hosted on an AS400.  What is the best course of action to reduce risk and allow for fast migration?","explanation":"For a fast migration with minimal risk, we would be looking for a lift-and-shift approach and not spend any time on re-architecting or re-platforming that we don't absolutely have to do.  Amazon MQ is JMS compatible and would provide a shorter path to the cloud than SQS.  DMS does not support DB2 as a target.","links":[{"url":"https://aws.amazon.com/amazon-mq/features/","title":"Amazon MQ Features – Amazon Web Services (AWS)"},{"url":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html","title":"Targets for Data Migration - AWS Database Migration Service"}],"answers":[{"id":"21cbd80d1d890d3668ba4f841d7901df","text":"Use a physical-to-virtual tool to convert the AIX DB2 server into a virtual machine.  Use AWS CLI to import the VM into AWS and launch the VM.  Deploy the Java program as a Lambda function.  Launch a version of IBM MQ from the AWS Marketplace.","correct":false},{"id":"6d8610d6575127564b286722b73ce4be","text":"Install DB2 on an EC2 instance and use DMS to migrate the data.  Encapsulate the Java program in a Docker container and deploy it on ECS.  Spin up an instance of Amazon MQ.","correct":false},{"id":"3700f4c1ab4e0778c4d0ae131d9c277d","text":"Deploy the Java processes as Lambda functions.  Install DB2 on an EC2 instance and migrate the data by doing an export and import.","correct":false},{"id":"4dba67b80e52ce08ef39ca56bf0ddd57","text":"Install DB2 on an EC2 instance and migrate the data by doing an export and import.  Spin up an instance of Amazon MQ in place of IBM MQ.  Install the Java process on a Linux-based EC2 system.","correct":true},{"id":"fbc4f7d74e4d62a2bdce9ca3f55f9fd2","text":"Use DMS and SCT to migrate DB2 to Aurora.  Update the Java application to use SQS and install it on a LInux-based EC2 system.  ","correct":false}]},{"id":"78111d9b-922f-435f-8e91-4ae84e990761","domain":"awscsapro-domain3","question":"A hotel chain has decided to migrate their business analytics functions to AWS to achieve higher agility when future analytics needs change, and to lower their costs. The primary data sources for their current on-premises solution are CSV downloads from Adobe Analytics and transactional records from an Oracle database. They've entered into a multi-year agreement with Tableau to be their visualization platform. For the time being, they will not be migrating their transactional systems to AWS. Which architecture will provide them with the most flexible analytics capability at the lowest cost?","explanation":"AWS Database Migration Service can be configured with an on-premises Oracle database as a source and S3 as a target. It can provide continuous replication between the two. AWS Glue can aggregate the data from S3 according to desired reporting dimensions and store the summaries in Redshift. Keeping the transactional detail in S3 and only keeping the aggregate information in Redshift will save on costs. The same is true for keeping transactional detail in S3 instead of RDS Oracle. AWS Glue is a great solution for transforming the Adobe Analytics CSV files to Parquet format in S3. Parquet's columnar organization will provide excellent performance for Redshift Spectrum queries that join between Redshift tables and S3. Tableau's Redshift connector supports Redshift Spectrum queries. For this use case, using Amazon QuickSight would not make sense since the company has already committed payments to Tableau via their multi-year agreement.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/glue/","title":"AWS Glue"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"86c10f6cca438461e60f5c04886f57c9","text":"Configure AWS Database Migration Service to continuously replicate Oracle transactional data to Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Redshift. Use Amazon QuickSight to query the data for visualization.","correct":false},{"id":"dc33d336682223190f2d8cb22449cf81","text":"Implement AWS Database Migration Service to continuously replicate Oracle transactional data to an Amazon RDS Oracle instance. Use AWS Glue to write the Adobe Analytics data to the RDS Oracle instance. Install Tableau on Amazon EC2 and write queries against the RDS Oracle database.","correct":false},{"id":"1bc635be85a059a6135751bf21fd3550","text":"Use Oracle Data Guard to continuously replicate Oracle transactional data to an Oracle instance on Amazon EC2. Configure AWS Glue to aggregate the transactional data from the Oracle instance for each dimension into Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Redshift. Use Amazon QuickSight to query the data for visualization.","correct":false},{"id":"e1f7ec66baca2c4e27cf072a8ca91424","text":"Employ AWS Database Migration Service to continuously replicate Oracle transactional data to Amazon S3. Configure AWS Glue to aggregate the transactional data from S3 for each dimension into Amazon Redshift. Use AWS Glue to write the Adobe Analytics data to Amazon S3 in Parquet format. Install Tableau on Amazon EC2 and write queries to Amazon Redshift Spectrum.","correct":true}]},{"id":"0abe2292-3f6e-47e1-93d9-6af24d5ea4c2","domain":"awscsapro-domain4","question":"A graphic design company has purchased eighteen m5.xlarge regional Reserved Instances and sixteen c5.xlarge zonal Reserved Instances. They receive their monthly AWS bill and find the invoice amount to be significantly higher than expected. Upon investigation, they discover RI discounted and non-discounted charges for nine m5.xlarge instances, nine m5.2xlarge instances, eight c5.xlarge instances, and eight c5.2xlarge instances. The business will need all of this capacity for at least the next twelve months. As their consultant, what would you advise them to do to maximize and monitor their RI discounts?","explanation":"Regional Reserved Instances allow for application of RI discounts within instance families, so all of the m5 instances are covered. Zonal Reserved Instances only provide discounts for specific instance types and sizes. So purchase of additional RIs would lower costs on the eight c5.2xlarge instances. Unused Reserved Instances are contractual and cannot be cancelled, so looking for another place to use them is the right approach. They could possibly be sold on the Reserved Instance Marketplace. AWS Budgets reservation budgets provide visibility and alerting on RI coverage specifically. Cost budgets and usage budgets may be useful, but they won't target RI coverage specifically.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html","title":"Regional and Zonal Reserved Instances (Scope)"},{"url":"https://aws.amazon.com/blogs/aws-cost-management/launch-instance-family-coverage-budgets/","title":"Launch: Instance Family Coverage Budgets"}],"answers":[{"id":"8e0e716570fad7bcaf6fe8bebe4f9d49","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Cancel the Reserved Instances for the nine unused m5.xlarge instances and eight unused c5.xlarge instances. Create an AWS Budgets cost budget that sends notification whenever costs exceed 80% of usage expectation","correct":false},{"id":"1c87a2204ad8da355c8ca0890aa06785","text":"Purchase an additional nine m5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances. Create an AWS Budgets usage budget that sends notification whenever RI coverage drops below 60%","correct":false},{"id":"4b6d77d8423cd16b8e711b482dbcafbf","text":"Purchase an additional nine c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":true},{"id":"8c1f6db4cd04a86fd43efa1bc97640ea","text":"Purchase an additional nine m5.2xlarge Reserved Instances and an additional eight c5.2xlarge Reserved Instances. Look for upcoming projects that can use nine m5.xlarge instances and eight c5.xlarge instances. Create an AWS Budgets reservation budget that sends notification whenever overall RI coverage drops below 60%","correct":false}]},{"id":"401cbed4-e977-4303-9344-586af01a4180","domain":"awscsapro-domain2","question":"You have been contracted by a manufacturing company to create an application that uses DynamoDB to store data collected in a automotive part machining process.  Sometimes this data will be used to replay a process for a given serial number but that's always done within 7 days or so of the manufacture date.  The record consists of a MACHINE_ID (partition key) and a SERIAL_NUMBER (sort key).  Additionally, there is a CREATE_TIMESTAMP attribute that contains the creation timestamp of the record and a DATA attribute that contains a BASE64 encoded stream of machine data.  To keep the DynamoDB table as small as possible, the industrial engineers have agreed that records older than 30 days can be purged on a continual basis.  Given this, what is the best way to implement this with the least impact on provisioned throughput.","explanation":"Using DynamoDB Time to Live feature is a perfect way to purge out old data and not consume any WCU or RCU.  Other methods of deleting records would impact the provisioned capacity units.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","title":"Time To Live - Amazon DynamoDB"}],"answers":[{"id":"7c29d6c8effb51be4df54033ce45d01f","text":"Enabled Lifecycle Management on the DynamoDB table.  Create a rule that deletes any records where CREATE_TIMESTAMP attribute is greater than 30 days old.","correct":false},{"id":"ef62d6ec88d117a0ac0cb7c99cd1abbd","text":"Use DynamoDB Streams to trigger a Lambda function when the record ages past 30 days.  Use the DynamoDB SDK in the Lambda function to delete the record.","correct":false},{"id":"41f68b5f48ef97cc437ebfe50ae10882","text":"Use AWS Batch to execute a daily custom script which queries the DynamoDB table and deletes those records where CREATE_TIMESTAMP is older than 30 days.  ","correct":false},{"id":"081f76fb4967f9d10a3799ae400ad898","text":"Update the table to add a attribute called EXPIRE  Change the application to store EXPIRE as CREATE_TIMESTAMP + 30 days.  Enable Time to Live on the DynamoDB table for the EXPIRE attribute.","correct":true},{"id":"0a945c4865c940ffaddaeade6f6bbdaf","text":"Use Step Functions to track the lifecycle of DynamoDB records.  Once 30 days has elapsed, branch to a Delete step and trigger a Lambda function to remove the record.","correct":false}]},{"id":"79f2f5be-b591-44e6-957b-eb0383640d7d","domain":"awscsapro-domain5","question":"You have a standard SQS queue to receive messages from the frontend application. The backend application is JAVA based and the AWS SDK is used to get the messages from the queue for processing. The SQS queue is not busy most of the time. According to the backend application logs, there is a high number of empty ReceiveMessageResponse instances returned. You want to adjust the settings to minimize the number of empty responses and reduce the cost. How would you implement this? ","explanation":"Amazon SQS long polling is preferable to short polling in most of the cases. Long polling requests let the consumers receive messages as soon as they arrive in the queue. It can help to reduce the number of empty responses. In order to enable long polling, the attribute ReceiveMessageWaitTimeSeconds should be more than 0. Short polling is incorrect. Visibility timeout and delivery delay do not address the problem of empty responses.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html","title":"Amazon SQS short and long polling"}],"answers":[{"id":"203fa6faf2e6bf53939b43300ec6dac2","text":"Increase the default visibility timeout of the queue to reduce the possibilities that the messages become visible to consumers again. The application can also use the ChangeMessageVisibility API to specify a suitable timeout value.","correct":false},{"id":"220352e5b3779c1f2030cfd4b391b19e","text":"Modify AWS SDK to get the messages in the SQS queue by short polling. The ReceiveMessage call from the consumer sets the WaitTimeSeconds attribute to 0. As a result, the empty responses are eliminated.","correct":false},{"id":"c3cbf51c591cb7fa17bd023ab814f95c","text":"Consume the messages in the SQS queue using long polling. Set the queue attribute ReceiveMessageWaitTimeSeconds to be more than 0. Amazon SQS will wait until there is an available message in a queue before sending a response.","correct":true},{"id":"a5cdcd2968c3566cbb7fc7bcd5fef01a","text":"Add a delivery delay in the SQS queue such as 1 minute. The delay helps to postpone the delivery of new messages to the queue for some time. When the JAVA application polls the messages from the queue, there will be a lower chance to get an empty response.","correct":false}]},{"id":"3440b6ff-6fe9-495d-a765-f69f6b82a628","domain":"awscsapro-domain3","question":"You work for a health record management company which operates a view-only portal for end-users to check their health records online. Users can also raise disputes if anything is incorrect. This 2-tier website that supports only HTTPS will be moved to the AWS Cloud. There are 5 web servers on-premises and an F5 Load Balancer that controls traffic to these web servers. SSL is terminated at the web servers. An Oracle Real Application Cluster (RAC) serves as the database tier. Due to the sensitive nature of personal health information, the web site uses mutual authentication - the server requests browsers for a valid client certificate before establishing a trusted session.\nSelect two alternate working architectures for this application to be migrated to AWS so that code changes are minimized. Choose two responses each of which can act as an independent and functional solution, and also result in minimum application code changes.","explanation":"The areas tested by this question are as follows.\nFirst, if an ELB is not terminating SSL, its listener protocol cannot be HTTPS - it has to be TCP. Additionally, AWS ELB does not support terminating client-side certificates. Therefore, if a website requires client SSL certificates, and if it also uses AWS ELB, the ELB must let the target EC2 instances to terminate SSL and validate the client certificate. This requires the protocol to be TCP/443. Both these facts (one, SSL is not terminated at the Load Balancer level, and two, mutual authentication is used) are stated explicitly in the question so that the candidate identifies at least one of them and is thus able to conclude that HTTPS is not the correct protocol choice for ELB. Hence, amongst the two choices that use ELB, the one that says TCP is the correct one.\nSecondly, RDS does not support Oracle RAC. Hence, the database tier must use EC2 instances. Thus, for the second alternate solution that uses Route 53 multi-value answer records instead of ELB, we should select the option that deploys EC2 instances for the database tier.","links":[{"url":"https://forums.aws.amazon.com/thread.jspa?threadID=109180","title":"Discussion Forums - HTTPS Client certificate validation while using client ELB"},{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"RDS FAQ-s, search for phrase - Is Oracle RAC supported on Amazon RDS"}],"answers":[{"id":"7250b61db9c9e69abf4f9e7bd2bfb268","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring HTTPS/443 as listener","correct":false},{"id":"307f49d40547367c203a0fcfa1a46be8","text":"Migrate the database to RDS Oracle and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":false},{"id":"e7a1d88af1880a82062437a4c1005adf","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":true},{"id":"11a4ad1f948d7b2fd3631cb763fa8a00","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring TCP/443 as listener","correct":true}]},{"id":"33803c8a-b588-4dca-8067-e500383254f3","domain":"awscsapro-domain4","question":"You work for a retail services company that has 8 S3 buckets in us-east-1 region. Some of the buckets have a lot of objects in them. There are Lambda functions and EC2-hosted custom application code where the names of these buckets are hardcoded. Your manager is worried about disaster recovery. As part of her business continuity plan, she has requested you to set up Cross-Region Replication of these S3 buckets to us-west-1, ensuring that the replicated objects are using a less expensive Storage Class because they would not be accessed unless disaster strikes. You are worried that in the event of failover due to the entire us-east-1 region being unavailable, the application code, once deployed in us-west-1, must continue to work while trying to access the S3 buckets in the new region. She has also requested you to start taking periodic snapshots of EBS Volumes and make these snapshots available in the us-west-1 region so that EC2 instances can be launched in us-west-1 using these snapshots if needed. How would you ensure that (a) the launching of EC2 instances works in us-west-1 and (b) your application code works with the us-west-1 S3 buckets?","explanation":"This question presents two problems - (1) how to ensure that EBS snapshots are created periodically and are also made available in a different region for launching required EC2 instances in case of failure of the primary region (2) how to deal with application code where S3 bucket names are hardcoded and whether this hardcoding will impact disaster recovery while trying to run in a different region. Both of these problems are real-life issues AWS customers face when designing and planning their disaster recovery solutions.\n(1)Remember that Data Lifecycle Manager can only schedule snapshot creation in the same Region. If we want to copy that snapshot into a different region, we must write our own scripts or Lambda functions for doing that. Hence, the choices that state that DLM can be used to directly create the snapshot into different regions are eliminated. Additionally, only root volume snapshots can be used to create an AMI. Non-root EBS Volume snapshots cannot be used to generate an AMI. Hence, the choices that specify using non-root volume snapshots are eliminated.\n(2)Remember that S3 bucket names are globally unique. Hence, one cannot create a second S3 bucket in the DR Region with the same name as the bucket in the primary region. Hence, the options that hint the creation of S3 buckets by the same name are eliminated. This results in a problem if S3 names are hardcoded in the application - that application will simply not run in a new region, it will fail. Hence, it is best to avoid hardcoding, and fetch the S3 bucket name from a key-value storage service like AWS Systems Manager Parameter Store at runtime. Creating this Parameter Store in each region and storing the correct bucket names in them can help in designing this non-hardcoded solution. Additionally, enabling Cross-Region Replication does not copy pre-existing content. Hence, the choices that suggest that pre-existing content will be automatically copied are eliminated.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-launch-snapshot.html","title":"Launching an Instance from a Backup"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/ec2/copy-snapshot.html","title":"Copy-snapshot documentation"}],"answers":[{"id":"02e8aed40d51e4bf9256f1ed436aa069","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false},{"id":"fc1a2efe00ef610249a55dadb0dd64fe","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the non-root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Pre-existing objects are copied over automatically while setting up Cross-Region Replication","correct":false},{"id":"824dc187ab89444593b50521f60b8ff3","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager. Then, set up a Lambda function to copy these snapshots to the us-west-1 region using the copy-snapshot API. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create corresponding S3 buckets with different names in us-west-1. Change the application code to not hardcode the names of S3 buckets. Instead, read the S3 bucket names from AWS Systems Manager Parameter Store. Set up a Parameter Store in us-west-1 with the same keys but containing the us-west-1 bucket names. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":true},{"id":"fae496411f2d461e0926abfdf8ad8b64","text":"To ensure that EC2 instances can be launched in us-west-1 when needed, schedule periodic creation of EBS snapshots of both root and non-root volumes using Data Lifecycle Manager such that the snapshots are created directly in us-west-1 region. Use the root volume snapshots to create an AMI in us-west-1 region when needed. Launch EC2 instances from this AMI.\nTo ensure application compatibility with S3 buckets in us-west-1, create the S3 buckets in us-west-1 with the same names as the corresponding ones in us-east-1, so that application code does not break. Set up Cross-Region Replication and specify that the object be moved to Infrequent Access Storage Class in the destination bucket. Run a script to copy pre-existing objects over as they are not copied automatically while setting up Cross-Region Replication","correct":false}]},{"id":"d2b0a9d5-1875-4a55-968d-3a2858601296","domain":"awscsapro-domain2","question":"You currently are using several CloudFormation templates. They are used to create stacks that include the resources of VPC subnets, Elastic Load Balancers, Auto Scaling groups, etc. You want to deploy all the stacks with a root stack so that all the resources can be configured at one time. Meanwhile, you need to isolate information sharing to within this stack group, which means other stacks outside of the stack group can not import its resources. For example, one stack creates a VPC subnet resource and this subnet can only be referenced by the stack group. What is the best way to implement this?","explanation":"As the stack outputs should be limited within the stack group, nested stacks should be chosen. The export stack outputs cannot prevent other stacks to use them. The AWS::CloudFormation::Stack resource type is used in nested stacks to provide dependencies. The DependsOn attribute is not used for configuring nested stacks.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html","title":"Exporting stack output values"}],"answers":[{"id":"aca679ff150a8a8176faf99cc057e825","text":"Create nested stacks with the \"AWS::CloudFormation::Stack\" resources. Use the outputs from one stack in the nested stack group as inputs to another stack in the group if needed.","correct":true},{"id":"e44006dac54e63b93a8804a4e632eeb5","text":"Upload the root and all child stack templates to an S3 bucket under the same directory. Use the \"DependsOn\" attribute in the root template to add dependencies. When the root stack is created, all the child stacks are created first. ","correct":false},{"id":"fe3bf9eee211c59e9c64ad17a2d32e27","text":"Export output values for each child stack if needed. Create a parent stack to use the exported values from child stacks to deploy and manage all resources at one time.","correct":false},{"id":"e208251ba3e5646edab96df0da85794c","text":"Upload stack templates to an S3 bucket. Create a root CloudFormation stack to use the uploaded templates with the resource type of \"AWS::CloudFormation::Template\". Configure the \"TemplateURL\" field with the template location in S3.","correct":false}]},{"id":"4ebf4191-8562-4f67-945d-ea5aae2f9f26","domain":"awscsapro-domain3","question":"You are consulting for a client who is trying to define a comprehensive cloud migration roadmap.  They have a legacy custom ERP system written in RPG running on an AS400 system.  RPG programmers are becoming rare so support is an issue.  They run Lotus Notes email which has not been upgraded in years and thus out of support.  They do have a web application that serves as their CRM created several years ago by a consulting group.  It is a Java and JSP-based application running on Tomcat with MySQL as the data layer hosted on a Red Hat Linux server. The company is in a real growth cycle and realizes their current platforms cannot sustain them.  So, they are about to launch a project to implement SAP as a replacement for their legacy ERP system over the next year.  What migration strategy would you recommend for their landscape that would allow them to modernize as soon as possible?","explanation":"In this case, retiring Lotus Notes is the better move because it would just prolong the inevitable by simply migrating to EC2.  The CRM system is fairly new and can be re-platformed on Elastic Beanstalk.  Due to the impending ERP upgrade, it makes no sense to do anything with the legacy ERP.  It would take lots of work to port over an RPG application to run on AWS--if it's even possible.","links":[{"url":"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/","title":"6 Strategies for Migrating Applications to the Cloud | AWS Cloud Enterprise  Strategy Blog"}],"answers":[{"id":"c7be9ce93a4a7a74b44e281cb697dcc4","text":"Begin a product search for a new CRM system that is cloud-ready.  Once identified, migrate the existing CRM into the new CRM system.  Migrate Lotus Notes to Workmail using the AWS Migration Hub.  Invest in training the IT staff about AWS through a Certified AWS Training Partner.  Provision and run the various SAP environments from scratch using EKS.  Do nothing to the legacy ERP until the SAP implementation is complete.","correct":false},{"id":"20ef21af4ab4ecda70e90e90861b154c","text":"Retire the Lotus Notes email and implement AWS Workmail.  Replatform the CRM application Tomcat portion to Elastic Beanstalk and the data store to MySQL RDS.  Invest time in training Operations staff CloudFormation and spend time architecting the landscape for the new SAP platform.  Do nothing to the legacy ERP platform until the SAP implementation is complete.  ","correct":true},{"id":"7579c185856bdbbcffbe49a649866488","text":"Rehost Lotus Notes mail on EC2 instances.  Refactor the CRM application to make use of Lambda and DynamoDB.  Use a third-party RPG to Java conversion tool to create Java versions of the legacy ERP to make it more supportable. Invest time in training developers continuous integration and continuous deployment concepts.  Because SAP implementations always take longer than estimated, rehost the legacy ERP system on EC2 instances so the AS400 can be retired.","correct":false},{"id":"480496ebe4100958e2f466291752ae2d","text":"Retire the CRM application and migrate the MySQL data over to Aurora.  Use QuickSight to provide access to the application for users.  Pay back support agreements to bring Lotus Notes back into support so it can be upgraded.  Migrate Notes email to EC2 instances.  Invest time in training Operations staff CloudFormation.  Create the complete SAP landscape as scriptable elements.  Do nothing to the legacy ERP platform until the SAP implementation is complete.","correct":false}]},{"id":"c3ac5de9-a343-4cde-af1b-6c9f89824d2f","domain":"awscsapro-domain5","question":"An external auditor is reviewing your process documentation for a Payment Card Industry (PCI) audit.  The scope of this audit will extend to your immediate vendors where you store, transmit or process cardholder data.  Because you do store cardholder data in the AWS Cloud, the auditor would like to review AWS's PCI DSS Attestation of Compliance and Responsibility.  How would you go about getting this document? ","explanation":"AWS Artifact provides on-demand downloads of AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use. You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls.","links":[{"url":"https://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html?icmpid=docs_artifact_console","title":"What Is AWS Artifact? - AWS Artifact"}],"answers":[{"id":"1d16d307ee989a80e421198a01993a9c","text":"AWS IAM Console","correct":false},{"id":"d9208942349d1c6f7dbaba3661069bc1","text":"AWS WorkDocs","correct":false},{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false},{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":true},{"id":"d7cb47dd1f374d3ed079b14cc6f2cd75","text":"Submit a Support Case requesting the document","correct":false},{"id":"09e838e873f25f954fef911d50b3d1ab","text":"AWS Pinpoint","correct":false},{"id":"63df0d05cd43af35c95cf04d92aaf685","text":"AWS Legal Services website","correct":false}]},{"id":"230f422f-7118-4096-8dce-59c642fb55c8","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a new Direct Connect connection.  The connection is up and you can ping the AWS peer IP address, but the BGP peering session cannot be established.  What should be your next logical troubleshooting steps?","explanation":"Because the connection is up and we can ping the AWS peer, the problem must be at a higher level on the OSI model than the Physical or Data layers.  BGP uses TCP port 179 to communicate routes so we should check that no NACL or SG is blocking it.  Additionally, we should make sure the ASNs are properly configured in the proper ranges.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/Troubleshooting.html","title":"Troubleshooting AWS Direct Connect - AWS Direct Connect"}],"answers":[{"id":"16e5aea88df69cc18f99e3f066ec99c1","text":"Ensure that the local ASNs and AWS-side ASNs are properly configured.","correct":true},{"id":"8fc27418eee2ce07b64bc672007d2c1b","text":"Contact the co-location provider and request a written report for the Tx/Rx optical signal across the cross connect.","correct":false},{"id":"45d4c1753395277878b9a17343628c52","text":"Ensure that the VLAN is configured properly between your on-prem router the provider. ","correct":false},{"id":"81977d7a1eb5714746851077b93f44d6","text":"Power cycle all the equipment to clear ARP table cache.","correct":false},{"id":"3d2a55832b90f19a2137e8715525d717","text":"Make sure no firewalls or ACLs are blocking TCP port 179 or any high-numbered ephemeral ports.","correct":true},{"id":"edd3f9408cecbbf9182678ccc51d7981","text":"Ask your network provider to provide you with a cross connect completion notice and compare the ports with those listed on your LOA-CFA","correct":false}]},{"id":"b06ef2a9-b122-4b47-b5be-b6d604e78405","domain":"awscsapro-domain2","question":"You are working with a customer to implement some better security policies.  They have a group of remote employees working on a confidential project that uses some proprietary Windows software and stores data in S3.  The Chief Information Security Officer is concerned about the threat of the desktop software or confidential data being smuggled out to a competitor.  What architecture would you recommend to best address this concern? ","explanation":"Using a locked down virtual desktop concept would be the best way to manage this.  AWS WorkSpaces provides this complete with client software to log into the desktops.  These Workspaces can be walled off from the Internet.  Using policies, you could allow access from only those in the Workspaces VPC.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html","title":"What Is Amazon WorkSpaces? - Amazon WorkSpaces"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html","title":"Endpoints for Amazon S3 - Amazon Virtual Private Cloud"}],"answers":[{"id":"21d0f58369770fb84d6bff8bfcf9c265","text":"Use Service Catalog to deploy and manage the proprietary Windows software to the remote employees.  Create an OpenVPN server instances within a VPC.  Create an VPC Interface Endpoint to S3 and use a security group to only permit traffic from the OpenVPN server security group.  Supply the remote employees with instructions to install and login using OpenVPN client software.","correct":false},{"id":"36e6b9fb8ba6dd4b59c9f032ba6f78f7","text":"Provision Windows 2016 instances in a private subnet.  Create a specific security group for the Windows machines permitting only SSH inbound.  Create a NACL which allows traffic to S3 services and explicitly deny all other network traffic to and from the subnet.  Assign an S3 bucket policy that only allows access for members of the Windows machine security group.","correct":false},{"id":"b5b5a93dca37986129c444b7654d600f","text":"Provision Amazon Workspaces in a secured private VPC.  Do not enable Internet access for the Workspaces.  Create a VPC Gateway Endpoint to S3 and implement an endpoint policy that explicitly allows access to the required bucket.  Assign an S3 bucket policy that denies access unless the sourceVpce matches the VPC endpoint.  Supply the users with instructions on downloading and login into the Workspaces instances.","correct":true},{"id":"f8c297565cec662fd215e6c551daca36","text":"Create a bucket policy using the sourceIP condition to only allow access from a specific VPC CIDR.  Apply a NACL which only permits inbound port 22 and outbound ephemeral ports.  Deploy Amazon Workspaces in the VPC and disable internet access.  Supply the users with instructions on downloading and login into the Workspaces instances.","correct":false}]},{"id":"edf9ffa9-02ec-4341-a179-577cd590543e","domain":"awscsapro-domain1","question":"You work for a technology product company that owns two AWS Accounts - Prod and DevTest, both belonging to the same Organizational Unit (OU) under AWS Organizations Root. There are three different teams in your company - Dev Team, Testing Team and Ops Team. While Dev and Testing Team members have IAM Users created in the DevTest account, the Ops Team members have IAM Users created in the Prod account. There is an S3 bucket created in the Prod account that Testing Team members need access to - they need both read and write access. What is the best way to give the Testing Team members access to the Prod account S3 bucket?","explanation":"Cross-Account Access is best achieved using IAM Cross-Account Roles. The solution that suggests that the Testing Team members have to sign in to a different AWS account every time they need to access the S3 bucket is not correct as it is inefficient and unproductive.\nThere is no such AWS Organization feature that can directly enable Cross-Account Access from the console. There no way to select or deselect groups or users in this manner. Hence, the option that suggests using these is eliminated.\nThe remaining two options are a play in words. Carefully read both options. The Role must be created in the Trusting Account, in this case Prod Account because it has the Resource (S3 bucket) that needs to be accessed by the someone from another AWS Account, i.e., the Trusted Account.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html","title":"Tutorial - Delegate Access Across AWS Accounts Using IAM Roles"},{"url":"https://aws.amazon.com/organizations/","title":"AWS Organizations"},{"url":"https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/","title":"How to Enable Cross-Account Access to the AWS Management Console"}],"answers":[{"id":"87953f71b8915388e4ecb3f7b2378374","text":"Create an IAM Role in the DevTest Account that allows access to the Prod Account, thus establishing trust between the Accounts. Attach a Trust Policy to the Role that grants the Testing Team members permission to assume the Role. Attach an Access Policy to the Testing Team's IAM Users in the DevTest Account that allows them to call STS AssumeRole for the specific Resource whose value is the ARN of the Role in Prod Account. Attach a Bucket Policy to the S3 Bucket that specifies the IAM Users of Testing Team members as Principal.","correct":false},{"id":"56e4fe89e8a8ffb36bfcd67126afa0d5","text":"Create an IAM Role in the Prod Account that allows the DevTest Account to assume it, thus establishing trust between the Accounts. Attach an Access Policy to the Role that allows access to the S3 bucket. Attach a Bucket Policy to the S3 Bucket that specifies the ARN of the Role as Principal. Attach an Access Policy to the Testing Team's IAM Users in the DevTest Account that allows them to call STS AssumeRole for the specific Resource whose value is the ARN of the Role in Prod Account.","correct":true},{"id":"e91ff3381d499ccd06e7891c37a77891","text":"Create IAM Users for the Testing Team members in the Prod account. Create a Testing IAM Group in the Prod account and add the IAM Users of the Testing team members to the Group. Assign an access policy to the Testing Group in the Prod account that grants Read and Write access to the correct S3 bucket. Testing team members will sign into the Prod account to access the S3 bucket.","correct":false},{"id":"e661ebc45484fb5a715bc96c884b4024","text":"Enable Cross-Account Access at the AWS Organizational Unit (OU) level from the console. Deselect the Dev Team UAM Users from Cross-Account Access Setup Wizard. This will allow only the Testing Team members to be able to access the Prod account. Write a bucket policy for the S3 bucket that lists the Testing Team members as Principals who are allowed to access the bucket.","correct":false}]},{"id":"f02ba751-479b-4ff0-a09b-8f18a63177b5","domain":"awscsapro-domain3","question":"An automotive supply company has decided to migrate their online ordering application to AWS. The application leverages a Model-View-Controller architecture with the user interface handled by a Tomcat server and twenty thousand lines of Java Servlet code. Business logic also resides in two thousand lines of PL/SQL stored procedure code in an Oracle database. The company's technology leadership has directed your team to move the database to a more cost-effective offering, and to adopt a more cloud-native architecture. Business objectives dictate that the application must be live in the AWS cloud in sixty days. Which migration approach will provide the most scalable architecture and meet the schedule objectives?","explanation":"This solution will require trade-offs between schedule requirements and architectural desires. Converting twenty thousand lines of Model-View-Controller code to a serverless architecture in sixty days is unreasonable, so moving the Tomcat MVC as-is to EC2 for the initial migration is the best approach. We can migrate to a serverless user interface in a later phase. Database Migration Service will suit our needs well for moving the application data to Aurora, but the most scalable architecture strategy is to migrate the stored procedure code out of the database so that database nodes won't need to be resized when the business logic needs more compute resources. Under normal circumstances, recoding two thousand lines of PL/SQL code to Python Lambda functions within a sixty day time frame will not be a problem.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/blogs/database/migrate-your-procedural-sql-code-with-the-aws-schema-conversion-tool/","title":"Migrate Your Procedural SQL Code with the AWS Schema Conversion Tool"},{"url":"https://aws.amazon.com/lambda/","title":"AWS Lambda"}],"answers":[{"id":"db222d8a15bda541fc4147908131cfd6","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service to move the application data into Amazon Aurora. Convert the stored procedure code to AWS Lambda Python functions, and modify the Servlet code to invoke them","correct":true},{"id":"97a348ed01424c357958b49bcc030935","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"2cf84f7f8daee7548143ad181423c7cb","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"5ab40b2a54c4e82a7aafa05c8fc9a458","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service to migrate the application data and stored procedures to an Amazon RDS Oracle instance","correct":false}]},{"id":"f37f4967-9ef0-4cec-b63f-15b52dc44ca2","domain":"awscsapro-domain2","question":"You are an AWS solutions architect in a company. A team is building up a new application using AWS resources including application load balancers. In order to capture detailed information about the requests to the load balancers, all application load balancers need to activate the access logs and save the log files in an S3 bucket. The access logs should be encrypted when they are stored in the S3 bucket for data protection. How would you enable the encryption for access logs?","explanation":"The encryption at rest should be configured in the S3 bucket rather than the ELB access logs. For access logs of application load balancers, only the server-side encryption with Amazon S3-managed encryption keys (SSE-S3) is supported. Users cannot store the access logs in an S3 bucket where encryption with SSE-KMS is configured. For ELB access logs, you cannot perform the client-side encryption before the files are transferred in the S3 bucket. As a summary, the server-side encryption with SSE-S3 is the correct method.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html","title":"Access Logs for Your Application Load Balancer"}],"answers":[{"id":"01a3559ab89c1a90f64a2d125b0ca2d4","text":"Create a customer managed key (CMK) in AWS Key Management Service (KMS). Add the \"delivery.logs.amazonaws.com\" service as the key user. Enable server-side encryption with AWS KMS-Managed Keys (SSE-KMS) by selecting the CMK in the S3 bucket.","correct":false},{"id":"6050b8b3764542173c87d57af013f345","text":"Create a Lambda function using Python boto3 to enable access logging for the application load balancers and activate the encryption feature. Configure an S3 bucket policy to allow the \"delivery.logs.amazonaws.com\" service to put objects.","correct":false},{"id":"efeff66ea347b9c1586198badebdd3f1","text":"Generate a symmetric encryption key locally, upload the key to S3 and enable client-side encryption with Amazon S3 by modifying the default encryption in the bucket properties. Make sure the encryption key is stored safely.","correct":false},{"id":"7105831f473423801649e7f1302abac6","text":"In the AWS S3 Management Console, enable server-side encryption with Amazon S3-Managed Keys (SSE-S3) by modifying the default encryption to be AES-256.","correct":true}]},{"id":"969eccbb-023f-4b5d-ba4c-d14c22eadc02","domain":"awscsapro-domain4","question":"Your company has been running its core application on a fleet of r4.xlarge EC2 instances for a year.  You are confident that the application has a steady-state performance and now you have been asked to purchase Reserved Instances (RIs) for a further 2 years to cover the existing EC2 instances, with the option of moving to other Memory or Compute optimised instance families when they are introduced.  You also need to have the option of moving Regions in the future. Which of the following options meet the above criteria whilst offering the greatest flexibility and maintaining the best value for money.","explanation":"When answering this question, it's important to exclude those options which are not relevant, first.  The question states that the RI should allow for moving between instance families and this immediately rules out Standard and Scheduled RIs as only Convertible RIs can do this.  Of the 2 Convertible RI options, on can be ruled out as it suggests selling unused RI capacity on the Reserved Instance Marketplace, but this is not available for Convertible RIs and therefore that only leaves one answer as being correct.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html","title":"Types of Reserved Instances (Offering Classes) - Amazon Elastic Compute  Cloud"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html","title":"Scheduled Reserved Instances - Amazon Elastic Compute Cloud"}],"answers":[{"id":"b46dcc09115bb8c1683455addbc4fa46","text":"Purchase a 1 year Convertible RI for each EC2 instance, for 2 consecutive years running","correct":true},{"id":"41620e7974f0d2cca7f57d1972e30387","text":"Purchase a Convertible RI for 3 years, then sell the unused RI on the Reserved Instance Marketplace","correct":false},{"id":"89987ea3ddc88d352336561f25d83387","text":"Purchase a 1 year Standard Zonal RI for 3 years, then sell the unused RI on the Reserved Instance Marketplace","correct":false},{"id":"188dc576a2e69c4182330092ab7f3786","text":"Purchase a Scheduled RI for 3 years, then sell the unused RI on the Reserved Instance Marketplace","correct":false}]},{"id":"0a4b2449-9275-4c2f-af02-0f8c51614f3a","domain":"awscsapro-domain2","question":"You are part of a business continuity team at a consumer products manufacturer.  In scope for the current project is the company web server which serves up static content like product manuals and specification sheets which customers can download.  This landscape consists only of a single NGINX web server and 5TB of local attached storage for the static content.  In the case of a failover, RTO has been defined as 15 minutes with RPO as 24 hours as the content is only updated a few times a year.  Staff reductions and budget constraints for the year mean that you need to carefully evaluate and choose the most cost-effective and most automated solution in the case of a failover.  Which of the following would be the most appropriate given the situation?","explanation":"In this case, the most cost-effective and most automated way to ensure the reliability statistics would be to migrate the static content to S3.  This option has built-in robustness and will cost less than any other option presented.","links":[{"url":"http://d36cz9buwru1tt.cloudfront.net/AWS_Disaster_Recovery.pdf","title":"Using Amazon Web Services for Disaster Recovery"}],"answers":[{"id":"bceaccaea071754d3724eaf31f0f6189","text":"Migrate the website content to an S3 bucket configured for static web hosting.  Create a Route 53 alias record for the web server domain.  End-of-life the on-prem web server.","correct":true},{"id":"46b84866da301243767946743c6024a1","text":"Download and configure the AWS Storage Gateway, creating a volume which can be replicated to AWS S3.  Attach that volume to the web server via iSCSI and migrate the content to that Storage Gateway volume.  Locate an AMI from the AWS Marketplace for NGINX.  If a failover is required, manually launch the AMI and run an RSYNC between the on-prem server and the EC2 server to migrate the content.","correct":false},{"id":"925fb4f48c2c90011e7e1f92d3412dcd","text":"Migrate the static content to an EFS share.  Mount the EFS share via NFS from on-prem to serve up the web content.  Configure another EC2 instances with NGINX to also mount the same share.  Upon fail-over, manually redirect the Route 53 record for the web server to the IP address of the EC2 instance.","correct":false},{"id":"2d14eb477a2f2c9dc3605ea5740297cd","text":"Create a small pilot-light EC2 instance and configure with NGINX. Configure a CRON job to run every 24 hours that syncs the data from the on-prem web server to the pilot-light EC2 EBS volumes.  Configure an Application Load Balancer to direct traffic to the on-prem web server until a health check fails.  Then, the ALB will redirect traffic to the pilot light EC2 instances. ","correct":false},{"id":"69fd92fc5be4948bfc0128d02ed2f392","text":"Install the CloudWatch agent on the web server and configure an alarm based on a health check.  Create an EC2 replica installation of the web server and stop the instances.  Create a Lambda function that is triggered by the health check alarm which starts the dormant EC2 instance and updates a DNS entry in Route 53 pointing to the new server.","correct":false}]},{"id":"1727d24a-6bf1-4fc6-b99b-24c0bc4552e9","domain":"awscsapro-domain2","question":"NextGen Appliances Corporation is developing a new smart refrigerator that sends telemetry data to a backend application running on AWS. The refrigerator will also be able to receive commands from a mobile dashboard app that will be provided with the product. Features of the mobile dashboard app include data query, analytics, notifications, and settings commands. The mobile dashboard app will be developed using AWS Amplify and will be deployed to an Amazon S3 bucket configured for web hosting. The software on the refrigerator will be developed with the AWS IoT Device SDK, and will communicate with AWS IoT Core. How should the company architect the backend application to provide management capabilities for the refrigerator from the mobile dashboard app?","explanation":"The smart refrigerator solution has an AWS IoT Core rule set up to route messages to a Lambda function which stores event data in DynamoDB and sends desired notifications to an SNS topic for viewing in the mobile dashboard app. The mobile dashboard app can be developed in AWS Amplify, and deployed to an S3 bucket configured for web hosting. CloudFront can be used to provide public access to the bucket. The app can send RESTful API requests to the backend through API Gateway to a Lambda function that performs the DynamoDB queries and sends commands to the refrigerator through IoT Core. Analytics requests can be routed to QuickSight Mobile, which uses IOT Analytics as its data source. The mobile app won't be able to send settings commands to IoT Core directly. API Gateway and Lambda need to be involved for that. QuickSight can't use Kinesis Data Analytics as a data source. The data will need to be written to S3 for QuickSight queries. IoT Analytics will provide analytics capabilities more suited to this use case than general solutions like Kinesis Data Analytics and Redshift. Amazon Pinpoint is used for sending personalized marketing messages, not general notifications. ","links":[{"url":"https://aws.amazon.com/iot-analytics/","title":"AWS IoT Analytics"},{"url":"https://aws.amazon.com/amplify/","title":"AWS Amplify"},{"url":"https://docs.aws.amazon.com/quicksight/latest/user/using-quicksight-mobile.html","title":"Using the Amazon QuickSight Mobile App"},{"url":"https://aws.amazon.com/solutions/smart-product-solution/?did=sl_card&trk=sl_card","title":"Smart Product Solution"}],"answers":[{"id":"57c04a93b4f460af88471726dcca35b2","text":"Send IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and publishes notifications to an Amazon Simple Notification Service topic. Also write IoT Core messages to AWS IoT Analytics. Have the mobile dashboard app call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses IoT Analytics as its source.","correct":true},{"id":"af8854f867f77abde2a38d7b3d1d84b7","text":"Route IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and sends notifications to Amazon Pinpoint. Also write IoT Core messages to an Amazon S3 bucket to be loaded into Amazon Redshift by AWS Glue. Have the mobile dashboard app call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses Redshift as its source.","correct":false},{"id":"5840ee0fec04ee6fca146272a5e75892","text":"Write IoT Core messages to Amazon Kinesis Data Streams with Amazon Kinesis Data Analytics as one consumer and Amazon EC2 instances with Auto Scaling as a second consumer. Have the EC2 instances write product event data to Amazon DynamoDB and publish notifications to an Amazon Simple Notification Service topic. Configure the mobile dashboard app to call RESTful APIs hosted by Amazon API Gateway, which are backed by other Lambda functions to perform data queries against DynamoDB, and to send settings commands to the refrigerator. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses Kinesis Data Analytics as its source.","correct":false},{"id":"f0ebf0f93a7448dab82c61c11dcc2c4a","text":"Deliver IoT Core messages to an AWS Lambda function that writes product event data to Amazon DynamoDB and publishes notifications to an Amazon Simple Notification Service topic. Also write IoT Core messages to AWS IoT Analytics. Have the mobile dashboard app send settings commands to AWS IoT Core. Configure the AWS Mobile Hub NoSQL Database option and register the DynamoDB event table for app data queries. Have the app redirect analytics requests to Amazon QuickSight Mobile, which uses IoT Analytics as its source.","correct":false}]},{"id":"1239c235-107c-4f5e-8bac-9dc824c00680","domain":"awscsapro-domain5","question":"You are helping a client with some process automation.  They have managed to get their website landscape and deployment process encapsulated in a large CloudFormation template.  They have recently contracted with a third-party service to provide some automated UI testing.  To initiate the test scripts, they need to make a call out to an external REST API.  They would like to integrate this into their existing CloudFormation template but not quite sure of the best way to do that.  Help them decide which of the following ideas is feasible and incurs the least extra cost.","explanation":"To integrate external services into a CloudFormation template, we can use a custom resource.  Lambda makes a very good choice for this scenario because it can handle some logic if needed and make a call out to an external API.  Using an EC2 instances to make this call is excessive and we likely would not have the ability to configure the third-party API to poll an SQS queue.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html","title":"AWS Lambda-backed Custom Resources - AWS CloudFormation"}],"answers":[{"id":"f761e84ee0cd0f689465458a41b69fae","text":"Include an SQS queue definition in the CloudFormation template.  Define a User Script on the deployed EC2 instance which will insert a message into the SQS queue only once it has fully booted.  Configure the external REST API to use long polling to check the queue for new messages in order to initiate the testing process.","correct":false},{"id":"6b3b26e17f2323a91f04f792f0c2d20c","text":"Create a Lambda function which issues a call out to the external REST API using the POST method.  Define a custom resources in the CloudFormation template and associate the Lambda function and execution role with the custom resource.  Include DependsOn to ensure that the function is only called after the other instances are ready.","correct":true},{"id":"43569df3ec4b7db0265dea4051c04644","text":"Add a small EC2 instance definition to the CloudFormation template.  Define a User Script for that instance which will install a custom application from S3 to call out to the external REST API endpoint using the POST method to trigger the testing process.  Add a CleanUp parameter to the EC2 instance definition that will shut down the instance once the activity has completed.","correct":false},{"id":"97a123d11bbf3be0e3e1788e2f0874ac","text":"Add an API Gateway deployment to the CloudFormation template.  Add the DependsOn parameter to the API Gateway resource to ensure that the call to the external API only happens after all the other resources have been created.  Create a POST method and define it as a proxy for the external REST API endpoint.  Using SWF, call the API Gateway endpoint to trigger the testing process.","correct":false}]},{"id":"6da286f8-23a6-4e8a-a3a4-c7b496a06523","domain":"awscsapro-domain5","question":"An online health foods retailer stores its product catalog in an Amazon Aurora database. The catalog contains over 6,000 products. They'd like to offer a product search engine on the website using Amazon Elasticsearch Service. They'll use AWS Database Migration Service (DMS) to perform the initial load of the Elasticsearch indexes, and to handle change data capture (CDC) going forward. During the initial load of the indexes, the DMS job terminates with an Elasticsearch return code of 429 and a message stating 'Too many requests'. What must be done to load the Elasticsearch indexes successfully?","explanation":"When the ElasticSearch indexing queue is full, a 429 response code is returned and an es_rejected_execution_exception is thrown. The DMS load task then terminates. Throttling the DMS input stream based on the number of Elasticsearch indexes, shards, and replicas to be loaded will result in a successfully completed job. The DMS MaxFullLoadSubTasks parameter indicates how many source tables to load in parallel, and the ParallelLoadThreads parameter determines the number of threads that can be allocated for a given table. Increasing Elasticsearch shards without modifying DMS subtask and thread parameters could still overrun the request queue. Changing the DMS stream buffer count won't help with this issue. Amazon Elasticsearch currently doesn't provide support for AWS Glue as a source, so integration would require significant effort. Increasing Elasticsearch EBS volume IOPS won't solve an ingress queue overrun problem. The DMS batch split size parameter sets the maximum number of changes applied in a single batch, but doesn't reduce the total number of requests.","links":[{"url":"https://aws.amazon.com/dms/","title":"Amazon Database Migration Service"},{"url":"https://aws.amazon.com/elasticsearch-service/","title":"Amazon Elasticsearch Service"},{"url":"https://aws.amazon.com/blogs/database/scale-amazon-elasticsearch-service-for-aws-database-migration-service-migrations/","title":"Scale Amazon Elasticsearch Service for AWS Database Migration Service migrations"}],"answers":[{"id":"ee750f72f1e70b83f6d83819f2d504f5","text":"Raise the baseline IOPS performance of the Elasticsearch cluster EBS volumes to enable more throughput. Increase the DMS batch split size parameter to send more data in each request and reduce the number of total requests","correct":false},{"id":"62ac117860ebe5397e04bad8ea29a5fb","text":"Replace DMS with AWS Glue for the initial index load and ongoing change data capture. Enable parallel reads when the ETL methods are called in the Glue jobs","correct":false},{"id":"c1e1a85b26a30433a6af5d30c8bb8d76","text":"Calculate the number of queue slots required for the Elasticsearch bulk request as a product of the number of indexes, shards, and replicas. Adjust DMS subtask and thread parameters accordingly","correct":true},{"id":"157e8733386382289486b3592774442f","text":"Increase the number of Elasticsearch shards for each index to increase distribution of the load. Change the DMS stream buffer count parameter to match the number of Elasticsearch shards","correct":false}]},{"id":"b00cb57f-7191-4f17-aa6d-ac687c418332","domain":"awscsapro-domain5","question":"You have a running EC2 instance and the name of its SSH key pair is \"adminKey\". The SSH private key file was accidentally put into a GitHub public repository by a junior developer and may get leaked. After you find this security issue, you immediately remove the file from the repository and also delete the SSH key pair in AWS EC2 Management Console. Which actions do you still need to do to prevent the running EC2 instance from unexpected SSH access?","explanation":"Although the SSH key pair is deleted in EC2, the public key content is still placed on the instance in an entry within ~/.ssh/authorized_keys. Someone can SSH to the instance if he has a copy of the leaked SSH private key. Users should not configure the instance to support another key pair as the old key pair still works. The correct method is deleting the instance immediately to prevent it from being compromised and launching another instance with a new SSH key pair. There is no need to use the AWS CLI command delete-key-pair as the key is already deleted from AWS EC2.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html","title":"Amazon EC2 Key Pairs"}],"answers":[{"id":"4f9620d4caefa25fafabc98f00c6b192","text":"Use AWS CLI delete-key-pair to completely delete the key pair so that no one can use it to SSH to the instance. Configure CloudWatch logs to monitor the SSH logging events and filter the logs with the SSH key ID to see if the key pair is still used by someone.","correct":false},{"id":"ee7464becbe88068a5f848419e621bba","text":"Stop and terminate the instance immediately as someone can still SSH to the instance using the key. Launch a new instance with another SSH key pair. SSH to the EC2 instance using the new key.","correct":true},{"id":"a928b9732684f81d9ce046842965f1f6","text":"No action is required as the SSH key pair \"adminKey\" is already deleted from AWS EC2. Even if someone has the SSH private key, he still cannot use the key to access the instance.","correct":false},{"id":"bc01758fc758191425928382b18697ca","text":"Create another SSH key pair via AWS EC2 or a third party tool such as ssh-keygen. Stop the instance and configure the instance with this new key pair in AWS Management Console. Restart the instance to activate the key pair.","correct":false}]},{"id":"2c034786-9b7e-4933-aad2-d0c4b1d89ca8","domain":"awscsapro-domain2","question":"A beach apparel company has begun an initiative to improve their sales analytics capabilities using AWS services. They'll need to be able to visualize summary sales data by product line, territory, and sales channel for each day, month, and year, and they'll need to be able to drill-down with ad-hoc queries on individual sales records. There are multiple data sources that provide transactional information in different formats. The company has chosen Amazon QuickSight as their visualization tool for the summary information. Visualizations and drill-down queries will require three years of rolling sales history, which estimates to seven petabytes of data. Which architecture will provide the best performance and cost efficiency?","explanation":"Using S3 to store the detailed sales transaction data and using Lambda to standardize data formats is the most cost effective option. Storing the summary data in Redshift provides a high performance option for reads from QuickSight, and keeping the detailed transaction data out of Redshift allows for smaller node sizes and lower cost. Amazon Redshift Spectrum can be used for drill-down queries that join tables from both Redshift and S3. For answer number two, Redshift will be a better option than Aurora for OLAP query performance due to it's columnar organization. Answer number four provides no simple way to perform ad-hoc drill down queries.","links":[{"url":"https://aws.amazon.com/redshift/","title":"Amazon Redshift"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"a08e38f138d23c0f1759ab2d1801f67e","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Firehose and load them into Amazon Redshift. Run AWS Glue jobs to format the transaction data in a standard way and perform aggregate functions to write the data into summary tables in Redshift","correct":false},{"id":"b3deac265c2195cb988c345d096800fd","text":"Ingest individual sales transactions from each data source into Amazon S3 with Amazon Kineses Data Firehose. Trigger an AWS Lambda function to format the transaction data in a standard way and redeposit the results in S3. Run AWS Glue jobs to aggregate the summary data into Amazon Redshift","correct":true},{"id":"ebf4a6c248510d05a046c8f0ea4298b7","text":"Use Amazon Kinesis Data Analytics to format the data source transactions in a standard way and load it into Amazon Aurora. Invoke Lambda functions to aggregate the data and write it into summary tables in Aurora","correct":false},{"id":"4740b70569f15040edf0916e47386757","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Streams and write them to Amazon Elastic Block Store on EC2 instances in Auto Scaling Groups. Perform data format standardization and summary aggregation on EC2, and write the summary results to Amazon Redshift tables","correct":false}]},{"id":"d8b88385-e15f-4313-bc53-e9fb82f89cc3","domain":"awscsapro-domain2","question":"You are developing an application to be hosted on EC2. The application uses some environmental configuration data and other necessary parameters when running. For example, the application needs to get the correct username and password in order to communicate with a RDS database. You want to find a free AWS service to store these parameters. To meet security requirements, these stored parameters must be encrypted by the AWS Key Management Service. Which of the following methods is the best?","explanation":"You can manage parameters in Systems Manager Parameter Store. The type of the parameters must be SecureString so that they are encrypted by KMS. Parameter Store has standard tier and advanced tier. In this scenario, standard tier is enough and advanced tier is not a free service. AWS Secrets Manager does not have the concept of standard or secure parameter. It also charges you per secret per month.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html","title":"How AWS Systems Manager Parameter Store uses AWS KMS"}],"answers":[{"id":"c08527f2eeacd9f56133d1dfed701f1a","text":"Create secure parameters in AWS Secrets Manager. Secrets Manager protects secrets by integrating with KMS and all stored parameters are automatically encrypted by the AWS managed key \"aws/secretsmanager\". You can also configure Secrets Manager to rotate the secrets.","correct":false},{"id":"4f357d94d3f20ee0d0c3c7c892ef0d70","text":"Create standard parameters in AWS Secrets Manager. Use the advanced tier as it uses envelope encryption to encrypt the stored parameters with KMS. You can also configure Secrets Manager to rotate the stored secrets or API keys automatically.","correct":false},{"id":"41e6f55938cd5c32278c6f4b6708641e","text":"Create standard string parameters in AWS Systems Manager Parameter Store as it is a free service. The parameters are automatically encrypted with envelope encryption by the default AWS managed key (aws/ssm) in KMS. Use AWS Encryption SDK in the application to fetch the parameters.","correct":false},{"id":"3295f034025fe6d21155d228ee3dd0a2","text":"Store secure string parameters in AWS Systems Manager Parameter Store so that the parameters are encrypted by KMS. Use the standard tier as there is no additional charge for it. Use AWS Encryption SDK in the application to get the parameters.","correct":true}]},{"id":"c1333471-d052-4710-bcdb-facadc095d70","domain":"awscsapro-domain5","question":"You are setting up a corporate newswire service for a global news company.  The service consists of a REST API deployed on EC2 instances where customers can retrieve the latest news articles in real-time that happen to contain their company name.  This allows companies to monitor all news sources for stories where they are mentioned.  Because of the worldwide reach of the new site, you want to position servers around the globe.  You want to publish one subdomain name globally (api.domain.com) and have the requesters directed to the nearest region based on latency.  In each region, you want to be able to accommodate blue-green deployments without downtime as well.  What steps do you take?","explanation":"We want to use weighted routing records for local instances so we have the ability to adjust weights and shift traffic during blue-green deployments.  Latency-based routing would take care of funneling requests to the site with the lowest latency.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html","title":"How Health Checks Work in Complex Amazon Route 53 Configurations - Amazon  Route 53"}],"answers":[{"id":"b850f1c18972d022213271c5d673e07f","text":"First setup weighted routing records for the local instances in the region in Route 53.  Assign equal weights with all sharing the same regional subdomain name (us-east-2.api.domain.com).  Next, create latency alias records by creating multiple entries for api.domain.com--each pointing to the regional subdomains.","correct":true},{"id":"41e30b8e30cbd737ced85953d7e3e939","text":"Use CloudFormation to create a distribution of the website.  Create an alias record for the subdomain (api.domain.com) in Route 53 and assign it to the CloudFront distribution.  To ensure no lag in news retrieval, set the maximum TTL on the CloudFront distribution to 0.","correct":false},{"id":"a60aaaf971cbd546c1ec57e08ea38274","text":"We would first create geo-spatial records for the local resources in each region (us-east-2.api.domain.com) and assign equal weights.  Next, we create latency-based routing records for the top level subdomain (api.domain.com) and direct those to the regional records as an alias.  We must also disable Health Check on the latency record to ensure the localized Health Check is used.","correct":false},{"id":"6e56101de6ed2ca97550d5025ddf559a","text":"Using Route 53, we first create the top-level api.domain.com with a geolocation policy.  We then create latency-based routing records for the instances in each region (us-east-2.api.domain.com).  Next, we configure the countries closest to each region in the geolocation policy to direct them to the regional records.","correct":false}]}]}}}}
