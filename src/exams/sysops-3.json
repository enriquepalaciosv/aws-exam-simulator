{"data":{"createNewExamAttempt":{"attempt":{"id":"80a424b0-c805-455d-9aed-3e55b66d294d"},"exam":{"id":"39ac468e-d891-43b4-9a0d-2491082eb64a","title":"AWS Certified SysOps Administrator - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"113a7914-1249-4e12-a748-03392c0570e8","domain":"high-avail","question":"You are a Security Administrator for your company. Your CIO wants to ensure that company data is highly available in multiple AWS Regions. What would you suggest to your CIO as the most effective approach?","explanation":"Deploying a multi-AZ RDS instance would only make it fault tolerant between Availability Zones, and not AWS Regions. Creating a Lambda function and creating/deploying EBS snapshots into different AWS Regions would both be an administrative and operational burden. The easiest, most effective, way is to utilize S3 Cross Region Replication","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html","title":"S3 Cross Region Replication"}],"answers":[{"id":"a4ca2ada50730e36dd0f3302f60ab0dc","text":"Create multiple snapshots of your company data on EBS volumes. Deploy those EBS volumes on EC2 instance in different AWS Regions.","correct":false},{"id":"6e22da724261694b4ecf8fa105ef5174","text":"Enable Cross-Region Replication on your bucket to copy objects to a destination bucket in another AWS Region.","correct":true},{"id":"fbde7989c5f0da5bb91bb5593f9c8e4e","text":"Create a Lambda function that downloads data from your S3 Bucket and executes a PUT operation to upload copied objects into a new bucket in a new Region.","correct":false},{"id":"f876473fb6289d5a1e3e6d449713b0e7","text":"Copy the company data to an RDS instance. Deploy a multi-AZ configuration for your RDS instance to make it highly available.","correct":false}]},{"id":"8c197177-2b38-41eb-9b6c-d7c52de0808c","domain":"security-comp","question":"Which of the following statements is correct?","explanation":"AWS is responsible for physical controls, you are responsible for patching and updating your own operating systems","links":[{"url":"https://aws.amazon.com/compliance/shared-responsibility-model/","title":"Shared Responsibility Model"}],"answers":[{"id":"25147b4a8382ebb44937e157a61975fc","text":"You are responsible for Physical and Environmental controls for your AWS infrastructure","correct":false},{"id":"be49b24b55936f276a5224988013f961","text":"You are responsible for the security configuration and management tasks for any VPCs, EC2 instances and S3 buckets you create","correct":true},{"id":"0f245a67bb96c5c2cfe2a8892b81a868","text":"AWS is responsible for patching and updates of any Operating System you install on your EC2 instances","correct":false},{"id":"d891cc198c4681e4512d4855f795f22e","text":"AWS is responsible for the hardware, software, networking and facilities that run AWS Cloud services","correct":true}]},{"id":"fcccd90f-956b-41b6-b618-bc81dade5d80","domain":"security-comp","question":"You have an EC2 instance in a private subnet. You connect to this instance via SSH using a bastion host in a public subnet. You notice from the logs that other SSH connections are being made from other private IP addresses, which is strange because only the bastion host should be able to connect to this instance. The private IP address of the bastion host is 10.0.1.117. You review the security group, which of the following rules could be causing the unauthorised SSH connections?;","explanation":"By allowing access to port 22 for SGXXXXXXXX this allows any host configured with this security group to access the instance using SSH","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html","title":"Security Groups"}],"answers":[{"id":"80fb1a7299d4a1135830c6f51a49fe82","text":" SSH 22 10.0.1.117/32","correct":false},{"id":"f7bc2e53005e98bab5ea06b5b23bfcf7","text":" HTTP 80 10.0.1.117/32","correct":false},{"id":"c3b35087d573e2c25bc7f358f1bf77d0","text":"MySQL 1433 10.0.1.117/32","correct":false},{"id":"b0172715fbf8cb2b18f5db13e284ca91","text":"SSH 22  SGXXXXXXXX","correct":true}]},{"id":"50f6b26c-7aa5-4d19-89a1-6d042000b087","domain":"data-man","question":"You would like to use SQL to query your CloudWatch logs. Which service can you use to do this?","explanation":"You can export CloudWatch data to S3 and use Athena to query the data using standard SQL.","links":[{"url":"https://aws.amazon.com/athena/faqs/","title":"Athena FAQs"}],"answers":[{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":false},{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":false},{"id":"582ca45acfd3e21caca8b786c1413850","text":"Athena","correct":true},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":false}]},{"id":"f253314b-765f-471a-a5de-bc5f6095164a","domain":"dep-prov","question":"An application server running in an autoscaling group is terminating and relaunching every few minutes. What is the most likely cause?","explanation":"When an instance is seen to be terminating and relaunching regularly (commonly known as 'flapping' or 'thrashing'), the most likely cause is that the Autoscaling group is marking the instance as unhealthy to trigger a replacement.  This can be caused if the Load Balancer health check has been improperly configured- for instance if a missing security group rules means the ALB cannot perform health checks. or the health check too aggressively marks instances as unhealthy before launch has completed (i.e. before userdata has finished.  Termination of an autoscaling instance when using Spot fleet is common, but to see the regular launch and termination would suggest this is health-check related rather than due to spot price changes.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html#health-check-grace-period","title":"EC2 AutoScaling - Replacing Unhealthy Instances"}],"answers":[{"id":"ef10fce024d97295098b2e7c8b9de34e","text":"The Launch Configuration is using an unsupported AMI for your Availability Zone.","correct":true},{"id":"ceb2643dc2123b2807f446e103bad7ff","text":"The price for the EC2 spot instance has increased to above the maximum price for your autoscaling group Launch Configuration.","correct":false},{"id":"22b774b395936efd71fe7ddaf41a3f8a","text":"The autoscaling health check is marking the instance as unhealthy before it has time to initalise fully.","correct":false},{"id":"18d69cdccb048f832573792064f79345","text":"There is a temporary outage in the AWS Autoscaling service in that region.","correct":false}]},{"id":"60a05f6f-23b7-4f0d-b2bf-0e29c276f8ce","domain":"automation","question":"You are working on an application that has multiple phases: development, staging, and production. Each phase runs on Amazon EC2 with an Amazon EBS volume. You use an Application Load Balancer to manage the application's traffic and CloudWatch metrics to collect metric data for each phase. You need to find an efficient way to manage the services, and modify the settings of each phase of your application. What is the most effective way of doing this?","explanation":"You can use a single page to view and manage your resources using AWS Resource Groups. Check your resources for each stage of your application by opening the resource group. View the consolidated information on your resource group page. To modify a specific resource, choose the resource's links on your resource group page to access the service console that has the settings that you need. Without Resource Groups, you would have to access multiple consoles but this is unncessary. OpsWorks is configuration management service that provides managed instances of Chef and Puppet. Service Catalog is to create and manage catalogs of IT services that are approved for use on AWS for compliance.","links":[{"url":"https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html","title":"What Is AWS Resource Groups?"}],"answers":[{"id":"47d6536a1b47257e2ed3d370744d7291","text":"Use AWS OpsWorks to automate operational tasks across your AWS resources, view operational data for monitoring and troubleshooting, and take action on your groups of resources.","correct":false},{"id":"2dbb34771a820b660e255fc85e30bb10","text":"Create portfolios of products related to each phase of the application. Group the products to easily manage and update the services within a portfolio using version control.","correct":false},{"id":"1ed632c93b70e6d21a83ffdc04373d13","text":"Create a custom console in AWS Resource Groups that organizes and consolidates information based on criteria specified in tags, or the resources for each phase of your application.","correct":true},{"id":"8b78b4f2fe4a52d464dc6d4e66bc684d","text":"Open multiple consoles to check the status of your services and to apply the necessary modifications to each phase's settings.","correct":false}]},{"id":"a8410bf2-3c51-49cc-8e44-ca3a1241eff2","domain":"dep-prov","question":"A developer has a monolithic Python application which is being migrated and refactored to use a microservice architecture. The developer has decided to use multiple AWS Lambda functions to run the Python code. During the preparation of the Lambda functions, the developer has noticed that the code requires several third party dependencies which are not part of the standard library. What needs to be done to ensure that the Lambda function code referencing the third party libraries can be executed properly?","explanation":"In order for a Lambda function to run scripts that involve third party libraries, a deployment package needs to be generated in a Lambda-like environment and uploaded to the Lambda or an S3 bucket. This is automatically done by tools like the SAM CLI. Third party libraries are not stored in EFS, ECR, or AppSync.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html","title":"Lambda Python - How to Create a Deployment Package"}],"answers":[{"id":"de93e97f2545dd02641eecd1a42fba8d","text":"Store the third party libraries in EFS and link the Lambda function to the EFS store.","correct":false},{"id":"e45dd8b567270f7faf25f4111f41fa9e","text":"Generate a deployment package containing the code and the third party libraries.","correct":true},{"id":"b41396a5bc20297186c72b266cf8be9e","text":"Use AppSync to manage and store the third party libraries.","correct":false},{"id":"d3ae725bd59d297fd27e71f3db582c01","text":"Store the third party libraries in ECR and link the Lambda function to the ECR repo.","correct":false}]},{"id":"3a6ecc9f-2e62-4807-b2f7-0d4f0e032cc3","domain":"networking","question":"You're configuring an Elastic Load Balancer. What can you do to ensure that a user request always goes to the same server?","explanation":"You can use the sticky session feature (also known as session affinity) to enable the load balancer to bind a user's session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html","title":"Sticky Sessions for Your Classic Load Balancer"}],"answers":[{"id":"55c493073aa412bee5b26fa084e13092","text":"Enable Zonal Failover","correct":false},{"id":"4ff5c8a41331ce8a8718cf02c632ff5d","text":"Enable sticky sessions","correct":true},{"id":"fe11d265f3c285c0d795d272a9eff45a","text":"Use Multi-Zone Load Balancing","correct":false},{"id":"6586c5993a69b3d3955cc5bf228a0792","text":"Enable Connection Draining","correct":false}]},{"id":"67a5ebe3-a3e7-426e-b6f1-ae8e7e58ba45","domain":"dep-prov","question":"You're using AWS CloudFormation template to provision an environment that consists of Amazon EC2 instances behind a load balancer. You want to output the DNS name of the load balancer in the Outputs section. Which intrinsic function should you use?","explanation":"The Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template. To return a string containing the DNS name of the load balancer with the logical name myELB, you could use \"Fn::GetAtt: [ myELB, DNSName ]\" in a YAML template.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html","title":"Fn::GetAtt"}],"answers":[{"id":"f39ff73d166b2d89703ba99b860cc1d2","text":"Fn::Join","correct":false},{"id":"fdf1a9a682e23e506b7afbdd8a427126","text":"Fn::GetAtt","correct":true},{"id":"45e374d45acef37653e6212bf389674d","text":"Fn::FindInMap","correct":false},{"id":"9755e3403594515213433801d07f5476","text":"Fn::Select","correct":false}]},{"id":"a5022f74-8a68-451a-8291-b3df925d05ac","domain":"security-comp","question":"You start your new job at CISO at a company which specialises in creating underground tunnels using electric boring/tunnelling machines. The company would like to revolutionise the transport industry by creating underground tunnels across major metropolitan cities at a fraction of the cost that is currently available. Your job is quite boring, however to add some excitement to your work you’ve been asked to automatically identify any S3 buckets that are public. Which AWS service can you use to quickly determine this?","explanation":"AWS Config includes rules that allow you to identify buckets that have public read or write enabled.","links":[{"url":"https://aws.amazon.com/blogs/aws/aws-config-update-new-managed-rules-to-secure-s3-buckets/","title":"AWS Config and S3"}],"answers":[{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":false},{"id":"7c90c8f2a24f3a1a28525f19fb2c75ab","text":"AWS Inspector","correct":false},{"id":"5714e9332e476d05d9a1763a1b10be50","text":"AWS CloudWatch","correct":false},{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":true}]},{"id":"8a1eae53-9489-4424-904d-6cf15dc1a0a0","domain":"security-comp","question":"The R&D group at your company has begun developing on AWS. Most applications have a short lifespan due to initiatives either not moving forward or entering a product life cycle in another department. Security policy dictates that AWS IAM roles be used for authentication and authorization of AWS services used by all applications. Policy also requires the removal of IAM roles that have had no activity for sixty days. Which solution will provide the most operationally efficient way to identify roles that don't comply with security policy?","explanation":"The IAM API provides information about when a role has last been used. A Lambda function invoked by an AWS Config custom rule can cycle through IAM roles to identify which are compliant and which are non-compliant. Trusted Advisor doesn't provide a check for inactive IAM roles. CloudWatch Alarm metrics don't exist for IAM role activity. Filtering CloudTrail events will miss non-compliant roles that have no API records written.","links":[{"url":"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules.html","title":"AWS Config Custom Rules"},{"url":"https://aws.amazon.com/blogs/security/continuously-monitor-unused-iam-roles-aws-config/","title":"Continuously monitor unused IAM roles with AWS Config"}],"answers":[{"id":"2799421eb19492036b61a12bcb8445d4","text":"Create an AWS Config custom rule that invokes an AWS Lambda function to identify IAM roles that have been inactive for sixty days or more. Have the Lambda function write compliance status back to AWS Config.","correct":true},{"id":"d7ec8812d614132d00b53e4d6ae7e537","text":"Regularly invoke a Lambda function to refresh AWS Trusted Advisor security checks. Create Amazon CloudWatch Events rules that monitor the Trusted Advisor checks and send status events to an Amazon Simple Notification Service topic when a role is no longer compliant.","correct":false},{"id":"08d10e1bbf40689f1b82d560dabddbbc","text":"Employ AWS CloudTrail to publish to Amazon CloudWatch Logs. Filter the logs for IAM role activity and publish metrics which will alarm whenever a role hasn't been accessed for sixty days or more.","correct":false},{"id":"444fe0024cfc19432a6f4acf80e90d76","text":"Configure an Amazon CloudWatch Alarm to publish metrics to an Amazon Simple Notification Service topic when the role last used date exceeds the sixty-day compliance threshold for role activity.","correct":false}]},{"id":"de2610da-0b20-4258-b215-146e96c134d3","domain":"automation","question":"Which section of a CloudFormation template allows you to set up differing instance types based on environment type (e.g. 'Production' or 'QA')?","explanation":"","links":[{"url":"http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"CloudFormation Template Anatomy"}],"answers":[{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false},{"id":"229eb04083e06f419f9ac494329f957d","text":"Conditions","correct":true},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":false}]},{"id":"386d9e39-b0a3-498b-9157-f845a768869a","domain":"mon-rep","question":"Which of the following are valid EC2 Auto-Scaling instance health statuses?","explanation":"Auto-scaling EC2 instances are either healthy or unhealthy.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/latest/userguide/healthcheck.html","title":"Health Checks for Auto Scaling Instances"}],"answers":[{"id":"2b329fc84084b45754e42488aca3114b","text":"Unhealthy","correct":true},{"id":"a9409139570c30da21e2629ed2d65361","text":"Compromised","correct":false},{"id":"396d45b57c2fbe3318e7b93272a2686b","text":"Healthy","correct":true},{"id":"46c4c4d980dfe025ae5b35aa0011dde4","text":"Alarm","correct":false}]},{"id":"7d7ac6a5-ba31-4418-9d4c-6da76657dbf2","domain":"networking","question":"You are designing a network with a bastion host (jump box) for security. Your network admins will SSH in to the bastion host and then on to other EC2 instances in a private subnet. You need your bastion host to be highly available. How should you build this environment?","explanation":"There has been is a much discussion about resilient Bastion design. The ELB does not add much value in this situation. Although you can get around it the ELB session timeouts will cause an SSH session to become disconnected if idle.  The answer with two AZs is a trap of the type you will see on the exam.  While 2x AZs would be ideal, the WHOLE answer must be correct, not just part of it. You should never use an ELB IP address for business as it is ephemeral and may change at any time.  DNS convectional round-robin will achieve the resiliency needed, as would an R53 Failover policy. The answer with two subnets does not exclude 2x AZs even though it does not stipulate it. Another design option you might see is EC2 Auto-Recovery or an Autoscaling group of Max=1 & Min=1 so that if the Bastion Host fails it is recreated automatically. ","links":[{"url":"https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html","title":"Bastion Hosts on AWS - Architecture"},{"url":"https://en.wikipedia.org/wiki/Round-robin_DNS","title":"DNS convectional round-robin"},{"url":"hhttps://aws.amazon.com/blogs/aws/new-auto-recovery-for-amazon-ec2/","title":"Auto Recovery for Amazon EC2"}],"answers":[{"id":"d5cfd03aad43f4cb7329ceb07c3b288d","text":"Create 2 Bastion EC2 instances in the same subnet. Create a DNS entry in Route53 which uses Round Robin DNS and points to each instance. Tell your SysAdmins to connect using the new DNS entry.","correct":false},{"id":"cb402e295d67552ab9dfcffabcfb0dcb","text":"Create 1 Bastion EC2 instance in a private subnet. Connect to this EC2 instance using a site to site VPN. Configure your router to automatically reconnect if the VPN is dropped.","correct":false},{"id":"01a3b5d266f7455e0d0cfc25070de365","text":"Create 2 Bastion EC2 instances in separate availability zones. Place these instances behind an elastic load balancer, and ask your SysAdmins to connect to the ELB's public IP Address.","correct":false},{"id":"c87a02a4d83b0d5310563e4b700e303a","text":"Create 2 Bastion EC2 instances in different subnets. Create a DNS entry in Route53 which uses Round Robin DNS and points to each instance. Tell your SysAdmins to connect using the new DNS entry.","correct":true}]},{"id":"c893ba29-4728-4483-889e-2fcf7d355ece","domain":"security-comp","question":"A developer has a script on a local machine accessing existing DynamoDB tables that needs to be transferred and executed inside an EC2 instance. The CTO has mandated that the use of access keys to access the DynamoDB tables is limited and avoided as much as possible and proper security measures are put into place. How can the developer accomplish this?","explanation":"IAM roles allow EC2 instances and similar resources such as Lambda functions to perform operations on other resources without the need for access keys and secrets. IAM groups are not the same as IAM roles and IAM groups can not be associated with EC2 instances to perform a custom action with the necessary permissions. Storing keys inside S3 buckets and RDS instances is not the priority option as the storage and usage of keys (code level) is being avoided.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html","title":"Lambda Permissions"}],"answers":[{"id":"0fb57732f6c727dbc92d9956be80c8ff","text":"Store the access key and secret access key inside a file in an S3 bucket. Load the keys upon initialization of the script to access the content of the DynamoDB tables.","correct":false},{"id":"405c9b5f9abc23f4f294b39af89e5e32","text":"Create an IAM group with the required privileges to access the DynamoDB tables. Associate the group to the EC2 instances running the script.","correct":false},{"id":"d13c898e30e31300eb42860362a84c72","text":"Create an IAM role with the required privileges to access the DynamoDB tables. Associate the role to the EC2 instances running the script.","correct":true},{"id":"2a2a7b84f186817d03e3e2cc9deabf79","text":"Store the access key and secret access key inside an RDS instance. Load the keys upon initialization of the script to access the content of the DynamoDB tables.","correct":false}]},{"id":"e05ee44b-cd10-4658-9853-ff5cea9c9d32","domain":"automation","question":"The company has started experiencing deployment issues due to the increasing complexity of the application and the lack of a structured testing and release process. The DevOps team of the company plans to set up a continuous integration pipeline in AWS to improve the stability of the releases and through the enforcement of the use of automated tests. The Head of DevOps has been instructed to use managed services as much as possible to reduce the maintenance overhead of the continuous integration pipeline. How can the DevOps team accomplish this?","explanation":"CodeBuild and CodePipeline are managed services that can be used to easily build a continuous integration pipeline. CodeBuild can run the tests and CodePipeline can manage the pipeline steps for the CI testing and deployment pipeline. Given that managed services are preferred, running Jenkins in an EC2 instance is not the priority option. AppSync is for building GraphQL powered APIs and is not used for continuous integration pipeline requirements.","links":[{"url":"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html","title":"Use CodePipeline with CodeBuild to Test Code and Run Builds"}],"answers":[{"id":"4c32bcc37cbaab42d77738b8b6875d19","text":"Use AppSync and CodeBuild for the continuous integration pipeline.","correct":false},{"id":"ce1176f044958de744d567d7ac7d0534","text":"Use Jenkins in an EC2 instance and AWS Step Functions for the continuous integration pipeline.","correct":false},{"id":"6574dc863e2d0855e66841be7aee54e7","text":"Use CodeBuild and CodePipeline for the continuous integration pipeline.","correct":true},{"id":"8adba880488851800b50283a6de55215","text":"Use Jenkins in an EC2 instance and CodePipeline for the continuous integration pipeline.","correct":false}]},{"id":"582d1290-8099-4ef5-8f80-ea6043cc32d7","domain":"security-comp","question":"You are a Sys Ops Administrator for your organization. During a routine security audit, you discovered several vulnerabilities in the operating systems of your EC2 fleet. Your EC2 fleet consists of over 250 instances. How would you resolve the security issues within your EC2 fleet in the most effective manner?","explanation":"AWS Systems Manager Patch Manager automates the process of patching managed instances with both security related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. Amazon GuardDuty is a security monitoring service that analyzes VPC Flow Logs, CloudTrail Events, and DNS logs. It would not be effective in applying patches to EC2. Amazon Inspector could help identify EC2 with security vulnerabilities, but the findings generated by Amazon Inspector depend on your choice of rules packages included in each assessment template, the presence of non-AWS components in your system, and other factors. You are responsible for the security of applications, processes, and tools that run on AWS services. Similar to Amazon Inspector, AWS Config would not help in applying patches to your entire fleet.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html","title":"Systems Manager"}],"answers":[{"id":"20ebac0f0e1509a9f9b321e86861fdff","text":"Deploy Amazon GuardDuty which will create a unique finding ID for each vulnerability in CloudWatch. Automate security patch updates with Lambda to the instances that are associated with a GuardDuty finding ID.","correct":false},{"id":"2161bb4bdfc887d3c242cee80467ae30","text":"Deploy the security patch using RunCommand with AWS Systems Manager for the entire fleet of EC2 instances.","correct":true},{"id":"b966c9e66f79563a38283cf5493db53e","text":"Use AWS Config to identify the EC2 instances with vulnerabilities based on security rules. Isolate these instances and install the patch updates to fix the vulnerabilities.","correct":false},{"id":"6256bbd45632da9b813aac4c9411f564","text":"Have Amazon Inspector assess the instances and send metric data to CloudWatch. Set up a CloudWatch Event to trigger a Lambda function that will install the patch to instances with the vulnerability.","correct":false}]},{"id":"8f82dce4-8d2c-47d8-accd-ce503e1b86f4","domain":"dep-prov","question":"Which of the following might help resolve an InstanceLimitExceeded error when launching an instance?","explanation":"If you get an InstanceLimitExceeded error when you try to launch a new instance or restart a stopped instance, you have reached the limit on the number of instances that you can launch in a region. When you create your AWS account, AWS set default limits on the number of instances you can run on a per-region basis. Requesting an instance limit increase will help with resolving an InstanceLimitExceeded error. Submitting a new launch request without specifying an Availability Zone or submitting a new launch request using a different instance type will help address InsufficientInstanceCapacity errors but will not help resolve InstanceLimitExceeded errors. Requesting an increase to an Amazon EBS volume limit will help with a VolumeLimitExceeded error but will not help with an InstanceLimitExceeded error.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html#troubleshooting-launch-capacity","title":"Troubleshooting Instance Launch Issues"}],"answers":[{"id":"8ed67781418ab44be37b294251671f76","text":"Submit a new launch request without specifying an Availability Zone","correct":false},{"id":"154758dad301ec2c98aa017346291b16","text":"Submit a new launch request using a different instance type","correct":false},{"id":"f24bc29b6cb6dcab18a780c8e6d48a0c","text":"Request an instance limit increase for that region","correct":true},{"id":"6466bc1f56521a662522f33ad6f71f30","text":"Request an increase to the Amazon EBS volume limit","correct":false}]},{"id":"d1f23ad4-6255-449e-ba3e-93f4ea692185","domain":"dep-prov","question":"You encounter problems while detaching an EBS volume through the Amazon EC2 console. What can help you diagnose the issue?","explanation":"The 'describe-volumes' CLI command can be used to gather further information to help with diagnosing volume issues and is therefore the most appropriate. The 'detach-volume --force' CLI command forces detachment if the previous detachment attempt did not occur cleanly (for example, logging into an instance, unmounting the volume, and detaching normally). This option can lead to data loss or a corrupted file system. Use this option only as a last resort to detach a volume from a failed instance. The 'Dismount-EC2Volume' PowerShell command will perform the same action as the EC2 console and will therefore not help with additional diagnosis. The 'fix-volumes' command is not a valid CLI command.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-detaching-volume.html","title":"Detaching an Amazon EBS Volume from an Instance"}],"answers":[{"id":"8f52ce0a3a674b594aaed05357a3d828","text":"Run the 'detach-volume --force' CLI command.","correct":false},{"id":"75f13dd600a356952c8f887bd5bcb626","text":"Run the 'Dismount-EC2Volume' PowerShell command.","correct":false},{"id":"6dab4100f13c64be47fc0b254385f5ab","text":"Run the 'describe-volumes' CLI command.","correct":true},{"id":"8f6461e3ace3c5f5cab7f6c269e12420","text":"Run the 'fix-volumes' CLI command.","correct":false}]},{"id":"4a4c6ce8-b38c-4905-8705-d5bbfb0ee4b2","domain":"mon-rep","question":"You have an Auto Scaling group resource in your AWS account. The desired number of instances has been changed from 2 to 0 recently and all instances were terminated because of it. You want to know when and how the resource was created and who modified the desired number. Which service can help you to quickly get the information?","explanation":"You can quickly get the configuration history in the AWS Config configuration timeline including who and when the resource was created or modified. A new CloudTrail does not help as it only records new events. Athena may work however it is not as easy as AWS Config. CloudWatch metrics cannot provide the required configuration information.","links":[{"url":"https://docs.aws.amazon.com/config/latest/developerguide/view-manage-resource-console.html","title":"Viewing configuration details in AWS Config"}],"answers":[{"id":"9a2b6d3211e9c6531d8bf6f6ff2e8351","text":"Save all CloudTrail events to an S3 bucket. Perform SQL queries in the bucket via Athena.","correct":false},{"id":"8d084da5294ed862175582c9670af840","text":"Create a new CloudTrail and save the events to CloudWatch Logs. Search for the Auto Scaling group resource in the logs.","correct":false},{"id":"25787092d8b6523e05e0155400bcc0bf","text":"Check the Auto Scaling group resource in AWS Config and inspect the configuration timeline for the resource.","correct":true},{"id":"65d77f35794d925b20ff78961bb4c1df","text":"Check the CloudWatch metrics for the Auto Scaling group resource. CloudWatch metrics can record the data for 6 weeks.","correct":false}]},{"id":"11e35452-b0ac-48fe-842b-faeedd3d58f1","domain":"mon-rep","question":"A SaaS company has 30 running EC2 instances that produces a significant quantity of web application logs on a daily basis. Due to the SLAs and other constraints given to the engineering team, the logs of these instances need to be analyzed in near real-time and the deployed versions of the applications need to be rolled back if there are issues observed in the logs from the latest deployment. How can the DevOps team accomplish this?","explanation":"Services such as Amazon Kinesis streams would solve the real-time processing requirement of this scenario. Amazon Kinesis Firehose has a 1-min lag therefore it cannot be used to solve the real-time requirement. All other options involve running a log processing workload every X minutes which cannot be considered real-time.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/streaming-cloudwatch-logs/","title":"Streaming CloudWatch Logs"}],"answers":[{"id":"4b7916aca484485ba20548fe4006a647","text":"Stream the logs to an Amazon Kinesis Firehose that stores data in CloudWatch logs. Use Amazon EMR to process the logs accordingly.","correct":false},{"id":"5502483a37b221760b333eb62c65239f","text":"Stream the logs to an S3 bucket. Use Amazon Athena to process the logs and generate reports every 30 minutes.","correct":false},{"id":"508417d6030658c25449604daac66ce6","text":"Stream the logs to a DynamoDB table using DynamoDB streams. Use Amazon EMR to process the logs and generate reports every 30 minutes.","correct":false},{"id":"f410ecf126408d1c95ad85902ed7b6ff","text":"Stream the logs to an Amazon Kinesis stream. Have the users and consumers analyze and process the logs accordingly.","correct":true}]},{"id":"5dddbe8f-a62f-4692-b8a5-61e1ff70f722","domain":"high-avail","question":"You run a popular desktop application which is hosted on a fleet of EC2 instances behind an autoscaling group. After three years you are about to release version 9, which millions of people have been waiting for with excitement. When you released version 8, 8 years ago your website crashed from the demand. You need to prevent this from happening on this release. What AWS service could you use to assist with this?","explanation":"CloudFront works together with your website and speeds up delivery of your content by caching it at Edge Locations local to your users","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html","title":"CloudFront"}],"answers":[{"id":"e5e2e10e37ff47a42272a2c0335fca65","text":"Use Cloudfront to cache the software update at Edge Locations so as to keep up with the demand","correct":true},{"id":"e994c20e96f7f92075129815a13774c3","text":"Host the update in AWS Aurora and turn on Aurora Accelerator to keep pace with the demand","correct":false},{"id":"52c99d94be86aed6ff572f5dd85fc5c9","text":"Use AWS Shield to protect the software update from too many users attempting to download it at once","correct":false},{"id":"3ea178bab502b44ed89d0a22b2fcfa78","text":" Host the update on a single T2 nano instance on EC2 and publish the public IP address to your customers to download","correct":false}]},{"id":"b63a9012-82a9-4a3f-9c32-2766b7adbdf6","domain":"networking","question":"You are a SysOps Administrator setting up a VPN connection between your on-premises data center and with AWS. You currently have an Amazon VPC setup with a Virtual Private Gateway. You have installed a customer gateway to your on-prem data center and router for your on-premises network is showing status OK. When you try to connect the EC2 instance in your Amazon VPC to a virtual machine in your data center it does not work. How should you set up the route table in the Amazon VPC?","explanation":"To enable instances in your VPC to reach your customer gateway, you must configure your route table to include the routes used by your Site-to-Site VPN connection and point them to your virtual private gateway. You can enable route propagation for your route table to automatically propagate those routes to the table for you.","links":[{"url":"https://docs.aws.amazon.com/vpn/latest/s2svpn/SetUpVPNConnections.html","title":"AWS Site-to-Site VPN"}],"answers":[{"id":"06625c579c97e66a67f1dda1fb750f64","text":"Configure a route to the NAT gateway.","correct":false},{"id":"7607506e7811e80615e897ab0057315b","text":"Configure a route to the customer gateway.","correct":false},{"id":"92b30688af3ba53d2f0a13cd249bd865","text":"Configure a route to the virtual private gateway.","correct":true},{"id":"39e27dda89073759c58b8260fe34ffb8","text":"Configure a route to the internet gateway.","correct":false}]},{"id":"4a2b50c3-b064-4235-a27f-e6a57dab2536","domain":"automation","question":"A non-emergency medical transport provider receives their AWS bill and would like to reduce their monthly spend. Upon investigation, they discover they are paying for many orphaned EBS volumes not attached to EC2 instances. As they dig deeper, they determine that none of the volumes are needed any longer. They'd like to automate the removal process for these volumes and any others that become orphaned for more than ninety days. Which solution will keep them from paying for unneeded EBS volumes in the future?","explanation":"A periodically scheduled Lambda function can be written to determine how long an EBS volume has been orphaned by looking at CloudTrail actions. Instructions for how to create snapshots and delete EBS volumes can be found in the EC2 API Reference. Passing the volume's ARN as an AWS Systems Manager Automation resolution target will not create a snapshot and delete a volume. You can use Amazon CloudWatch Events to automate the monitoring of Trusted Advisor activity, but not CloudWatch Logs. AWS Data Pipeline is used to move and process data between different services, not to manage EBS resources.","links":[{"url":"https://aws.amazon.com/ebs/","title":"Amazon Elastic Bock Store"},{"url":"https://aws.amazon.com/blogs/mt/controlling-your-aws-costs-by-deleting-unused-amazon-ebs-volumes/","title":"Controlling your AWS costs by deleting unused Amazon EBS volumes"}],"answers":[{"id":"628e11e6d88eb8d10d63cd3f003d04a2","text":"Schedule an AWS Data Pipeline workflow that leverages the AWS-provided pre-condition for orphaned EBS volumes. Pass it a time threshold parameter value of ninety days, and assign actions to create a snapshot and delete the volume.","correct":false},{"id":"92788951cb2a5d4a2d4d91fb8c19fe27","text":"Create an AWS Systems Manager workflow to examine EBS volumes. For volumes not attached to EC2 instances for over ninety days, pass the volume's ARN as a resolution target to the automation's execution logic to create a snapshot and delete the volume.","correct":false},{"id":"9dfd7bacb72bcd7c38c22bb76dbd0321","text":"Detect changes in AWS Trusted Advisor checks for underutilized EBS volumes with Amazon CloudWatch Logs. For volumes identified as underutilized for over ninety days, invoke an AWS Lambda function to create a snapshot of the volume and delete it using EBS APIs.","correct":false},{"id":"855c39931a8bd19bfec582fd45fb7334","text":"Schedule an AWS Lambda function to run periodically via Amazon CloudWatch Events. Have the Lambda function read AWS CloudTrail actions to identify detached EBS volumes in an available state for over ninety days. Create a snapshot of the volume and delete it using EBS APIs.","correct":true}]},{"id":"c4f661ee-5ea1-4e69-9440-52bea0321a6a","domain":"mon-rep","question":"You have set CloudWatch billing alarms for your instances running in eu-west-2. However, when you try to access the billing information and alarms, no information is visible. Why might this be?","explanation":"Billing and Alarm data can be accessed only from the us-east-1 region.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/free-tier-alarms.html","title":"Creating a Billing Alarm"}],"answers":[{"id":"cd2c9fa4324b5861276dbbf7f4f593a8","text":"You need to login as the root user to see such information.","correct":false},{"id":"fb7a7d16c3f39960e7afde6babd422e1","text":"Billing and Alarm data can be accessed only from the us-east-1 region.","correct":true},{"id":"b5ff6f2dfd759f7e70f27d7529e4462b","text":"You need to login as the account owner to see such information.","correct":false},{"id":"7f3a3688c3f0cddb24c07255b9d13767","text":"Billing and Alarm data can be accessed only from the us-west-1 region.","correct":false}]},{"id":"795314ba-4e90-418a-af2c-ee9d66f916d8","domain":"data-man","question":"An engineer has been instructed to design and build an application that processes and stores summary reports of financial transactions daily. The CTO has mandated that these reports are protected from accidental deletion when reports with the same name are generated and stored. How can the engineer accomplish this?","explanation":"Using the versioning feature of S3, accidental deletions are prevented as delete markers are just placed for deleted versions of objects. Kinesis automatically deletes records after 7 days and is not used to store critical data. The EC2 instance store and the ElastiCache clusters are not persistent storage options as well.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html","title":"S3 Versioning"}],"answers":[{"id":"e524707a83030888491be6ca959ec580","text":"Store the reports as JSON documents in an ElastiCache cluster.","correct":false},{"id":"a198447d96a130961d8a3f7c3f73d325","text":"Store the reports inside an EC2 instance store. Use a proper naming convention to prevent reports from being overwritten.","correct":false},{"id":"8cfcc5e04e55c8460deac0663173053b","text":"Store the reports as JSON documents in Amazon Kinesis.","correct":false},{"id":"92a6d1a8e51989a926f98031621f78c6","text":"Store the reports inside an S3 bucket and enable versioning.","correct":true}]},{"id":"a4edb410-e5e5-40b4-9335-1da82639c2e3","domain":"networking","question":"You work for an investment bank, supporting a mission critical stock market data processing application running on EC2 and consuming real-time data feeds from your on-premises systems. Your traders are complaining that the system is sometimes very slow to refresh the data and you suspect that this is due to fluctuations in available network bandwidth between AWS and your datacentre. What improvement can you suggest to give users a consistent experience and improve performance for users?","explanation":"AWS Direct Connect is a network service that provides an alternative to using the Internet to connect customer's on premise sites to AWS.","links":[{"url":"https://aws.amazon.com/directconnect/faqs/","title":"Direct Connect FAQs"}],"answers":[{"id":"35461a11ab55ba8f2b7eb1d3fcc15eff","text":"Scale out your application servers","correct":false},{"id":"a60c49fc87050d8b3c698515938d624b","text":"Configure S3 Transfer Acceleration to move the data into AWS much faster","correct":false},{"id":"3225172eff8d104a4744f0ee6f50d836","text":"Configure an additional Elastic IP for each of your application servers to increase the network bandwidth","correct":false},{"id":"5d5c067abd490006c21d11ff221c552a","text":"Configure a Direct Connect connection between your data center and AWS","correct":true}]},{"id":"bff7e3f1-155b-475f-b05d-04a051ab9325","domain":"dep-prov","question":"You are experiencing issues with an Application Load Balancer that has been updated in the past week by another member of your team. Access logging is enabled on the ALB. You need to determine what configuration changes have been made. Which is the most appropriate monitoring tool to use?","explanation":"You can use AWS CloudTrail to capture detailed information about the calls made to the Elastic Load Balancing API for configuration and store them as log files in Amazon S3. You can use these CloudTrail logs to determine which calls were made, the source IP address where the call came from, who made the call, when the call was made, and so on. Amazon CloudWatch can be used to retrieve statistics about data points for your load balancers and targets as an ordered set of time-series data, known as metrics. You can use these metrics to verify that your system is performing as expected but they do not provide information about configuration changes. Access Logs capture detailed information about requests sent to your load balancer from clients, but are also not relevant to configuration. You can use request tracing to track HTTP requests but this does not provide additional configuration information.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html","title":"Monitor Your Application Load Balancers"}],"answers":[{"id":"6c5c81f47915de5f03d2577e8fae1c34","text":"Access logs","correct":false},{"id":"6a292fdb897093b64ef80b39e7db0a4a","text":"Request tracing","correct":false},{"id":"50ed91980adb1dac23689554eb719277","text":"CloudWatch metrics","correct":false},{"id":"8c6ded942a243b91e65d037ab4e21f7d","text":"CloudTrail logs","correct":true}]},{"id":"05d71be4-026e-433e-bd8b-eb4a3929ba63","domain":"automation","question":"A development team wants to use the latest Windows AMI whenever they launch an EC2 instance. Which service will allow them to query the AWS-managed Parameter Store namespace to retrieve the newest AMI for their CloudFormation template?","explanation":"AWS publish the latest AMI IDs for Operating Systems in AWS-managed parameters in the Parameter Store.  By using a Custom Resource in Lambda you can retrieve the relevant AMI ID and return it to the CloudFormation service, that way ensuring that your templates always use the newest AMI.","links":[{"url":"https://aws.amazon.com/blogs/mt/query-for-the-latest-windows-ami-using-systems-manager-parameter-store/","title":"Select AMI using Systems Manager Parameter Store"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html","title":"AWS Lambda-backed Custom Resources"}],"answers":[{"id":"8c19fb5ff9d451c3f315e96ca8563b84","text":"CloudFormation Linked Parameters","correct":false},{"id":"dc0efa07b1be89f7cfd1ab666df2f949","text":"CloudFormation Custom Resource using Lambda","correct":true},{"id":"7eb8f6238570dc713a360eae3029648f","text":"CloudFormation Mappings","correct":false},{"id":"2751cfe1530d4333f0bdac2d7b7c21bd","text":"CloudFormation using AWS Systems Manager Parameter Store","correct":true},{"id":"cdf3a2f6faa3abf891b952dde17eb469","text":"CloudFormation Template Transformation","correct":false}]},{"id":"7fc0d8c0-5fc9-463e-94ff-54f852f1d819","domain":"data-man","question":"An S3 bucket stores some files for an application. As the files need to be read from users on the internet, the bucket should have Public Access. However when you modify the Bucket Policy to be public in AWS console, the operation is blocked with an \"Access denied\" error. Your IAM user has enough permissions to modify Bucket Policies. How would you troubleshoot the issue?","explanation":"Users can configure S3 buckets to block public access. When users try to enable the public access through Access Control Lists or Bucket Policies, the operation is denied. In this scenario, you should check if the public access through Bucket Policy is blocked.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access.html","title":"Block Public Access to S3 Buckets"}],"answers":[{"id":"416e41b2d06f37e643dd26b55ea9bc53","text":"Check the syntax of Bucket Policy and ensure that the Action field only contains s3:GetObject and the Principal field is a wildcard.","correct":false},{"id":"ba324e78ebf4d884332673942ca4bee0","text":"Check the bucket public access settings to see if the public access through Bucket Policy is blocked. Make sure the public access is not blocked by the settings.","correct":true},{"id":"92ffb0eb54566b1970433d74106174e3","text":"Use AWS CLI s3api put-bucket-policy to modify the Bucket Policy.","correct":false},{"id":"2e51a73ed0e5f8cf18b5f4047b358a94","text":"User should configure the public access in S3 Access Control List rather than Bucket Policy. In ACL, enable the read access to the group of Everyone.","correct":false}]},{"id":"f1f36583-612f-4946-ac77-a6e78aede912","domain":"networking","question":"You have an Amazon VPC with one private subnet, one public subnet and one network address translation (NAT) server. You are creating a group of EC2 instances that configure themselves to deploy an application via GIT. Which of the following setups provides the highest level of security?","explanation":"You should use EC2 instances in private subnet; no EIPs; and route outgoing traffic via the NAT.","links":[{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_NAT_Instance.html#basics","title":"NAT Instance Basics"}],"answers":[{"id":"1f9416e0df4342a7c5298d9f43d4d392","text":"Amazon EC2 instances in a private subnet; assign EIPs; route outgoing traffic via the internet gateway (IGW).","correct":false},{"id":"6a084c8ddbef4573cc627373b1a04456","text":"Amazon EC2 instances in public subnet; assign EIPs; route outgoing traffic via the NAT.","correct":false},{"id":"9767e7ce6f652652e1ec42f2d2a48af1","text":"Amazon EC2 instances in public subnet; no EIPs; route outgoing traffic via the internet gateway (IGW).","correct":false},{"id":"03808744fcc2ec07927563e827ba457a","text":"Amazon EC2 instances in private subnet; no EIPs; route outgoing traffic via the NAT.","correct":true}]},{"id":"aba20312-a023-419f-b495-12e7dd2c6577","domain":"mon-rep","question":"In which of the following are your CloudTrail logs stored?","explanation":"Logs are stored in S3. You must specify a storage  bucket name to enable CloudTrail.","links":[{"url":"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.html","title":"Finding Your CloudTrail Log Files"}],"answers":[{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":false},{"id":"7b1fb630c85b556e31fa54e3d2b6201a","text":"An EBS Volume","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":false},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true}]},{"id":"bf96ed55-16e4-466e-af19-219458837cba","domain":"security-comp","question":"Which of the following is not a possible use case for AWS inspector","explanation":"Amazon Inspector in an agent-based service which allows you to automate security vulnerability assessments throughout your development and deployment pipeline or against static production systems. You cannot install the Inspector agent on an RDS instance.","links":[{"url":"https://aws.amazon.com/inspector/faqs/","title":"Inspector FAQs"}],"answers":[{"id":"c0f6489b2e3fb0571e505c7a5560a3c3","text":"To scan your EC2 instances for Common Vulnerabilities and Exposures ","correct":false},{"id":"8543a5ae6daf2a49147c11e5e7422e9f","text":"Use a CloudWatch Event to trigger AWS inspector to run an assesment","correct":false},{"id":"80b4d5c55b7c39f3d2aa5c485154a2a8","text":"Automate the assessment of your EC2 instance on a regular basis","correct":false},{"id":"2c7f5727b64a2fdb39bb7c27b505ad87","text":"The inspection of an RDS instance","correct":true}]},{"id":"55cd4e20-346b-4ab5-920b-bf4720db7ee5","domain":"automation","question":"You are working as a SysOps Administrator for your company and are working on writing a CRON job on an application running on EC2. The CRON expression requires the instance to provide its public IP address to pass to another application running on a second EC2 instance. How would you obtain the IP address?","explanation":"Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you're writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application. User data are the parameters you specify when configuring your instance. Instance store is a type of instance and the AMI does not contain the IP address.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html","title":"Instance Metadata and User Data"}],"answers":[{"id":"263ce3ebb0453f2bf28edc6a6d3bbf05","text":"From the instance user data using the curl command.","correct":false},{"id":"6544d86f167272ed59ebf831074f047f","text":"From the instance store data using the curl command.","correct":false},{"id":"45dbd3d67172bc00ea7cf86e81faf0f5","text":"From the instance metadata using the curl command.","correct":true},{"id":"6248d35cdbd86a27a1dd1517a621d1d2","text":"From the instance AMI data.","correct":false}]},{"id":"cb3a3e4a-87a1-4810-a732-ab1818b84b1d","domain":"data-man","question":"Your company's on-premises CRM software uses file storage. The marketing department would like to add a new social media module which will require a large increase in storage capacity. There is no capital budget to purchase additional storage servers, so the decision is made to place the new module's data on Amazon Elastic File System (EFS). A leader of the application team raises concerns about the latency that could be incurred with a single application accessing data both on-premises and in the cloud. How would you architect the solution to minimize the possibility of poor application performance?","explanation":"Either AWS Direct Connect or AWS VPN is required to connect on-premises servers to Amazon EFS. Bursting Throughput mode is the default mode for EFS file systems, scaling up as the size of a filesystem grows. Provisioned Throughput mode can provide higher throughput for applications with requirements greater than those of Bursting Throughput mode. Max I/O Performance mode will only benefit highly parallelized applications, which CRM generally is not.","links":[{"url":"https://aws.amazon.com/efs/","title":"Amazon Elastic File System"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/performance.html#throughput-modes","title":"Amazon EFS Performance"}],"answers":[{"id":"ed115154d20878d7d538a3557109c6e2","text":"Connect the CRM application to the AWS cloud over an AWS Direct Connect. Configure the EFS file system in Max I/O Performance mode","correct":false},{"id":"9d857d0961d3644ba26dec63c2d911e2","text":"Have the CRM application connect to the AWS cloud over an AWS Direct Connect. Configure the EFS file system in Provisioned Throughput mode","correct":true},{"id":"6ebf148cfe4cebe47a593afb84737882","text":"Connect the CRM application to EFS through an AWS Service Endpoint. Deploy the EFS file system in both Provisioned Throughput and Max I/O Performance modes","correct":false},{"id":"7b828048a4ab8b3c896ce413de929519","text":"Have the CRM application connect to the AWS cloud over an AWS VPN. Implement the EFS file system in both Bursting Throughput and Max I/O Performance modes","correct":false}]},{"id":"82163b46-cb90-4a66-b0fc-70e4be51a532","domain":"security-comp","question":"Consolidated Medical Devices runs thousands of EC2 instances for their application portfolio. The server fleet consists of many different operating systems and instance families. Their homegrown script-based server management scheme can no longer scale with their infrastructure growth. In addition to automating systems administration tasks, they need a solution that will simplify compliance tasks related to security assessments and patching. Which architecture will provide the management capabilities they need with the least amount of ongoing maintenance?","explanation":"Amazon Inspector automatically assesses systems for exposure, vulnerabilities, and deviations from best practices. Inspector can publish to an SNS topic, which can trigger a Lambda function to query the non-compliant EC2 agent IDs. A second SNS topic and Lambda function can assess the Inspector findings and invoke AWS Systems Manager Patch Manager capabilities to patch the non-compliant instances. AWS Systems Manager doesn't currently have the capability to integrate with Inspector automatically. Amazon GuardDuty is a threat detection service, and is not used for vulnerability assessments.","links":[{"url":"https://aws.amazon.com/inspector/","title":"Amazon Inspector"},{"url":"https://aws.amazon.com/systems-manager/","title":"AWS Systems Manager"},{"url":"https://aws.amazon.com/solutions/server-fleet-management-at-scale/?did=sl_card&trk=sl_card","title":"Server Fleet Management at Scale"}],"answers":[{"id":"82ae7c1a5ab7c1d3ac7a76b47966f852","text":"Install the AWS Systems Manager agent and the Amazon Inspector agent on all EC2 instances. Configure daily assessment runs in the Inspector management console. Configure AWS Systems Manager to automatically receive the agent IDs of non-compliant instances and trigger Patch Manager to bring the instances into compliance","correct":false},{"id":"f33a28baafce146de4e99dc5e600e088","text":"Create an Amazon CloudWatch event to trigger Amazon Inspector to run daily security assessments. Have Inspector publish a message to an Amazon SNS topic, which triggers an AWS Lambda function. Have the Lambda function query Inspector for the agent IDs of non-compliant instances and publish them to a second SNS topic which triggers a second Lambda function. Configure the second Lambda function to invoke AWS Systems Manager to patch the non-compliant instances","correct":true},{"id":"3b16dc801e9751c29f18e70c557ca982","text":"Load the AWS Systems Manager agent and the Amazon GuardDuty agent on all EC2 instances. Configure daily assessment runs in the GuardDuty management console. Configure AWS Systems Manager to automatically receive the instance IDs of non-compliant servers and trigger Patch Manager to bring the instances into compliance","correct":false},{"id":"aa2b9bb0b72691ec56685a805a9f2f8e","text":"Configure daily assessment runs in the Amazon GuardDuty management console. Have GuardDuty publish a message to an Amazon SNS topic, which triggers an AWS Lambda function. Have the Lambda function query GuardDuty for the instance IDs of non-compliant servers and publish them to a second SNS topic which triggers a second Lambda function. Configure the second Lambda function to invoke AWS Systems Manager to patch the non-compliant instances","correct":false}]},{"id":"3f043cde-54b2-42c7-9a19-7153fe5b6e95","domain":"high-avail","question":"Your customer has asked about cost-savings opportunities with AWS. They've noted that their EC2 instances are on most, if not all, the time but metrics show that aggregate CPU utilization is low. Demand for their application is also unpredictable. They want to cut costs around their EC2 fleet. Which of the below suggestions would you recommend to your customer to maximize savings?","explanation":"AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. AWS Auto Scaling makes scaling simple with recommendations that allow you to optimize performance, costs, or balance between them. With auto scaling you can scale-out Amazon EC2 instances seamlessly and automatically when demand increases, shed unneeded Amazon EC2 instances automatically and save money when demand subsides, and scale dynamically based on your Amazon CloudWatch metrics, or predictably according to a schedule that you define. Purchasing reserved instances, although cheaper than on-demand, would not necessarily cut costs. Since demand is unpredictable you may be purchasing a commitment that you may not use. There is no indication that the application will be running for at least a year. This is true even after right-sizing. Storing snapshots of EBS in S3 is indeed cheaper than storing EBS volumes but that does not address the issue of the EC2 instances themselves.","links":[{"url":"https://aws.amazon.com/autoscaling/","title":"AWS Auto Scaling"}],"answers":[{"id":"a8dd2caf5c16147c7b1b6bd321b6558d","text":"Take snapshots of the EBS volumes attached to the EC2 instances and store them in S3. Delete the EBS volumes as storing in S3 is a cheaper alternative than EBS storage costs.","correct":false},{"id":"76a2f03f07c2ade7d896075a501c6bc5","text":"Utilize auto scaling groups for the EC2 fleet. Set up a scaling policy that will launch EC2 instances when CUP utilization is above a threshold, and release instances when CPU utilization is below a threshold.","correct":true},{"id":"89a84b7404990525469a3025ab3b5116","text":"Decrease the instance sizes for those instances with low CPU utilization. Purchase standard reserved instances after right-sizing the instances.","correct":false},{"id":"99040729979583a2eb85e3502b484989","text":"Purchase convertible reserved instances for your EC2 fleet. They will experience up to 66% savings compared to on-demand costs and will have the option to change their instance types if the application needs change.","correct":false}]},{"id":"ba52dee8-0d6c-4faf-9121-0e64c18bbf1a","domain":"high-avail","question":"Your website is evenly distributed across 10 EC2 instances in 5 AWS regions. How could you configure your site to maintain high-availability with minimum downtime if one of the 5 regions was to lose network connectivity for an extended period of time?","explanation":"If you are designing to check for loss of contact with the instances you need to use \"Evaluate Target Health\" to confirm connectivity.  The Latency policy will eventually detect the unavailability; however it is not a real-time test.","links":[{"url":"http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html","title":"How Health Checks Work in Complex Amazon Route 53 Configurations"}],"answers":[{"id":"dc1f9d897e82bca789bb2983fa7ce22d","text":"Create a Route 53 Latency-based Routing Record Set that resolves to an Elastic Load Balancer in each region. Set an appropriate health check on each ELB.","correct":false},{"id":"b5494c49d052d62119e11eab7d8499c7","text":"Create an Elastic Load Balancer to place in front of each EC2 instance. Set an appropriate health check on each ELB.","correct":false},{"id":"90875d19db265ba7b3931ecbd5a5d813","text":"Establish VPN Connections between the instances in each region. Rely on BGP to failover in the case of a region-wide connectivity outage.","correct":false},{"id":"0e2faa25f8eb6371b513d8d442513b10","text":"Create a Route 53 Latency-based Routing Record Set that resolves to Elastic Load Balancers in each region and has the Evaluate Target Health flag set to \"True\".","correct":true}]},{"id":"39376479-c045-4faf-a0b1-6dfddce09dba","domain":"mon-rep","question":"Which of the following is a valid AWS namespace?","explanation":"CloudWatch namespaces are containers for metrics.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/aws-namespaces.html","title":"AWS Namespaces"}],"answers":[{"id":"96f49c22a5ef2aba5cfdcdf4d4c7cce1","text":"AWS/DynamoDB","correct":true},{"id":"be6132c54d50d8ce33a982685bcdb6fd","text":"CUSTOM/KMS","correct":false},{"id":"f7986f757fc3e62f5d2152aafbb2dde0","text":" AWS/ApiGateway","correct":true},{"id":"15b7736c9e06de8c034b67c82b7bee1b","text":"AMAZON/EC2","correct":false}]},{"id":"10c87ec7-567f-4119-8817-e484af01182e","domain":"high-avail","question":"Your development team has written an application to pick up and process images taken from an SQS queue. The app is growing in popularity and your manager wants to do this as cheaply as possible. What is the best way to achieve cheap and timely processing?","explanation":"The solution must respond in a timely manner to increases in workload and running during ‘off-peak’ hours is not appropriate. Using a lambda to perform the scaling is not necessary since it is supported out-of-box by AWS’ Autoscaling service. Since the application is growing in demand there isn’t a defined capacity to pre-purchase via Reserved Instances.  Therefore the best solution is to use Autoscaling to scale in and out based on queue length.","links":[{"url":"https://docs.aws.amazon.com/en_pv/autoscaling/ec2/userguide/as-using-sqs-queue.html","title":"Scaling Based on Amazon SQS"}],"answers":[{"id":"a9f53d8ff41f6f104afe7dfd446f8cf0","text":"Deploy the app into an Autoscaling Group and scale in and out based on an SQS queue length","correct":true},{"id":"e00965d574d10c8a9d991c96eb786139","text":"Deploy the app into an Autoscaling Group and run the processing during off-peak hours","correct":false},{"id":"905df3db934191d38ccbd1dbec2e459a","text":"Deploy the app to EC2 and use lambda to stop and resize the instance based on the SQS queue length","correct":false},{"id":"d3dd710c9e556214f0ae216bd74a6982","text":"Deploy the app into an Autoscaling Group and use reserved instances (RI) to pre-purchase a year’s worth of processing capacity","correct":false}]},{"id":"4fa7de17-2f03-4b83-a519-e00b011ca644","domain":"security-comp","question":"You are a SysOps Administrator for your organzation. The organization has one AWS account that all users share. You are asked by the CISO to make access to AWS secure and manageable. What would be the best solution?","explanation":"AWS Single Sign-On makes it easy to centrally manage access to all of your AWS accounts. It supports Security Assertion Markup Language (SAML) 2.0 so you don't have to create IAM users for every individual in your organization. The other solutions would be an administrative and unnecessary burden, while encrypted login credentials using KMS is not used for AWS access.","links":[{"url":"https://docs.aws.amazon.com/en_pv/singlesignon/latest/userguide/iam-auth-access.html &https://docs.aws.amazon.com/cli/latest/reference/sts/assume-role-with-saml.html","title":"AWS Single Sign-On"}],"answers":[{"id":"0949ef9615ffc62f343f10d27d0bea2e","text":"Encrypt Active Directory logins using AWS KMS. Store encrypted logins in an RDS table. When users access AWS, set up permissions to decrypt the credentials to securely access AWS resources.","correct":false},{"id":"b18f21a1b9a12f45b180d3ba09d175a0","text":"Configure a SAML federation between AWS and your organization's Active Directory. Set up Active Directory groups with AWS IAM groups to manage user permissions.","correct":true},{"id":"678d5bab1e8e14c5f9a7055f83d2aa56","text":"Set up an AWS Organization to manage accounts and apply permissions boundaries. Set up IAM users and group them in the appropriate OU.","correct":false},{"id":"48371df9efaa3423dc8c9153f0af03c3","text":"Group Active Directory users and mirror their permissions with IAM policies. Create IAM users in AWS and group them into IAM groups that correspond to the Active Directory Group.","correct":false}]},{"id":"745e3728-9374-48a2-b7ab-7ee0c4ad4ad6","domain":"security-comp","question":"Your AWS Organization includes multiple AWS accounts. To meet the security compliance, AWS Config should be enabled in all accounts. The data needs to be recorded in all regions as well. You prefer using a central place to view all the resource configurations and compliance data recorded in AWS Config. How would you configure it?","explanation":"An aggregator is an AWS Config resource type that collects AWS Config data across multiple accounts and multiple regions. You can easily add an AWS Organization and select all regions in the aggregator. After that, you can get an aggregated view of the configuration information of AWS resources, an overview of Config rules and their compliance state. You do not need to manually enable AWS Config in all regions and all accounts. And AWS Config cannot be enabled in the AWS Organizations panel.","links":[{"url":"https://docs.aws.amazon.com/config/latest/developerguide/setup-aggregator-console.html","title":"Setting up an aggregator in AWS Config"}],"answers":[{"id":"a856258c8005e338f9c1b40c80449899","text":"Enable AWS Config in all regions and all accounts. Select an S3 bucket to store all the configuration data.","correct":false},{"id":"2317a132eeda89bc0a88eb9b628b6d06","text":"In AWS Organizations panel, enable AWS Config for all the regions. View the centralized configuration data in AWS Organizations.","correct":false},{"id":"6bfb56d27b2d920c4cc3001c82233f29","text":"Create an aggregator in AWS Config. Add the AWS Organization to the aggregator and select all AWS regions.","correct":true},{"id":"4787f6c13efd28a83db2125402fa372b","text":"Enable AWS Config in all regions for each account. Configure AWS QuickSight to view the aggregated data.","correct":false}]},{"id":"aa8af241-5322-486d-bd34-12b2d348b2ce","domain":"data-man","question":"A security officer needs to know when IAM keys in his company’s AWS Accounts have been around for more than three months.  Which service will easily help him with this?","explanation":"Trusted Advisor will give you details of IAM access key rotation.","links":[{"url":"https://aws.amazon.com/blogs/mt/query-for-the-latest-windows-ami-using-systems-manager-parameter-store/","title":"Query for the Latest Windows AMI using Systems Manager Parameter Store"}],"answers":[{"id":"1f530f3e77a170663f032cffb98d0f26","text":"AWS Security Advisor","correct":false},{"id":"8db02bec6e61573a2849575f23459da0","text":"AWS IAM Inspector","correct":false},{"id":"05f43441d2d29ae2bb38fc8596ca6ff7","text":"AWS Trusted Advisor","correct":true},{"id":"49e614cc047c88063f1780aa1c1e7c0a","text":"AWS CloudWatch Sentinel","correct":false}]},{"id":"4672c614-c5ab-464a-9de0-5ef85a8d081e","domain":"automation","question":"You are a SysOps Administrator for your company. The company's CIO was on vacation and didn't know that there was an AWS Region outage during her time off. She returned having no idea of the impact and wants to be alerted the next time an outage occurs whether or not she is on vacation. How would you implement a solution?","explanation":"You can use Amazon CloudWatch Events to detect and react to changes in the status of AWS Personal Health Dashboard (AWS Health) events. Then, based on the rules that you create, CloudWatch Events invokes one or more target actions when an event matches the values that you specify in a rule. Depending on the type of event, you can send notifications, capture event information, take corrective action, initiate events, or take other actions. Creating a Lambda function may be possible but is overly complicated. An AWS Config rule may also work but is not as efficient as using AWS Health directly. Amazon Inspector is used to assess security for applications deployed on EC2 and is not appropriate for this case.","links":[{"url":"https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html","title":"Monitoring AWS Health Events with Amazon CloudWatch Events"}],"answers":[{"id":"d34ecb714b1da223e3ec16bc80d4193b","text":"Configure an AWS Config rule that checks to see if any Regions are suffering outages. Have the configure trigger a Lambda function that will send an email to the CIO.","correct":false},{"id":"74245b5d3f6507fb9fbf04a80d0edc0a","text":"Send custom text or SMS notifications to the CIO with Amazon SNS when an AWS Health event happens by using Lambda and CloudWatch Events.","correct":true},{"id":"0825ebcd89b98147868bbea3888611bd","text":"Use Amazon Inspector to assess service health. Have Amazon Inspector produce reports for you to review and forward these reports to the CIO for those containing outages.","correct":false},{"id":"984716f59e8039a5c670fd67e008a4e7","text":"Create a Lambda function that parses through the AWS Service Health Dashboard to identify outages in certain Regions. Have the Lambda function email the CIO using SES.","correct":false}]},{"id":"85c8beb3-4040-4c16-80a2-28699caea7f7","domain":"mon-rep","question":"Your company is running dozens of EC2 instances. What kind of a solution would give near real-time visualizations of multiple EC2 instance metrics at once?","explanation":"You can gather the necessary metrics together in CloudWatch Dashboards for complete operational visibility.","links":[{"url":"https://aws.amazon.com/cloudwatch/","title":"CloudWatch"}],"answers":[{"id":"eaa240abb4f0731fca4c2e20cbbbfefe","text":"Add the metrics into a CloudWatch Dashboard.","correct":true},{"id":"af39109a1cd1a96dc8fc03cad521e886","text":"Visualize the metrics with QuickSight.","correct":false},{"id":"1ac4cde26a39fcabf5405c352b7bdff9","text":"Send the metrics to S3 and visualize them with S3 analytics.","correct":false},{"id":"beef56bb880b285559c0254491f4d5c9","text":"Organize the metrics into a CloudFront panel.","correct":false}]},{"id":"7x37ji7v-n162-1qmy-13gi-0uo4m7wzzhxv","domain":"data-man","question":"You need to upload a 7 TB object to S3. What's the best way to go about this?","explanation":"In S3, you can upload objects of up to 5 GB in size in a single operation. For objects greater than 5 GB you must use the multipart upload API. Using the multipart upload API you can upload objects up to 5 TB each.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectOperations.html","title":"Operations on S3 Objects"}],"answers":[{"id":"7945f6a9bf6e09f798e85934b6d3bad4","text":"This can be done in one updload.","correct":false},{"id":"ad38fdc937c75d02a8ce9f99dd2038ff","text":"Use Transfer Acceleration to expedite the transfer of the large object.","correct":false},{"id":"cc539f0a12c590bfaf4b0090f879ed46","text":"Use Snowball to deliver the large object to S3.","correct":false},{"id":"1f04ed6b37b4cb31587dbcf7bceb1483","text":"This cannot be done: the maximum S3 object size is 5TB.","correct":true}]},{"id":"3eaf9c7f-ec35-42cb-af59-f26b9b358432","domain":"automation","question":"Which of the following sections is required for a CloudFormation template to be valid?","explanation":"The Resources section is the only required section. It specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resources-section-structure.html","title":"CloudFormation - Resources"}],"answers":[{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":true},{"id":"229eb04083e06f419f9ac494329f957d","text":"Conditions","correct":false},{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":false},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false}]},{"id":"88de3a5c-b9ce-46eb-928f-f0f909bfe5dd","domain":"networking","question":"You own the registered domain name cloudcanines.com and are trying configure Route53 to map the DNS name to the DNS name of your Elastic Load Balancer. Which Route53 record can you use to achieve this?","explanation":"cloudcanines.com is a Zone Apex and you can't create a CNAME record at the zone apex. Instead you need to use an alias record to map the Zone Apex to the Elastic Load Balancer","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html","title":"Route53 Record sets"}],"answers":[{"id":"adc4bfdb0829dae99e3699393e3fbaa4","text":"CNAME","correct":false},{"id":"effdb9ce6c5d44df31b89d7069c8e0fb","text":"Alias","correct":true},{"id":"de46eab399f3ea0bbf1912c1d14a1544","text":"Zone Apex","correct":false},{"id":"0b98720dcb2cc6fd60358a45dfbc5b87","text":"MX","correct":false}]},{"id":"8b30bb8d-7142-4b59-8d3c-4d5033f31a8f","domain":"mon-rep","question":"You are working for a company which is migrating all of its data into S3, the migration is underway but your Security Architect is concerned that not all buckets are secure and wants you identify all buckets which allow public read or write. Which service can you use to find out?","explanation":"AWS Config allows gives you a view of the configuration of your AWS infrastructure and compares it for compliance against rules you can define","links":[{"url":"https://aws.amazon.com/blogs/aws/aws-config-update-new-managed-rules-to-secure-s3-buckets/","title":"AWS Config Rules to Secure S3"}],"answers":[{"id":"7c90c8f2a24f3a1a28525f19fb2c75ab","text":"AWS Inspector","correct":false},{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":true},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":false}]},{"id":"8473f19c-c3f7-4f2b-93a5-55ed10d99f83","domain":"mon-rep","question":"You run a hybrid environment with some servers in AWS and other Servers on Premise. Your boss has been impressed with your CloudWatch dashboard which shows the performance of all your EC2 instances around the world, however it does not show any metrics for your on premise servers. What could you do to rectify this?","explanation":"AWS Systems Manager Agent (SSM Agent) is Amazon software that runs on your Amazon EC2 instances and your hybrid instances that are configured for Systems Manager (hybrid instances). You can manually install SSM Agent on servers or virtual machines in your on-premises environment","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html","title":"SSM Agent"}],"answers":[{"id":"02a9311d5b1ac89de02acaf3c528c90a","text":"Install a third party monitoring agent and export performance data to the third party platform. Use AWS datapipeline to create a pipeline of this data from the third parties platform, in to CloudWatch","correct":false},{"id":"2b914a41bdba2bd7ceb93d6306696cd4","text":"Use AWS Storage Gateway and configure Gateway Monitoring Mode. Publish the monitoring statistics to S3 and use AWS Datapipeline to import this data in to CloudWatch","correct":false},{"id":"5aa17ac5ff5e522d0cbd19818e8b854d","text":"It is not yet possible to monitor on premise servers using AWS CloudWatch","correct":false},{"id":"38763eaec7d5f8126a56abeb236b9d27","text":"Install and run the SSM agent on your on premise servers. Once finished, install and run the CloudWatch agent on the on premise servers. Create the required role in IAM and verify that the agent is publishing data to CloudWatch. Add the widget for the on premise tools","correct":true}]},{"id":"17885db5-c61d-4edf-b0e3-e9d449d8e618","domain":"mon-rep","question":"Which of the following EC2 instance metrics are sent to Amazon CloudWatch by default? Select three.","explanation":"CPU utilization, disk I/O and network traffic are visible to the hypervisor running the instance and are sent to CloudWatch by default. For the others, you would need to install CloudWatch Agent on the instance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html","title":"Available CloudWatch Metrics for Your Instances"}],"answers":[{"id":"ec1e54ae04652319df5c011f228c07ac","text":"Free disk space","correct":false},{"id":"2105454033539f83d3b07265aac88d7a","text":"The amount of swap space currently in use","correct":false},{"id":"613b1188dd73dfdb768f39cfad3cc9a3","text":"Memory utilization","correct":false},{"id":"b4e5bb2b6842990e919682b3d6d5726c","text":"Volume of incoming and outgoing network traffic","correct":true},{"id":"fb8326e1edbd06b1bf6ea0332e089055","text":"CPU utilization","correct":true},{"id":"c4903df1e41e0ba0b4636e753d8c7661","text":"Disk read and write operations","correct":true}]},{"id":"95b5fb6b-4742-4c0a-8563-4dc9dc116700","domain":"mon-rep","question":"You have been asked to monitor custom metrics generated by your own applications and services. There is a need for these metrics to be collated and returned to the product owner each week so they can plan for the following sprint's requirements. What services would you use to be able to provide this automated report?","explanation":"You can set up monitoring of custom metrics in CloudWatch, from this you can use CloudWatch also to trigger a Lambda Function at a scheduled time, which will collect use AWS SDK to pull the metrics into a report that will be sent via email using the SNS service","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"Publishing Custom Metrics"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html","title":"Using AWS Lambda with Amazon CloudWatch Events"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-sns-example.html","title":"Tutorial: Using AWS Lambda with Amazon Simple Notification Service"}],"answers":[{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"f79368a88745f4dba5e0fc92aa545c61","text":"EKS","correct":false},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":true},{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":true},{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":true}]},{"id":"066a63c9-1a0c-454f-8eeb-628657c4b7b3","domain":"security-comp","question":"As a security administrator for your company, the development team has asked for your advice on protecting their web product running on AWS against SQL injection attacks. Recently, there have been several cases where attackers have tried to insert certain malicious SQL queries to extract data from a database that stores confidential customer data. The development team manages and runs the database on EC2 running behind a load balancer. What advice would you give to the team to proactively protect against these kinds of attacks?","explanation":"There are several firewall services that AWS has provided including AWS WAF, AWS Shield, and AWS Firewall Manager. But in this case an ACL with WAF would be the most appropriate. AWS Firewall Manager simplifies your administrative and maintenance tasks across multiple accounts and resources for AWS WAF. AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to a load balancer. You can block/allow all requests except the ones your specify. AWS Shield Advanced would protect against DDOS attacks. AWS Config would only provide notifications and thus would be a reactive solution to attacks.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager?"}],"answers":[{"id":"3e1ffae7dc069fcf4bdce431a89b4792","text":"Create a rule in AWS Firewall Manager to explicitly block the IP addresses that were listed as the attackers.","correct":false},{"id":"3b888c9ab75dfc1fd31b6fdcec298c41","text":"Use AWS Config to monitor the application. Set a rule to notify the development team when a malicious attack occurs.","correct":false},{"id":"c84b52c0e32a45fc862e101beb233230","text":"Activate AWS Shield Advanced. Although costly, it will protect the application with a 24/7 response team from AWS, and full system and financial restoration after an attack.","correct":false},{"id":"5cfd6d4faff387749033b7f3e493f870","text":"Create a WAF Access Control List (ACL) with a rule to explicitly block the SQL injection attacks. Attach the ACL to the load balancer.","correct":true}]},{"id":"4c7fb750-0ec0-4c9f-8dc6-378122edf97a","domain":"automation","question":"The engineering team of a digital marketing company has a lot of AWS Lambda functions directly created and managed using the AWS Console. The CTO has mandated that the code and the deployments are managed using templates and the code is stored in a code repository to enable proper version control processes. How can the team achieve this?","explanation":"SAM templates and CloudFormation templates can be used to manage the Lambda function code. Out of all the options, only CodeCommit can be used directly as a managed service for a code repository. S3 buckets are not used directly as a code repository.","links":[{"url":"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html","title":"What is SAM"}],"answers":[{"id":"f33ec453646a2074aa930a61f466fc1a","text":"Use CloudFormation templates to manage the lambda function code. Use ECR for the code repository.","correct":false},{"id":"a85a26cd33fe99adb584452fa780dce2","text":"Use CloudFormation templates to manage the lambda function code. Use S3 buckets for the code repository.","correct":false},{"id":"c5fe89ee64d944e98f26b2db3fa7cea2","text":"Use SAM templates to manage the lambda function code. Use CodeCommit for the code repository.","correct":true},{"id":"b670d764967a3aa65a1424279e37291c","text":"Use SAM templates to manage the lambda function code. Use S3 buckets for the code repository.","correct":false}]},{"id":"b2d3b949-889b-4bbd-8ec9-c65b764c47c3","domain":"mon-rep","question":"You are a SysOps Administrator monitoring a web app that lets users upload high-quality images and use them online. Each image requires resizing and encoding. The images are placed in an Amazon SQS queue for processing by an EC2 instance. It processes the images and then publishes the processed images where they can be viewed by users. When you monitor the EC2 instance you see that the CPU utilization is consistently at 90% and that image processing time is being delayed. The team is looking for a cost-effective solution. What would you recommend?","explanation":"You can use an Auto Scaling group to manage EC2 instances for the purpose of processing messages from an SQS queue. Set a custom metric to send to Amazon CloudWatch that measures the number of messages in the queue per EC2 instance in the Auto Scaling group, and then set a target tracking policy that configures your Auto Scaling group to scale based on the custom metric and a set target value. CloudWatch alarms invoke the scaling policy. Increasing the size of the instance may work but is not a cost-effective solution since Auto Scaling gives you the option to scale down during low demand. Kinesis Data Streams are best suited for real-time data processing, and they have a size limit of 1MB which would be too low for high-quality images. Migrating the data to DynamoDB would not be a viable, let alone cost-effective, solution.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html","title":"Scaling Based on Amazon SQS"}],"answers":[{"id":"40fe2365fa7bae167e628c9ef29bd6ca","text":"Move the images into Kinesis Data Streams where you'll be able to process the data in real time.","correct":false},{"id":"3a781b7e075f07b31a51c98b18d84a2e","text":"Increase the size of the instance and ensure that it is compute-optimized to boost it's capacity to process the images.","correct":false},{"id":"52bcbdff61b39eafa67d9496dc77ee09","text":"Migrate the image data into DynamoDB. Attach a role to the instance to be able to access the data from DynamoDB and process the images.","correct":false},{"id":"c1c44e18b3c0f81b8ec026e9e1ab5b38","text":"Place the instance in an Auto Scaling group. Use CloudWatch metrics to scale out the Auto Scaling group depending on the size of the SQS queue.","correct":true}]},{"id":"af9e096c-e1b0-4f1e-9102-84dac1fab349","domain":"dep-prov","question":"You are trying to delete an EBS volume with the volume ID of vol-129df77122c4d7208 using the AWS CLI. You need to make sure that you have the required permissions before you perform the delete action. Which of the following commands will achieve this?","explanation":"The '--dry-run' parameter checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation. Otherwise, it is UnauthorizedOperation. The '--generate-cli-skeleton' parameters prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value 'input', it prints a sample input JSON that can be used as an argument for the '--cli-input-json' flag. '--dryrun' and '--perform-action no' are not valid parameters.","links":[{"url":"https://docs.aws.amazon.com/cli/latest/reference/ec2/delete-volume.html","title":"EC2 CLI documentation: delete-volume"}],"answers":[{"id":"153993f9bb7a685925820e05c073cffd","text":"aws ec2 delete-volume --volume-id vol-129df77122c4d7208 --perform-action no","correct":false},{"id":"efbae1a8e8a0b8190dfd2fcc000acff5","text":"aws ec2 delete-volume --volume-id vol-129df77122c4d7208 --dry-run","correct":true},{"id":"21a0e55daf1816e8554b456fd8f16f9d","text":"aws ec2 delete-volume --volume-id vol-129df77122c4d7208 --dryrun","correct":false},{"id":"7b8ce5990ccec3f3375d86cb12a57fa4","text":"aws ec2 delete-volume --volume-id vol-129df77122c4d7208 --generate-cli-skeleton","correct":false}]},{"id":"08b69906-407e-4ec7-9995-849fef7431a3","domain":"dep-prov","question":"Your organization provides a service using an Application Load Balancer with 3 EC2 instances as registered targets. The organization no longer needs to provide the service so you attempt to delete the ALB via the AWS CLI. What will happen?","explanation":"Deleting a load balancer does not affect its registered targets. For example, your EC2 instances continue to run and are still registered to their target groups. The command will not fail as it is possible to delete an ALB registered targets.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-delete.html","title":"Delete an Application Load Balancer"}],"answers":[{"id":"c9e00101dcbdd43dd5940309daf0b02f","text":"The ALB will be removed and the target instances will be terminated","correct":false},{"id":"af18f004917f3bcc9a1a8dc6b867b1f0","text":"The command will fail because there are instances attached to the ALB","correct":false},{"id":"2a4e55639f55031ca059e7173f17bc5b","text":"It is not possible to delete an ALB using the CLI","correct":false},{"id":"2f4978a4d0f6296a0c354b89adb25f86","text":"The ALB will be deleted but the instances will still be running","correct":true}]},{"id":"9efe0b3f-2e6a-4918-81b2-c1a827654892","domain":"networking","question":"Which of the below statements about subnets and CIDR blocks as part of a VPC in AWS is correct?","explanation":"The maximum size for a subnet is /16 so this is a correct answer. Default behaviour is that every time you create a subnet it is added to the main route table - allowing other subnets using the main route table to route to it. As for the incorrect statements - the minimum size of a subnet is /28 (16 addresses) therefore /30 is incorrect, subnets are local to a single AZ and cannot span multiple AZs (although VPCs in which they live can). Every VPC will need an IPv4 CIDR allocated - even if the intent is just to use IPv6","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html","title":"VPC Subnets"}],"answers":[{"id":"32df7fa7ee27b28e38ab223822a82204","text":"The minimum size for a subnet is /30 with 4 addresses","correct":false},{"id":"ed1e31d6b69887380df735258ef1bd1a","text":"A subnet that will only be used for IPv6 doesn't need an IPv4 CIDR associated ","correct":false},{"id":"22b96919d7cd98b93972df376d49d356","text":"By default, all subnets you create can route to each other","correct":true},{"id":"badb9d21322bff4ed8278d92ebc7174b","text":"The maximum size for a subnet is /16 with 65,536 addresses","correct":true},{"id":"70426e4ee2fa35fde1533f02d72886a1","text":"A subnet can span multiple AZs","correct":false}]},{"id":"78ad6dfb-14bd-44bf-a5a4-4af2076aae23","domain":"networking","question":"As your company's lead network administrator, you are helping the development team set up a VPC for an application in their AWS account. The application requires a network configuration such that the web servers of the application have connectivity to the Internet, and the database servers have VPN-only connectivity to the corporate network servers. What VPC set up would support this desired configuration? (Select all that apply)","explanation":"The scenario requires a VPC with an internet gateway, a virtual private gateway, a public subnet, and a VPN-only subnet. One route table has a route to the virtual private gateway in a private subnet. Another route table is explicitly associated with the public subnet. The custom route table has a route to the internet (0.0.0.0/0) through the internet gateway. A NAT instance in another private subnet would not allow Internet connectivity. The Direct Connect connection is unnecessary. The requirements does not allow placing database servers in public subnets.","links":[{"url":"https://docs.aws.amazon.com/en_pv/vpc/latest/userguide/VPC_Route_Tables.html","title":"Route Tables"}],"answers":[{"id":"4d167d5d0ddda70960f354da47b72e33","text":"Place the web servers in a private subnet. Associate a route table to the private subnet that has a route to a NAT instance in another private subnet.","correct":false},{"id":"f8f4044343da77cd7ab9710d8b6f5067","text":"Place the web servers in a public subnet. Associate a route table to the public subnet that has a route to the Internet through the Internet Gateway.","correct":true},{"id":"d7235003c798151a119f9b00b31cce7f","text":"Place the database servers in a public subnet with Direct Connect. Set up a Direct Connect connection to the servers in your on-premises environment.","correct":false},{"id":"31d58d613e8532ad19cdb0c73912f36d","text":"Place the database servers in a private subnet. Associate a route table to the private subnet that has a route to a virtual private gateway.","correct":true},{"id":"ac748fb5daa7fcaff151922fb2c482a4","text":"Place the database servers in a public subnet. Associate a route table to the public subnet that has a route to the Internet through the Internet Gateway.","correct":false}]},{"id":"9f4a3688-de11-407f-99a7-f332ade4066e","domain":"dep-prov","question":"You are running an EC2 instance and have created and attached an EBS volume with default settings. You notice that the volume status check for the volume has failed and the instance can no longer access the volume. How can you access the information on the volume?","explanation":"When Amazon EBS determines that a volume's data is potentially inconsistent, it disables I/O to the volume from any attached EC2 instances by default. This causes the volume status check to fail, and creates a volume status event that indicates the cause of the failure. Switching on Enable Volume IO will allow the instance to access the volume. Switching on Auto-Enabled IO will also achieve the same outcome automatically but this is disabled by default. All other options are incorrect.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html#work_volumes_impaired","title":"Working with an Impaired Volume"}],"answers":[{"id":"628b701156eef0c168e27dc6d58a9d15","text":"Switch off Auto-Enabled IO","correct":false},{"id":"7de363a9824728f9b2e5c0a8efd8c6c3","text":"Switch off Enable Volume IO","correct":false},{"id":"8daa3d585fbafd5057d0e3bf2af521e0","text":"The volume can no longer be accessed","correct":false},{"id":"bf737cfd17a3b913cb405f896e5e9b7d","text":"Switch on Enable Volume IO","correct":true}]},{"id":"939386e8-1d77-444a-a32f-4191025a74e2","domain":"networking","question":"You are working on a project to launch a new online trading application. Many of your competitors have already suffered DDOS attacks and your Security Architect wants to know how you can mitigate against these sorts of attacks. What do you suggest?","explanation":"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS WAF provides protection against different types of attack like SQL injection and cross-site scripting but not specifically DDOS.","links":[{"url":"https://aws.amazon.com/shield/faqs/","title":"AWS Shield FAQs"}],"answers":[{"id":"f0f0e94fb03611cf4f38fe1117e6e76e","text":"Use AWS WAF","correct":false},{"id":"8517d0c67040a9d1b4358594409eeae0","text":"Use Security Groups and Network ACLs","correct":false},{"id":"83ad2fe9589162c814a040ab0d753c45","text":"Use AWS Shield","correct":true},{"id":"059fc274da3193f55a6bb268b21e06ce","text":"Host based IDS / IPS","correct":false}]},{"id":"5660d734-c8d8-404c-b683-3d43db3c17e1","domain":"mon-rep","question":"You are a SysOps Administrator for your company. Your CFO notices that costs have increased steadily for the past year, and tasks you with analyzing cost and usage of the company’s AWS environment based on tagged resources. What is the most effective way to analyze your company’s AWS spend?","explanation":"Choose Cost Explorer to track and analyze your AWS usage. Cost Explorer is free for all accounts and can filter by Region, purchase option, tags, among other things. Trusted Advisor provides areas to optimize costs but doesn't provide cost and budget reports. Developing a Lambda function to calculate spend would be an administrative burden, and so would comparing different AWS Config environments. Both would require manual efforts to leverage AWS' pricing API. It's much more efficient to utilize AWS' free Cost Explorer with built-in reporting functionality.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-getting-started.html","title":"AWS Billing and Cost Management"}],"answers":[{"id":"c307cf10883e9081648ee198aebeb387","text":"Download cost optimization and budgeting reports from Trusted Advisor as a CSV. Filter downloaded data by tags.","correct":false},{"id":"b1204a52a0664ddd8a75b740204e6b88","text":"Enable the Cost Explorer tool to track and analyze your AWS usage. Filter spend by tags.","correct":true},{"id":"dbbdf8cec2b6f9217bcf2d39ad3ac859","text":"Create a Lambda function that is invoked for every CloudTrail Event. Have the Lambda function calculate spend based on each AWS services’ API calls using the tag key.","correct":false},{"id":"343d6ae3fa3cd5f44bce786dbe8e8b8b","text":"Use AWS Config to evaluate the configuration of your AWS environment last year. Create a custom rule to analyze by tag. Compare last year’s configuration to this year's and calculate the difference in spend.","correct":false}]},{"id":"4871ce12-9da0-4e28-b1e4-373560dbdffa","domain":"automation","question":"A telecommunications company sends out monthly bills to their customers. Usage is accumulated during the month by nightly batch jobs that process call details. The company is in the process of migrating the billing system to AWS to reduce costs. What approach will provide them with the most cost effective solution for the compute portion of their nightly batch runs?","explanation":"AWS Batch provides allocation strategies to consider capacity and throughput in addition to cost when provisioning instances for jobs. This is a newer feature that provides more flexibility than the previous scheme that chose an instance that was the best fit based on vCPU, memory, and GPU requirements. Creating a pool of EC2 Reserved Instances might result in unused capacity if workload requirements change. Lambda is not currently available as a compute resource for AWS Batch.","links":[{"url":"https://aws.amazon.com/batch/","title":"AWS Batch"},{"url":"https://aws.amazon.com/blogs/compute/optimizing-for-cost-availability-and-throughput-by-selecting-your-aws-batch-allocation-strategy/","title":"Optimizing for cost, availability and throughput by selecting your AWS Batch allocation strategy"}],"answers":[{"id":"3220afaa7c6fd31e7a8d35ce1e2df1fa","text":"Configure AWS Batch to choose an instance type for each job based on vCPU, memory, and GPU requirements at the lowest cost.","correct":false},{"id":"96ef18a4d93470acb7dbd558eb666ca3","text":"Specify AWS Lambda as the compute resource for AWS Batch. Invoke the appropriate Lambda functions for each job.","correct":false},{"id":"0bb4d60773589240e55a5a506ee84275","text":"Use AWS Batch allocation strategies to define capacity, throughput, and cost priorities for instance type provisioning.","correct":true},{"id":"8633cd86b6633324660fa073362c2f98","text":"Schedule jobs with AWS Batch into a pool of EC2 Reserved Instances that contains enough servers for the minimum number of jobs that will be run on any one night. Use an Auto Scaling Group to provision Spot Instances to handle any additional demand.","correct":false}]},{"id":"6eb6dee0-9456-492e-afc7-b7c860b04c92","domain":"mon-rep","question":"You have a fleet of EC2 webservers behind an application load balancer. Your web application had some down time which involved some 5XX errors during a very important time in your business 1 week ago. Although you maintain application logs on individual EC2 instances, you do not store these logs anywhere central and unfortunately the EC2 instances that experienced the downtime have since been terminated. How could you review this log data?","explanation":"Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. These logs are encrypted and stored in S3.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html","title":"Application Load Balancer Access Logs"}],"answers":[{"id":"d53665e5cde1c8f661130ca2882d789a","text":"Open the AWS artifact service. Create a new artifact job and point the AWS artifact agents at the terminated EC2 instances. Download the metrics and review in CloudWatch.","correct":false},{"id":"ae59c599efa75d438daecb43cc08ad41","text":"Use AWS X-ray to restore the logs from the terminated EC2 instances","correct":false},{"id":"49774faac3030c4425f2aa345cb89dc0","text":"If access logs is turned on for your application load balancer you could review this data by reviewing the logs in S3.","correct":true},{"id":"c13c66278433ebb606e21e9c9f5250fd","text":"Create a new AWS inspector job to pull the snapshots of the EC2 instances from S3 and run a report in conjuction with AWS Athena.","correct":false}]},{"id":"6da1a2f4-4dc6-48be-8e42-c838e3815602","domain":"data-man","question":"Your CTO wants to store company data in the Cloud. The full migration plan includes moving 500 TB of data to Amazon S3. What would be the fastest, and most cost-effective way to move this amount of data to AWS?","explanation":"Snowball is a strong choice for data transfer if you need to more securely and quickly transfer terabytes to many petabytes of data to AWS. As a rule of thumb, if it takes more than one week to upload your data to AWS using the spare capacity of your existing Internet connection, then you should consider using Snowball. If you have a 100 Mb connection that you can solely dedicate to transferring your data and need to transfer 100 TB of data, it takes more than 100 days to complete data transfer over that connection. You can make the same transfer by using multiple Snowballs in about a week. Direct Connect would take longer and is more expensive, and there would be no need for the connection after moving the 500 TB. VPN is faster to set up than Direct Connect but would take even longer. Storage Gateway would have the same issues and is ideally meant to be used as a hybrid cloud storage service.","links":[{"url":"https://aws.amazon.com/snowball/faqs/","title":"AWS Snowball FAQs"}],"answers":[{"id":"6459fb2dead422bbe8c534a859c17e67","text":"Set up a Direct Connection connection between your on-premise data center and AWS. Transfer the data over the Direct Connect connection.","correct":false},{"id":"4d4f0e9f5d90bedd7d63f07e5d7102a7","text":"Transfer your data with multiple instances of Snowball. Install the Snowball client on multiple workstations and transfer data to the Snowball devices.","correct":true},{"id":"49d1df753d0b6625015b1b5a57b60b6a","text":"Set up a VPN connection between your on-premise data center and AWS. Transfer the data over IPSec Tunnel using encryption.","correct":false},{"id":"37724cf12ad7358208dd202bd2037e64","text":"Use AWS Storage Gateway using file gateway and copy the data to S3 using the file gateway mount point.","correct":false}]}]}}}}
