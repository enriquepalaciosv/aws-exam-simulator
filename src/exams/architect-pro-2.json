{"data":{"createNewExamAttempt":{"attempt":{"id":"b0d93926-3d38-467b-91cc-9af60d38862c"},"exam":{"id":"76f13057-60ed-4fa6-81e1-e2cb707e4168","title":"AWS Certified Solutions Architect - Professional Exam","duration":10800,"totalQuestions":77,"questions":[{"id":"93badb2c-68ab-4715-b29e-1209af0c7b27","domain":"awscsapro-domain2","question":"You are a developer for a Aerospace company.  As part of an outreach and education program, the company has financed the construction of a free public service that provides weather forecasts for the sun.  Anyone can make a call to this REST service and receive up-to-date information on forecasted sun flare or sun spots that might have an electromagnetic impact here on Earth.  You are in the final stages of developing this new serverless application based on DynamoDB, Lambda and API Gateway.  During performance testing, you notice inconsistent response times for the service.  You had expected the API to be relatively consistent since its just retrieving data from DynamoDB and returning it as JSON via the API Gateway.  What might account for this variation in response time?","explanation":"Inconsistent response times can have a few different causes.  The exact nature of the testing is not explained but we can anticipate a few causes.  If you have enabled API Gateway caching, the gateway can return a result from its cache without having to go back to a supplying service or database.  This can result in various response rates depending on if an item is in the cache or not.  (The question did not specify we had slow response...just inconsistent response which could be a response faster than we expected.)  When a Lambda function is run for the first time or after an update, AWS must provision the Lambda environment and pull in any external dependencies.  This can result in a slower response time at first but faster later.  Also, if we do not have sufficient RCU for our DynamoDB table, we could run into throttling of the reads which could appear as inconsistent response times.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html","title":"AWS Lambda Execution Context - AWS Lambda"},{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html","title":"Enable API Caching to Enhance Responsiveness - Amazon API Gateway"}],"answers":[{"id":"f13e59f5ef22ace9340e24f434eb09cb","text":"You are experiencing a cold start.","correct":true},{"id":"4c3254ba1ddb0fb7a3ce3824707a7105","text":"You have enabled caching on the API Gateway.","correct":true},{"id":"43fb818a88f47b6b3b136abdd40a15a6","text":"There are not enough open inbound ports in your VPC.","correct":false},{"id":"158838df2d14636ed1429fffbcc6825e","text":"Your DynamoDB RCUs are underprovisioned.","correct":true},{"id":"4922e0fbbf140cd0dbb6458bbadbf268","text":"The data is being updated on DynamoDB at the exact same time you are trying to read it.","correct":false},{"id":"3bd10d778b04bfa32a7338a04e832d38","text":"The CloudFront distribution used by API Gateway is not deployed fully yet.","correct":false},{"id":"b03fe2e16e6904dc4b5fa34c91c1f855","text":"You are using HTTP rather than HTTPS.","correct":false}]},{"id":"e8bba7f5-4c0d-42dd-ad7a-74f042ce3dd9","domain":"awscsapro-domain3","question":"Due to a dispute with their co-location hosting company, your client is forced to move some applications as soon as possible to AWS.  The main application uses IBM DB2 for the data store layer and a Java process on AIX which interacts via JMS with IBM MQ hosted on an AS400.  What is the best course of action to reduce risk and allow for fast migration?","explanation":"For a fast migration with minimal risk, we would be looking for a lift-and-shift approach and not spend any time on re-architecting or re-platforming that we don't absolutely have to do.  Amazon MQ is JMS compatible and would provide a shorter path to the cloud than SQS.  DMS does not support DB2 as a target.","links":[{"url":"https://aws.amazon.com/amazon-mq/features/","title":"Amazon MQ Features â€“ Amazon Web Services (AWS)"},{"url":"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.html","title":"Targets for Data Migration - AWS Database Migration Service"}],"answers":[{"id":"6d8610d6575127564b286722b73ce4be","text":"Install DB2 on an EC2 instance and use DMS to migrate the data.  Encapsulate the Java program in a Docker container and deploy it on ECS.  Spin up an instance of Amazon MQ.","correct":false},{"id":"3700f4c1ab4e0778c4d0ae131d9c277d","text":"Deploy the Java processes as Lambda functions.  Install DB2 on an EC2 instance and migrate the data by doing an export and import.","correct":false},{"id":"4dba67b80e52ce08ef39ca56bf0ddd57","text":"Install DB2 on an EC2 instance and migrate the data by doing an export and import.  Spin up an instance of Amazon MQ in place of IBM MQ.  Install the Java process on a Linux-based EC2 system.","correct":true},{"id":"21cbd80d1d890d3668ba4f841d7901df","text":"Use a physical-to-virtual tool to convert the AIX DB2 server into a virtual machine.  Use AWS CLI to import the VM into AWS and launch the VM.  Deploy the Java program as a Lambda function.  Launch a version of IBM MQ from the AWS Marketplace.","correct":false},{"id":"fbc4f7d74e4d62a2bdce9ca3f55f9fd2","text":"Use DMS and SCT to migrate DB2 to Aurora.  Update the Java application to use SQS and install it on a LInux-based EC2 system.  ","correct":false}]},{"id":"722221be-beb9-4a2b-8ae1-dff52b80125c","domain":"awscsapro-domain3","question":"You work for a technology company with two leased data centres (one on the east coast and one on the west coast) and one owned on-premises data centre. Management has decided to move the two leased data centres to the AWS cloud - one to us-east-1 and the other to us-west-1. The on-premises data centre will still continue running workloads which are not ready to move to the cloud.\nThis on-premises data centre must be always connected to the VPC-s in us-east-1 and us-west-1 for (a) the continuous replication of several databases and (b) the need to access some data residing on the on-premises data centre from applications running in both the AWS regions. The peak bandwidth required for these connections is (a) 500 Mbps between us-east-1 and on-premises, and (b) 8 Gbps between us-west-1 and on-premises. The applications would still be able to function at lower bandwidth, but the experience will be poor, which is not desirable. Both these connections must be Highly Available with 99.999% uptime. The connectivity solution must be cost-effective as well.\nAs the AWS Architect, what connectivity solution would you propose, so that all Bandwidth, HA and cost-effectiveness requirements are met?","explanation":"We can eliminate the VPC Peering solution immediately, as VPC Peering is for connecting two VPC-s on AWS. VPC Peering cannot be used to connect an AWS VPC with an on-premises network.\nOut of the remaining choices, the one that proposes connecting to us-east-1 using VPN and us-west-1 using Direct Connect comes very close to fulfilling all requirements. It suffers from two problems, however. One - the peak bandwidth requirement for us-east-1 is 500 Mbps. A VPN connection cannot be expected to provide 500 Mbps most of the time, as the true bandwidth someone can get from a VPN connection depends on a lot of factors including internet traffic it is sharing the route with. Secondly, if we are paying for a Direct Connect connection for the other region anyway, why not just use that one for this region too? Now, there is something called Direct Connect Gateways that makes it possible to share multiple AWS Regions using the same Direct Connect connection. The knowledge of Direct Connect Gateways is important for the AWS SA-P exam. Hence, this question tests this knowledge. The correct answer is the only one that uses Direct Connect Gateway.\nThe other choice that uses two separate Direct COnnect connections (one for each region) is not cost-effective, especially because since 2017, Direct Connect Gateways make it possible to connect to multiple AWS Regions using the same Direct Connect connection.\nRegarding HA, it is always a good practice to set up a VPN connection as a back-up for Direct Connect. The only requirement to do this is that the back-up VPN connection must also use the same Virtual Private Gateway on the AWS VPC side, otherwise traffic cannot fail over easily.\nNote about Direct Connect Gateways - they not only allow a customer to connect to two AWS Regions using a single Direct Connect connection, they also let the connected Regions communicate with each other! (This is why the VPC CIDR-s in us-east-1 and us-west-1 in the correct answer have to be non-overlapping.) There may be questions testing this aspect as well. Before Direct Connect Gateways existed, VPC Peering would be the only way for Inter-Region VPC Access. There is also another solution now - Transit Gateway, but this was announced late 2018. Usually, topics do not start appearing on the exam unless they have been more than 6 months in GA. Expect Transit Gateways to start appearing in questions now as well!","links":[{"url":"https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/","title":"AWS Direct Connect Gateway for Inter-Region VPC Access"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/","title":"Configuring VPN as back up of Direct Connect"},{"url":"https://aws.amazon.com/directconnect/sla/","title":"Direct Connect SLA puts uptime target at 99.9%. Therefore, if we need more than that, we should set up VPN as back up"}],"answers":[{"id":"f0671c30ae58d58c0d64162640e69527","text":"Use two Direct Connect connections - an 1 Gbps one between the on-premises data centre and us-east-1, and a 10 Gbps one between the on-premises data centre and us-west-1. For each Direct Connect connection, set up a back-up VPN connection that must use the same Virtual Private Gateway as the Direct Connect circuit","correct":false},{"id":"3ea4a9645ac22c0b2c6c427764a7ae01","text":"Set up a Direct Connect Gateway. Associate the Virtual Private Gateways from both the us-east-1 and us-west-1 VPC-s with this Direct Connect Gateway. Then set up a single 10 Gbps Direct Connect connection between the on-premises data centre and the Direct Connect Gateway, using a Private Virtual Interface. Ensure that the VPC CIDR-s in the two AWS Regions are non-overlapping. To increase HA, set up separate back-up VPN connections between the on-premises data centre and each of the two AWS Regions","correct":true},{"id":"608dc6c035921ef09c640ebb70de9ddd","text":"Use VPC Peering to connect the on-premises network with both the us-east-1 and us-west-1 VPC-s independently. Bandwidth provided by VPC Peering is virtually unlimited, limited only by the instance sizes used. Also, VPC peering connections are fault-tolerant and scalable, so no back-up connectivity is needed","correct":false},{"id":"85b6b49a01a28a90d71ef3c20ca9da8d","text":"Connect the on-premises data centre and us-east-1 using redundant site-to-site VPN connections as its bandwidth requirements do not require a costly Direct Connect connection. The redundant VPN connections must use different customer gateways and will provide an HA solution for that region. Connect the on-premises data centre with us-west-1 using a 10 Gbps Direct Connect circuit. Set up a back-up VPN connection for this region such that it uses the same Virtual Private Gateway as the Direct Connect circuit","correct":false}]},{"id":"c2d46981-3dac-4e68-81d1-9eedf0cbf264","domain":"awscsapro-domain2","question":"Your company is preparing a special event for its 100th year in business.  As part of that event, the event committee would like to create a kiosk where employees can browse the thousands of photographs captured over the years of the employees and company events.  In a brainstorming session, one event staff member suggests the crazy idea of allowing employees to quickly pull up photographs which they appear in.  What AWS service might be able to make this a reality?","explanation":"AWS Rekognition is a service that can detect and match faces in a photograph.  The kiosk could include a camera that allows event-goers to snap a picture of themselves and then it could scan the photo archive for facial matches.","links":[{"url":"https://aws.amazon.com/rekognition/","title":"Amazon Rekognition â€“ Video and Image - AWS"}],"answers":[{"id":"438e06c02cba48ce4ffbf026d97488b4","text":"Amazon DeepView","correct":false},{"id":"ade161cea9b509c72570ba6ae5238e5f","text":"Kinesis for Video","correct":false},{"id":"07c025c347c7483abdd039cd36be4220","text":"AWS Rekognition","correct":true},{"id":"cbe32e918d6a248e3e8ed74e6f0b72f6","text":"AWS Comprehend","correct":false},{"id":"42e6f83b4b8205e6c2e62ddafdd3bbe3","text":"Amazon Chime","correct":false}]},{"id":"f02ba751-479b-4ff0-a09b-8f18a63177b5","domain":"awscsapro-domain3","question":"An automotive supply company has decided to migrate their online ordering application to AWS. The application leverages a Model-View-Controller architecture with the user interface handled by a Tomcat server and twenty thousand lines of Java Servlet code. Business logic also resides in two thousand lines of PL/SQL stored procedure code in an Oracle database. The company's technology leadership has directed your team to move the database to a more cost-effective offering, and to adopt a more cloud-native architecture. Business objectives dictate that the application must be live in the AWS cloud in sixty days. Which migration approach will provide the most scalable architecture and meet the schedule objectives?","explanation":"This solution will require trade-offs between schedule requirements and architectural desires. Converting twenty thousand lines of Model-View-Controller code to a serverless architecture in sixty days is unreasonable, so moving the Tomcat MVC as-is to EC2 for the initial migration is the best approach. We can migrate to a serverless user interface in a later phase. Database Migration Service will suit our needs well for moving the application data to Aurora, but the most scalable architecture strategy is to migrate the stored procedure code out of the database so that database nodes won't need to be resized when the business logic needs more compute resources. Under normal circumstances, recoding two thousand lines of PL/SQL code to Python Lambda functions within a sixty day time frame will not be a problem.","links":[{"url":"https://aws.amazon.com/dms/","title":"AWS Database Migration Service"},{"url":"https://aws.amazon.com/blogs/database/migrate-your-procedural-sql-code-with-the-aws-schema-conversion-tool/","title":"Migrate Your Procedural SQL Code with the AWS Schema Conversion Tool"},{"url":"https://aws.amazon.com/lambda/","title":"AWS Lambda"}],"answers":[{"id":"97a348ed01424c357958b49bcc030935","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"db222d8a15bda541fc4147908131cfd6","text":"Migrate the Tomcat server and Servlet code to EC2. Use AWS Database Migration Service to move the application data into Amazon Aurora. Convert the stored procedure code to AWS Lambda Python functions, and modify the Servlet code to invoke them","correct":true},{"id":"2cf84f7f8daee7548143ad181423c7cb","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service and the AWS Schema Conversion Tool to migrate the application data and stored procedures to Amazon Aurora","correct":false},{"id":"5ab40b2a54c4e82a7aafa05c8fc9a458","text":"Convert the Servlet Code to JavaScript Lambda functions accessed through Amazon API Gateway. Use AWS Database Migration Service to migrate the application data and stored procedures to an Amazon RDS Oracle instance","correct":false}]},{"id":"8765bd56-057b-488c-9a0a-f5bd413dd240","domain":"awscsapro-domain5","question":"Due to new corporate policies on data security, you are now required to use encryption at rest for all data.  You have some EC2 Linux instances on AWS that were created without encryption for the root EBS volume.  What can you do that meet the requirement and reduce administrative overhead?","explanation":"AWS does support encrypted root volumes but conversion from unencrypted root to an encrypted root requires a bit of a process. You must first create an AMI then copy that newly created AMI to the same region, specifying that you want to encrypt the EBS volumes during the copy.  You can then create a new instance with an encrypted root volume from the copied AMI.  You can use either a generated key from KMS or your own CMK imported into KMS.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html","title":"AMIs with Encrypted Snapshots - Amazon Elastic Compute Cloud"},{"url":"https://aws.amazon.com/blogs/aws/new-encrypted-ebs-boot-volumes/","title":"New â€“ Encrypted EBS Boot Volumes | AWS News Blog"}],"answers":[{"id":"50c17b27f0bd0390ae321943f7db5c3d","text":"Create a certificate in CMS for the encryption key.  Stop the instances and temporarily detach the root volumes.  Via the AWS CLI, enable encryption on the root volumes using the \"ebs modify-volume\" argument with the flag of \"encryption=<CMS ARN>\" to specify the certificate.","correct":false},{"id":"1e30ccf0b75e9e70fd76c6e041510c75","text":"At present, EC2 does not support encrypted root volumes.  Create new encrypted EBS data volumes and attach the new volumes to the existing instances.  Use RSYNC to migrate all the non-OS data over to the encrypted data volumes.","correct":false},{"id":"180e9aecdb74f204b1df00ffe6fa8b56","text":"Stop the instances and create AMIs from the instances.  Copy the AMIs to the same region and select the \"Encrypt target EBS snapshots\".  Redeploy the instances using the AMI copies you made with encrypted root volumes.","correct":true},{"id":"9fbbb2e71386cb7f7a7ed77129a1a960","text":"Create an encrypted EFS instance and mount-points in the respective subnets.  Log into the instance and mount an encrypted EFS mount-point.  Copy all the root files over to the EFS mount point.  Edit the FSTAB file to mount the EFS mount point as the root volume instead of the root EBS device and reboot.","correct":false},{"id":"4430b7492a6c058c3574ce4e8ea43955","text":"Stop the instances and temporarily detach the EBS volumes.  Attach the root volumes to another EC2 instance and mount them a data volume.  Use a encryption tool like GPG or OpenPGP to recursively encrypt all the files on the mounted root volumes.  Detach and reattach the encrypted EBS volumes to the original instances and restart.  Import the encryption keys in KMS as a CMK.","correct":false}]},{"id":"54d12a9a-149b-42a7-8491-300583d5c2b8","domain":"awscsapro-domain5","question":"A client is trying to setup a new VPC from scratch.  They are not able to reach the Amazon Linux web server instance launched in their VPC from their on-prem network using a web browser.  You have verified the internet gateway is attached and the main route table is configured to route 0.0.0.0/0 to the internet gateway properly.  The instance also is being assigned a public IP address.  Which of the following would be another potential cause of the problem?","explanation":"For an HTTP connection to be successful, you need to allow port 80 inbound and allow the ephemeral ports outbound.  Additionally, it is possible that the subnet is not associate with the route table containing the default route to the internet.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html","title":"Troubleshooting Connecting to Your Instance - Amazon Elastic Compute Cloud"}],"answers":[{"id":"c7ca1b6a8fe855bda71123163488960b","text":"The customer has disabled the ec2-user account on the Amazon Linux instance.","correct":false},{"id":"2bde109ce87f4a4f513679f31116184d","text":"The instance does not have an elastic IP address assigned. ","correct":false},{"id":"32cdd854c9d71059eac396dd1249830e","text":"The subnet of the instance is not associated with the main route table.","correct":true},{"id":"d0f5e76f9fc753305b11c4c3a11e97ef","text":"The IAM role assigned to the LAMP instances does not have any policies assigned.","correct":false},{"id":"25d3393c550b8dbc2179367832573598","text":"The outbound network ACL allows port 80 and 22 only.","correct":true},{"id":"01b7463243eb231b840fcd4b737e044b","text":"The default route to the internet gateway is incorrect.","correct":false},{"id":"c9f886542d1dabe99bc64dd39c5e1615","text":"The inbound security group allows port 80 and 22 only.","correct":false}]},{"id":"2c034786-9b7e-4933-aad2-d0c4b1d89ca8","domain":"awscsapro-domain2","question":"A beach apparel company has begun an initiative to improve their sales analytics capabilities using AWS services. They'll need to be able to visualize summary sales data by product line, territory, and sales channel for each day, month, and year, and they'll need to be able to drill-down with ad-hoc queries on individual sales records. There are multiple data sources that provide transactional information in different formats. The company has chosen Amazon QuickSight as their visualization tool for the summary information. Visualizations and drill-down queries will require three years of rolling sales history, which estimates to seven petabytes of data. Which architecture will provide the best performance and cost efficiency?","explanation":"Using S3 to store the detailed sales transaction data and using Lambda to standardize data formats is the most cost effective option. Storing the summary data in Redshift provides a high performance option for reads from QuickSight, and keeping the detailed transaction data out of Redshift allows for smaller node sizes and lower cost. Amazon Redshift Spectrum can be used for drill-down queries that join tables from both Redshift and S3. For answer number two, Redshift will be a better option than Aurora for OLAP query performance due to it's columnar organization. Answer number four provides no simple way to perform ad-hoc drill down queries.","links":[{"url":"https://aws.amazon.com/redshift/","title":"Amazon Redshift"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html","title":"Getting Started with Amazon Redshift Spectrum"}],"answers":[{"id":"ebf4a6c248510d05a046c8f0ea4298b7","text":"Use Amazon Kinesis Data Analytics to format the data source transactions in a standard way and load it into Amazon Aurora. Invoke Lambda functions to aggregate the data and write it into summary tables in Aurora","correct":false},{"id":"b3deac265c2195cb988c345d096800fd","text":"Ingest individual sales transactions from each data source into Amazon S3 with Amazon Kineses Data Firehose. Trigger an AWS Lambda function to format the transaction data in a standard way and redeposit the results in S3. Run AWS Glue jobs to aggregate the summary data into Amazon Redshift","correct":true},{"id":"4740b70569f15040edf0916e47386757","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Streams and write them to Amazon Elastic Block Store on EC2 instances in Auto Scaling Groups. Perform data format standardization and summary aggregation on EC2, and write the summary results to Amazon Redshift tables","correct":false},{"id":"a08e38f138d23c0f1759ab2d1801f67e","text":"Read detailed sales transactions from each data source with Amazon Kinesis Data Firehose and load them into Amazon Redshift. Run AWS Glue jobs to format the transaction data in a standard way and perform aggregate functions to write the data into summary tables in Redshift","correct":false}]},{"id":"0bfd631e-c08c-406a-acf5-a07416aab129","domain":"awscsapro-domain5","question":"Your team uses a CloudFormation stack to manage AWS infrastructure resources in production. As the AWS resources are used by a large number of customers, the update to the CloudFormation stack should be very cautious. Your manager asks for additional insight into the changes that CloudFormation is planning to perform when it updates the stack with a new template. The change needs to be reviewed before being applied by a DevOps engineer. What is the best method to achieve this requirement?","explanation":"CloudFormation Change Sets are able to provide the information on how the running resources are affected by a stack update. The outputs can be reviewed before being executed. Users can view the Change Set through AWS Console or CLI. The Retain option in the DeletionPolicy, CloudFormation stack policy or termination protection helps on protecting the stack resources. However, they cannot provide a summary of  changes in a stack update.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html","title":"Updating Stacks Using Change Sets"}],"answers":[{"id":"550c80eaf15eeb89d49aa2a86eb747a6","text":"Enable termination protection in the CloudFormation stack so that the AWS resources cannot be accidentally deleted or modified. Disable the protection only if the changes are approved. Execute the changes in a maintenance window.","correct":false},{"id":"935df51ce2c7f07994c2b8a257489e00","text":"Create a CloudFormation Change Set using AWS Management Console or CLI, review the changes to see if the modifications are as expected and execute the changes to update the stack.","correct":true},{"id":"4657e544cc1daf4315865e230d92dd00","text":"For key AWS resources in the CloudFormation stack, add a Retain option in the DeletionPolicy attribute, which prevents the resources from being accidentally deleted by a stack update. Add a Delete option for the resources that you want to delete along with the stack.","correct":false},{"id":"700aa7cb0f0e8cfb417b67ae5d49e962","text":"Add a CloudFormation stack policy to prevent updates to stack resources. Only after the changes are reviewed and approved, change the stack policy to allow the stack update. Revert the stack policy after the change.","correct":false}]},{"id":"abd5a1c8-719f-4025-9804-8876b93d5633","domain":"awscsapro-domain2","question":"You are troubleshooting a CloudFront setup for a client.  The client has an Apache web server that is configured for both HTTP and HTTPS.  It has a valid TLS certificate acquired from LetsEncrypt.org.  They have also configured the Apache server to redirect HTTP to HTTPS to ensure a safe connection.  In front of that web server, they have created a CloudFront distribution with the web server as the origin.  The distribution is set for GET and HEAD HTTP methods, an Origin Protocol Policy of HTTP only, Minimum TTL of zero and Default TTL of 86400 seconds.  When a web browser tries to connect to the CloudFront URL, the browser just spins and never reaches the web server.  However, when a web browser points to the web server itself, we get the page properly. Which of the following if done by themselves would most likely fix the problem?","explanation":"With CloudFront only configured for HTTP Only, we have a loop when the web server redirects HTTP to HTTPS.  We can either enable HTTPS on CloudFront or disable the redirection policy on the Apache server.","links":[{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/custom-origin-cloudfront-fails/","title":"Troubleshoot Failing Custom Origins in CloudFront"}],"answers":[{"id":"f393ea6e3b2e4d003e821f57c52df1d2","text":"Change the CloudFront distribution origin protocol policy to use only HTTPS.","correct":true},{"id":"a24567748b0a6f2b185b2d69a0ced579","text":"Add the OPTION HTTP methods on the CloudFront distribution.","correct":false},{"id":"c7717815f23591b0fdaf1c616725535b","text":"Remove the redirection policy on the origin server and allow it to accept HTTP.","correct":true},{"id":"2970fc3adfc086f41b20364111c10e11","text":"Add POST and PUT HTTP methods on the CloudFront distribution.","correct":false},{"id":"caa85ea467e37e906dca5499b196b327","text":"Use an ALB instead of CloudFront to provide content caching.","correct":false},{"id":"961c2031ccad89e399f2ffee45459704","text":"Enable CloudFront to forward cookies and enable query string forwarding.","correct":false}]},{"id":"2afb0db9-a43f-4e97-8272-5ce423ded162","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They notice on their RDS console that one of their mission-critical production databases has an \"Available\" listed under the Maintenance column.  They are extremely concerned that any sort of updates to the database will negatively impact their DB-intensive mission-critical application.  They at least want to review the update before it gets applied, but they are not sure when they will get around to that.  What do you suggest they do?","explanation":"For RDS, certain OS updates are marked as Required. If you defer a required update, you receive a notice from Amazon RDS indicating when the update will be performed. Other updates are marked as Available, and these you can defer indefinitely.  You can also apply the maintenance items immediately or schedule the maintenance for your next maintenance window.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html","title":"Maintaining a DB Instance - Amazon Relational Database Service"}],"answers":[{"id":"35c3b3d1cb8d9866e1abc96dec8bfa3f","text":"Apply the maintenance items immediately.  AWS validates each update with each customer's RDS instances using a shadow image so there is little risk here.","correct":false},{"id":"b9f81cfac73d5c4d2871a7f969ecb9f3","text":"Disable the Maintenance Window so the updates will not be applied.","correct":false},{"id":"93df9eb71cb48a0c821fe555e35f5b62","text":"Defer the updates indefinitely until they are comfortable.","correct":true},{"id":"66ed278812c2a52913954afa52952b97","text":"The maintenance will be automatically performed during the next maintenance window.  They have no choice in the matter.","correct":false},{"id":"6aeada3a9046319bc858239b15031f66","text":"Backup the database immediate because the updates could come at any time.  If possible, create a Read Replica to act as a standby in case problems are introduced with the update.","correct":false}]},{"id":"b5f80e7e-7b92-402f-b74d-7397f0778eaf","domain":"awscsapro-domain4","question":"You are an AWS administrator and you want to enforce a company policy to reduce the cost of AWS usage. For the AWS accounts of testing environments, all instances in Auto Scaling groups should be terminated at 10:00PM every night and one instance in ASG should be launched at 6:00AM every morning so that the team can resume their work. At 10:00PM and 6:00AM, the team should get an email alert for the scaling activities. Which of the following methods would you use to implement this?","explanation":"A Lambda function can be used to modify the desired capacity at 10:00PM and 6:00AM. Only the desired capacity should be changed and the maximum capacity should be kept. If the maximum capacity is 1 during the day, the number of instances in ASG cannot be over 1. AWS CLI \"terminate-instances\" is inappropriate to terminate all instances in an ASG because ASG will automatically create new ones to maintain the desired capacity.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-manual-scaling.html","title":"Manual Scaling for Amazon EC2 Auto Scaling"}],"answers":[{"id":"0344a7b1df36d0f6e904c191acd31686","text":"Create a Lambda function that runs at 10:00PM and 6:00AM every day. The function terminates all EC2 instances using AWS CLI \"terminate-instances\" at 10:00PM and launches a new instance using AWS CLI \"run-instances\" at 6:00AM. Notify the team by publishing a message to an SNS topic.","correct":false},{"id":"5e1b982a6d6e29c160572d2430d8e3bb","text":"Use AWS CLI \"aws autoscaling put-scheduled-update-group-action\" to modify the desired capacity and maximum capacity to be 0 at 10:00PM and 1 at 6:00AM. Notify the team by configuring a CloudWatch Event rule.","correct":false},{"id":"32fa65e299613094d47b0e1a67a22636","text":"Create a cron job running in an EC2 instance. The cron job modifies the minimum, desired and maximum capacities to 0 at 10:00PM and changes the minimum, desired and maximum capacities back to 1 at 6:00AM. Configure a CloudWatch Event rule to notify the team.","correct":false},{"id":"8bbb4a7f1fc80cbb616df074b2984bd7","text":"Create a Lambda function that loops through all Auto Scaling groups, modifies the desired capacity to be 0 every 10:00PM and increases the desired capacity to be 1 every 6:00AM. Notify the team through an SNS notification in the Lambda function.","correct":true}]},{"id":"55b78611-5890-4835-8371-66af208d25a2","domain":"awscsapro-domain2","question":"ACME Company has decided to migrate their on-premises 800TB data warehouse and their 200TB Hadoop cluster to Amazon Redshift. The migration plan calls for staging all of the data in Amazon S3 before loading it into Redshift in order to accomplish the desired distribution across the compute nodes in the cluster. ACME has an AWS Direct Connect 500 Mbps connection to AWS. However, calculations are showing the effective transfer rate won't allow them to complete the migration during the three month time frame they have to complete the project. What migration approach should they implement to complete the project on-time and with the least amount of effort for the migration team?","explanation":"Loading multiple Snowball Edge devices concurrently over a number of waves can be accomplished in less than a month with reasonable internal network performance. Snowball Edge's onboard S3-Compatible Endpoint makes for seamless transfer from a data warehouse S3 interface, which most of the major data warehouse vendors support. Hadoop data can be copied to the S3 endpoint after mounting it on a staging workstation. Transferring 1PB over a 500Mbps Direct Connect will take approximately 8 months, so increasing it to a 1Gbps connection will complete in a minimum of 4 months. Loading concurrent Snowball devices will work, but the staging workstation won't be able to mount the data warehouse directly. The data will most likely need to be unloaded into delimited flat files on the workstation's filesystem. Using Database Migration Service won't overcome the network bandwidth issues.","links":[{"url":"https://aws.amazon.com/snowball-edge/","title":"AWS Snowball Edge"},{"url":"https://aws.amazon.com/blogs/storage/data-migration-best-practices-with-snowball-edge/","title":"Data Migration Best Practices with AWS Snowball Edge"}],"answers":[{"id":"e705d456884fba2f673d22a9e8e9b654","text":"Increase the Direct Connect capacity to a 1Gbps connection and copy the data from the data warehouse and the Hadoop cluster directly to S3 using native tools","correct":false},{"id":"8dfc8bb936df7dbaf648fa2d5760508e","text":"Attach multiple AWS Snowball devices to the on-premises network. Use a staging workstation to mount the data warehouse and the Hadoop filesystem as data sources. Use the AWS Snowball client to load the data onto the Snowball device","correct":false},{"id":"06696779bc2515fee9a6f91b4eb57c6b","text":"Reduce the project time frame by leveraging AWS Database Migration Service to load both the data warehouse and Hadoop cluster data directly from the on-premises data sources to Redshift, eliminating the S3 intermediate stage. Then, re-organize the data based on the distribution scheme afterwards","correct":false},{"id":"2b70a067963b17ed73d482ffd2fc810a","text":"Attach multiple Snowball Edge devices to the on-premises network. Load the data warehouse data with an S3 interface supported by the data warehouse platform. Mount the Hadoop filesystem from a staging workstation using native connectors and transfer the data through the AWS CLI","correct":true}]},{"id":"cb24982e-2c2d-43d4-872f-2dabbdb7e367","domain":"awscsapro-domain2","question":"You are helping an IT Operations group transition into AWS.  They will be created several instances based off the latest Amazon Linux 2 AMI.  They are unsure of the best way to enable secure connections for all members of the group and certainly do not want to share credentials. Which of the following methods would you recommend?","explanation":"Of the provided options, the only one that upholds AWS best practices for providing secure access to EC2 instances is to use AWS Session Manager.  ","links":[{"url":"https://aws.amazon.com/blogs/aws/new-session-manager/","title":"New â€“ AWS Systems Manager Session Manager for Shell Access to EC2 Instances  | AWS News Blog"}],"answers":[{"id":"cce617457cd8701c595d236b7fa4ed7c","text":"Allow each administrator to create their own SSH keypair and assign them all to the SSH Key for the instance upon each launch.","correct":false},{"id":"c594a26a8a9141cdd5615a0761fb2438","text":"Create a bastion host and use it like a jump-box.  Paste each administrators private key into the known_hosts file on the bastion host.","correct":false},{"id":"b1ab01927f47a067694506df9d5249e3","text":"Configure IAM role access for AWS Systems Manager Session Manager.","correct":true},{"id":"93ecb84b9fc90ec0bc4db84968ab5ebf","text":"Allow administrators to update the SSH key of the instance in the AWS console each time they need access to a system.","correct":false},{"id":"e36f501af80c8f03e08321d565cc900e","text":"Share the single private SSH key with each administrator in the group.","correct":false}]},{"id":"9564dd14-763c-45c7-8546-a8bbe687a55e","domain":"awscsapro-domain2","question":"You are helping a company design a new data analysis system.  The company captures data in the form of small JSON files from thousands of pollution sensors across the country every 10 minutes.  Presently, they use some BASH scripts to load data into an aging IBM DB2 database but they would like to upgrade to a more scalable cloud-based option.  A key requirement is the ability to replay the time-series data to create visualizations so they have to keep and query lots of detailed data. They intend to keep their current visualization tools which are compatible with any database that provides JDBC drivers.  Which of the following architectures IS NOT suited to help them Ingest and analyze the sensor data?","explanation":"For EMR, the task nodes do not store any HDFS data.  Task node storage is considered ephemeral so this would not be a good choice.","links":[{"url":"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html","title":"Understanding Master, Core, and Task Nodes - Amazon EMR"}],"answers":[{"id":"9ee2b80a7b38c15f156422085185bf91","text":"Deploy Amazon Aurora for MySQL and download the MySQL JDBC drivers. Use AWS Glue to Ingest the raw sensor files into Aurora.","correct":false},{"id":"23ad2d64a285f603e5ca0a29c6a444e7","text":"Spin up an EMR cluster.  Ingest the data into HDFS partitions on the EMR task nodes using Flume.  Use Hive to provide JDBC access.","correct":true},{"id":"0439c9dae7a15237bd7e1bfdca41bbd2","text":"Spin up a Redshift database.  Use Kinesis Firehose to load the data directly into Redshift.","correct":false},{"id":"98678aaeaf75f088f0d57ca7b321f688","text":"Use Lambda to read the sensor files and write them to a DynamoDB table.  Use a third-party JDBC driver to provide query access.","correct":false},{"id":"c552ecc7ce7256e713857b27429e7753","text":"Load the files into S3 and use AWS Athena as the database layer.","correct":false}]},{"id":"2c204ae8-6bba-49a1-b8f6-1aa4330c3d8c","domain":"awscsapro-domain5","question":"You are helping an IT organization meet some security audit requirements imposed on them by a prospective customer.  The customer wants to ensure their vendors uphold the same security practices as they do before they can become authorized vendors.  The organization's assets consist of around 50 EC2 instances all within a single private VPC.  The VPC is only accessible via an OpenVPN connection to an OpenVPN server hosted on an EC2 instance in the VPC.  The customer's audit requirements disallow any direct exposure to the public internet.  Additionally, prospective vendors must demonstrate that they have a proactive method in place to ensure OS-level vulnerability are remediated as soon as possible.  Which of the following AWS services will fulfill this requirement?","explanation":"AWS Macie is a service that attempts to detect confidential data rather than OS vulnerabilities.  Since there is no public internet access for the VPC, services like GuardDuty and Shield have limited usefulness. They help protect against external threats versus any OS-level needs.  AWS Artifact is simply a document repository and has no monitoring functions.  Only AWS Inspector will proactively monitor instances using a database of known vulnerabilities and suggest patches.","links":[{"url":"https://aws.amazon.com/inspector/faqs/","title":"FAQs - Amazon Inspector - Amazon Web Services (AWS)"},{"url":"https://aws.amazon.com/macie/","title":"Amazon Macie | Discover, classify, and protect sensitive data | Amazon Web  Services (AWS)"}],"answers":[{"id":"7ed3972608faa6c3dfc6fda7f151889c","text":"Enable AWS GuardDuty to monitor and remediate threats to my instances.","correct":false},{"id":"178912f5fbd90ab710621756a2ba18ff","text":"Employ AWS Macie to periodically assess my instances for vulnerabilities and proactively correct gaps.","correct":false},{"id":"81133f0650fa1ca2fbe1b920a6c67cc9","text":"Employ Amazon Inspector to periodically assess applications for vulnerabilities or deviations from best practices.","correct":true},{"id":"6a353f53758a3fd632209b5286a01086","text":"Enable AWS Shield to protect my instances from unauthorized access.","correct":false},{"id":"45d5e166ed185c1f7516650c714423dd","text":"Enable AWS Artifact to periodically scan my instances and prepare a report for the auditors.","correct":false}]},{"id":"3f7fa126-1155-4aa3-802d-e9eeb75f5e5a","domain":"awscsapro-domain3","question":"You work for a Clothing Retailer and have just been informed the company is planning a huge promotional sale in the coming weeks.  You are very concerned about the performance of your eCommerce site because you have reached capacity in your data center.  Just normal day-to-day traffic pushes your web servers to their limit.  Even your on-prem load balancer is maxed out, mostly because that's where you terminate SSL and use sticky sessions.  You have evaluated various options including buying new hardware but there just isn't enough time.  Your company is a current AWS customer with a nice large Direct Connect pipe between your data center and AWS.  You already use Route 53 to manage your public domains.  You currently use VMware to run your on-prem web servers and sadly, the decision was made long ago to move the eCommerce site over to AWS last.  Your eCommerce site can scale easily by just adding VMs, but you just don't have the capacity.  Given this scenario, what is the best choice that would leverage as much of your current infrastructure as possible but also allow the landscape to scale in a cost-effective manner?","explanation":"A Target Group for an ALB can contain instances or IP addresses.  In this case, we can define the private IP addresses of our on-prem web servers along side the private IP addresses of any EC2 instances we spin up.  The caveat is that we can only use private IP addresses when defining a target group in this way.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-application-load-balancing-via-ip-address-to-aws-on-premises-resources/","title":"New â€“ Application Load Balancing via IP Address to AWS & On-Premises  Resources | AWS News Blog"}],"answers":[{"id":"77592781918fa63474b5efbd5cc9555f","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an NLB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the NLB as an alias record.","correct":false},{"id":"6d3db4c52e96931f925f17fe8e9fd50f","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using public IP addresses of your on-prem web servers and additional EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false},{"id":"3e47f65e4524f53faba23e6995b592f5","text":"Use Server Migration Service to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define a target group using private IP addresses of your on-prem web servers and additional AWS-based EC2 instances created from the imported AMI.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":true},{"id":"4df24111c113846bfe0505ad0c84d9a3","text":"Use VM import to import a VM of a current web server into AWS as an AMI.  Create an ALB on AWS.  Define two target groups:  one containing the public IP addresses of your on-prem load balancer and one including an auto scaling group of additional EC2 instances created from the imported AMI.  Assign both target groups to the ALB using the same listener port.  Use Route 53 to update your public facing eCommerce name to point to the ALB as an alias record.","correct":false}]},{"id":"2c688b4f-f267-472d-a68f-db7c9070bfae","domain":"awscsapro-domain5","question":"An application has a UI automation test suite based on Selenium and the testing scripts are stored in a GitHub repository. The UI tests need a username and password to login to the application for the testing. You check the test scripts and find that the credentials are saved in the GitHub repository using plain text. This may bring in some potential security issues. You suggest saving the username and password in a secure, highly available and trackable place. Which of the following methods is the easiest one?","explanation":"AWS Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. The credentials can be stored as ciphertext. The service is highly scalable, available, and durable. It also integrates with CloudTrail so the usage is easy to track. DynamoDB, DocumentDB and S3 are not designed to store parameters. These services need more configurations and are not as simple as AWS Parameter Store.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html","title":"AWS Systems Manager Parameter Store"}],"answers":[{"id":"26e06a6fc6a107b0dd5aac356ad3908d","text":"Edit a JSON file to store the username and password and upload the file to an S3 bucket. Encrypt the S3 bucket with SSE-S3. Modify the S3 bucket policy to only allow the testing machines to get the file.","correct":false},{"id":"33442a701fc2a058eea89266ea439ee4","text":"Create a table in DynamoDB that has a primary key for username and a sort key for password. Select the encryption type as \"KMS - AWS managed CMK\". Enable the on-demand read/write capacity mode.","correct":false},{"id":"fed8d54a1ce26b883744ac61a13e5ac9","text":"Configure an Amazon DocumentDB cluster with the d5.r5.large instance class. Create a schema for username and password. Enable Auto Scaling for the cluster and set up the default number of instances to be 3.","correct":false},{"id":"d540b786961136abf5dfd186e84fcca0","text":"Save the username and password in AWS Parameter Store. Select the SecureString type to encrypt the data with the default key in Key Management Service (KMS). Modify the scripts to fetch the parameter values through AWS SDK.","correct":true}]},{"id":"95f1d7a8-c3d4-4fec-952a-72385aa8b4c8","domain":"awscsapro-domain5","question":"You are consulting for a company that performs specialized customer data analytics.  Their customers can upload raw customer data to a website and receive back demographic statistics.  Their application consists of a REST API created using PHP and Apache.  The application is self-contained and works in real-time to return results as a JSON response to the REST API call.  Because there is customer data involved, company policy states that data must be encrypted in transit and at rest.  Sometimes, there are data quality issues and the PHP application will throw an error.  The company wants to be notified immediately when this occurs so they can proactively reach out to the customer.  Additionally, many of the company's customers use very old mainframe systems that can only access internet resources using IP address rather than a FQDN.  Which architecture will meet these requirements fully?","explanation":"The requirement of a static IP leads us to a Network Load Balancer with an EIP.","links":[{"url":"https://aws.amazon.com/elasticloadbalancing/features/","title":"Elastic Load Balancing Features"}],"answers":[{"id":"5e4c5230c7e08202a0ea0575d5412d57","text":"Provision a Network Load Balancer with an EIP in front of your EC2 target group.  Install the CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.  Terminate SSL on the EC2 instances.","correct":true},{"id":"a9ed133e35b8332aea2bf603521b891a","text":"Provision an Application Load Balancer in front of your EC2 target group and offload SSL to CloudHSM.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch and configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from CloudHSM.","correct":false},{"id":"434ce04b2c3a4d2e679d37df43de2585","text":"Provision an Application Load Balancer with an EIP in front of your EC2 target group and terminate SSL at the ALB.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"0c8e5c32f081b0484e86b71651ae3642","text":"Provision a Network Load Balancer in front of your EC2 target group and terminate SSL at the load balancer using Certificate Manager.  Install CloudWatch Logging agent on the EC2 instances and stream logs to CloudWatch.  Configure notification via SNS when application errors are noticed in the system logs.  Configure the server AMI to use encrypted EBS volumes with a key from AWS KMS.","correct":false},{"id":"569eec0061a1a97be77e3bdab43a1756","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Offload SSL termination using AWS KMS.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false},{"id":"e53df806e37b325d7f61be27772875f1","text":"Deploy the web application on Lambda with API Gateway as the front-end.  Enabled SSL termination on the API Gateway using Certificate Manager.  Setup CloudWatch to alert via SNS if there are application exceptions.  Encryption at rest is not required as there is no data stored in this architecture.","correct":false}]},{"id":"f3efb368-9a43-4e1d-bf0a-1cf55bb918a8","domain":"awscsapro-domain4","question":"An AWS customer makes use of a wide variety of AWS services across multiple AWS regions. As the usage cost keeps increasing, the customer wants to get the detailed billing and cost information of his AWS account. In the meantime, the customer needs to quickly analyze the cost and usage data, visualize it, and share it through data dashboards so that he can get a better understanding of the billing and resource utilization. Which of the following methods would you choose?","explanation":"Users can get the billing and usage information from the AWS Cost and Usage Reports. The reports downloaded in an S3 bucket can be further processed and analyzed by QuickSight. The reports cannot be created in AWS Cost Explorer or billing dashboard. The question also asks for data visualization and dashboards. QuickSight, AWS's Business Intelligence service, is the most appropriate service for this requirement. Athena, Redshift or Elastic MapReduce are not suitable.","links":[{"url":"https://aws.amazon.com/blogs/aws/new-upload-aws-cost-usage-reports-to-redshift-and-quicksight/","title":"Upload AWS Cost & Usage Reports to QuickSight"}],"answers":[{"id":"fc09f5c144565de3b8472bde413e896a","text":"Create a data pipeline that downloads the monthly cost and usage reports from AWS Cost Explorer and uploads the reports to an S3 bucket. Set up a specific billing table with Amazon Athena and analyze the billing and usage data using SQL query commands.","correct":false},{"id":"36fb0d3b243e97c8ed14e32e6a2bc329","text":"Generate the billing reports from the AWS management console and upload the files to an S3 bucket. Create an Amazon Elastic MapReduce (EMR) cluster by specifying data inputs and outputs. Process the data in the cluster and generate reports for the admin IAM user.","correct":false},{"id":"78f31d3919e8b532d502611700598d75","text":"Create the hourly AWS Cost and Usage Reports and enable the data integration with Amazon QuickSight. Analyze and visualize the data in QuickSight by creating interactive dashboards and generating custom reports.","correct":true},{"id":"d3ef8ffbe6a0ff9378c5e248f2f7e165","text":"Download the AWS Cost and Usage Reports from the AWS billing dashboard in an S3 bucket which is owned by the administrators. Integrate the reports with an Amazon Redshift cluster. Analyze and view the data in the Redshift data warehouse.","correct":false}]},{"id":"083b20e3-95ff-4b8a-b655-aedf1de67c6c","domain":"awscsapro-domain4","question":"The security monitor team informs you that two EC2 instances are not compliant reported by an AWS Config rule and the team receives SNS notifications. They require you to fix the issues as soon as possible for security concerns. You check that the Config rule uses a custom Lambda function to inspect if EBS volumes are encrypted using a key with imported key material. However, at the moment the EBS volumes in the EC2 instances are not encrypted at all. You know that the EC2 instances are owned by developers but you do not know the details about how the instances are created. What is the best way for you to address the issue?","explanation":"The key must have imported key material according to the AWS Config rule. It should be a new key created in KMS. Existing KMS cannot import a new key material and AWS Managed Key such as aws/ebs cannot be modified either. CloudHSM is more expensive than KMS and is not required in this scenario. Besides, when the new encrypted EBS volume is attached, it should be attached to the same device name such as /dev/xvda1.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html","title":"How to import key material in AWS Key Management Service (AWS KMS)?"}],"answers":[{"id":"94975e581f509af38c350ecdd5b951f5","text":"Import a new key material to an existing Customer Managed Key (CMK) in KMS. Create an AMI from the EC2 instance. Then launch a new EC2 instance from the AMI. Encrypt the EBS volume in the new instance. Terminate the old instance after the new one is in service.","correct":false},{"id":"5a6b8e38c5d149a42e259d93323f12aa","text":"Modify the AWS Managed Key (AWS/EBS) in KMS to include an imported key material. Create a snapshot of the EBS volume. Then create a new volume from the snapshot with the volume encrypted. Detach the original volume and attach the new encrypted EBS to another device name of the instance.","correct":false},{"id":"62f46fb2afeb695bf73a050f1662cd44","text":"Create a Customer Managed Key (CMK) in KMS with imported key material. Create a snapshot of the EBS volume. Copy the snapshot and encrypt the new one with the new CMK. Then create a volume from the snapshot. Detach the original volume and attach the new encrypted EBS to the same device name of the instance.","correct":true},{"id":"03c530623f9019c80c05daa34ad8fab1","text":"Create a new EBS key from CloudHSM with imported key material. Create a new EBS volume encrypted with the new key. Attach the volume to the EC2 instance. Use Linux dd command to copy data from non-encrypted volume to encrypted volume. Unmount the old volume after the sync is complete.","correct":false}]},{"id":"9882b1ed-c0de-4205-8a00-54731bef3109","domain":"awscsapro-domain2","question":"You work for an organic produce importer and the company is trying to find ways to better engage with its supply chain.  One idea is to create a public ledger that all members of the supply chain could update and query as products changed hands along the journey to the customer.  Then, your company could create an app that would allow consumers to view the chain from producer to end retailer and have confidence in the product sourcing.  Which AWS service or services could most directly help realize this vision?","explanation":"Amazon Quantum Ledger Database (QLDB) is a fully-managed ledger database that provides a transparent, immutable and verifiable transaction log.  While other products could be used to create such a supply chain logging solution, QLDB is the closest to a ready-made solution.","links":[{"url":"https://aws.amazon.com/qldb/","title":"Amazon QLDB"}],"answers":[{"id":"680da36fb45c26a1d8e7996c6f5014cd","text":"Amazon CloudTrail and API Gateway","correct":false},{"id":"35b417ac310d92018a8db5515d4fae60","text":"Amazon DynamoDB and Lambda","correct":false},{"id":"ca649f7447f846ed8add8c396187b83a","text":"Amazon P2PShare and API Gateway","correct":false},{"id":"a9d83c7f8f0b0f2a8f67b7097ee73e3a","text":"Amazon QLDB","correct":true},{"id":"a8312fbd65c49606c59b53a8a062ecff","text":"Amazon Managed Blockchain","correct":false}]},{"id":"3440b6ff-6fe9-495d-a765-f69f6b82a628","domain":"awscsapro-domain3","question":"You work for a health record management company which operates a view-only portal for end-users to check their health records online. Users can also raise disputes if anything is incorrect. This 2-tier website that supports only HTTPS will be moved to the AWS Cloud. There are 5 web servers on-premises and an F5 Load Balancer that controls traffic to these web servers. SSL is terminated at the web servers. An Oracle Real Application Cluster (RAC) serves as the database tier. Due to the sensitive nature of personal health information, the web site uses mutual authentication - the server requests browsers for a valid client certificate before establishing a trusted session.\nSelect two alternate working architectures for this application to be migrated to AWS so that code changes are minimized. Choose two responses each of which can act as an independent and functional solution, and also result in minimum application code changes.","explanation":"The areas tested by this question are as follows.\nFirst, if an ELB is not terminating SSL, its listener protocol cannot be HTTPS - it has to be TCP. Additionally, AWS ELB does not support terminating client-side certificates. Therefore, if a website requires client SSL certificates, and if it also uses AWS ELB, the ELB must let the target EC2 instances to terminate SSL and validate the client certificate. This requires the protocol to be TCP/443. Both these facts (one, SSL is not terminated at the Load Balancer level, and two, mutual authentication is used) are stated explicitly in the question so that the candidate identifies at least one of them and is thus able to conclude that HTTPS is not the correct protocol choice for ELB. Hence, amongst the two choices that use ELB, the one that says TCP is the correct one.\nSecondly, RDS does not support Oracle RAC. Hence, the database tier must use EC2 instances. Thus, for the second alternate solution that uses Route 53 multi-value answer records instead of ELB, we should select the option that deploys EC2 instances for the database tier.","links":[{"url":"https://forums.aws.amazon.com/thread.jspa?threadID=109180","title":"Discussion Forums - HTTPS Client certificate validation while using client ELB"},{"url":"https://aws.amazon.com/rds/oracle/faqs/","title":"RDS FAQ-s, search for phrase - Is Oracle RAC supported on Amazon RDS"}],"answers":[{"id":"7250b61db9c9e69abf4f9e7bd2bfb268","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring HTTPS/443 as listener","correct":false},{"id":"11a4ad1f948d7b2fd3631cb763fa8a00","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Use an ELB as the load balancer, configuring TCP/443 as listener","correct":true},{"id":"307f49d40547367c203a0fcfa1a46be8","text":"Migrate the database to RDS Oracle and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":false},{"id":"e7a1d88af1880a82062437a4c1005adf","text":"Migrate the database to a cluster of EC2 instances and the web servers to EC2 instances. Assign each web server an Elastic IP Address. Set up Route 53 with multi-value answer routing to these IP addresses. Set up a Route 53 health check for each record","correct":true}]},{"id":"230f422f-7118-4096-8dce-59c642fb55c8","domain":"awscsapro-domain1","question":"You are helping a client troubleshoot a new Direct Connect connection.  The connection is up and you can ping the AWS peer IP address, but the BGP peering session cannot be established.  What should be your next logical troubleshooting steps?","explanation":"Because the connection is up and we can ping the AWS peer, the problem must be at a higher level on the OSI model than the Physical or Data layers.  BGP uses TCP port 179 to communicate routes so we should check that no NACL or SG is blocking it.  Additionally, we should make sure the ASNs are properly configured in the proper ranges.","links":[{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/Troubleshooting.html","title":"Troubleshooting AWS Direct Connect - AWS Direct Connect"}],"answers":[{"id":"16e5aea88df69cc18f99e3f066ec99c1","text":"Ensure that the local ASNs and AWS-side ASNs are properly configured.","correct":true},{"id":"45d4c1753395277878b9a17343628c52","text":"Ensure that the VLAN is configured properly between your on-prem router the provider. ","correct":false},{"id":"8fc27418eee2ce07b64bc672007d2c1b","text":"Contact the co-location provider and request a written report for the Tx/Rx optical signal across the cross connect.","correct":false},{"id":"edd3f9408cecbbf9182678ccc51d7981","text":"Ask your network provider to provide you with a cross connect completion notice and compare the ports with those listed on your LOA-CFA","correct":false},{"id":"81977d7a1eb5714746851077b93f44d6","text":"Power cycle all the equipment to clear ARP table cache.","correct":false},{"id":"3d2a55832b90f19a2137e8715525d717","text":"Make sure no firewalls or ACLs are blocking TCP port 179 or any high-numbered ephemeral ports.","correct":true}]},{"id":"f7d7767b-9159-4e53-8e37-ff9bf41ace17","domain":"awscsapro-domain5","question":"You are working with a client to help them design a future AWS architecture for their web environment.  They are open with regard to the specific services and tools used but it needs to consist of a presentation layer and a data store layer.  In a brainstorming session, these options were conceived.  As the consulting architect, which of these would you consider feasible?","explanation":"The only two options which contain feasible options are the Beanstalk and S3/Dynamo methods.  One would not create a new K8s deployment for for a new web update.  CodeBuild and AWS Config are not the correct tools for how they are being suggested.","links":[{"url":"https://aws.amazon.com/codebuild/","title":"AWS CodeBuild â€“ Fully Managed Build Service"}],"answers":[{"id":"f5c7f9be39386b15747c0fe57d5040ba","text":"Deploy Kubernetes on an auto-scaled group of EC2 instances.  Define pods to represent the multiple tiers of the landscape.  Use ElastiCache for Memcached to offload queries from a Multi-AZ RDS instance.  To deploy changes to the landscape, create a new EKS deployment containing all the updated service containers and deploy them to replace all the previous existing tiers.  Ensure the DevOps team understands the rollback procedures.","correct":false},{"id":"ca4cc92538f3b73d221c9b5d4378e1f8","text":"Setup a traditional three tier architecture with a CloudFormation template per tier and one master template to link in the others.  Configure a CodeBuild stack and set this stack to perform automated Blue Green deployments whenever any code change is made.","correct":false},{"id":"01a7f4c09f65e41884b0a72843a8d55b","text":"Deploy an auto scaling group of EC2 instances behind an Application Load Balancer.  Provision a Mulit-AZ RDS instance to act as the data store, configuring a caching layer to offload queries from the database.  Use a User Script in the AMI definition to download the latest web assets from S3 upon boot-up.  When changes are required, use AWS Config to automatically fetch a new version of web content from S3 when a new version is created.","correct":false},{"id":"c27a151a7715575fb1ebf0225c6aee09","text":"Use the AngularJS framework to create a single-page application.  Use the API Gateway to provide public access to DynamoDB to serve as the data layer.  Store the web page on S3 and deploy it using CloudFront.  When changes are required, upload the new web page to S3.  Use S3 Events to trigger a Lambda function which expires the cache on CloudFront.","correct":true},{"id":"d6a73928290b05c25d87e26ece9e94a6","text":"Create a monolithic architecture using Elastic Beanstalk configured in the console.  Create an RDS instance outside the Beanstalk environment and configure it for multi-AZ availability.  When a new landscape change is required, use a command line script to implement the change.","correct":true}]},{"id":"f3fef147-7b9d-45b6-8b2b-d943c90e8920","domain":"awscsapro-domain5","question":"You are assisting a company in the migration of their container-based web landscape over to Amazon.  They have a total of 21 containers which comprise their DEV, QA and Production environments.  All environment are identical in design and size.  Each environment consists of 3 web servers, 3 app servers and 1 datastore server.  Given the landscape, which of the provided options would be best for them to minimize maintenance?","explanation":"Deploying containers via ECS is a good option but we would want to use the EC2 hosted path.  Fargate is generally used for transient workloads and our datastore would be something we'd want to persist.  We might be able to deploy the data store with RDS, but the question does not make it clear if the data store is an RDS-supported database.  It could be a NoSQL data store or some other database unsupported by RDS.  Similarly, a MEAN stack under Elastic Beanstalk might not be compatible with our landscape either.","links":[{"url":"https://aws.amazon.com/ecs/resources/","title":"Resources for Amazon ECS - run containers in production"}],"answers":[{"id":"f05469f4c7578263f4271e7514c338ef","text":"Deploy the web and app servers in each environment using ECS.  Provision an RDS instance for each environment.  Use AWS Systems Manager to provide a common management console.","correct":false},{"id":"e0ea997f77cb156d35ec716cf772c49c","text":"Deploy the web, app and database containers using ECS.  Make use of Fargate for the underlying ECS infrastructure.","correct":false},{"id":"619957021a43a829fbb6228467323ca1","text":"Deploy the web, app and database servers using ECS on EC2.  Purchase 1-year reserved instance contracts for the required EC2 instances.","correct":true},{"id":"c1354e6d48fedccbf7b4e9c18854d980","text":"Redeploy the web landscape on a MEAN stack under Elastic Beanstalk, making use of auto-scaling groups to right-size the respective environments.  ","correct":false}]},{"id":"07f91ae7-094b-48a9-8924-a4d142cbbcb6","domain":"awscsapro-domain5","question":"On your last Security Penetration Test Audit, the auditors noticed that you were not effectively protecting against SQL injection attacks.  Even though you don't have any resources that are vulnerable to that type of attack, your Chief Information Security Officer insists you do something.  Your organization consists of approximately 30 AWS accounts.  Which steps will allow you to most efficiently protect against SQL injection attacks?","explanation":"Firewall Manager is a very effective way of managing WAF rules across many WAF instances and accounts.  It does require that the accounts be linked as an AWS Organization.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"ee9c067e3dea2fa4cd2e3968abaf86de","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations.  Use CloudFormation to implement request restrictions for SQL code on the CloudFront distributions across all accounts.  Setup a CloudWatch event to notify administrators if requests with SQL code are seen.","correct":false},{"id":"3927b32bfb85cc040d2b7dcb21015fc5","text":"Use AWS WAF to create an ACL that denies requests that include SQL code.  Assign the ACL to Firewall Manager instances in each account using AWS OpsWorks.","correct":false},{"id":"2238e9ae7489214f767fa479d013cd23","text":"Create a custom NACL filter using Lambda@Edge to check requests for SQL code.  Use OpsWorks to apply the NACL across all public subnets across the organization. ","correct":false},{"id":"9a5fa8e9a1a9be9149138c6307abde19","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service.  Use Firewall Manager to create an ACL rule to deny requests that contain SQL code.  Apply the ACL to WAF instances across all organizational accounts.","correct":true},{"id":"e072f443d93c569cce94eb4946a912af","text":"Ensure all sub-accounts are members of an organization in the AWS Organizations service and use Consolidated Billing. Subscribe to AWS Shield Advanced to automatically enable SQL injection protection across all sub-accounts.","correct":false}]},{"id":"edf9ffa9-02ec-4341-a179-577cd590543e","domain":"awscsapro-domain1","question":"You work for a technology product company that owns two AWS Accounts - Prod and DevTest, both belonging to the same Organizational Unit (OU) under AWS Organizations Root. There are three different teams in your company - Dev Team, Testing Team and Ops Team. While Dev and Testing Team members have IAM Users created in the DevTest account, the Ops Team members have IAM Users created in the Prod account. There is an S3 bucket created in the Prod account that Testing Team members need access to - they need both read and write access. What is the best way to give the Testing Team members access to the Prod account S3 bucket?","explanation":"Cross-Account Access is best achieved using IAM Cross-Account Roles. The solution that suggests that the Testing Team members have to sign in to a different AWS account every time they need to access the S3 bucket is not correct as it is inefficient and unproductive.\nThere is no such AWS Organization feature that can directly enable Cross-Account Access from the console. There no way to select or deselect groups or users in this manner. Hence, the option that suggests using these is eliminated.\nThe remaining two options are a play in words. Carefully read both options. The Role must be created in the Trusting Account, in this case Prod Account because it has the Resource (S3 bucket) that needs to be accessed by the someone from another AWS Account, i.e., the Trusted Account.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html","title":"Tutorial - Delegate Access Across AWS Accounts Using IAM Roles"},{"url":"https://aws.amazon.com/organizations/","title":"AWS Organizations"},{"url":"https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/","title":"How to Enable Cross-Account Access to the AWS Management Console"}],"answers":[{"id":"e91ff3381d499ccd06e7891c37a77891","text":"Create IAM Users for the Testing Team members in the Prod account. Create a Testing IAM Group in the Prod account and add the IAM Users of the Testing team members to the Group. Assign an access policy to the Testing Group in the Prod account that grants Read and Write access to the correct S3 bucket. Testing team members will sign into the Prod account to access the S3 bucket.","correct":false},{"id":"e661ebc45484fb5a715bc96c884b4024","text":"Enable Cross-Account Access at the AWS Organizational Unit (OU) level from the console. Deselect the Dev Team UAM Users from Cross-Account Access Setup Wizard. This will allow only the Testing Team members to be able to access the Prod account. Write a bucket policy for the S3 bucket that lists the Testing Team members as Principals who are allowed to access the bucket.","correct":false},{"id":"56e4fe89e8a8ffb36bfcd67126afa0d5","text":"Create an IAM Role in the Prod Account that allows the DevTest Account to assume it, thus establishing trust between the Accounts. Attach an Access Policy to the Role that allows access to the S3 bucket. Attach a Bucket Policy to the S3 Bucket that specifies the ARN of the Role as Principal. Attach an Access Policy to the Testing Team's IAM Users in the DevTest Account that allows them to call STS AssumeRole for the specific Resource whose value is the ARN of the Role in Prod Account.","correct":true},{"id":"87953f71b8915388e4ecb3f7b2378374","text":"Create an IAM Role in the DevTest Account that allows access to the Prod Account, thus establishing trust between the Accounts. Attach a Trust Policy to the Role that grants the Testing Team members permission to assume the Role. Attach an Access Policy to the Testing Team's IAM Users in the DevTest Account that allows them to call STS AssumeRole for the specific Resource whose value is the ARN of the Role in Prod Account. Attach a Bucket Policy to the S3 Bucket that specifies the IAM Users of Testing Team members as Principal.","correct":false}]},{"id":"0c3d85ee-ff65-46e7-86d4-9fb7dcd21176","domain":"awscsapro-domain4","question":"Per the requirements of a government contract your company recently won, you must encrypt all data at rest.  Additionally, the material used to generate the encryption key cannot be produced by a third-party because that could result in a vulnerability.  You are making use of S3, EBS and RDS as data stores, so these must be encrypted.  Which of the following will meet the requirements at the least cost?","explanation":"When possible, making use of KMS is much more cost-effective than CloudHSM.  We can import our own key material into KMS for creating Customer Master Keys.  Because KMS works natively with the services we will be using, we save on any sort of custom integration that CloudHSM would have required.","links":[{"url":"https://aws.amazon.com/kms/faqs/","title":"FAQs | AWS Key Management Service (KMS) | Amazon Web Services (AWS)"},{"url":"https://aws.amazon.com/cloudhsm/faqs/","title":"AWS CloudHSM FAQs - Amazon Web Services"}],"answers":[{"id":"a4e27f3b361ede72810f71b15168297e","text":"Generate a public and private key pair.  Upload the public key via the EC2 dashboard.  When creating EBS volumes, select encryption and select this public key.  When creating S3 buckets, implement a bucket policy which requires encryption at rest only, rejecting other files.  Create an RDS instance and select the public key from the dropdown in the setup wizard.","correct":false},{"id":"66a66f8efdd420438db68069645b1ae7","text":"Initialize a CloudHSM instance.  Use it to generate custom encryption keys for each service you will use.  When creating an S3 bucket, EBS volume or RDS instance, select the custom CloudHSM key from the dropdown in the setup wizard.","correct":false},{"id":"473891e603ed3e2091e789a13094399c","text":"Use AMS KMS to create a customer-managed CMK.  Create a random 256-bit key and encrypt it with the wrapping key.  Import the encrypted key with the import token.  When creating S3 buckets, EBS volumes or RDS instances, select the CMK from the dropdown list.","correct":true},{"id":"9052cdd68b68935da91384397a18351c","text":"Use AMS KMS to create a 256-bit encryption key.  Use a grant to only allow access by S3, RDS and EBS.  When creating an S3 bucket, select the SSE-KMS option and pick the key from the dropdown.  For EBS and RDS, use the CLI to assign the KMS key when creating those instances.","correct":false}]},{"id":"431e43bc-ccbc-480f-9915-210bc7773d2b","domain":"awscsapro-domain5","question":"You are in the process of migrating a large quantity of small log files to S3 for long-term storage.  To accelerate the process and just because you can, you have created quite sophisticated multi-threaded distributed process deployed across 100 VMs which can load hundreds of thousands of files at one time.  For some reason, the process seems to be throttled somewhere along the chain.  You try many things to try to uncover the source of the throttling but nothing works.  Reluctantly, you decide to turn off the KMS encryption setting for your S3 bucket and the throttling goes away.  You turn AMS-KMS back on and the throttling is back. Given the troubleshooting steps, what is the most likely cause of the throttling and how can you correct it?","explanation":"Through a process of elimination, it seems you have identified the variable that is causing the throttling.  KMS, like other AWS services, does have rate limiters which can be increased via Support Case.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/limits.html","title":"Limits - AWS Key Management Service"}],"answers":[{"id":"fe629daf7473efc279d7d8ee6f5a5806","text":"You are maxing out your SYNC requests to S3.  You need to request a limit increase via a Support Case.","correct":false},{"id":"05aeb0bc36d7b53aa30bf9e22b6cd120","text":"You are maxing out your PUT requests to S3.  You need to change over to multi-part upload as a workaround.","correct":false},{"id":"1dd4f25e52404e18ddec0b8711a82a13","text":"You are hitting the KMS encrypt request account limit.  You must request a limit increase via a Support Case.","correct":true},{"id":"01148eae3319190a0b228c6d02c9572c","text":"You are maxing out your network connection.  You must split the traffic over multiple interfaces.","correct":false},{"id":"60ff77ab365c15bb11771e94e3dc271d","text":"You have exceeded the number of API calls for your account.  You must create a new account.","correct":false}]},{"id":"375e7161-43df-4d2f-adab-75cc6166a453","domain":"awscsapro-domain5","question":"You build a CloudFormation stack for a new project. The CloudFormation template includes an AWS::EC2::Volume resource that specifies an Amazon Elastic Block Store (Amazon EBS) volume. The EBS volume is mounted in an EC2 instance and contains some important customer data and logs. However, when the CloudFormation stack is deleted, the EBS volume is deleted as well and the data is lost. You want to create a snapshot of the volume when the resource is deleted by CloudFormation. What is the easiest method for you to take?","explanation":"The easiest method is using the DeletePolicy attribute in the CloudFormation template. The \"Snapshot\" value ensures that a snapshot is created before the CloudFormation stack is deleted. The \"Retain\" value is incorrect as it keeps the volume rather than creates a snapshot. The EBS lifecycle manager can create daily snapshot however it is not required in the question. When the CloudWatch Event rule is triggered, the EBS volume may already be deleted and no snapshot can be taken. Besides, the CloudFormation deletion cannot be suspended by a Lambda function.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html","title":"DeletionPolicy Attribute"}],"answers":[{"id":"0719057e0f4aca49eb47cd94c333e599","text":"Add a DeletePolicy attribute in the CloudFormation template and specify \"Snapshot\" to have AWS CloudFormation create a snapshot for the EBS volume before deleting the resource.","correct":true},{"id":"692ea271f5ada1d555a5889be933267e","text":"Modify the CloudFormation template to create an EBS snapshot strategy in EBS lifecycle manager which creates a daily snapshot as backup and also another snapshot when the EBS volumeâ€™s CloudFormation stack is being deleted.","correct":false},{"id":"061af6db30e4b65a1d544d4e64546085","text":"Modify the CloudFormation template by adding a DeletePolicy variable for the AWS::EC2::Volume resource. Specify the value of \"Retain\" to automatically create a snapshot of the EBS volume before the stack is deleted.","correct":false},{"id":"555e7ef93e63402dd6a074edd5b8a16d","text":"Create a CloudWatch Event rule that checks the CloudFormation delete-stack event. Trigger a Lambda function that pauses the CloudFormation stack deletion, creates the EBS snapshot of the volume and resumes the stack deletion after the snapshot is created successfully.","correct":false}]},{"id":"760cbc05-d8ba-4d58-a555-250f26815963","domain":"awscsapro-domain4","question":"Your team is working on a long-term government project. You have purchased several reserved instances (c5.18xlarge) to host the applications to reduce costs. There is a requirement to track the utilization status of these instances and the actual utilization should be always over 80% of the reservation. When the utilization falls below 80%, the team members need to receive email and SMS alerts immediately. Which AWS services would you use to achieve this requirement?","explanation":"The AWS Budgets service is able to track the reserved instances utilization. Several optional budget parameters can be configured such as linked account, region and instance type. You can use an SNS notification to receive alerts when the actual usage is less than a defined threshold. You do not need to maintain a new Lambda function to get the usage data and send alerts. Besides, either CloudWatch alarm or Trusted Advisor cannot provide alerts based on the reserved instances utilization.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-create.html#create-reservation-budget","title":"Create a reservation budget"}],"answers":[{"id":"c13e13d0cd86acac35a5358fff11066a","text":"Configure the reserved EC2 instances to send the utilization metrics to CloudWatch. Create a CloudWatch dashboard based on the metrics and set up a CloudWatch alarm. Use an SNS topic to receive the alert when the alarm is on.","correct":false},{"id":"ba6dc1f65ab53b3ac3bb10944df422b4","text":"Create a Lambda function using Python boto3 to get the utilization status of the reserved instances. Execute the Lambda function at a regular basis such as every day and notify the team by an SNS topic if the utilization drops below 80%.","correct":false},{"id":"9cb24caa236c81ae714987bb14b0faee","text":"Create a new reservation budget in AWS Budgets service. Set the reservation budget type to be RI Utilization and configure the utilization threshold to be 80%. Use an SNS topic to notify the team when the utilization drops below 80%.","correct":true},{"id":"a4024e0dbac3a9e9e371ebfe3a511ba7","text":"In the Cost Optimization category of AWS Trusted Advisor, monitor the usage of the EC2 reserved instances. If the usage is less than 80% of the reservation, Trusted Advisor raises an action recommended alarm and sends a message to an SNS topic which notifies the team via email and SMS.","correct":false}]},{"id":"7c5f884f-c0f9-4028-a725-50819d704324","domain":"awscsapro-domain5","question":"You deploy an application load balancer and an Auto Scaling group (ASG) in production for a new project. When instances in the ASG have a high CPU utilization, a new instance is launched. However, the new instance fails the health check from the ASG and has been terminated after some time. You check the logs in the instance and find that the startup script does not finish yet before the instance is terminated. How would you resolve the problem?","explanation":"Amazon EC2 Auto Scaling waits until the health check grace period ends before checking the health status of the instance. The grace period timer should be increased to give the instance more time to finish the startup script. Increasing the healthy threshold makes the instance more difficult to become healthy. Decreasing the timeout value also does not help as the instance may become unhealthy very quickly. Modifying the health check type from ELB to EC2 is unsuitable as the ASG cannot get the instance status from the application level. Even if the instance shows as healthy in ASG, the application may not be ready yet.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Check Grace Period"}],"answers":[{"id":"e322fd6a55a044826665dfce4ae7020b","text":"Decrease the timeout value in the ELB health check from 5 seconds to 1 second so that when the ELB performs the health check on the backend instances, the instances are able to respond in time before a timeout occurs.","correct":false},{"id":"e7ad9ea949ae87c6a6001a94f5a9bf48","text":"Increase the health check grace period in the Auto Scaling group configurations. When a new instance boots up, it is given more time to execute the startup scripts and run applications before the health check from ASG.","correct":true},{"id":"b5ca84726bf6fcfd6522ab188c8224ea","text":"Increase the default healthy threshold in the health check of elastic load balancer from 5 to 10 so that the instance will become healthy more quickly once the startup script finishes in the new instance.","correct":false},{"id":"1d8964ced10da94d49d1a1efe02bbbca","text":"Modify the health check type from ELB to EC2 in the Auto Scaling group. Configure ASG to check the EC2 instance status. As long as the instance does not have a system level issue, it will not fail the health check in the ASG even when the startup script is still running.","correct":false}]},{"id":"5f6d53a1-1b9c-46fe-9a8e-a5706e72914d","domain":"awscsapro-domain4","question":"Which of the following is an example of buffer-based approach to controlling costs?","explanation":"The buffer-based approach to controlling costs is discussed in the Cost Optimization Pillar of the AWS Well-Architected Framework.  A buffer is a mechanism to ensure that applications can communicate with each other when they are running at different rates over time.  By decoupling the throughput rate of a process, you can better govern and smooth demand--creating a less volatile and reactionary landscape.  As a result, costs can be reduced by optimizing for the steady state. ","links":[{"url":"https://aws.amazon.com/architecture/well-architected/","title":"AWS Well-Architected - Build secure, efficient, cloud enabled applications"}],"answers":[{"id":"5bc62ebd024ca5594793ca76f08cd960","text":"A public-facing API is created using API Gateway and Lambda.  As a serverless architecture, it scales seamlessly in step with demand.","correct":false},{"id":"f867e23e60a3917c1ebe5e2c4ced818c","text":"An auto-scaling fleet is created to dynamically adjust available compute resources based network connection events as reported by CloudWatch.","correct":false},{"id":"26437f21964d56c3e373f55d997101ed","text":"A mobile image upload and processing service makes use of SQS to smooth an erratic demand curve.","correct":true},{"id":"878d4fde3965b2e5f84c543b2cca1dfc","text":"A production ERP landscape is scaled up during the month-end financial close period to provide some padding for the additional processing and reports so they do not impact the normal business processes.","correct":false}]},{"id":"338ce579-a236-4282-9670-8da3b0baf2e9","domain":"awscsapro-domain3","question":"You are helping a Retail client migrate some of their assets over to AWS.  Presently, they are in the process of moving their Enterprise Data Warehouse.  They are planning to re-host their very large Oracle data warehouse on EC2 in a high availability configuration across AZs.  They presently have several Scala scripts that process some detailed Point of Sale data that is collected each day.  The scripts perform some aggregation on the data and import the aggregate into their Oracle database.  They want to move this process to AWS as well.  Which option would be the most cost-effective way for them to do this?","explanation":"AWS Glue is a fully managed extract, translate and loading service and is compatible with Scala.  EMR could do this but represents more overhead than necessary.  Lambda is not compatible with Scala and migrating to Redshift does not bring anything in this case if the customer wants to retain their Oracle database.","links":[{"url":"https://aws.amazon.com/glue/faqs/","title":"AWS Glue Features - Amazon Web Services"}],"answers":[{"id":"1a4de6676c8c078310e08aad71d9dce6","text":"Migrate the processing to AWS EMR.","correct":false},{"id":"a445a1a877009cd7c31858687a818116","text":"Import your Scala scripts into AWS SCT for processing.","correct":false},{"id":"04578ae8419780f9dc441d01fe11582d","text":"Create Lambda functions using your Scala scripts.","correct":false},{"id":"8b01d948d5ad2f4b1c8e817c2d98e7c2","text":"Migrate the processing to AWS Glue.","correct":true},{"id":"4f1ff8b853c3ba363bdd2bda53538ab4","text":"Migrate from Oracle to Redshift and use Kinesis Firehose.","correct":false}]},{"id":"401cbed4-e977-4303-9344-586af01a4180","domain":"awscsapro-domain2","question":"You have been contracted by a manufacturing company to create an application that uses DynamoDB to store data collected in a automotive part machining process.  Sometimes this data will be used to replay a process for a given serial number but that's always done within 7 days or so of the manufacture date.  The record consists of a MACHINE_ID (partition key) and a SERIAL_NUMBER (sort key).  Additionally, there is a CREATE_TIMESTAMP attribute that contains the creation timestamp of the record and a DATA attribute that contains a BASE64 encoded stream of machine data.  To keep the DynamoDB table as small as possible, the industrial engineers have agreed that records older than 30 days can be purged on a continual basis.  Given this, what is the best way to implement this with the least impact on provisioned throughput.","explanation":"Using DynamoDB Time to Live feature is a perfect way to purge out old data and not consume any WCU or RCU.  Other methods of deleting records would impact the provisioned capacity units.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html","title":"Time To Live - Amazon DynamoDB"}],"answers":[{"id":"0a945c4865c940ffaddaeade6f6bbdaf","text":"Use Step Functions to track the lifecycle of DynamoDB records.  Once 30 days has elapsed, branch to a Delete step and trigger a Lambda function to remove the record.","correct":false},{"id":"081f76fb4967f9d10a3799ae400ad898","text":"Update the table to add a attribute called EXPIRE  Change the application to store EXPIRE as CREATE_TIMESTAMP + 30 days.  Enable Time to Live on the DynamoDB table for the EXPIRE attribute.","correct":true},{"id":"ef62d6ec88d117a0ac0cb7c99cd1abbd","text":"Use DynamoDB Streams to trigger a Lambda function when the record ages past 30 days.  Use the DynamoDB SDK in the Lambda function to delete the record.","correct":false},{"id":"41f68b5f48ef97cc437ebfe50ae10882","text":"Use AWS Batch to execute a daily custom script which queries the DynamoDB table and deletes those records where CREATE_TIMESTAMP is older than 30 days.  ","correct":false},{"id":"7c29d6c8effb51be4df54033ce45d01f","text":"Enabled Lifecycle Management on the DynamoDB table.  Create a rule that deletes any records where CREATE_TIMESTAMP attribute is greater than 30 days old.","correct":false}]},{"id":"5af539b7-b132-4a3a-bc80-406c620e7325","domain":"awscsapro-domain1","question":"A food service business has begun an initiative to migrate all applications and data to the AWS cloud. Governance needs to be established before any migrations can occur. Business units such as sales, marketing, and product management have fluctuating infrastructure capacity and security requirements, while other business units like finance, operations, and human resources have more static demand. Security policies and compliance needs vary by project group within each business units. Each business unit is responsible for it's own cost center, and the finance group would like cost reporting to be as streamlined as possible. Which AWS account structure will best satisfy the company's governance needs?","explanation":"Leveraging AWS Organizations to manage an account structure with a core Organizational Unit and Organizational Units for each business unit provides flexibility for future organizational changes. Creating an account for each project group facilitates security policy differences within business units, and limits the exposure of a single security event. Managing differing security requirements by project group in a single account will require more governance maintenance. Creating billing, shared services, and log archive accounts in multiple Organizational Units will result in duplication of services, and can be done at the core level.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-laying-the-foundation/introduction.html","title":"Laying the Foundation: Setting Up Your Environment for Cost Optimization"},{"url":"https://aws.amazon.com/solutions/aws-landing-zone/?did=sl_card&trk=sl_card","title":"AWS Landing Zone"}],"answers":[{"id":"a66d8391267460b5800c5c3d07921767","text":"Use AWS Organizations to create Organizational Units for each business unit. Create a billing account, a shared services account, and a log archive account in each Organizational Unit. Create accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false},{"id":"bd400ff0d22480599228a0442d2bb8d4","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Create an Organizational Unit for each business unit that contains accounts for each project group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":true},{"id":"03705913700b8d76205d4203c58dc5e1","text":"Use AWS Organizations with a single Organizational Unit to consolidate costs. Create a billing account, a shared services account, and a log archive account in the Organizational Unit. Create individual accounts for each business unit. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs","correct":false},{"id":"a8f7d8fbb7c6c3a1a14c91577dff42e1","text":"Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account, and a log archive account. Place business units with similar security requirements in shared Organizational Units. Create accounts for each business unit in the shared Organizational Units. Manage security requirements for each project group with VPC networking services such as Security Groups and Network ACLs. Establish standard tags to sort the AWS Detailed Billing report by cost center","correct":false}]},{"id":"05e085a9-4de3-46fe-9470-10c7f2faba57","domain":"awscsapro-domain5","question":"You are consulting with a client who is in the process of migrating over to AWS.  Their current on-prem Linux servers use RAID1 to provide redundancy.  One of the big benefits they are looking forward to with moving to AWS is the ability to create snapshots of EBS volumes without downtime.  Right now, they intend on migrating the servers over to AWS and retaining the same disk configuration.  What is your advice for them?","explanation":"Because RAID is based upon multiple volumes being in sync, taking snapshots of an individual volume that's part of a active and mounted RAID array would not create a proper backup.  You must first unmount the RAID volume and then create the snapshots of the component volumes.  This of course means any data on the RAID volume would be unavailable.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html","title":"RAID Configuration on Linux - Amazon Elastic Compute Cloud"}],"answers":[{"id":"99a9d29ef15a0ced996c1510ff6d8f6a","text":"EC2 does not support RAID configurations.","correct":false},{"id":"73b207bc7a947de1eed26bc058b4b67b","text":"If snapshots without downtime are the priority, do not use RAID.","correct":true},{"id":"bee3a756d6bfddce4e9917e171a4b0e2","text":"Consider using RAID0 when on AWS for performance reasons.","correct":false},{"id":"21a6a5218cf9778e0184eed7897c54ce","text":"Consider using RAID6 rather than RAID1 on AWS for performance reasons.","correct":false},{"id":"ead9938f5d4fc4d2df30763406b6a8e5","text":"Consider using RAID10 when on AWS because it offers the best of both RAID0 and RAID1.","correct":false}]},{"id":"c3ac5de9-a343-4cde-af1b-6c9f89824d2f","domain":"awscsapro-domain5","question":"An external auditor is reviewing your process documentation for a Payment Card Industry (PCI) audit.  The scope of this audit will extend to your immediate vendors where you store, transmit or process cardholder data.  Because you do store cardholder data in the AWS Cloud, the auditor would like to review AWS's PCI DSS Attestation of Compliance and Responsibility.  How would you go about getting this document? ","explanation":"AWS Artifact provides on-demand downloads of AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and Service Organization Control (SOC) reports. You can submit the security and compliance documents (also known as audit artifacts) to your auditors or regulators to demonstrate the security and compliance of the AWS infrastructure and services that you use. You can also use these documents as guidelines to evaluate your own cloud architecture and assess the effectiveness of your company's internal controls.","links":[{"url":"https://docs.aws.amazon.com/artifact/latest/ug/what-is-aws-artifact.html?icmpid=docs_artifact_console","title":"What Is AWS Artifact? - AWS Artifact"}],"answers":[{"id":"fefa18704e871eb671528fd4b7bc6ca2","text":"AWS Macie","correct":false},{"id":"63df0d05cd43af35c95cf04d92aaf685","text":"AWS Legal Services website","correct":false},{"id":"d9208942349d1c6f7dbaba3661069bc1","text":"AWS WorkDocs","correct":false},{"id":"60b018772cea138af5a8c452ed694734","text":"AWS Artifact","correct":true},{"id":"d7cb47dd1f374d3ed079b14cc6f2cd75","text":"Submit a Support Case requesting the document","correct":false},{"id":"09e838e873f25f954fef911d50b3d1ab","text":"AWS Pinpoint","correct":false},{"id":"1d16d307ee989a80e421198a01993a9c","text":"AWS IAM Console","correct":false}]},{"id":"9382645e-ce3e-4063-b95e-1a72d0b644d1","domain":"awscsapro-domain2","question":"You have built an amazing new machine learning algorithm that you believe would be of benefit to many paying business customers.  You want to expose it as a REST API to your customers and offer three different consumption levels: Silver, Gold and Platinum.  The backend is completely serverless using Lambda functions. What is the most efficient and least cost way to make your API available for paying customers with a per-request pricing model?","explanation":"Since 2016, AWS has allowed developers to monetize their APIs in AWS Marketplace using API Gateway.  The developer must first create a Developer Portal to provide a method for customers to register for access and then associate the assigned Product Code, received when the developer registers the API in the Marketplace, to the desired usage plan within API Gateway.  AWS then handles accounting for the usage and billing.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/sell-api-as-saas-on-aws-marketplace.html","title":"Sell Your API Gateway APIs through AWS Marketplace - Amazon API Gateway"},{"url":"https://aws.amazon.com/blogs/compute/monetize-your-apis-in-aws-marketplace-using-api-gateway/","title":"Monetize your APIs in AWS Marketplace using API Gateway | AWS Compute Blog"}],"answers":[{"id":"93741211ae6c981d6c73be4eac40c975","text":"Port your Lambda functions over to a Docker container and deploy using EKS.  Setup metered usage for each customer you expect to subscribe and deploy unique API keys to those customers.  Use CloudTrail to generate usage data for the API containers and import into RedShift for aggregation and processing.  Use the Amazon Pay API to issue invoices to customers based on monthly queries of the RedShift data.","correct":false},{"id":"ea9b822cf5c0fc7fc226c7130821d810","text":"Deploy your API using API Gateway using the \"managed-service\" mode.  Use AWS Batch to export usage logs to S3.  Use AWS Glue to aggregate and transform the raw logs into daily usage and save in DynamoDB.  Build a Payment Gateway using the AWS SDK to read the DynamoDB billing table and prepare invoices for customers.  Use SES to email invoices to customers.","correct":false},{"id":"9ade23b86bae0ad5bcbc882074feedb2","text":"Use API Gateway to configure a usage plan for the production stage of the API.  Register as a seller with AWS Marketplace and define three different levels of service and pricing. Assign the respective product code to the proper usage plan in the API Gateway console.","correct":false},{"id":"bca3af4da0c0286bf50f5dde56d2a017","text":"Setup your own API Gateway Serverless Developer Portal to create API keys for subscribers.  Register as a seller with AWS Marketplace and specify the usage plans and developer portal.  Submit a product load form with a dimension named \"apigateway\" of the \"requests\" type.  Create a metering IAM role to allow metrics to be sent to AWS Marketplace.  Associate your provided Product Code with the corresponding usage plan.","correct":true},{"id":"54bb1f265f3f353d8a065327131b21af","text":"Deploy your API to an S3 bucket using the Static Hosting feature.  Enable \"requester pays\" for the bucket to handle billing.  Create a serverless customer portal that will allow customers to register for API access and dynamically create an IAM role for them using Lambda.","correct":false}]},{"id":"a4d41d3b-abbe-4121-8e1d-5567b1ec7294","domain":"awscsapro-domain5","question":"You are an AWS administrator and you need to maintain multiple Amazon Linux EC2 instances. You can SSH to the instances with a .pem key file created by another colleague. However, as the colleague will leave the company shortly, the SSH key pair needs to be changed to a new one created by you. After the change, users should be able to access the instances only with the new .pem key file. The old key should not work. How would you get the key pair replaced properly?","explanation":"You do not need to launch new instances as you can simply paste the public key content in the \".ssh/authorized_keys\" file to enable the new key pair. You cannot directly change the key through AWS Management Console by clicking the \"change SSH key\" button. You are also not allowed to change the SSH key when stopping and starting instances. Users can only select an SSH key pair when they launch a new instance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#replacing-key-pair","title":"Adding or Replacing a Key Pair for Your Instance"}],"answers":[{"id":"b216d43c3e1e71b67af45a138184b181","text":"Create a new SSH key pair, get the public key information from it, paste it in the \".ssh/authorized_keys\" file in the Amazon Linux EC2 instances and remove the old public key.","correct":true},{"id":"4fa9ce5fdb3a2215ed8798d2e734fd42","text":"In AWS Management Console, create a new key pair and download the .pem file at a safe place. Select the Amazon EC2 instances and click \"change SSH key\" to get the key replaced. The EC2 instances are automatically restarted after the change.","correct":false},{"id":"521ec8da4f5fa8bee591740d44a8427e","text":"Create a new key pair locally or through AWS EC2 service. Take AMIs of the instances as backups. Stop the instances and choose the new key pair when restarting the instances. Notify the team to start using the private key for SSH connections.","correct":false},{"id":"be5d2e07f838b7752b235431feaae361","text":"The SSH key pair cannot be changed after EC2 instances are launched. Take AMIs or EBS snapshots from existing instances, terminate the instances, launch new ones with the AMIs/snapshots and select another SSH key pair.","correct":false}]},{"id":"b401741c-5b37-4b47-8e61-7802fbc9d7d6","domain":"awscsapro-domain1","question":"You are helping a client consolidate several separate accounts into a single account.  This consolidation will result in approximately 50 new VPCs in their one account.  They want to continue to use Route 53 for DNS but only want it accessible privately. How can you accomplish this most efficiently?","explanation":"Private Hosted Zones provide DNS services to VPCs but cannot be access from the internet.  They can be associated with VPCs either by the console, CLI or programmatically via SDK.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html","title":"Associating More VPCs with a Private Hosted Zone - Amazon Route 53"}],"answers":[{"id":"315372936e7ffba65896da15d0f45c2d","text":"Create a Private Hosted Zone within Route 53.  As the new VPCs are created, associate them with the Private Hosted Zone.","correct":true},{"id":"cbffb64b6b43e6fe45496b6e77ce17b8","text":"Create a Private Hosted Zone within Route 53 for each respective VPC.  Configure replication between the private hosted zones to keep records in sync.","correct":false},{"id":"82d5ef7e7176200aa4350ef90dd4c354","text":"Create a Public Hosted Zone within Route 53 and associate it to each VPC.  Configure a NACL on each VPC to deny inbound DNS queries (UDP port 53).","correct":false},{"id":"f74daa2300ac594c111ca9fce198f19c","text":"Create a central DNS server using EC2 and BIND.  Configure Route 53 to reference this DNS server as a resolver.  Update DNS records at the registrar to point to the central DNS.","correct":false},{"id":"3cc28b12f45b3dee8f7f16a0f93d00ce","text":"Install BIND on an EC2 instance in a single VPC.  Create VPC peering connections between the DNS VPC and any new VPCs.  Configure a DHCP Option Set to assign a DNS and link that to each VPC.","correct":false}]},{"id":"343601ea-0262-46dc-baab-511550237b8f","domain":"awscsapro-domain2","question":"You work for a distributed large enterprise that uses Splunk as their log aggregation, management and analysis software. The company has recently shown a keen interest in adopting a microservice based architecture and wants to convert some of its applications to Docker containers. They have selected AWS ECS as the orchestration platform for these containers. They are interested only in EC2 Launch Type as the Security Team wants to harden the EC2 instances with policies they want to control. However, the Chief Architect is concerned about hardcoding the Splunk token inside the container code or configuration file. Splunk requires any logging request to include this token. The Chief Architect wants to know if there is a way to pass the production Splunk token to ECS at runtime so that the container tasks can continue logging to Splunk without exposing the production secret to all developers, and if yes, how. Select the best answer.","explanation":"ECS Task Definition file supports two ways of specifying secretOptions in its logConfiguration element - AWS Systems Manager Parameter Store and AWS Secrets Manager. This question only deals with only one of these two ways, as it does not mention AWS Secrets Manager at all. However, the real focus of the question is how KMS is used by AWS Systems Manager Parameter Store. There is actually nothing special about how Parameter Store deals with KMS. Hence, the question actually is just about knowing the ways KMS Keys can be used - they can either be customer-managed, or AWS managed or both AWS and customer-managed.\nAnother aspect tested by the question is whether to use Task Role or EC2 Instance Role while granting an ECS Cluster permission to access resources during container execution. AWS recommends using the Task Role in this scenario, hence we can eliminate the two choices that specify using EC2 Instance Roles.\nThe option that says that the encryption key must only be stored by the customer is incorrect, as we can use KMS to store the key. The options that state the only supported management options for the KMS-stored keys are either only customer-managed or only AWS managed are both incorrect. The correct option identifies that KMS keys can either be customer-managed or AWS managed.\nOne thing to note is that this question does not need any prior knowledge of Splunk. Everything that needs to be known about Splunk is stated as part of the question. The AWS SA-P exam normally does not require the candidate to know about any 3rd-party product or services, but such services can be explicitly named.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html#secrets-logconfig","title":"Injecting Sensitive Data in a Log Configuration"},{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html","title":"How AWS Systems Manager Parameter Store Uses AWS KMS"},{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html","title":"IAM Roles for Tasks"}],"answers":[{"id":"ee244c2ca9783d96974f0c603a4aa548","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS but must be solely AWS-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. The task definition must include a task role that allows the container task to read AWS Systems Manager Parameter Store and decrypt using an AWS-managed KMS key","correct":false},{"id":"c3037df22b608e2533a356125fab5322","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS but must be solely customer-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. Each EC2 node must have an associated EC2 Instance Role that allows it to read AWS Systems Manager Parameter Store and decrypt using a customer-managed KMS key","correct":false},{"id":"a161521ba68b3fbb6a60532e1bf664c0","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can be stored in KMS and can either be customer-managed or be AWS-managed. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. The task definition must include a task role that allows the container task to read AWS Systems Manager Parameter Store and decrypt using a KMS key, this latter permission being required only if the key is customer-managed and not AWS managed","correct":true},{"id":"8a8783b1fab2fb72bc61295df2630a46","text":"Yes, the Splunk token can be stored in AWS Systems Manager Parameter Store as an encrypted key-value pair. The encryption key can only be stored and managed by the customer - therefore the value must be encrypted by the customer before writing to the Parameter Store and decrypted by the customer after reading from the Parameter Store. The task definition file must specify Splunk as the log driver, and can additionally pass the Systems Manager parameter name inside the secretOptions element of the logConfiguration attribute. Each EC2 node must have an associated EC2 Instance Role that allows it to read AWS Systems Manager Parameter Store","correct":false}]},{"id":"6dc7fe81-03aa-45d6-b8e1-6dc3b70914e0","domain":"awscsapro-domain1","question":"A company owns multiple AWS accounts managed in an AWS Organization. You need to generate daily cost and usage reports that include the activities of all the member accounts. The reports should track the AWS usage for each resource type and provide estimated charges. The report files also need to be delivered to an Amazon S3 bucket for storage. How would you create the required reports?","explanation":"The consolidated billing feature in AWS Organization does not generate billing reports automatically. You need to configure the AWS Cost and Usage Reports in the master account and use an S3 bucket to store the reports. The generated reports include activities for all the member accounts and it is not required to create a report in each member's account. The option of CloudWatch Event rule and Lambda function may work however it is not a straightforward solution.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-reports-costusage.html","title":"AWS Cost and Usage Report"}],"answers":[{"id":"b66bf1016802f0edb247437b5fda31cb","text":"Login in each AWS account using the root IAM user, configure the daily Cost and Usage Report and set up a central S3 bucket to save the reports from all AWS accounts. Store the reports in different folders in the S3 bucket.","correct":false},{"id":"149237c5674e21794204a4a0ca00bee2","text":"Create a CloudWatch Event rule that runs every day. Register a Lambda function target which calls the PutReportDefinition API to get cost reports of all AWS accounts and store them in an S3 bucket.","correct":false},{"id":"033f176fcb8f66b1ee9fb950c8741cda","text":"Enable the consolidated billing feature in the AWS Organization which automatically generates a daily billing report. Predefine an S3 bucket to store the reports. Make sure the S3 bucket has a bucket policy to allow the AWS Organization service to write files.","correct":false},{"id":"967b7874a080c033470777ce955a4550","text":"In the master account of the AWS Organization, generate the AWS Cost and Usage Reports and save the reports in an S3 bucket. Modify the bucket policy to allow the billing reports service to put objects.","correct":true}]},{"id":"24937858-d37e-4f9b-b195-d87c42b3f1ca","domain":"awscsapro-domain2","question":"You are designing a workflow that will handle very confidential healthcare information.  You are designing a loosely coupled system comprised of different services.  One service handles a decryption activity using a CMK stored in AWS KMS.  To meet very strict audit requirements, you must demonstrate that you are following the Principle of Least Privilege dynamically--meaning that processes should only have the minimal amount of access and only precisely when they need it.  Given this requirement and AWS limitations, what method is the most efficient to secure the Decryption service?","explanation":"Grants in KMS are useful for dynamically and programmatically allowing a process the ability to use the key then revoking after the need is over.  This is more efficient than manipulating IAM roles or policies.","links":[{"url":"https://docs.aws.amazon.com/kms/latest/developerguide/grants.html","title":"Using Grants - AWS Key Management Service"}],"answers":[{"id":"3cc7443088dcd1d7935eaf578a49d078","text":"Create an IAM key policy that explicitly denies access to the Decryption operation of the CMK.  Assign that policy to a role that is then assigned to the process executing the Decryption service.  Use a Lambda function to programmatically remove and add the IAM policy to the role as needed by the decryption process.","correct":false},{"id":"2202679b0d915deca81caf1d431477a4","text":"Create a IAM key policy that explicitly allows access to the CMK and assign that to a role.  Assign the role to the process that is executing the Decryption service.  At the end of the day, programmatically revoke that role until the start of the next day.","correct":false},{"id":"6c5638772104a82518657e9bfbc0970d","text":"In the step right before the Decryption step, programmatically apply a grant to the CMK that allows the service access to the CMK key.  In the step immediately after the decryption, explicitly revoke the grant.","correct":true},{"id":"b5f054e3c6369a2c4a9256e55359f9ff","text":"The current AWS platform services are not well suited for implementing Principle of Least Privilege in a dynamic manner.  Consider a different design that makes use of a more monolithic architecture rather than services.","correct":false},{"id":"45b8475be8fa894d6c7145b5536d21fe","text":"Use a grant constraint to deny access to the key except for the service account that is running the workflow processes.  Enable CloudTrail alerts if any other role attempts to access the CMK.","correct":false}]},{"id":"1520156f-0918-4ab4-a759-ce33a931c744","domain":"awscsapro-domain5","question":"Your company has an online shopping web application. It has adopted a microservices architecture approach and a standard SQS queue is used to receive the orders placed by the customers. A Lambda function sends orders to the queue and another Lambda function fetches messages from the queue and processes them. On some occasions the message in the queue cannot be handled properly. For example, when an order has a deleted production ID, the message cannot be consumed successfully and is returned to the queue. The problematic messages in the queue keep growing and the ability to process normal messages is affected. You need a mechanism to handle the message failure and isolate error messages for further analysis. Which method would you choose?","explanation":"It is not a good idea to adjust the retention period or simply delete the messages that fail to be processed as the question asks for a mechanism to isolate the messages for further troubleshooting. A redrive policy should be used to auto-forward error message to a dead letter queue. Then you can analyze the contents of messages to diagnose the producerâ€™s or consumerâ€™s issues. One thing to note is that a standard queue can only have another standard queue as the dead letter queue. Therefore a FIFO dead letter queue is incorrect as this scenario uses a standard SQS queue and requires a standard dead letter queue.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html","title":"Amazon SQS dead-letter queues"}],"answers":[{"id":"64fd54fe78b534f0eac9222a6e32747d","text":"Create a FIFO (First-In-First-Out) queue as the dead letter queue and use a redrive policy to forward problematic messages to this new queue. Create a Lambda function to read the message contents in the FIFO queue for further analysis.","correct":false},{"id":"3c8453a6c57faf61e761f771bab6f1af","text":"Create a standard queue as the dead letter queue and configure a redrive policy to put error messages to the dead letter queue. Analyze the contents of messages in the dead letter queue to diagnose the issues.","correct":true},{"id":"a7eb53a7df09334677590165f666c58f","text":"Modify the error handling logic of the Lambda function to delete the messages whenever the processing is unsuccessful with an error or exception. The error messages do not return to the queue and the normal message handling is not blocked.","correct":false},{"id":"f7b3898cfcb4851b120c9b14d044ab90","text":"Decrease the message retention period of the queue to 1 day. When the messages are not processed properly and put back in the queue, they can be quickly deleted when the retention period expires.","correct":false}]},{"id":"482e75c9-071e-4a10-83f4-575f9c15b885","domain":"awscsapro-domain5","question":"A client calls you in a panic.  They have just accidentally deleted the private key portion of their EC2 key pair.  Now, they are unable to SSH into their Amazon Linux servers.  Unfortunately the keys were not backed up and are considered gone for good.  What can this customer do to regain access to their instances?","explanation":"The two methods that AWS recommends if you lose a private key for an EC2 key pair are using Systems Manager Automation or using a secondary instance to edit the authorized_keys file.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2reset.html","title":"Reset Passwords and SSH Keys on Amazon EC2 Instances - AWS Systems Manager"}],"answers":[{"id":"509a77ae9827c5fcb60ccecc62fc9853","text":"Stop the instances, detach its root volume and attach it as a data volume to another instances.  Modify the authorized_keys file, move the volume back to the original instance and restart the instances.","correct":true},{"id":"be45655cb1d64dff71a97aa729bc4e4a","text":"Open the TELNET port (port 23) on the Security Group for the server.  Use a TELNET client to attach to the instances using the root account and password.  Modify the authorized_key file with the new public key.","correct":false},{"id":"bec3d01a56c851f61a1a09c852635db7","text":"Generate and upload a new key pair.  Stop the instances and select the new key pair from the dropdown on the Instance Settings sub-menu in the Console.","correct":false},{"id":"79457a1b908d4a36cfeba625be909d40","text":"Use AWS Systems Manager Automation with the AWSSupport-ResetAccess document to create a new SSH key for your current instance.","correct":true},{"id":"492c38d2fe2c3b96608bb8436592fe26","text":"Use the AWS CLI with the EC2 ModifyInstance action to enable SSH password-only access for the ec2-user account.  Attach using a password rather than an SSH key.  Modify the authorized_key file for the new public key.","correct":false},{"id":"f17aa014620843abd81fa849982566b0","text":"Create a new key pair in KMS then assign the new public key to the required EC2 instance.","correct":false}]},{"id":"8aa313fe-cd0f-4899-a2f4-e8f2fd64c245","domain":"awscsapro-domain4","question":"Your business depends on AWS S3 to host three kinds of files - images, documents and compressed installation packages. These files are accessed and downloaded by end-users from all US regions and west EU, though the compressed installation packages are downloaded rarely as users tend to access the service from their browsers instead of installing anything on their machines. Each installation package bundles several images and documents, and also includes binaries that are downloaded from a 3rd party service while creating the package files.\nThe images and documents range from a few KBs to a few hundred KBs in size and they are mostly static in nature. However, the compressed installation package files are generated every few hours because of changes done by the 3rd party service to their binaries, and some of them are as large as a few hundred GB-s. The installation package files can be regenerated from the images and documents fairly quickly if required. It is important to be able to retrieve older versions of the images and documents.\nWhich of the following storage solutions is the most cost-effective approach to design the storage for these files?","explanation":"The areas tested by this question are:\\n1. Versioning cannot be enabled at the object level. It is a bucket-level feature. This rules out the choice where we have a single bucket and selectively turn on versioning on for some objects only.\\n2. If you enable Versioning for a bucket containing large objects that are frequently created/uploaded, it will result in higher storage cost as all the previous versions will result in storage volume growing quickly because of frequent writes. In the given scenario, the compressed installation package files are large and also frequently generated (every few hours). There is no requirement to version them, as they can be quickly generated on-demand. Hence, putting them in a bucket that has Versioning enabled is not a good cost-effective solution. This rules out two choices - one where we have a single versioned bucket, the other where we enable versioning for both buckets.\\n3. Note that all options except one correctly identify the storage class requirements - the compressed installation package files should be stored as One-Zone IA because durability is not a prime requirement for these files (simply because they can be regenerated on-demand easily). They are rarely downloaded, hence IA is the correct class. Combined with low durability, One Zone IA is the most cost-effective solution. Only one option uses the incorrect storage tier for these files - note that IA is more expensive than One-Zone IA, and the question is about cost-effectiveness.\nHence, the only correct answer is the one that addresses both Versioning and Storage Class requirements correctly.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html","title":"Documentation on Object Versioning"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-howtoset","title":"Setting the Storage Class of an Object"}],"answers":[{"id":"fd1d330daf4d05b4fa4c888a0584130f","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the bucket. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to One-Zone Infrequent Access while uploading compressed installation package files","correct":false},{"id":"90f6b37d063a0158682b034d578bf29b","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for both the buckets. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":false},{"id":"9e88398b12bdf0dd2fdc12a19f4962a1","text":"Store the images and documents in one bucket (A) and the compressed installation package files in another bucket (B). Turn on versioning for Bucket A only. Set Storage Class to Standard S3 while uploading objects to Bucket A. Set Storage Class to One-Zone Infrequent Access while uploading objects to Bucket B","correct":true},{"id":"2e4e12f367286b12f14ae74b1fd4e350","text":"Store all three kinds of files in a single S3 bucket. Turn on versioning for the image and document objects only, but not for the compressed installation package files. Set Storage Class to Standard S3 while uploading images and documents. Set Storage Class to Infrequent Access while uploading compressed installation package files","correct":false}]},{"id":"c9f44641-660b-4c42-9380-9e7f6b0a9ba4","domain":"awscsapro-domain4","question":"As the solution architect, you are assisting your customer design and develop a mobile application using API Gateway, Lambda and DynamoDB. S3 buckets are being used to serve static content. The API created using API Gateway is protected by WAF. The development team has just staged all components to the QA environment. They are using a load testing tool to generate short bursts of a high number of concurrent requests sent to the API Gateway method. During the load testing, some requests are failing with a response of 504 Endpoint Request Timed-out Exception.\nWhat is one possible reason for this error response from API Gateway endpoint?","explanation":"The SA-P exam sometimes focuses on knowledge of response codes from API Gateway and what each distinct HTTP response code could mean.\nThe key to answering this question correctly is being able to distinguish between 4XX and 5XX HTTP error response codes. Though AWS has not been entirely consistent in their error code assignment philosophy, 4XX usually happens any time throttling kicks in because the request in that case never makes to an instance of Lambda function. 5XX happens when a Lambda function is actually instantiated, but some error (like time out) happened inside the Lambda function. One sneaky way to remember this is the fact that 5XX errors are called server errors in HTTP-land, so to generate a 5XX a server process must exist (and must have failed). Of course, in this context, the HTTP server process is a Lambda function - so in scenarios where throttling prevented a Lambda function from getting spawned, the response code cannot be 5XX. This is not consistently followed by AWS API Gateway error design, though, as we can see that AUTHORIZER_CONFIGURATION_ERROR and AUTHORIZER_FAILURE are both 500, though no Lambda function is actually spawned in either case. However, the candidate must remember that throttling always results in 4XX codes. An Endpoint Request Timed-out Exception (504) suggests that the requests in question actually made its way past the API Gateway into a Lambda function instance.\nFor the scenario where request rate exceeds API Gateway limits, the request would be blocked by API Gateway itself. The response would be 429. The exact knowledge of the code 429, however, is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe scenario where 1000 Lambda functions are already running is a similar example of throttling - the 1001st Lambda function will not even be spawned. The response, again, will be 429. However, the exact knowledge of the code 429 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThe WAF scenario is yet another example of the request not even crossing the protections placed at the gateway level. If WAF is activated on API Gateway, it will block requests when the rate exceeds the HTTP flood rate-based rule (provided all such requests come from a single client IP address). However, the response, again, will be in the 4XX area (specifically, 403 Forbidden) - however, the exact knowledge of the code 403 is not needed to eliminate this choice. It is expected of the candidate to know that any kind of throttling always results in 4XX response codes, so this choice must be incorrect.\nThis leaves Lambda time-out as the only correct answer. The mention of 30 seconds or more is a diversion tactic, in case candidate believes that the relevant Lambda time-out is 5 minutes. A given Lambda function instance may have a time-out limit of 5 minutes, but when it is invoked from API Gateway, the timeout imposed by API Gateway is 29 seconds. If a Lambda function runs for longer than 29 seconds, API Gateway will stop waiting for it and return 504 Endpoint Request Timed-out Exception.","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html","title":"Amazon API Gateway Limits and Important Notes"},{"url":"https://aws.amazon.com/blogs/compute/amazon-api-gateway-adds-support-for-aws-waf/","title":"Amazon API Gateway adds support for AWS WAF"}],"answers":[{"id":"98daaa701198f7e1c541fe8051799129","text":"The Lambda function is sometimes taking 30 seconds or more to finish executing","correct":true},{"id":"370770015087b4ff70656089fc9e3316","text":"The test is triggering too many Lambda functions concurrently. AWS imposes a soft limit of 1000 concurrent Lambda functions per region","correct":false},{"id":"8090c6b0fe9036ec84fce24b16a7dc10","text":"The load testing tool has exceeded the soft limit for request rate allowed by API Gateway","correct":false},{"id":"82f6a6eb1a4aeef5dd34f21fcd2069ef","text":"The number of requests generated by the load testing framework has exceeded the threshold for the HTTP flood rate-based rule set in the WAF settings for the stage in question","correct":false}]},{"id":"ebcf9ff3-82a1-48f8-b2fc-5d2aeb1d018c","domain":"awscsapro-domain3","question":"You are consulting with a company who is at the very early stages of their cloud journey.  As a framework to help work through the process, you introduce them to the Cloud Adoption Framework.  They read over the CAF and come back with a list of activities as next steps.  They are asking you to validate these activities to keep them focused.  Of these activities, which would you recommend delaying until later in the project?","explanation":"External communication usually comes much later in the process once project plans are defined and specific customer impact is better understood.","links":[{"url":"https://aws.amazon.com/professional-services/CAF/","title":"The AWS Cloud Adoption Framework"}],"answers":[{"id":"2f063bd85b14ae1bbeec72bf0f6c06f5","text":"Work with the Human Resources business partners to create new job roles, titles and compensation/remuneration scales.","correct":false},{"id":"937d0c376f475ce2eac7a3356601b8fb","text":"Investigate the need for training for Program and Project Management staff around agile project management.","correct":false},{"id":"199907bba5306a10dbadf5a330d5f1f6","text":"Hold a workshop with IT business partners about the creation of an IT Service Catalog concept.","correct":false},{"id":"153eadc71676701cd67fdf00dc6c4723","text":"Work with Marketing business partners to design an external communications strategy to be used during potential outages during the migration.","correct":true},{"id":"525fde29088ab22597d7d8063c7dadf6","text":"Work with internal Finance business partners to design a transparent chargeback model.","correct":false}]},{"id":"bbcb9a8c-f84d-4424-b199-9047a4625e15","domain":"awscsapro-domain2","question":"Your company's DevOps manager has asked you to implement a CI/CD methodology and tool chain for a new financial analysis application that will run on AWS. Code will be written by multiple teams, each team owning a separate AWS account. Each team will also be responsible for a Docker image for their piece of the application. Each team's Docker image will need to include code from other teams. Which approach will provide the most operationally efficient solution?","explanation":"AWS CodePipeline, AWS CodeCommit, and AWS CodeBuild all allow cross-account access once the appropriate resource-level permissions have been granted. Orchestrating deployments from a single DevOps account will provide the most operationally efficient solution, resulting in less need for coordination of services and configurations across development team accounts.","links":[{"url":"https://aws.amazon.com/products/developer-tools/","title":"Developer Tools on AWS"},{"url":"https://aws.amazon.com/blogs/devops/how-to-use-cross-account-ecr-images-in-aws-codebuild-for-your-build-environment/","title":"How to Use Cross-Account ECR Images in AWS CodeBuild for Your Build Environment"}],"answers":[{"id":"c32df9c97fbc40e07515d4ca41de63e2","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in the team accounts","correct":false},{"id":"4a4bd59860afb498d97f0d01cff52b7a","text":"Implement AWS CodePipeline in each team account. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform deployments from AWS CodeDeploy in a single DevOps account","correct":false},{"id":"b9ae951b67f2fed4a145bd7f591c8631","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the DevOps account to AWS CodeCommit in the team accounts to get the latest code. Perform cross-account access from AWS CodeBuild in the DevOps account to AWS CodeBuild in the team accounts to get the Docker images. Perform deployments from AWS CodeDeploy in the DevOps account","correct":true},{"id":"83443cdcabe51198b91ed96d84eed4a6","text":"Implement AWS CodePipeline from a single DevOps account to orchestrate builds in the team accounts. Perform cross-account access from AWS CodeCommit in the team accounts to get the latest code from AWS CodeCommit in the other team accounts. Use AWS CodeBuild in the team accounts to create the container images. Perform all deployments from AWS CodeDeploy in the DevOps account","correct":false}]},{"id":"8b70203c-6d35-44fb-9575-9b4bb2958269","domain":"awscsapro-domain1","question":"A financial services company operates in all fifty U.S. states. They've decided to deploy part of their application portfolio in the AWS us-east-1, us-east-2, and us-west-2 regions. Multiple AWS accounts will be created, one for each of the company's four business units. The applications need to be able to communicate with each other across the accounts and across the regions. The applications also need to communicate with systems in the corporate data center. Which networking approach with provide the best operational efficiency?","explanation":"Creating a hub-and-spoke network topology minimizes network management overhead. Transit Gateway would be the best approach. Routing AWS traffic for many VPCs through Transit Gateway is now possible and will allow for smooth integration of environments without the technical overhead of managing separate VPN or multiple Direct Connect connections.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-multiple-region-multi-vpc-connectivity/","title":"Multiple Region Multi-VPC Connectivity"},{"url":"https://aws.amazon.com/transit-gateway/","title":"AWS Transit Gateway"},{"url":"https://docs.aws.amazon.com/directconnect/latest/UserGuide/multi-account-associate-tgw.html","title":"Associating a Transit Gateway Across Accounts"}],"answers":[{"id":"639783dcd838facfacfca10603c4ac87","text":"Create VPC Peering connections between the VPCs in the different regions and different accounts. Use AWS Direct Connect Gateway to interface the corporate data center network to the different AWS regions","correct":false},{"id":"6f2b464e20f643061f4f98c2c84d1df5","text":"Deploy an AWS Transit Gateway as a network hub to manage the connections between the VPCs in the different regions, the different accounts, and the corporate data center network","correct":true},{"id":"5e87ca266342ca7d4c0d7a033f63da87","text":"Deploy a transit VPC in a shared account with EC2-based appliances that create hub-and-spoke VPN connections to VPCs in the other accounts, the other regions, and the corporate data center network","correct":false},{"id":"d8df048014675105d92ca9f84fa7b760","text":"Route the AWS cross-account, cross-region traffic through the corporate data center network via VPN connections to leverage existing network infrastructure","correct":false}]},{"id":"071d48ba-80e7-420e-969e-98cb2bcfbaa3","domain":"awscsapro-domain2","question":"Across your industry, there has been a rise in activist hackers launching attacks on companies like yours.  You want to be prepared in case some group turns its attention toward you.  The most common attack, based on forensic work security researchers have done after other attacks, seems to be the TCP Syn Flood attack.  To better protect yourself from that style of attack, what is the least cost measure you can take?","explanation":"AWS Shield Standard is offered to all AWS customers automatically at no charge and will protect against TCP Syn Flood attacks without you having to do anything - this meets the requirements of protecting TCP Syn Flood attacks at the lowest cost possible, as described in the question. A more robust solution which is better aligned to best practice would involve a load balancer in the data path, however as this would provide more functionality than required at a higher cost, is not the correct option for this question.","links":[{"url":"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html","title":"What Are AWS WAF, AWS Shield, and AWS Firewall Manager? - AWS WAF, AWS  Firewall Manager, and AWS Shield Advanced"}],"answers":[{"id":"6f87c7b3eda4188070a6635a49710939","text":"Implement AWS WAF and configure filters to block cross-site scripting match conditions.","correct":false},{"id":"aec18df10393205d72a60b52cf05bab9","text":"This type of attack is automatically addressed by AWS.  You do not need to take additional action.","correct":true},{"id":"82f7ef9f4b3e8e05aef157162915fecf","text":"Subscribe to a Business or Enterprise Support Plan.  Engage AWS DDoS Response Team and arrange for a custom mitigation.","correct":false},{"id":"7d8280921ead2af66cf214b774f94306","text":"Re-architect your landscape to use an application load balancer in front of any public facing services.","correct":false},{"id":"c50dd258b1fac18bd9368b07bf0fbc11","text":"Implement AWS Shield Advanced and configure it to generate CloudWatch alarms when malicious activity is detected.","correct":false}]},{"id":"312b233b-ecc8-4e70-ae91-665159c7f77b","domain":"awscsapro-domain2","question":"You work for an automotive parts manufacturer as a Cloud Solutions Architect and you are in the middle of a design project for a new quality vision system.  To \"help out\", your parent company has insisted on contracting with a very expensive consultant to review your application design.  (You suspect that the consultant has more theoretical knowledge than practical knowledge however.)  You explain that the system uses video cameras and special polarizing filters to identify defects on fuel injectors.  As the part passes each station, an embedded RFID serial number is read and included with the PASS/FAIL vision test result in a JSON record written to DynamoDB.  The DynamoDB table is exported to Redshift on a monthly basis.  If a flaw is detected, the part can sometimes be reworked and sent back through the process--but it does retain its unique RFID tag.  Only the latest tests need to be kept for the part.  The consultant reviews your design and seems slightly frustrated that he is unable to recommend any improvement.  Then, he smiles and asks \"How are you ensuring idempotency?  In case a part is reprocessed?\"    ","explanation":"Idempotency or idempotent capability is a design pattern that allows your application to deal with the potential of duplicate records.  This can happen when interfaces fail and some records need to be reprocessed.  In this case, we are using a unique RFID serial number as our identifier for the part.  In DynamoDB, we would just overwrite the record with the latest record using a UpdateItem SDK method.  For Redshift, an UPSERT function allows us to either insert as a new record or update if a record of the same key already exists.  Redshift can do this using a merge operation with a staging table.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateItem.html","title":"UpdateItem - Amazon DynamoDB"},{"url":"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-upsert.html","title":"Use a Staging Table to Perform a Merge (Upsert) - Amazon Redshift"}],"answers":[{"id":"8f594d180a595027ddef4e33f2784b0f","text":"You will be using CloudWatch to monitor the DynamoDB tables for capacity concerns.  If needed, you can enable DynamoDB auto scaling to accommodate the extra volume that reprocessing might introduce.","correct":false},{"id":"d87a9f08720c0fd91dee999f81f6f0ed","text":"You could change your design to write the message first to an SQS queue with FIFO enabled.  The records would then be guaranteed to process in the order they arrived.","correct":false},{"id":"9dc68479e32f89bfe0afde00f41ae6c3","text":"For the target DynamoDB table, you have defined the unique RFID string as the partition key.  When copying to Redshift, you use table merge method to perform a type of UPSERT operation.","correct":true},{"id":"5a65801823c8e80af868a9ca05e34e18","text":"You will be using API Gateway and Lambda for the insert into DynamoDB so scaling is not a concern. The part can be processed as many times as needed and Lambda will scale as needed.","correct":false}]},{"id":"4b00251a-a278-4d88-b715-955b4752a79a","domain":"awscsapro-domain2","question":"You'd like to create a more efficient process for your company employees to book a meeting room.  Which of the following is the most efficient path to enabling this improved business experience?","explanation":"With Alexa for Business, you can enlist Alexa-enabled devices to perform tasks for employees like retrieve information, start conference calls and book meeting rooms.","links":[{"url":"https://aws.amazon.com/blogs/business-productivity/announcing-room-booking-for-alexa-for-business/","title":"Announcing Room Booking for Alexa for Business | Business Productivity"}],"answers":[{"id":"12cb35ea295a9a96bfba94741cb4f0df","text":"Sign-up for Amazon Chime.  Create conference rooms in the console and place speakerphones in each conference room.","correct":false},{"id":"a723a332c2ed052968becb9b6824e2a4","text":"Sign-up for AWS Alexa for Business. Create conference rooms in the console and place an Alexa device in each conference room.","correct":true},{"id":"5445641c568edaf760839d9f5bb7169c","text":"Configure an Alexa device with a custom skill backed by a Lambda function.  Use Amazon Lex to convert the audio sent to the Lambda function into an actionable skill.  ","correct":false},{"id":"e43eb2c75536fac2d714b862f4fec490","text":"Invest in a voice-to-text API from the AWS Marketplace.  Create a custom Lambda function that calls the API and books a conference room.  Equip each conference room with Amazon Dash buttons and configure them to invoke the Lambda function.","correct":false}]},{"id":"4c49e888-8f76-4b15-b267-7f6ec35579ca","domain":"awscsapro-domain5","question":"A client has asked you to review their system architecture in advance of a compliance audit.  Their production environment is setup in a single AWS account that can only be accessed through a monitored and audited bastion host. Their EC2 Linux instances currently use AWS-encrypted EBS volumes and the web server instances sit in a private subnet behind an ALB that terminates TLS using a certificate from ACM. All their web servers share a single Security Group, and their application and data layer servers similarly share one Security Group each. Their S3 objects are stored with SSE-S3.  The auditors will require all data to be encrypted at rest and will expect the system to secure against the possibility that TLS certificates might be stolen by would-be spoofers.  How would you help this client pass their audit in a cost effective way? ","explanation":"All the measures they have taken with Certificate Manager, S3 encryption and the EBS volumes meet the audit requirements.  There is no need for LUKS, CloudHSM or client-side encryption.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html","title":"Amazon EBS Encryption - Amazon Elastic Compute Cloud"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html","title":"Protecting Data Using Server-Side Encryption with Amazon S3-Managed  Encryption Keys (SSE-S3) - Amazon Simple Storage Service"}],"answers":[{"id":"0c31fb48e65443ba5bfa312a7dcc117c","text":"Encrypt the S3 objects with OpenPGP locally before re-uploading them to S3.","correct":false},{"id":"bdddd09d0832e16504afd5c88136cf7e","text":"Leave the S3 objects alone.","correct":true},{"id":"ab9ad7bfd57b97954a6f861d872c6137","text":"Continue to use the ACM for the TLS certificate.","correct":true},{"id":"92194f6603feb3f83a46b00bda37de5a","text":"Deploy CloudHSM and migrate the TLS keys to that service.","correct":false},{"id":"93113c2b6ca9be67acbd3561eef56481","text":"Reconfigure the EC2 EBS volumes to use LUKS OS-Level encryption.","correct":false},{"id":"d9447af4853ab8736e49349138cac8fb","text":"Make no changes to the EBS volumes.","correct":true}]},{"id":"0e57f592-af77-4ff7-b32a-752702278ab5","domain":"awscsapro-domain2","question":"Several years ago, the company you are consulting for started an SOA concept to enable more modularity.  At that time, they chose to deploy each microservice as separate LAMP stacks launched in Elastic Beanstalk instances due to the ease of deployment and scalability.  They are now in the process of migrating all the services over to Docker containers.  Which of the following options would make the most efficient use of AWS resources?","explanation":"ECS might be a possible choice, but pods are a concept with Kubernetes and not ECS.  Elastic Beanstalk deployments run in their own self-contained environment and don't share resources.  Of the choices presented, the only feasible choice is K8s on EC2.","links":[{"url":"https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html","title":"Pod Networking - Amazon EKS"},{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html","title":"What Is AWS Elastic Beanstalk? - AWS Elastic Beanstalk"}],"answers":[{"id":"661aa72d0ab709e1931e20010a6e3ad7","text":"Use the Elastic Container Repository and AWS Spectrum to fully automate the deployment and scaling of the Docker containers.","correct":false},{"id":"a473fc9c398c6c24b4c0514eb9fb546a","text":"Configure Elastic Beanstalk to more efficiently run the existing landscapes across a single auto-scaled group rather than separate instances.","correct":false},{"id":"d93d65de15f35d86079a7a18bda05be2","text":"Package the containers as JAR files and deploy them with Lambda to take advantage of the pay-per-use model","correct":false},{"id":"25ffdcfeb30ada0ea8a578689b8d0a49","text":"Create an ECS cluster and configure each container to exist in its own pod.  Use the Calico add-on to manage access to each service.","correct":false},{"id":"f9f924e7f824f3e085294f8e97e7b362","text":"Create an auto-scaled group of EC2 instances and run Kubernetes across them to orchestrate the containers.","correct":true}]},{"id":"e24f7f76-6908-4ad9-820a-11790bfdcec6","domain":"awscsapro-domain3","question":"You are helping a client migrate over an internal application from on-prem to AWS.  The application landscape on AWS will consist of a fleet of EC2 instances behind an Application Load Balancer.  The application client is an in-house custom application that communicates to the server via HTTPS and is used by around 40,000 users globally across several business units.  The same exact application and landscape will be deployed in US-WEST-2 as well as EU-CENTRAL-1.  Route 53 will then be used to redirect users to the closest region.  When the application was originally built, they chose to use a self-signed 2048-bit RSA X.509 certificate (SSL/TLS server certificate) and embedded the self-signed certificate information into the in-house custom client application.  Regarding the SSL certificate, which activities are both feasible and minimize extra administrative work?","explanation":"You can import private certificates into Certificate Manager and assign them to all the same resources you can with generated certificates, including an ALB.  Also note that Certificate Manager is a regional service so certificates must be imported in each region where they will be used.  The other options in this question would either require you to update the certificate on the client or requires unnecessary steps to resolve the challenge.","links":[{"url":"https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html","title":"Importing Certificates into AWS Certificate Manager - AWS Certificate  Manager"}],"answers":[{"id":"28694bd7f280694a43741563f6933ad6","text":"Create a new public SSL/TLS certificate using Certificate Manager and configure the common name and OU to match the existing certificate.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false},{"id":"0663551d15f5b15af587ac8bf75a2566","text":"Purchase a new public SSL/TLS certificate from a third-party CA.  Upload the certificate to Certificate Manager and assign that certificate to the Application Load Balancers.","correct":false},{"id":"0c4320d1dd787a5bae2b43479b645d94","text":"Use Service Catalog to push an update of the in-house app which includes an updated certificate and CA chain.  Generate a new private certificate using OpenSSL. Import the new certificate to Certificate Manager in US-EAST-1.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false},{"id":"d4976a6c33ee189e6b681dffc83cbac5","text":"Import the existing certificate and private key into Certificate Manager in both regions.  Assign that imported certificate to the Application Load Balancers using their respective regionally imported certificate.","correct":true},{"id":"111997579381183b07a22fad8574e76c","text":"Create a new Certificate Authority within Certificate Manager and import the existing certificate.  Generate a new certificate, CA chain and private key and push an update for the application.  Assign the new certificate to the Application Load Balancers in all regions.","correct":false}]},{"id":"dc82c397-347d-4f69-bb06-03822238c7a0","domain":"awscsapro-domain1","question":"You are consulting for a large multi-national company that is designing their AWS account structure.  The company policy says that they must maintain a centralized logging repository but localized security management.  For economic efficiency, they also require all sub-account charges to roll up under one invoice.  Which of the following solutions most efficiently addresses these requirements?","explanation":"Service Control Policies are an effective way to broadly restrict access to certain features of sub-accounts.  Use of a single separate logging account is an effective way to create a secure logging repository.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html","title":"Service Control Policies - AWS Organizations"}],"answers":[{"id":"0c9b5a803a99a3d2ef53869b6857c0e0","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use ACLs to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.  Establish trust relationships between the Consolidated Billing account and all sub-accounts.","correct":false},{"id":"cbec34b5388f7f183659e82c20fb3abf","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Use an SCP to restrict sub-accounts from changing CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create localized IAM Admin accounts for each sub-account.","correct":true},{"id":"74a6c6df518100b16da3f16e870b5d5c","text":"Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  Create localized IAM policies to restrict modification of CloudWatch and CloudTrail configuration.  Configure consolidated billing under a single account and register all sub-accounts to that billing account.  Create a centralized security account and establish trust relationships between each sub-account.","correct":false},{"id":"871870fa49beedfb95106595e4a1c9f4","text":"Configure billing for each account to load into a consolidated RedShift instance.  Create a centralized security account and establish trust relationships between each sub-account.  Configure admin roles within IAM of each sub-account for local administrators.  Create a stand-alone consolidated logging account and configure all sub-account CloudWatch and CloudTrail activity to route to that account.  ","correct":false}]},{"id":"8d0d69dd-35f2-468a-883b-18ad1135b564","domain":"awscsapro-domain2","question":"You are providing a security administrator with some on-the-job training regarding IAM, roles and policies.  The security administrator asks just what the policy evaluation logic is when a request is made?  For example, when a user tries to use the AWS Management Console, what is the process that AWS goes through to determine if that request is allowed?","explanation":"Knowing this logic flow can help troubleshoot security issues.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html","title":"Policy Evaluation Logic - AWS Identity and Access Management"}],"answers":[{"id":"f3087f569306369e9cfa2b1f6806acbc","text":"The AWS service receives the request.  AWS first authenticates the principal.  Next, AWS determines which policy to apply to the request.  Then, AWS evaluates the policy types and arranges an order of evaluation. Finally, AWS then processes the policies against the request context to determine if it is allowed.","correct":true},{"id":"e13db2078457fb9b19c98ad9ae06563c","text":"Upon receiving the request, AWS will first evaluate the permissions of the principal and the request.  Next, any STS assumed role policies are evaluate.  Then, specific user or role permission boundaries are evaluated against the request and the principal.  Finally, any Organizational boundaries (SCPs) are evaluated against the request.  If there are no explicit deny actions, the request is allowed.","correct":false},{"id":"eb4b98badf50f00af2105b3132c620af","text":"First AWS evaluates organizational boundaries of the request in context of the principal.  Next, any STS assumed role policies are evaluated.  Then, permission boundaries are evaluated against the principals request.  Finally, AWS then issues a decision of allow if there were no explicit deny.","correct":false},{"id":"5afa15bfdd702c70736460429675418c","text":"The AWS service receives the request from the principal.  AWS first evaluates the authority of the principal in context of the service.  Next, any identity-based or resource-based policies are evaluated.  Then, explicit Action statements are evaluated in context of the request.  Finally, AWS issues a decision on whether to allow or deny the request.","correct":false}]},{"id":"edb30172-3f76-4423-a6bb-78a3d2fdeb42","domain":"awscsapro-domain2","question":"Your team is managing hundreds of Linux and Windows EC2 instances in different environments such as development, QA, staging and production. You need a tool to help you automate the process of patching instances so that the operating systems have latest patches and meet the compliance policies. You want to manage the patching in different groups depending on the environment. For example, patches should be deployed and tested in the QA environment first before the production environment. How would you achieve this requirement through an AWS service?","explanation":"AWS Systems Manager Patch Manager is the most appropriate tool to manage the patching for large groups of EC2 or on-premises instances. For different environments, users can configure patch groups using the \"Patch Group\" tag and then establish a patch baseline for each patch group. It is better to manage instances with the \"Patch Group\" tag rather than other customized tags. AWS SSM Session Manager and AWS SSM Run Command are not suitable to deploy patches across a large number of instances. The AWS-RunRemoteScript command is also incorrect as it is used to execute scripts stored in a remote location.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-patchgroups.html","title":"Patch Manager Patch Groups"}],"answers":[{"id":"08b9feba2a12ba67141a0ffe02938798","text":"Add patch group tags in the EC2 instances. Perform the patching using the command AWS-RunRemoteScript in AWS SSM Run Command. Patch on the QA environment first by selecting the QA patch group tag.","correct":false},{"id":"32ac8d27221ec8e699eb4e872d3f6ed0","text":"Centrally manage the instances in AWS SSM Managed Instances and divide them into different categories. Perform the patching activities from AWS SSM Session Manager in a maintenance window.","correct":false},{"id":"6c4763d45c6cb24622b0db9533f95e0c","text":"Create environmental tags in EC2 instances such as a tag key named \"env\". In AWS SSM Patch Manager, configure patching activities by selecting the instances using the tag. Patch on the QA environment first and perform the necessary testing.","correct":false},{"id":"b99f4271e073e1e030f3c26c383c5959","text":"In AWS Systems Manager Patch Manager, create different patch groups using the tag key \"Patch Group\" and configure a patch baseline for each patch group. Schedule the patching in a maintenance window by selecting a patch group.","correct":true}]},{"id":"580790f0-3491-4b4c-a9b8-42b36a787cf5","domain":"awscsapro-domain2","question":"A new project needs a simple, scalable Amazon Elastic File System (EFS) to be used by several Amazon EC2 instances located in different availability zones. The EFS file system has a mount target in each availability zone within a customized VPC. You have already attached a security group to EC2 instances that allows all outbound traffic. In the meantime, how would you configure the EFS volume to allow the ingress traffic from these EC2 instances?","explanation":"EC2 instances connect to the Amazon EFS file system through the mount targets using the Network File System (NFS) port. The mount targets use a security group to control the access from EC2 instances. It should have an inbound rule to allow the NFS port (TCP:2049). The source of the security group would be the EC2 security group rather than the VPC CIDR. Either IAM role or network ACL cannot be used to control inbound access to an EFS system.","links":[{"url":"https://docs.aws.amazon.com/efs/latest/ug/network-access.html","title":"Security Groups for Amazon EC2 Instances and EFS Mount Targets"}],"answers":[{"id":"070a2bfc6b40165751997ed4d5d4e295","text":"Attach a security group to the mount targets in all availability zones. Allow the NFS port 2049 for its inbound rule. Identify the EC2 security group name as the source in the rule of the security group.","correct":true},{"id":"0fb49dc49facb19b17db23bdff392a1d","text":"Configure a new security group to allow inbound traffic from the VPC CIDR IP range such as 10.10.0.0/16. Associate the security group with the EFS file system directly to allow the ingress traffic from EC2 instances in the VPC.","correct":false},{"id":"b346d4a177db41a1cfb51b80ae95708a","text":"Add an inbound rule in the network ACL to allow the ingress traffic from the NFS TCP port 2049. The source of the rule should be 0.0.0.0/0. Apply the network ACL in all the subnets where the EFS mount targets exist.","correct":false},{"id":"10fa29df072c2421e53a66048a92cbe3","text":"Make sure the attached IAM role in EC2 instances has the \"elasticfilesystem:*\" permission. The ingress traffic in the EFS volume is automatically allowed if the IAM role has enough permissions to interact with Elastic File Systems.","correct":false}]},{"id":"374fde7d-232a-4cfe-b5d6-7755d564c6ca","domain":"awscsapro-domain1","question":"You are consulting for a company that has decided to partially migrate some resources to AWS from their two data centers (DC1 and DC2).  Their first order of business is to design a robust, redundant and cost-effective network connection between their data centers and AWS.  They already have redundant links between DC1 and DC2.  Which of the following architectures provides the highest availability at the least cost?","explanation":"A common and cost effective way to provide a redundant link to AWS with Direct Connect is a VPN connection.  In the event that the Direct Connect path fails at DC1, your on-prem router can redirect traffic over the VPN at DC2 via the DC1-DC2 link.  Having dual Direct Connect links is definitely redundant but more expensive than a VPN.","links":[{"url":"https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/","title":"Multiple Data Center HA Network Connectivity â€“ AWS Answers"}],"answers":[{"id":"abd8b38f393b6a0d0b42f68dca5ec24d","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure BGP to dynamically route traffic across the nearest Direct Connect link.","correct":false},{"id":"77c9fb7459e1b19f6f655d63556016e8","text":"Configure a Direct Connect connection from DC1 to a Virtual Private Gateway on AWS.  Setup a VPN connection from DC2 to a Virtual Private Gateway on AWS.  Configure a dynamic route across DC1 and DC2 for both paths with a route priority favoring the Direct Connect path to AWS.","correct":true},{"id":"425adb8435a7c170eef3698fa729f5ea","text":"Configure a Direct Connect connection from both DC1 and DC2 to a Virtual Private Gateway on AWS. Configure a default route in both DC1 and DC2 to route traffic to the local Direct Connect link.","correct":false},{"id":"bcac2f1d84c5f367ede3342f0adda492","text":"Ensure that DC1 and DC2 have separate ISPs.  Setup VPN connections from DC1 and DC2 to a Virtual Private Gateway on AWS.  Create static routes at each DC to use the local VPN to AWS.  Use CloudTrail to monitor traffic on the Virtual Private Gateway and trigger a script to update the static route if one of the VPN connections goes down.","correct":false}]},{"id":"08a68d51-48ba-43b7-b0c3-c24e04bb33a8","domain":"awscsapro-domain3","question":"You have just completed the move of a Microsoft SQL Server database over to a Windows Server EC2 instance.  Rather than logging in periodically to check for patches, you want something more proactive.  Which of the following would be the most appropriate for this?","explanation":"The default predefined patch baseline for Windows servers in Patch Manager is AWS-DefaultPatchBaseline.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html","title":"Default and Custom Patch Baselines - AWS Systems Manager"}],"answers":[{"id":"42c46b2fcd3034cf79b83f0d5dc37f7d","text":"Make use of Patch Manager and the AWS-DefaultPatchBaseline pre-defined baseline","correct":true},{"id":"78beead90b3e8deb4bf1ee7e3544a309","text":"Make use of AWS Batch to apply patches as they appear on the RSS feed from Microsoft","correct":false},{"id":"4b5715f588231d7445bb512181ea13a2","text":"Make use of Server Manager and the AWS-LinuxWindowsDefaultPatchBaseline pre-defined baseline","correct":false},{"id":"b40084bb5d5139b353505e5f942afc34","text":"Make use of Patch Manager to apply patches as you have defined in the Patch Groups","correct":false},{"id":"5ee836bac8680ff00d457a8d7f90fad6","text":"Make use of Patch Manager and the AWS-WindowsDefaultPatchBaseline pre-defined baseline","correct":false}]},{"id":"6b6689f4-b150-482a-aa96-eab1674cb232","domain":"awscsapro-domain5","question":"Quality Auto Parts, Inc. has installed IoT sensors across all of their manufacturing lines. The devices send data to both AWS IoT Core and Amazon Kinesis Data Streams. Kinesis Data Streams triggers a Lambda function to format the data, and then forwards it to AWS IoT Analytics to perform monitoring and time-series analyses, and to take actions based on business processes. After an equipment failure on one of the manufacturing lines causes tens of thousands of dollars in revenue losses, it's determined that alarms for a specific piece of equipment where received seventy-five seconds after the issue originated, and that automated corrective action within a few seconds of the problem could have avoided the financial losses altogether. What changes should be made to the architecture to improve the latency of device alerts?","explanation":"AWS IoT Analytics is useful for understanding long-term device performance, performing business reporting, and identifying predictive fleet maintenance needs, but common latencies run from seconds to minutes. If you need to analyze IoT data in real-time for device monitoring, use Kinesis Data Analytics, which provides latencies in the millisecond to seconds range. A Lambda function can be used as the destination for Kinesis Data Analytics to perform corrective actions. IoT Core rules can write messages to a Kinesis stream, but not directly to Kinesis Data Analytics. Having a Lambda function perform anomaly detection will work, but will require more logic to be written for query setup and execution than using a specialized service like Kinesis Data Analytics. With Amazon CloudWatch Alarms, an alarm will watch a single metric over a period time, but will not provide the capabilities of SQL to detect complex anomaly conditions.","links":[{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/aws-reference-architecture-time-series-processing.pdf?did=wp_card&trk=wp_card","title":"Processing IoT Time Series Data on AWS"},{"url":"https://aws.amazon.com/iot-analytics/faq/","title":"AWS IoT Analytics FAQs"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/05/introducing-real-time-iot-device-monitoring-with-kinesis-data-analytics/","title":"Introducing Real-Time IoT Device Monitoring with Kinesis Data Analytics"}],"answers":[{"id":"522af3cd7d520d3e94e97c02d19c0672","text":"Create an AWS IoT Core rule to write the message to Amazon CloudWatch Alarms to detect anomalies in the data. Invoke another AWS Lambda function from CloudWatch Alarms to perform device corrective action when needed.","correct":false},{"id":"7e2eb8f3a96a390aab88e66000821c26","text":"Create an AWS IoT Core rule to write the message to Amazon Kinesis Data Analytics to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":false},{"id":"71ade679994c0c1e6e55b3853194e4c5","text":"Add another AWS Lambda function as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Have the Lambda function write the anomalies to Amazon DynamoDB and perform device corrective action when needed.","correct":false},{"id":"fe8ff20697982ca413f81ea14472e603","text":"Add Amazon Kinesis Data Analytics as a second consumer of the Kinesis Data Stream to detect anomalies in the data. Invoke another AWS Lambda function from Kinesis Data Analytics to perform device corrective action when needed.","correct":true}]},{"id":"58fa6440-b521-4dff-b6be-b1f8818c718d","domain":"awscsapro-domain2","question":"You currently manage a website that consists of two web servers behind an Application Load Balancer.  You currently use Route 53 as a DNS service.  Going with the current trend of websites doing away with the need to enter \"www\" in front of the domain, you want to allow your users to simply enter your domain name. What is required to allow this?","explanation":"Route 53 allows you to create a record for a zone apex.  In this case, we have created an alias record for the ALB.","links":[{"url":"https://docs.aws.amazon.com/govcloud-us/latest/ug-west/setting-up-route53-zoneapex-elb.html","title":"Setting Up Amazon Route 53 Zone Apex Support with an AWS GovCloud (US-West)  Elastic Load Balancing Load Balancer - AWS GovCloud (US-West) User Guide"}],"answers":[{"id":"5c48fe59299d582ae6074d269711de20","text":"Route 53 does not currently support zone apex records.  You must use a third-party DNS provider.","correct":false},{"id":"0dc522eb50b1c1320900ac02fcfb4dd6","text":"Create an S3 bucket as a static web host.  Create simple HTML file that redirects to the www subdomain.  Use CloudFront custom origins as a front-end for your top-level domain name.","correct":false},{"id":"9e87834a987d5b3556ce3e9f35ac6a82","text":"Create an A record for your top-level domain name as an alias for the ALB.","correct":true},{"id":"f70aff68786523bc13fd44cb89feb502","text":"Create an A record for your top-level domain name using the public IP of your ALB.","correct":false},{"id":"935c6d54a7555f482da11c1ded5dfc9c","text":"Create a CNAME record for the root domain and configure it to resolve to www subdomain name.","correct":false}]},{"id":"de88bc69-44a8-4a12-b28f-0a5e86db3939","domain":"awscsapro-domain1","question":"You have been entrusted to act as the interim AWS Administrator following the departure of the erstwhile Administrator in your company. You notice that there are several existing roles called role-engineer, role-manager, role-qa, role-dba, role-data-scientist, etc. When a new person joins the company, the new IAM user simply assumes the right role while using AWS - this allows central management of permissions and eliminates the need to manage permissions on a per-user basis.\nA new QA hire joins the company a few days later. You create an IAM User for her. You attach a Policy to the new IAM User that allows Action STS AssumeRole on any Resource. However, when this employee logs in the same day and tries to switch roles to role-qa, she is denied and is unable to assume the role-qa Role.\nWhat could be one reason why this is happening and how can it be best fixed?","explanation":"In order to allow an IAM User to successfully assume an IAM Role, two things must happen. First, the Policy attached to the User must allow the action STS AssumeRole. This is already true according to the question. Second, the Trust Policy of the Role itself must allow the User in question to assume the Role. This second condition can be met if we specify the arn of the User in the Principal element of the Trust Policy. In general, this question can be answered if the candidate is familiar with the concept of Principal in a Role, see link - A Principal within an Amazon IAM Role specifies the user (IAM user, federated user, or assumed-role user), AWS account, AWS service, or other principal entity that is allowed or denied to assume or impersonate that Role. Trust Policy is different than the Policy permissions - think of Policy Permissions as [what can be accessed] and Trust Policy as [who can access].\nTrust Policy cannot belong to an IAM User, hence the choice that claims the problem to be an unmodified User Trust Policy is incorrect. IAM changes are instantly effective, so the choice that points at the need of a time delay is also incorrect. Among the other two choices, the knowledge needed to pick the right one is an awareness of the Principal element.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html","title":"AWS JSON Policy Elements - Principal"},{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html","title":"IAM Roles"}],"answers":[{"id":"765f20e64b35dbc08c2bc319bcbe7e1a","text":"Sufficient time has not passed since you made the changes. It takes up to 12 hours to propagate IAM role changes. To fix this, ask her to try again the next day.","correct":false},{"id":"b3bf261fca3a734ad312d3ac0e5d0589","text":"You have not modified the Trust Policy of the IAM User to trust the Role role-qa. To fix this, add a Condition to the IAM Policy attached to the new user that filters on the role and specify the arn of role-qa","correct":false},{"id":"dbb48c05e238c18bdb9c17ee265e387b","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Condition element of the Trust Policy of the Role","correct":false},{"id":"e6dacaf19a289e1f73855c5e904b21fb","text":"You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role. To fix this, add the arn of the new IAM User to the Principal element of the Trust Policy of the Role","correct":true}]},{"id":"0a4b2449-9275-4c2f-af02-0f8c51614f3a","domain":"awscsapro-domain2","question":"You are part of a business continuity team at a consumer products manufacturer.  In scope for the current project is the company web server which serves up static content like product manuals and specification sheets which customers can download.  This landscape consists only of a single NGINX web server and 5TB of local attached storage for the static content.  In the case of a failover, RTO has been defined as 15 minutes with RPO as 24 hours as the content is only updated a few times a year.  Staff reductions and budget constraints for the year mean that you need to carefully evaluate and choose the most cost-effective and most automated solution in the case of a failover.  Which of the following would be the most appropriate given the situation?","explanation":"In this case, the most cost-effective and most automated way to ensure the reliability statistics would be to migrate the static content to S3.  This option has built-in robustness and will cost less than any other option presented.","links":[{"url":"http://d36cz9buwru1tt.cloudfront.net/AWS_Disaster_Recovery.pdf","title":"Using Amazon Web Services for Disaster Recovery"}],"answers":[{"id":"bceaccaea071754d3724eaf31f0f6189","text":"Migrate the website content to an S3 bucket configured for static web hosting.  Create a Route 53 alias record for the web server domain.  End-of-life the on-prem web server.","correct":true},{"id":"925fb4f48c2c90011e7e1f92d3412dcd","text":"Migrate the static content to an EFS share.  Mount the EFS share via NFS from on-prem to serve up the web content.  Configure another EC2 instances with NGINX to also mount the same share.  Upon fail-over, manually redirect the Route 53 record for the web server to the IP address of the EC2 instance.","correct":false},{"id":"69fd92fc5be4948bfc0128d02ed2f392","text":"Install the CloudWatch agent on the web server and configure an alarm based on a health check.  Create an EC2 replica installation of the web server and stop the instances.  Create a Lambda function that is triggered by the health check alarm which starts the dormant EC2 instance and updates a DNS entry in Route 53 pointing to the new server.","correct":false},{"id":"2d14eb477a2f2c9dc3605ea5740297cd","text":"Create a small pilot-light EC2 instance and configure with NGINX. Configure a CRON job to run every 24 hours that syncs the data from the on-prem web server to the pilot-light EC2 EBS volumes.  Configure an Application Load Balancer to direct traffic to the on-prem web server until a health check fails.  Then, the ALB will redirect traffic to the pilot light EC2 instances. ","correct":false},{"id":"46b84866da301243767946743c6024a1","text":"Download and configure the AWS Storage Gateway, creating a volume which can be replicated to AWS S3.  Attach that volume to the web server via iSCSI and migrate the content to that Storage Gateway volume.  Locate an AMI from the AWS Marketplace for NGINX.  If a failover is required, manually launch the AMI and run an RSYNC between the on-prem server and the EC2 server to migrate the content.","correct":false}]},{"id":"d8bfc54e-024a-4fbb-9daa-9a218a10b738","domain":"awscsapro-domain5","question":"Your company has an Inventory Control database running on Amazon Aurora deployed as a single Writer role. Over the years more departments have started querying the database and you have scaled up when necessary.  Now the Aurora instance cannot be scaled vertically any longer, but demand is still growing.  The traffic is 90% Read based.  Choose an option from below which would meet the needs of the company in the future.","explanation":"This question is about scaling, and if you have scaled up to the maximum level (db.r4.16xlarge) the next step is to consider scaling out.  In this case the application is Read heavy, which lends itself perfectly to adding extra Read replicas and using Read-Write splitting to help future growth.  Changing the max_connections value or using Query plan optimisation may make performance more efficient, but they are not long term solutions. Adding Multi-AZ simply adds High Availability.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Performance.html","title":"Managing Performance and Scaling for Aurora DB Clusters"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-replicas-adding.html","title":"Adding Aurora Replicas to a DB Cluster"}],"answers":[{"id":"15cd367246a8991991de371856908334","text":"Create multiple additional Readers within the Aurora cluster and alter the application to make use of Read-Write splitting","correct":true},{"id":"ce644fa65dde55c85ae051c5081ba860","text":"Use Query plan management to allow the optimizer to choose the most efficient plan for each job and make transactions quicker","correct":false},{"id":"c6229282cfe5ed1024d22e629709b94d","text":"Increase the maximum number of connections into the database by changing the max_connections parameter","correct":false},{"id":"d1fe9035612273c2d0c6b9f4229e1d3d","text":"Convert Aurora to a Multi-AZ Deployment in three or more zones","correct":false}]},{"id":"3e23b8a9-ce64-430f-936f-9a318a85da75","domain":"awscsapro-domain4","question":"To be sure costs of AWS resources are allocated to the proper budgets, you are trying to come up with a way to allocate the AWS bill to the proper cost centers.  Which of the following would be most effective for your organization?","explanation":"Using Service Catalog is a good way to automatically enforce and apply a tagging strategy and it requires no special effort from the product consumers.","links":[{"url":"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/tagoptions-launching.html","title":"Launching a Product with TagOptions - AWS Service Catalog"}],"answers":[{"id":"683fce4e1eed553dd4cfa34806ffcbc6","text":"Use API Gateway to create a proxy for the API of the resources your users will deploy.  Insert some custom logic using VTL to automatically append a cost center tag to the request based on the cost center of the IAM user making the request.","correct":false},{"id":"f065124beaeb3f8cde1976516f7fb97a","text":"Make use of AWS Artifact to analyse the spending pattern over the month and identify the IAM users responsible for the most costs.  Cross-reference that with the cost centers to which IAM users belong.","correct":false},{"id":"37c28a1741830444dec6469aa71f605c","text":"Deploy products within AWS Service Catalog and only allow users to deploy resources using the catalog.  Use TagOptions to provide the users a list from which they can select their cost center.  Activate the cost center tag in the Billing Console.","correct":true},{"id":"874d7ac46d1adeb7650db0db8dc9d2c1","text":"Use an SCP at the organizational level to require a cost center tag be applied to every resource.  Activate the cost center tag in the Billing Console and allocate costs based on that.","correct":false},{"id":"e2f5a49c87b9eac8673cfe95968f124d","text":"Use AWS Batch to periodically run a custom Lambda function that scans all resources and deletes any without proper tagging for cost center.","correct":false}]},{"id":"2d9a7fba-ee40-4128-8f36-b32ef55ce662","domain":"awscsapro-domain3","question":"You have just been informed that your company's data center has been struck by a meteor and it is a total loss.  Your company's applications were not capable of being deployed with high availability so everything is currently offline.  You do have a recent VM images and DB backup stored off-site.  Your CTO has made a crisis decision to migrate to AWS as soon as possible since it would take months to rebuild the data center.  Which of the following options will get your company's applications up and running again in the fastest way possible?","explanation":"The Server Migration Service uses the Server Migration Service Connector which is an appliance VM that needs to be loaded locally in vCenter.  We don't have a VMware system...only a backup of an image so this won't work.  The best thing we can do is import the VM and restore the database.","links":[{"url":"https://docs.aws.amazon.com/server-migration-service/latest/userguide/prereqs.html","title":"Server Migration Service (SMS) Requirements - AWS Server Migration Service"},{"url":"https://aws.amazon.com/ec2/vm-import/","title":"VM Import/Export"}],"answers":[{"id":"e50f3fc14c0feac5ff24b30bf605d687","text":"Use Server Migration Service to import the VM into EC2.  Use DMS to restore the backup to an RDS instance on AWS.","correct":false},{"id":"af2dfb68e123596c40e5014f3e8dc491","text":"Copy the VMs into AWS and create new AMIs from them.  Create a clustered auto scaling group across multiple AZs for your application servers.  Provision a multi-AZ RDS instance to eliminate the single-point-of-failure problem.  Restore the data from the backups using the database admin tools.","correct":false},{"id":"364a256b0572da0bd43823d35b22e01d","text":"Call your data communications provider and order a Direct Connect link to your main office.  Order a Snowball Edge to serve as a mobile data center.  Restore the VM image to the Snowball Edge device as an EC2 instance.  Restore the backup to an RDS instance on the Edge device.  When the Direct Connect link is installed, use that to smoothly migrate to AWS.","correct":false},{"id":"c3692f609e87c0dc0416f0ad05897b3f","text":"Explain to company stakeholders that it is not possible to migrate from the backups directly to AWS.  Recommend that we first find a co-location site, procure similar hardware as before the disaster and restore everything there.  Then, we can carefully migrate to AWS.","correct":false},{"id":"b8015467f854fe29575bd8fd26c819a5","text":"Use VM Import to upload the VM image to S3 and create the AMI of key servers.  Manually start them in a single AZ.  Stand-up a single AZ RDS instance and use the backup files to restore the database data.","correct":true}]},{"id":"bdd6d6a9-48a5-45f8-bb74-873f266d85df","domain":"awscsapro-domain2","question":"A hospital would like to reduce the number of readmissions for high risk patients by implementing an interactive voice response system to provide reminders about follow up visit requirements after patients are discharged. The hospital has the capability to automatically send HL7 messages that include the patient's phone number and follow up visit information from its medical records application via Apache Camel. They've chosen to deploy the solution on AWS. They already have a VPN connection to AWS, and all aspects of the application need to be HIPAA eligible. Which architecture will provide the most resilient and cost effective solution for the automated call system?","explanation":"S3 provides a low cost repository for the HL7 messages received. Having Lambda write the object keys to SQS, and having another Lambda function retrieve and parse the messages gives the architecture asynchronous workflow. Amazon Connect provides the capability to define call flows and perform IVR functions. Each of these services is HIPAA eligible. DynamoDB is also a good option for storing message information, but will be more expensive than S3. Amazon Pinpoint can place outbound calls, but is not able to perform interactive voice response functions. Amazon Comprehend Medical doesn't create call flow sequences.","links":[{"url":"https://aws.amazon.com/compliance/hipaa-eligible-services-reference/","title":"HIPAA Eligible Services Reference"},{"url":"https://aws.amazon.com/connect/","title":"Amazon Connect"},{"url":"https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/reduce-hospital-readmissions-ra.pdf?did=wp_card&trk=wp_card","title":"Reducing Hospital Readmissions"}],"answers":[{"id":"9585e2e5994ba5cdc2db33c22d9230cf","text":"Set up Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Connect to place the call to the patient.","correct":false},{"id":"a8c26f088ade23063b2bf5f202a74cce","text":"Have Apache Camel write the HL7 messages to Amazon Kineses Data Streams. Configure a Lambda function as a consumer of the stream to parse the HL7 message and write the information to Amazon DynamoDB. Trigger another Lambda function to pull the patient data from DynamoDB and send it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"c9bee4c5e4463b67e2726d3bed2ceed1","text":"Configure Apache Camel to write the HL7 messages to Amazon Kineses Data Firehose, which stores the patient information in Amazon S3. Trigger a Lambda function to read the patient information from S3 and write it to Amazon Comprehend Medical. Use Comprehend Medical's machine learning capabilities to create the appropriate call flow sequence and forward it to Amazon Pinpoint to place the outbound call.","correct":false},{"id":"8121c4e6e285161311cacf7b8031d5af","text":"Configure Apache Camel to write the HL7 messages to Amazon S3. Trigger a Lambda function to write each HL7 message object key to Amazon Simple Queue Service FIFO. Have another Lambda function read messages in sequence from the SQS queue and use the object key to retrieve and parse the HL7 messages. Use that same Lambda function to write patient information to Amazon Connect to place the call using an established call flow.","correct":true}]},{"id":"ef90df82-b36a-41f5-9294-904edfb5830e","domain":"awscsapro-domain1","question":"You are helping a client design their AWS network for the first time.  They have a fleet of servers that run a very precise and proprietary data analysis program.  It is highly dependent on keeping the system time across the servers in sync.  As a result, the company has invested in a high-precision stratum-0 atomic clock and network appliance which all servers sync to using NTP.  They would like any new AWS-based EC2 instances to also be in sync as close as possible to the on-prem atomic clock as well.  What is the most cost-effective, lowest maintenance way to design for this requirement?","explanation":"DHCP Option Sets provide a way to customize certain parameters that are issued to clients upon a DHCP request.  Setting the NTP server is one of those parameters.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_DHCP_Options.html","title":"DHCP Options Sets - Amazon Virtual Private Cloud"}],"answers":[{"id":"2b2197df0ae72d4d462432d7c2f65c98","text":"Create a bridged network tunnel from the on-prem time server to the VPCs on AWS.  Configure the VPC route tables to route NTP (UDP 123) over the tunnel.","correct":false},{"id":"40efb671ff85878c28d1fa99329f15c7","text":"Configure your Golden AMI to use Amazon Time Sync Server at 169.254.169.123 and require this AMI to be used.  Use AWS Config to periodically audit the NTP configuration of all AWS assets.","correct":false},{"id":"d5ea0e4895105bc1137c03c8bfb28ebe","text":"Configure a DHCP Option Set with the on-prem NTP server address and assign it to each VPC.  Ensure NTP (UDP port 123) is allowed between AWS and your on-prem network.","correct":true},{"id":"c950359dab3520d6c8c8f4e16a925860","text":"Deploy a third-party time server from the AWS Marketplace.  Configure it to sync from the on-prem time server.  Ensure NTP (UDP port 123) is allow inbound in the NACLs for the VPC containing the third-party server.","correct":false},{"id":"d94eaf4f8670e4d3f42fbae47250e3e6","text":"Create a dedicated host instance on AWS and place it within a transit VPC.  Configure the server to run NTP as a stratum-2 server.  Ensure NTP (UDP port 123) is allowed inbound and outbound in the Security Groups local to the stratum-2 server.","correct":false}]},{"id":"663fbd6a-87bd-4fa6-a0ea-428ba2de5b51","domain":"awscsapro-domain5","question":"You manage a relatively complex landscape across multiple AZs.  You notice that the incoming requests vary mostly depending on the time of day but also there is a more unpredictable component resulting in smaller spikes and valleys for your resources.  Fortunately, you manage this landscape via OpsWorks Stacks.  What options, if any, are available to you as part of the OpsWorks featureset.","explanation":"OpsWorks Stacks offers three types of scaling: 24/7 for instances that remain on all the time; time-based for instances that can be scheduled for a certain time of day and on certain days of the week; and load-based scaling which will add instances based on metrics.  All this can be configured from within the OpsWorks Stack console.","links":[{"url":"https://docs.aws.amazon.com/opsworks/latest/userguide/best-practices-autoscale.html","title":"Best Practices: Optimizing the Number of Application Servers - AWS OpsWorks"}],"answers":[{"id":"3622d494dceb973760a46dea038d1dc2","text":"If you need the ability to dynamically scale, you will need to use OpsWorks for Chef Automate.  OpsWorks Stacks does not support scaling.","correct":false},{"id":"b7ff5b06f51facca179494cb2bb00e55","text":"You can enabled CloudFormation Anticipated Scaling that uses past CloudWatch metrics and machine learning to automatically design a scaling policy optimized for the incoming request patterns.","correct":false},{"id":"216c997091da6e24174ad1b83d0be8b9","text":"You would define a baseline level of resources within the OpsWorks Stack Console to cover the average load.  But for the periodic load, that requires a scheduled auto-scaling policy.  Similarly, for the volatile spikes, you must use a stepped auto-scaling policy defined in an auto scaling group. ","correct":false},{"id":"75ab4de4ea42c1971b0ee09ae04ca591","text":"You would define a baseline level of resources and configure them for 24/7 instances.  Then you could define a time-based instances to cover certain times of day.  Finally, you could cover the volatile spikes with a load-based instances.  All this can be done within OpsWorks Stacks.","correct":true}]},{"id":"1eb605a0-e0bc-4666-9fe9-aa249901bcb5","domain":"awscsapro-domain3","question":"Your company currently runs SharePoint as it's internal collaboration platform. It's hosted in the corporate data center on VMware ESXi virtual machines. To reduce costs, IT leadership has decided not to renew its VMware license agreement for the coming year. They've also decided on an AWS cloud-first approach going forward, and have ordered an AWS Direct Connect for connectivity back to the corporate network. On-premises Active Directory handles SharePoint authentication, and will continue to do so in the future. You've been tasked with determining the best way to deliver SharePoint to the company's users after the VMware agreement expires. How will you architect the solution in a cost effective and operationally efficient way?","explanation":"Deploying SharePoint Web Front Ends in separate Availability Zones behind a Network Load Balancer, with SharePoint App Servers in those same subnets, provides a highly reliable solution. RDS SQL Server supports Always On Availability Groups. Since RDS is a managed service, operational efficiency is achieved. Amazon Workspaces also provides managed service benefits for remote desktops, and gives the company the opportunity to have users use lower cost hardware. It can all be authenticated through an AWS Managed AD trust relationship with the on-premises Active Directory. The Managed AD managed service provides better operational efficiency than creating Domain Controllers on EC2. Introducing VMware Cloud on AWS for the database layer results in more networking complexity, and is not necessary since RDS supports Always On clusters. Remote Desktop Gateways will require higher cost end-user hardware.","links":[{"url":"https://d1.awsstatic.com/VMwareCloudonAWS/SharePoint-Hybrid_Reference-Architecture.pdf?did=wp_card&trk=wp_card","title":"SharePoint Reference Architecture - AWS and VMware Cloud on AWS"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html","title":"Multi-AZ Deployments for Microsoft SQL Server"},{"url":"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_microsoft_ad.html","title":"AWS Managed Microsoft AD"}],"answers":[{"id":"56ace4242f33e5150f0a40b3627f9568","text":"Use an Application Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers and SQL Server instances on EC2 instances in the same subnets as the SharePoint Web Front Ends. Configure the SQL Server instances as Always On clusters. Use AWS Directory Service AD Connector for authentication from the on-premises Active Directory. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":false},{"id":"1ff58ba530d63513506d1a8a58cb91b9","text":"Deploy a Network Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Run an Amazon RDS SQL Server Always On cluster. Use AWS Directory Service Managed AD for authentication in a trust relationship with the on-premises Active Directory. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":true},{"id":"140b5814920d29ba818858f73c97577b","text":"Implement a Network Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers, SQL Server instances, and Active Directory Domain Controllers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Configure the SQL Server instances as Always On clusters. Join the Domain Controllers to the on-premises AD forest. Implement Amazon Workspaces to enable domain joined hosted Windows desktops.","correct":false},{"id":"c2b436befa546d5de6bf07d1d3eb3766","text":"Configure an Application Load Balancer to distribute traffic to SharePoint Web Front Ends on EC2 instances in two different Availability Zones. Place SharePoint App Servers on EC2 instances in the same subnets as the SharePoint Web Front Ends. Run SQL Server Always On clusters on VMware Cloud on AWS. Use AWS Directory Service AD Connector for authentication from the on-premises Active Directory. Implement Remote Desktop Gateways in each subnet to provide connectivity for Windows desktops.","correct":false}]},{"id":"79f2f5be-b591-44e6-957b-eb0383640d7d","domain":"awscsapro-domain5","question":"You have a standard SQS queue to receive messages from the frontend application. The backend application is JAVA based and the AWS SDK is used to get the messages from the queue for processing. The SQS queue is not busy most of the time. According to the backend application logs, there is a high number of empty ReceiveMessageResponse instances returned. You want to adjust the settings to minimize the number of empty responses and reduce the cost. How would you implement this? ","explanation":"Amazon SQS long polling is preferable to short polling in most of the cases. Long polling requests let the consumers receive messages as soon as they arrive in the queue. It can help to reduce the number of empty responses. In order to enable long polling, the attribute ReceiveMessageWaitTimeSeconds should be more than 0. Short polling is incorrect. Visibility timeout and delivery delay do not address the problem of empty responses.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html","title":"Amazon SQS short and long polling"}],"answers":[{"id":"203fa6faf2e6bf53939b43300ec6dac2","text":"Increase the default visibility timeout of the queue to reduce the possibilities that the messages become visible to consumers again. The application can also use the ChangeMessageVisibility API to specify a suitable timeout value.","correct":false},{"id":"a5cdcd2968c3566cbb7fc7bcd5fef01a","text":"Add a delivery delay in the SQS queue such as 1 minute. The delay helps to postpone the delivery of new messages to the queue for some time. When the JAVA application polls the messages from the queue, there will be a lower chance to get an empty response.","correct":false},{"id":"220352e5b3779c1f2030cfd4b391b19e","text":"Modify AWS SDK to get the messages in the SQS queue by short polling. The ReceiveMessage call from the consumer sets the WaitTimeSeconds attribute to 0. As a result, the empty responses are eliminated.","correct":false},{"id":"c3cbf51c591cb7fa17bd023ab814f95c","text":"Consume the messages in the SQS queue using long polling. Set the queue attribute ReceiveMessageWaitTimeSeconds to be more than 0. Amazon SQS will wait until there is an available message in a queue before sending a response.","correct":true}]},{"id":"c1333471-d052-4710-bcdb-facadc095d70","domain":"awscsapro-domain5","question":"You are setting up a corporate newswire service for a global news company.  The service consists of a REST API deployed on EC2 instances where customers can retrieve the latest news articles in real-time that happen to contain their company name.  This allows companies to monitor all news sources for stories where they are mentioned.  Because of the worldwide reach of the new site, you want to position servers around the globe.  You want to publish one subdomain name globally (api.domain.com) and have the requesters directed to the nearest region based on latency.  In each region, you want to be able to accommodate blue-green deployments without downtime as well.  What steps do you take?","explanation":"We want to use weighted routing records for local instances so we have the ability to adjust weights and shift traffic during blue-green deployments.  Latency-based routing would take care of funneling requests to the site with the lowest latency.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html","title":"How Health Checks Work in Complex Amazon Route 53 Configurations - Amazon  Route 53"}],"answers":[{"id":"b850f1c18972d022213271c5d673e07f","text":"First setup weighted routing records for the local instances in the region in Route 53.  Assign equal weights with all sharing the same regional subdomain name (us-east-2.api.domain.com).  Next, create latency alias records by creating multiple entries for api.domain.com--each pointing to the regional subdomains.","correct":true},{"id":"6e56101de6ed2ca97550d5025ddf559a","text":"Using Route 53, we first create the top-level api.domain.com with a geolocation policy.  We then create latency-based routing records for the instances in each region (us-east-2.api.domain.com).  Next, we configure the countries closest to each region in the geolocation policy to direct them to the regional records.","correct":false},{"id":"a60aaaf971cbd546c1ec57e08ea38274","text":"We would first create geo-spatial records for the local resources in each region (us-east-2.api.domain.com) and assign equal weights.  Next, we create latency-based routing records for the top level subdomain (api.domain.com) and direct those to the regional records as an alias.  We must also disable Health Check on the latency record to ensure the localized Health Check is used.","correct":false},{"id":"41e30b8e30cbd737ced85953d7e3e939","text":"Use CloudFormation to create a distribution of the website.  Create an alias record for the subdomain (api.domain.com) in Route 53 and assign it to the CloudFront distribution.  To ensure no lag in news retrieval, set the maximum TTL on the CloudFront distribution to 0.","correct":false}]}]}}}}
