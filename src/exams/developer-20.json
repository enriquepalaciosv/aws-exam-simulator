{"data":{"createNewExamAttempt":{"attempt":{"id":"6de3d173-c0d6-4680-a5c6-28de8891ea83"},"exam":{"id":"d69cabe2-8401-40e4-9c39-92445eee6e6b","title":"AWS Certified Developer - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"6f11e8ea-c824-427c-9cf9-7bebec4fe4b6","domain":"development","question":"Where should the appspec.yml be stored?","explanation":"The AppSpec file (appspec.yml) must always be in the root or your application source directory otherwise the deployment will not work. The .ebextensions folder is used to set custom environment variables in Elastic Beanstalk, not CodeDeploy.","links":[{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html","title":"CodeDeploy AppSpec File"}],"answers":[{"id":"0a2d3304fa88233c14e8c9ebffba0882","text":"In the .ebextentions folder","correct":false},{"id":"b007c30133faeba98cd0fbcedbf4fa6c","text":"In the root of your application source directory","correct":true},{"id":"8ee84dac450c4097f3cce035b8ede57d","text":"In the config directory in your application source directory","correct":false},{"id":"57f3232fcb44408176cbdb5c8b7e4b06","text":"In /opt","correct":false}]},{"id":"cd8831e1-d079-43e5-a45d-4d0770135faa","domain":"refactoring","question":"You are working on a flight booking application which runs on a number of EC2 instances. Recently one of your servers crashed which meant all of your users lost their sessions and had to log in again. Many of your customers have complained that they had to start their session again from the beginning because your application does not store session state anywhere. Which of the following could you use to persist session state and stop this from happening?","explanation":"Many applications store session state data in memory. However, this approach doesn't scale well. After the application grows beyond a single web server, the session state must be shared between servers. DynamoDB provides an effective and scalable solution for sharing session state across web servers.","links":[{"url":"https://docs.aws.amazon.com/sdk-for-net/v3/ndg/web-dynamodb-session.html","title":"Managing Session State with Amazon DynamoDB"}],"answers":[{"id":"4def2a084469f97f6372bfaf0823941b","text":"Glacier","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true},{"id":"248c1b0bcb74394beb4a08030c4a6847","text":"EBS","correct":false},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":false}]},{"id":"075f4eef-bd5d-49b2-ba18-f94cc15c377f","domain":"deployment","question":"Which of the following statements is correct?","explanation":"A primary key can either be a single-attribute partition key or a composite partition-sort key.","links":[{"url":"https://aws.amazon.com/dynamodb/faqs/#Getting_Started","title":"DynamoDB Query Functionality"}],"answers":[{"id":"d7f04a911ddfe5da2f4356ffbd52b25a","text":"In DynamoDB, a primary key can be a single-attribute partition key","correct":true},{"id":"a6bead04f6018113a477c5e6ebb0e82d","text":"In DynamoDB, a primary key can be composite partition/sort key.","correct":true},{"id":"20cdfac45e66ccd0a06ccea3d171de20","text":"In DynamoDB, a primary key can be a range of values.","correct":false},{"id":"e2240879e0c017447c6a1c59d0abd816","text":"In DynamoDB, a primary key must be a single-attribute","correct":false}]},{"id":"f956f10b-d615-4d75-a2a4-dd978268221b","domain":"deployment","question":"You are a developer running an application on AWS Elastic Beanstalk. You are implementing an application update and need to use a deployment policy. The requirements are to maintain full capacity, deploy five instances at once for the new version, and to terminate instances running the old version once the new instances are running successfully. How would you implement this deployment policy?","explanation":"To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances. All at Once deployment takes the instances in your environment out of service for a short time. Rolling deployment also takes a batch of servers out of service while deploying the new version in batches. Blue/Green deployments are for cases when you want to have two versions live simultaneously and be able to swap between the two versions.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html","title":"Deployment Policies and Settings"}],"answers":[{"id":"e693d0e841ddc0314ab81115baff0563","text":"Set the deployment policy as Rolling with Additional Batch. Set the batch size type as Fixed, and the batch size as 5.","correct":true},{"id":"da598c0ec61581331132eb0291fa30cd","text":"Set the deployment policy as Blue/Green. Set the timeout as 900, and the batch size as 5.","correct":false},{"id":"ee833ae676b93a46ca5aafc9ce391102","text":"Set the deployment policy as Rolling. Set the batch size as 5.","correct":false},{"id":"4a909b7335924524609c65e522fe581a","text":"Set the deployment policy as All at Once. Set the batch size type as Fixed, and the batch size as 5.","correct":false}]},{"id":"9548796c-789c-42ea-9e90-3da3a9252c1b","domain":"security","question":"Your Security team have recently reviewed the security standards across your entire AWS environment. They have identified that a number of EC2 instances in your development environment have read and write access to an S3 bucket containing highly confidential production data. You have been asked to help investigate and suggest a way to remedy this. Which of the following can you use to find out what is going on so that you can suggest a solution?","explanation":"With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies attached to IAM users, groups, or roles in your AWS account. You can test which actions are allowed or denied by the selected policies for specific resources.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html","title":"Testing IAM Policies with the IAM Policy Simulator"}],"answers":[{"id":"58f20da8b31f9e9da2e5a374608338ef","text":"Use the CLI or console to check the public access permissions of the S3 bucket","correct":false},{"id":"aac02aded2357ef60d7cc0b8f32df947","text":"Use CloudTrail and Athena to identify which role or policy is granting access","correct":false},{"id":"5bc514eb754ea13140481ed2afc68d45","text":"Use the VPC flow logs to identify which EC2 instances are attempting to access the bucket","correct":false},{"id":"17d9101067c49b2ec4b35087e24ce8eb","text":"Use the IAM Policy Simulator to identify which role or policy is granting access","correct":true}]},{"id":"6a4ef89c-dbe0-491d-88d4-11b09f0a272a","domain":"development","question":"What is NOT the best practice when deploying production applications using Elastic Beanstalk?","explanation":"It is a good practice to decouple an Amazon RDS instance from an Elastic Beanstalk environment, especially in production environment.  Launching an RDS database as part of an Elastic Beanstalk environment may be suitable for development or PoC environments. However, in general, it isnâ€™t ideal as this means that termination of the Elastic Beanstalk environment will result in termination of the database as well. To protect important data from potential data loss, Amazon RDS database should be launched outside of the Elastic Beanstalk environment. With this approach, we decouple the life-cycle of the database from the life-cycle of the Elastic Beanstalk environment. This method also allows us to connect multiple environments to the same RDS instance. This may be useful for performing advanced deployment scenarios such as blue-green deployments. Storing RDS connection string in an encrypted, secured, and controlled S3 bucket and using Elastic Beanstalk configuration files is a valid method that can be used to securely store this data outside of the application code. Protecting the RDS databases from accidental deletion by enabling Delete Protection is always a good practice.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html","title":"Using Elastic Beanstalk with Amazon RDS"},{"url":"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/","title":"How do I decouple an Amazon RDS instance from an Elastic Beanstalk environment without downtime, database sync issues, or data loss?"},{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/rds-external-credentials.html","title":"Storing the connection string in Amazon S3"}],"answers":[{"id":"9e19947230060c5fdc4610175989a39e","text":"Amazon RDS Connection String should be stored in a controlled S3 bucket.","correct":false},{"id":"24e8e8250f45def68ec55670ac2390e7","text":"Amazon RDS Delete Protection should be enabled.","correct":false},{"id":"1a265bcccdb4ad642b0a09989c931196","text":"Amazon RDS database should be launched outside of the Elastic Beanstalk environment as that provides more flexibility.","correct":false},{"id":"f8ac7ea397ed3bf0d0dcf3a3d5123521","text":"Amazon RDS databases should be included in the Elastic Beanstalk environment as that maintains the same life cycle for all components of the environment.","correct":true}]},{"id":"098e4897-54dd-493d-ae35-d28374c03576","domain":"security","question":"You application can be accessed using multiple devices, for example, laptop, tablet, iPhone or Android devices. You would like to be able to identify and track when your users access your site using different devices. Which of the following AWS technologies can enable you to do this?","explanation":"Cognito enables developers to remember the devices on which end-users sign in to their application. You can see the remembered devices and associated metadata through the console. In addition, you can build custom functionality using the notion of remembered devices. For example, with a content distribution application (e.g., video streaming), you can limit the number of devices from which an end-user can stream their content.","links":[{"url":"https://aws.amazon.com/blogs/mobile/tracking-and-remembering-devices-using-amazon-cognito-your-user-pools/","title":"Tracking and Remembering Devices Using Amazon Cognito"}],"answers":[{"id":"6cbb574c9488bb59fdd64fa8f509fce8","text":"Use Cognito","correct":true},{"id":"5b46051451752459a684abff24663b39","text":"Store a unique session ID in ElastiCache","correct":false},{"id":"0d6127433a099d6824bfd30c14ad4096","text":"Create a unique user ID and associate it with the device metadata","correct":false},{"id":"8204565cad3d39f0897a0ce85182016f","text":"Store a unique session ID in DynamoDB","correct":false},{"id":"95e34ddc7b57bc86318fc7a45f0d2e2d","text":"Use a Lambda function to store a unique device ID in DynamoDB and associate it with the user session ID","correct":false}]},{"id":"68d79e4d-f827-496d-9fc3-606615dd6fe5","domain":"security","question":"When using Web Identity Federation and Cognito to allow a user to access an AWS service (such as an S3 bucket), which of the following is the correct order of steps?","explanation":"A user authenticates with Facebook first. They are then given an ID token by Facebook. An API call, AssumeRoleWithWebIdentity, is then used in conjunction with the ID token. A user is then granted temporary security credentials.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_manual.html","title":"Using Web Identity Federation APIs"}],"answers":[{"id":"13bdd19ca71c9978a4642bd95fcb92c5","text":"A user authenticates with Facebook first. They are then given an ID token by Facebook. An API call, AssumeRoleWithWebIdentity, is then used in conjunction with the ID token. A user is then granted temporary security credentials.","correct":true},{"id":"811872e18604a3d8118cc1c9381bf702","text":"Users cannot use Facebook credentials to access the AWS platform.","correct":false},{"id":"b66b1e5f0bf94eca60bce37aa0702f6e","text":"A user logs in to the AWS platform using their Facebook credentials. AWS authenticates with Facebook to check the credentials. Temporary Security Access is granted to AWS.","correct":false},{"id":"e95a8dc9c210c0c1e6e36f8215ab4978","text":"A user makes the AssumeRoleWithWebIdentity API Call. The user is then redirected to Facebook to authenticate. Once authenticated, the user is given an ID token. The user is then granted temporary access to the AWS platform.","correct":false}]},{"id":"e466b1d0-1858-40d8-aeca-e48e5e44ae2d","domain":"deployment","question":"Your application is using Kinesis to ingest data from a number of environmental sensors which continuously monitor for pollution within a 1 mile radius of a local primary school. An EC2 instance consumes the data from the stream using the Kinesis Client Library. You have recently increased the number of shards in your stream to 6 and your project manager is now suggesting that you need to add at least 6 additional EC2 instances to cope with the new shards. What do you recommend?","explanation":"Re-sharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. You should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances. When re-sharding increases the number of shards in the stream, the corresponding increase in the number of record processors increases the load on the EC2 instances that are hosting them. If the instances are part of an Auto Scaling group, and the load increases sufficiently, the Auto Scaling group adds more instances to handle the increased load.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html","title":"Kinesis Data Streams Terminology"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html","title":"Resharding, Scaling, and Parallel Processing"}],"answers":[{"id":"7561e4f93183899bc8757da9db13ae60","text":"One worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances","correct":true},{"id":"c11271a6921d20363c624ddde1790781","text":"You should decrease the number of shards to match the number of consumer instances","correct":false},{"id":"6c565b117fa64cb3b8ddaf38c670efed","text":"You should increase the number of instances to match the number of shards","correct":false},{"id":"1c8f531c39885147b4935b0e64880b97","text":"The number of instances should be greater than number of shards","correct":false}]},{"id":"007a648d-faa9-4464-9816-0b646386047f","domain":"mon-trb","question":"A three-tier application consists of a presentation and application tier deployed on EC2 instances in a public VPC subnet, and a data tier hosted on an RDS databases in a private VPC subnet.  When attempting to establish a connection to the RDS database, the application times out.  What could be the source of this problem?","explanation":"A VPC Network Security Groups are a network security capability providing firewall functionality.  They control inbound and outbound traffic on EC2 or RDS instances. NSG rules applied on the RDS database must be configured to allow inbound traffic from the NSG on the EC2 instances on the database port. If the NSG's on the RDS database are not configured correctly, any connection attempting to access the RDS instance will time out as the database will be unreachable. If database credentials were incorrect, the application would not time out. A connection to the RDS instance would be established, an the database would return an error code.  The question specifically states that there is an issue establishing a connection. VPC peering is used to establish a secure connection between two VPCâ€™s, and is not suitable in this scenario. Similarly, Internet Gateway is used in public subnets to route outbound traffic to the internet and is not relevant to the situation.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html","title":"Controlling Access with Security Groups"}],"answers":[{"id":"caf6ad7f31cbd20a32d3eea156a6cffb","text":"The public subnet does not have Internet Gateway configured.","correct":false},{"id":"2954caaf920be59b173a0e00624c47f0","text":"VPC Peering is not configured properly.","correct":false},{"id":"b7adf16356fbe2a22cd0c74e0b333b26","text":"Database NSG is not configured to allow traffic from EC2 instances.","correct":true},{"id":"77a29c8aaf1cae7a01a6aa1960c59878","text":"Database credentials are incorrect.","correct":false}]},{"id":"21efb969-377d-45b1-a907-8994e94aa26b","domain":"development","question":"An organization is considering making use of AWS Fargate in their next project. Which of the following statements best describes AWS Fargate?","explanation":"AWS Fargate is a compute engine for Amazon ECS that allows you to run containers without having to manage servers or clusters.","links":[{"url":"https://aws.amazon.com/fargate/","title":"AWS Fargate"}],"answers":[{"id":"940863780b5c57669b92b1fc551543ca","text":"Automates management of the control plane within a Kubernetes cluster.","correct":false},{"id":"9996f92567fdefae4123ba718e107bbf","text":"Deploys Compute logic to an AWS Edge location.","correct":false},{"id":"fbe3eadaa96811fdb58081b395eb6164","text":"Stores Docker containers within a registry, making them available for use by AWS ECS.","correct":false},{"id":"5c10facdecd8cb3ff129afb77d315739","text":"Deploys Docker containers within AWS, without having to manage underlying EC2 instances.","correct":true}]},{"id":"bd31be6d-dcf3-4103-b281-72d44c552186","domain":"refactoring","question":"You are working on a social media application which allows users to share BBQ recipes and photos. You would like to schedule a Lambda function to run every 10 minutes which checks for the latest posts and sends a notification including an image thumbnail to users who have previously engaged with posts from the same user. How can you configure your function to automatically run at 10 minute intervals?","explanation":"You can direct AWS Lambda to execute a function on a regular schedule using CloudWatch Events. You can specify a fixed rate - for example, execute a Lambda function every hour or 15 minutes, or you can specify a cron expression.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html","title":"Using AWS Lambda with Amazon CloudWatch Events"}],"answers":[{"id":"2d428d9cc2b8ba9e3f0fb6f7b7c5d585","text":"Use AWS SWF to schedule the function","correct":false},{"id":"34d04b0eb9dd5f8f69d5c60eb65f6bc8","text":"Use Lambda with cron to schedule the function","correct":false},{"id":"2255149fa723010e237589bd73a85d2d","text":"Use EC2 with cron to schedule the function","correct":false},{"id":"547b2838c15c420120f6d664b99d6dbb","text":"Use CloudWatch Events to schedule the function","correct":true}]},{"id":"70e8f135-19fb-4ad6-82bc-8ec0ed899d83","domain":"security","question":"You are developing a video streaming application which users can access using multiple devices, for example, laptop, tablet and cell phone. You would like to be able to track usage across the different devices and limit the number of devices from which a user can stream content. Which of the following AWS technologies could you use to achieve this?","explanation":"Cognito enables developers to remember the devices on which end-users sign in to their application. You can see the remembered devices and associated metadata through the console. In addition, you can build custom functionality using the notion of remembered devices. For example, with a content distribution application (e.g., video streaming), you can limit the number of devices from which an end-user can stream their content.","links":[{"url":"https://aws.amazon.com/blogs/mobile/tracking-and-remembering-devices-using-amazon-cognito-your-user-pools/","title":"Tracking and Remembering Devices Using Amazon Cognito"}],"answers":[{"id":"425684337e67b0a12f438c06f1c5818d","text":"Use MFA on the device","correct":false},{"id":"3a73d44e5a5b2804c4cc5dd0b6c0bfe5","text":"Use S3 to store metadata about the device and link it to session state held in DynamoDB","correct":false},{"id":"3cff09984a222738fa34702f699ab961","text":"Use a Lambda function to store session state and device type in DynamoDB","correct":false},{"id":"5348de0d4df2bd40fa189bd4e5ff1b6c","text":"Store device metadata linked to session state in ElastiCache","correct":false},{"id":"6cbb574c9488bb59fdd64fa8f509fce8","text":"Use Cognito","correct":true}]},{"id":"422c80de-8e21-4706-ab03-ce11c4cfa083","domain":"deployment","question":"You are developing a completely serverless application using Lambda and API Gateway. You need a place to persist data as key-value pairs and your application will need low latency access to the data. Which of the following is the best option for storing this data?","explanation":"DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. RDS does not store data as key-value pairs, a JSON document in S3 is not an efficient way to store the data and using an EC2 instance would not be serverless and would not be scalable.","links":[{"url":"https://aws.amazon.com/dynamodb/","title":"DynamoDB Overview"}],"answers":[{"id":"023289eeb117bb428a4288ca492fd5e8","text":"Store the data in a DynamoDB table","correct":true},{"id":"243a3f875904542a9e0117f28650b59f","text":"Store the data in an RDS database","correct":false},{"id":"5d9c9d9ca04e432986c67415ccfdb791","text":"Store the data in JSON format on an EC2 instance","correct":false},{"id":"866706615c3cfcbd5b3dfc3bbd116f85","text":"Store the data in JSON format in an S3 bucket","correct":false}]},{"id":"4834c926-4c2e-11ea-b77f-2e728ce88125","domain":"development","question":"You want to create a continuous delivery pipeline with a build tool recommended by your Project Manager. However, you anticipate your build project to be large and complex. Which of the following AWS services will enable you to orchestrate complex pipelines?","explanation":"True to its naming, AWS CodePipeline is what you must use with the recommended build tool to make it easier to create your delivery pipelines. CodeDeploy is used for automatically deploying code, not creating pipelines. CodeBuild is for building and testing code. Although Jenkins is a build tool, it is not an AWS service.","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html","title":"What Is AWS CodePipeline?"},{"url":"https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf","title":"Practicing Continuous Integration and Continuous Delivery on AWS"}],"answers":[{"id":"7be6a0f173a965818e92c1df46c6626e","text":"AWS CodeDeploy","correct":false},{"id":"ebf976dafc560e096501509079ba441c","text":"AWS CodePipeline","correct":true},{"id":"ff02d3f07f94fff5aa3bdd045211b9e2","text":"AWS CodeBuild","correct":false},{"id":"2e54334c0a5ce2e3e5a5845df3ab3ada","text":"Jenkins","correct":false}]},{"id":"4b9c267c-f41d-4325-919e-7863e0abb6f3","domain":"development","question":"A three-tier web application is deployed using CloudFormation template. How can the CloudFormation developer ensure that the database resource is saved for backup purposes upon stack deletion?","explanation":"The DeletionPolicy attribute can be used to preserve a specific resource when its stack is deleted. The DeletionPolicy Retain option can be used to ensure AWS CloudFormation keeps the resource without deleting the resource.  The Stack Termination Protection feature enables protection against accidental deletion of an entire stack, not preservation of a specific resource. Similarly, the 'cloudformation:DeleteStack' Action applies to entire stack(s).","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html","title":"DeletionPolicy Attribute"}],"answers":[{"id":"eee064b70f79422e8511f18b251253ef","text":"Create IAM Policy with Effect of Deny for 'cloudformation:DeleteStack' Action.","correct":false},{"id":"6983aeb4c0d42bb293c8ec93966aedbb","text":"Set the DeletionProtection to True in the CloudFormation template.","correct":false},{"id":"558cc27970e2b64a29bdc85f381b8cb9","text":"Set the DeletionPolicy to Retain in the CloudFormation template.","correct":true},{"id":"8093dacb1a766d0f91fa60e48baa87ed","text":"Set Stack Termination Protection to Enable.","correct":false}]},{"id":"8856df48-5866-4ee3-a5a7-2033444e21eb","domain":"security","question":"You have provisioned an RDS database and then deployed your application servers using Elastic Beanstalk. You now need to connect your application servers to the database. What should you do?","explanation":"As you are connecting to a database that was not created within your Elastic Beanstalk environment, you will need to create the Security Group yourself and also provide connection string and credentials to allow your application servers to connect to the database","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html#rds-external-defaultvpc","title":"Elastic Beanstalk And RDS"}],"answers":[{"id":"1aa1798c4c20c22832e9a949af74ada3","text":"Configure Elastic Beanstalk to install a database client on your application servers","correct":false},{"id":"26b897f7474a08158506b32e7c566323","text":"Provide the ip address of the RDS instance to Elastic Beanstalk","correct":false},{"id":"50021ae41e50f1b10069ebcccab3c0ef","text":"Provide the database connection information to your application","correct":true},{"id":"24e204c62e09f215959e144cbb8611d4","text":"Configure a security group allowing access to the database and add it to your environments auto-scaling group","correct":true}]},{"id":"313867da-7161-4083-9745-77950b6208dd","domain":"development","question":"You are using CodeBuild to create a Docker image and add the image to your Elastic Container Registry. Which of the following commands should you include in the buildspec.yml?","explanation":"Use the docker push command to add your image to your Elastic Container Registry","links":[{"url":"https://aws.amazon.com/blogs/devops/build-a-continuous-delivery-pipeline-for-your-container-images-with-amazon-ecr-as-source/","title":"Build a Continuous Delivery Pipeline for Your Container Images with Amazon ECR as Source"}],"answers":[{"id":"c1520af238ed7b988b389f732c0d6287","text":"docker build -t $REPOSITORY_URI:latest .","correct":true},{"id":"ec1f85b16af01c1d7ec994f6ba6efa32","text":"docker push $REPOSITORY_URI:latest","correct":true},{"id":"585a787c56d351517a27b9d64d3db8ae","text":"docker add $REPOSITORY_URI:latest","correct":false},{"id":"87c7975f8fe350f6f133c1f3d3b6b9f2","text":"aws codebuild docker -t $REPOSITORY_URI:latest .","correct":false},{"id":"d2d39a1ed4490154f045931eee5d18e4","text":"aws ecr push $REPOSITORY_URI:latest","correct":false}]},{"id":"7d99fa38-f31e-4527-95a6-a88611f7731c","domain":"mon-trb","question":"A developer deployed a serverless application consisting of an API Gateway and Lambda function using CloudFormation. Testing of the application resulted in a 500 status code and 'Execution failed due to configuration' error. What is a possible cause of the error?","explanation":"When you build an API Gateway API with standard Lambda integration using the API Gateway console, the console automatically adds the required permissions. However, when you set up a stage variable to call a Lambda function through your API, you must manually add these permissions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html","title":"Invoke"},{"url":"https://docs.aws.amazon.com/apigateway/api-reference/handling-errors/#api-error-codes","title":"Error Codes (Client and Server Errors)"}],"answers":[{"id":"eef7bf395052a4cb6bf0286cc9578779","text":"API Gateway is not authorized to invoke the Lambda function.","correct":false},{"id":"dddca3f487612188440bbfe93d1ef829","text":"IAM policy is restricting the user from invoking the API Gateway API endpoint.","correct":false},{"id":"8fa26152f6981113cb014274ea900af1","text":"Too many API Gateway requests were created exceeding the allowed limit.","correct":false},{"id":"d69e5b40405e5538753a24c15bb80a0e","text":"The Lambda function's resource-based policy doesn't include permission for your API to invoke the function.","correct":true}]},{"id":"1710c298-c975-4762-8948-da98b2900d8a","domain":"development","question":"You are developing a web application which has been deployed using Lambda. Today you updated the code and uploaded the new version of your code to the Lambda console. Your test team have begun testing but have reported today that the application seems to still be using the original code. What could be the reason for this?","explanation":"The problem is that the application is referencing the function using an alias pointing to a previous version of the code. When you use versioning in AWS Lambda, you can publish one or more versions of your function. So that you can use different variations of your Lambda function in your development workflow, such as development, beta, and production. Lambda also supports creating aliases for each of your Lambda function versions. Conceptually, an AWS Lambda alias is a pointer to a specific Lambda function version. You can update aliases to point to different versions of functions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda Function Versioning and Aliases"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/how-to-manage-versioning.html","title":"Lambda Versioning"}],"answers":[{"id":"ce472c670d174e72811d57156684bb1e","text":"You forgot to publish the version","correct":false},{"id":"078229fa3cd7070c357a105fd9f1584d","text":"Your application is referencing the function using a qualified ARN","correct":false},{"id":"b099c8a69bf8eac2d39f02ac07d53702","text":"Your application is referencing the function using an alias which points to a previous version of the code","correct":true},{"id":"2fcd244fd0d8e7833b6bf94d70f01295","text":"Your application is referencing the function using an unqualified ARN","correct":false},{"id":"b67108883bf2590e9f79b9ab834a84cd","text":"Your application is referencing the function using $LATEST","correct":false}]},{"id":"fe61a353-eb18-40dc-9e65-30f7106d3e6e","domain":"mon-trb","question":"Your application is running on EC2 and on Linux virtual machines in your own data center. You would like to configure your application to send data to X-Ray for troubleshooting and performance analysis. Which of the following steps will you need to complete?","explanation":"You need the X-Ray SDK and the X-Ray daemon on your EC2 instances and on-premises systems, you then need to instrument your application to send the required data to X-Ray","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html","title":"X-Ray Developer Guide"}],"answers":[{"id":"6ffb986aa2b49eb856b38668f9a44fc2","text":"Install the AWS CLI, then instrument your application to send data to X-Ray.","correct":false},{"id":"ff31ac1442cb9f28ad8f65a358816a25","text":"Install the AWS SDK and the X-Ray CLI, then instrument your application to send data to X-Ray.","correct":false},{"id":"da1eaa5b340d64b20d5b526dc0de4b85","text":"Install the X-Ray SDK and the X-Ray daemon, then instrument your application to send data to X-Ray.","correct":true},{"id":"998a1f353d1923ec24b66b6ae427d343","text":"Install the X-Ray daemon, then instrument your application to send data to X-Ray.","correct":false}]},{"id":"46c2faa3-c022-4c5e-bb6b-fe84f70b3dc4","domain":"deployment","question":"Which of the following AWS services enables you to capture a time-ordered sequence of any modifications which happened to the items in your DynamoDB table over the past 24 hours?","explanation":"DynamoDB Streams captures a time-ordered sequence of item-level modifications in a DynamoDB table and durably stores the information for up to 24 hours.","links":[{"url":"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/","title":"DynamoDB Streams Use Cases"}],"answers":[{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"1f60690ba7c488a02416a7bf195f900b","text":"DynamoDB Streams","correct":true},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"52d5592ad24a2cda379ce38e9c218d65","text":"DynamoDB TTL","correct":false}]},{"id":"5cf3b6d4-9a0e-4602-8dab-26e687207049","domain":"development","question":"Which of the following platforms are supported in ElasticBeanstalk?","explanation":"Elastic beanstalk supports common platforms like including Tomcat, Passenger, Puma and Docker","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html","title":"Elastic Beanstalk Supported Platforms"}],"answers":[{"id":"bf2528a296adb62d041a7519aa77f248","text":"Passenger","correct":true},{"id":"c5fd214cdd0d2b3b4272e73b022ba5c2","text":"Docker","correct":true},{"id":"7345f7045e4668138112c100f25517a4","text":"JBoss","correct":false},{"id":"8f72e28063c30c7468fb6af4653f4f9c","text":"Tomcat","correct":true}]},{"id":"87896ba2-d675-4fce-97b9-f144f05d42f1","domain":"security","question":"You are building an S3 hosted website and your website is accessing javascript and image files located in another S3 bucket. How can you enable this? ","explanation":"Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html","title":"Cross-Origin Resource Sharing (CORS)"}],"answers":[{"id":"0e0c1a7e0b6fe582226b82afc8eec89b","text":"S3 bucket policies","correct":false},{"id":"39e38061c46bfab6a44c6c8b5482763f","text":"Cross Origin Resource Sharing (CORS)","correct":true},{"id":"bef5f130edd0f036b2ca659d3295d5c7","text":"IAM roles","correct":false},{"id":"be48e3ddc7b57744c693982774a47dad","text":"S3 ACLs","correct":false}]},{"id":"08861bdf-6813-43f1-9def-4255492b4533","domain":"mon-trb","question":"Your application is using SQS to send and receive messages, your application needs to receive the messages as soon as they arrive and you need to ensure the architecture is as cost efficient as possible. Which of the following approaches will optimise the cost and performance of the application?","explanation":"In almost all cases, Amazon SQS long polling is preferable to short polling and results in higher performance and reduced cost in the majority of use cases.","links":[{"url":"https://aws.amazon.com/sqs/faqs/","title":"SQS FAQs"}],"answers":[{"id":"33e24a8e438593d0ed4994ce9ea68ccd","text":"Enable Long Polling","correct":true},{"id":"8757c7e091bc29e7b9225334f00531ac","text":"Reduce the total number of message queues","correct":false},{"id":"9ef993bf7e61fa02b5f6ac0fc8da6b18","text":"Enable Short Polling","correct":false},{"id":"64dab6674843dc03406bdc90b1c5b21d","text":"Lower the message Visibility Timeout","correct":false}]},{"id":"4750f3bd-de92-4efb-ad08-06c9ea71eccb","domain":"refactoring","question":"Kinesis allows consumer applications to consume records in which order?","explanation":"Kinesis gives you the ability to consume records according to a sequence number applied when data is written to the Kinesis shard","links":[{"url":"https://aws.amazon.com/kinesis/data-streams/faqs/","title":"Kinesis FAQ"}],"answers":[{"id":"b1e05f0db70da1b8dee8592d942ed2e1","text":"According to the timestamp assigned when the record is written to the stream","correct":false},{"id":"689f6f887e0c13fb07b437de23303cac","text":"Last In First Out","correct":false},{"id":"cb1ab21304b72197113dbe18c491e9c2","text":"According a sequence number assigned when the record is written to the stream","correct":true},{"id":"e71e8f5a50819b4773c68991d0c9f602","text":"Records are processed in no particular order","correct":false}]},{"id":"14015b82-f869-4335-88ac-d2edf4da7f0b","domain":"security","question":"Your main application currently stores its credentials as a text file on an EC2 server.  Your Manager has informed you that this is an insecure practice and has told you to store these credentials in an AWS managed service instead.  AWS Systems Manager Parameter Store and AWS Secrets Manager can be used for the secure storage of credentials.  Of the below features, which apply to both Secrets Manager and Parameter Store?","explanation":"Many aspects of Parameter Store and Secrets Manager appear very similar, but Secrets Store charges you for storing each secret and also provides a secret rotation service whereas Parameter Store does not.  Therefore these are the only two answers related to both services.","links":[{"url":"https://aws.amazon.com/systems-manager/faq/#Parameter_Store","title":"AWS Systems Manager FAQs"},{"url":"https://aws.amazon.com/secrets-manager/faqs/","title":"AWS Secrets Manager FAQs"}],"answers":[{"id":"c954aba0b83727b5f6892234e837686d","text":"Integrated with Identity and Access Management","correct":true},{"id":"a6c8669080382c473c6c3331a4d3e832","text":"Supports encryption at rest using customer-owned KMS keys","correct":true},{"id":"12697ea16ebbdb808c191988acdbecba","text":"Manages rotation and lifecycle of credentials","correct":false},{"id":"da5f35495ba99b6d624c0dadf1bd7626","text":"Available at no additional charge providing you store less than 10,000 credentials","correct":false},{"id":"5a6c9e66d676a1b933ff0f18e23874d1","text":"Can store credentials in hierarchical form","correct":true}]},{"id":"235fa5cd-42b5-4017-af9a-e62d0503651a","domain":"security","question":"You are working on a Lambda function which needs to access data in RDS, which of the below are valid approaches for securely storing the encrypted database connection strings and other secrets which your function needs to use?","explanation":"Parameter Store provides secure storage for configuration data, connection strings, passwords and secrets management. None of the other options are secure.","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html","title":"AWS Systems Manager Parameter Store"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/tutorial-env_console.html","title":"Create a Lambda Function Using Environment Variables To Store Sensitive Information"}],"answers":[{"id":"913d4a9f910c7f6b836d623b39131480","text":"Use Systems Manager Parameter Store","correct":true},{"id":"53708b71a9f9d0c3994b3bd1d470b254","text":"Use DynamoDB to store the encrypted connection string and secrets","correct":false},{"id":"f2523ec5429fa4e1be124d650a69a0f5","text":"Store the encrypted connection string and other secrets in S3","correct":false},{"id":"ca7c47bd30833fb42e5bc78f6c7583be","text":"Use Lambda Environment Variables","correct":true}]},{"id":"69332ca4-69c7-4b5c-b3fd-3dcc1b2154c6","domain":"security","question":"You would like to test the effect of a new IAM policy which you are planning to attach to a group of developers in your team. Which of the following can you use to check that the policy works as expected?","explanation":"With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies that are attached to IAM users, groups, or roles in your AWS account. You can test which actions are allowed or denied by the selected policies for specific resources.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html","title":"Testing IAM Policies with the IAM Policy Simulator"}],"answers":[{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"b6632fa69795d5fabc908fe75210b177","text":"IAM Policy Simulator","correct":true},{"id":"543096643aa6d28d9fac278e9257783d","text":"Amazon Inspector","correct":false}]},{"id":"6593608d-3801-4b5d-8947-89efa395825a","domain":"refactoring","question":"You need to monitor application-specific events every 10 seconds. How can you configure this?","explanation":"You need to configure a custom metric to handle application specific events and if you want to monitor at 10 second intervals, you need to use high-resolution metrics. Detailed monitoring reports metrics at 1 minute intervals.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"CloudWatch Custom Metrics"}],"answers":[{"id":"8625460160031630c52a466b7a91f16b","text":"configure a high-resolution custom metric in CloudWatch","correct":true},{"id":"da902bf148db5983868bf3383162183a","text":"Select high-resolution metrics in CloudWatch","correct":false},{"id":"f3e15ffbefd8415dda26321a2912dca1","text":"Select detailed monitoring in CloudWatch","correct":false},{"id":"886f04b8880657a2d07a12c6355639a7","text":"Configure the application to send notifications using SNS every 10 seconds","correct":false}]},{"id":"97b2f9cc-962b-4c75-82c3-a44815644776","domain":"mon-trb","question":"You are developing a new application using Lambda, API Gateway, S3 and DynamoDB. You would like to record information about incoming and outgoing HTTP requests as well as latency incurred by each component. You have multiple versions of the application to cater for your Development, UAT, Performance Test and Production environments. What is the most efficient way to collect this information and group it according to which environment it relates to?","explanation":"AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, micro-services, databases and HTTP web APIs. When you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segment document as annotations and metadata. Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations","title":"X-Ray Annotations and Metadata"},{"url":"https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html","title":"Searching for Traces in the AWS X-Ray Console with Filter Expressions"}],"answers":[{"id":"8f0f2ab2fa58b6ddef1f41eb2fe56872","text":"Use CloudWatch to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false},{"id":"c57540f9c7ee62b718518b73d8ea536c","text":"Use X-Ray to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":true},{"id":"7596c1ed59077a183e47b002a53bb84a","text":"Use CloudFormation to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false},{"id":"d2ae15c70b6c5a546c571203b1a18474","text":"Use CloudTrail to view the information, configure annotations to indicate which environment the traces relate. Group the data according to environment.","correct":false}]},{"id":"0e4eaa2b-a121-40aa-ab67-94f768660e76","domain":"deployment","question":"Your application regularly sends a number of large messages exceeding 1GB in size to SQS. You would like to store these messages in S3 rather than in SQS. Which of the following can you use to manage large SQS messages stored in S3?","explanation":"You can use Amazon S3 and the Amazon SQS Extended Client Library for Java to manage Amazon SQS messages stored in S3. This includes specifying when messages should be stored in S3, referencing message objects stored in S3, getting them, and deleting them.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html","title":"Managing Large Amazon SQS Messages Using Amazon S3"}],"answers":[{"id":"a69f5cdbab8efbb9ff8de1f9db3d29fb","text":"SQS Extended HTTP API for Java","correct":false},{"id":"9cda9035f194932d72b2b650415eccb3","text":"SQS Extended Client Library for Java","correct":true},{"id":"a7be1fa1b34770b56b6b0cf5a4ff2842","text":"SQS Extended API for Java","correct":false},{"id":"6c93ec556a138f4181f32250f309a0a8","text":"SQS Extended CLI for Java","correct":false}]},{"id":"16387bfd-386c-4d6e-a808-8f42694fb73d","domain":"deployment","question":"Your application is using Kinesis to ingest click-stream data relating to your products from a variety of social media sites. Your company has been trending this quarter because a high profile movie star has recently signed a contract to endorse your products. As a result, the amount of data flowing through Kinesis has increased, causing you to increase the number of shards in your stream from 4 to 6. The application consuming the data runs on a single EC2 instance in us-east-1a with a second instance in us-east-1b which is used as a cold standby in case the primary instance fails. How many consumer instances will you now need in total to cope with the increased number of shards?","explanation":"Re-sharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. You should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances. When re-sharding increases the number of shards in the stream, the corresponding increase in the number of record processors increases the load on the EC2 instances that are hosting them. If the instances are part of an Auto Scaling group, and the load increases sufficiently, the Auto Scaling group adds more instances to handle the increased load.","links":[{"url":"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html","title":"Kinesis Data Streams Terminology"},{"url":"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html","title":"Resharding, Scaling, and Parallel Processing"}],"answers":[{"id":"35e98ea97449addda5a6f1eecf072e4a","text":"6 instances in us-east-1a and 1 instance in us-east-1b","correct":false},{"id":"205a5d33fbfbd8a66d4bbd686e7455b9","text":"3 instances in us-east-1a and 3 instances in us-east-1b","correct":false},{"id":"834a08c5c4b013c0a9e06105fc8c873a","text":"6 instances in us-east-1a and 6 instances in us-east-1b","correct":false},{"id":"df38b78549933ce6cdce41ce65a061ba","text":"1 instance in us-east-1a and 1 instance in us-east-1b","correct":true}]},{"id":"1afa6661-6523-49f7-912c-46b740714117","domain":"deployment","question":"You are deploying a number of Lambda functions using CloudFormation. Which section of the CloudFormation template should you use to define your Lambda resources?","explanation":"Use the Resources section of the CloudFormation template to define the resources you are going to deploy, e.g. EC2 instances, S3 buckets, IAM roles, Lambda functions etc.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"CloudFormation Template Anatomy"},{"url":"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-lambda-state-machine-cloudformation.html#lambda-state-machine-cfn-step-2","title":"Example CloudFormation Template"}],"answers":[{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":false},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":true},{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false}]},{"id":"d069e7a2-ff20-46e7-9fd8-64d3fc2e0c8d","domain":"development","question":"You have a load balancer configuration that you use for most of your CloudFormation stacks. This load balancer always sits in front of your application running on EC2 as it has the important function of forwarding HTTPS requests on port 443 to HTTP requests on port 80 on the instance. As demand for the application grows you need to reuse this load balancer configuration in multiple other deployments of the application and you need to use CloudFormation to do this in an automated way. What is the most efficient way to deploy the load balancer configuration?","explanation":"Nested stacks are stacks created as part of other stacks. You create a nested stack within another stack by using the AWS::CloudFormation::Stack resource. For example, assume that you have a load balancer configuration that you use for most of your stacks. Instead of copying and pasting the same configurations into your templates, you can create a dedicated template for the load balancer. Then, you just use the resource to reference that template from within other templates. Lambda would not be able to deploy infrastructure resources as efficiently as CloudFormation nested stacks. AWS CloudFormation provides two methods for updating stacks: direct update or creating and executing change sets. When you directly update a stack, you submit changes and AWS CloudFormation immediately deploys them. Use direct updates when you want to quickly deploy your updates. With change sets, you can preview the changes AWS CloudFormation will make to your stack, and then decide whether to apply those changes.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html","title":"Working with Nested Stacks"}],"answers":[{"id":"97cc8d8d6d33e845e203f2c7d07e776e","text":"Use AWS CloudFormation direct updates to quickly deploy the same load balancer configuration in multiple environments.","correct":false},{"id":"d8c538a0c914101c9d913c3644dfeecd","text":"Use AWS CloudFormation change sets to change the load balancer configuration based on Region/AZ where you want to deploy a copy of the application.","correct":false},{"id":"54ac82e3cb0e853af8ce69674edafd64","text":"Instead of CloudFormation, use Lambda. Let the load balancer trigger a Lambda function that has the infrastructure code embedded to deploy the configuration when prompted.","correct":false},{"id":"67c3f9750fe6b5768341534dcf97ba93","text":"Use AWS CloudFormation nested stacks by creating a dedicated template for the load balancer and refer to that template within other templates.","correct":true}]},{"id":"4a430e9e-4943-4d13-9511-ba93d66d9eaf","domain":"development","question":"A developer is configuring CodeDeploy to deploy an application to an EC2 instance. The application's source code is stored within AWS CodeCommit.\n\nWhat permissions need to be configured to allow CodeDeploy to perform the deployment to EC2?","explanation":"CodeDeploy interacts with EC2 via the CodeDeploy Agent, which must be installed and running on the EC2 instance. During a deployment the CodeDeploy Agent running on EC2 pulls the source code from CodeCommit. The EC2 instance accesses CodeCommit using the permissions defined in its instance profile role; therefore, it is the EC2 instance itself that needs CodeCommit access.\n\nThe specific CodeCommit permission needed to pull code is `codecommit:GitPull`.","links":[{"url":"https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control-permissions-reference.html","title":"CodeCommit Permissions"},{"url":"https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-configure.html","title":"Configure an Amazon EC2 Instance to Work with CodeDeploy"}],"answers":[{"id":"a5f1b9fe7264b6a78932a24f195b1064","text":"Create an IAM Policy with an acton to allow `codecommit:CreatePullRequest` on the required repository. Attach the policy to CodeDeploy's Service role.","correct":false},{"id":"3801756a25b44dc5e1eaaf3a55f6c0fe","text":"Create an IAM policy with an acton to allow `codecommit:GitPull` on the required repository. Attach the policy to the EC2 instance profile role.","correct":true},{"id":"ae4d1eac013f4a67d18129084085b41b","text":"Create an IAM policy with an acton to allow `codecommit:CreatePullRequest` on the required repository. Attach the policy to the EC2 instance profile role.","correct":false},{"id":"dcaaa5b141981d140fe32ea458888500","text":"Create an IAM Policy with an acton to allow `codecommit:GitPull` on the required repository. Attach the policy to CodeDeploy's Service role.","correct":false}]},{"id":"427e94b9-4fba-461b-aea9-d1a0c40a150e","domain":"development","question":"A DynamoDB table is configured in provisioned throughput mode with 500 RCU and 100 WCU.  How much data can be read and written to the table each second?","explanation":"One read capacity unit is equivalent to one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. One write capacity unit is equivalent to one write per second for an item up to 1 KB in size.  Therefore, 500 RCU is equivalent to: 1) 500 RCU * 4KB = 2000 KB per second for strongly consistent read operations; 2) 500 RCU * 4KB = 2000 KB per second * 2 = 4000 KB per second for eventually consistent read operations; 3) 100 WCU * 1KB = 100 KB per second for write operations.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.ProvisionedThroughput.Manual","title":"Provisioned Mode"}],"answers":[{"id":"3e1674995e5d9212840ca5a62310ccf9","text":"500 KB for strongly consistent read operations, 1000 KB for eventually consistent read operations, 100 KB for write operations.","correct":false},{"id":"3d31cbf18bacabdeabf60932252ab4e0","text":"2000 KB for strongly consistent read operations, 4000 KB for eventually consistent read operations, 400 KB for write operations.","correct":false},{"id":"c233b8f618b2e1721b960acd0f8a9c81","text":"500 KB for strongly consistent read operations, 1000 KB for eventually consistent read operations, 400 KB for write operations.","correct":false},{"id":"28e9c06ef3d9789d4e86426c1a59bbcf","text":"2000 KB for strongly consistent read operations, 4000 KB for eventually consistent read operations, 100 KB for write operations.","correct":true}]},{"id":"5f0e91e8-9de9-4bfe-b30d-6c00cb4e25ee","domain":"deployment","question":"Your application stores files in an S3 bucket located in us-east-1, however many of your users are located in ap-south-1. The files are less than 50MB in size, however users are frequently experiencing delays when attempting to upload files. Which of the following options will maximize the upload speed?","explanation":"S3 Transfer Acceleration is recommended to increase upload speeds and especially useful in cases where your bucket resides in a Region other than the one in which the file transfer was originated. Multipart upload is a good option for large files, e.g. >100MB in size.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html","title":"S3 Transfer Acceleration"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html","title":"Multipart Upload"}],"answers":[{"id":"34188758a0d842537d7d14f43c94c315","text":"Design the application to use multipart upload, so that the file is split in to multiple parts which are then uploaded simultaneously.","correct":false},{"id":"137ef2be67b8313e80b25b2c3ea64ec0","text":"Implement a third party CDN solution.","correct":false},{"id":"27cbe99b103845434c5d99034be17b10","text":"Utilize S3 Transfer Acceleration.","correct":true},{"id":"c1b6778485fc5a390b6a08f16f22afec","text":"Require the users to use Direct Connect in order to use to application so as to maximize the upload bandwidth.","correct":false}]},{"id":"66c6afb4-04e0-4eda-aa5b-745c464d5cad","domain":"mon-trb","question":"You deployed a new Lambda function a few days ago and your code seems to be executing successfully, however when you check CloudWatch there isn't any log data for your function. What could be the reason for this?","explanation":"A service needs to have permissions to write log data to CloudWatch logs, Lambda is associated with an execution role which needs to grant the relevant IAM permissions","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html","title":"Using CloudWatch"}],"answers":[{"id":"0261bd378f9288b971d24a9076d27d22","text":"The execution role for the Lambda function did not grant permissions to write log data to CloudWatch Logs","correct":true},{"id":"122cf71e8d5e76b7cfa4409633b0e4bf","text":"There is an issue with S3 in your region","correct":false},{"id":"ec216d5f36d3705950b7957bbb31d565","text":"The CloudWatch agent has stopped","correct":false},{"id":"dfa1603ecab090efaf75db7e3a0456e3","text":"Your code is taking too long to execute, it could be that your function does not have enough compute resources to generate the log files","correct":false}]},{"id":"1a8f6ca4-1edf-4cb3-8dce-63550ef898d0","domain":"security","question":"You are working on a web application which handles confidential financial data. The application runs on a few EC2 instances which are behind an Elastic Load Balancer. How can you ensure the data is encrypted end-to-end in transit between your ELB and EC2 instances?","explanation":"Terminating secure connections at the load balancer and using HTTP on the backend might be sufficient for your application. However, if you are developing an application that needs to comply with strict external regulations, you might be required to secure all network connections. First, add a secure listener to your load balancer, then configure the instances in your environment to listen on the secure port and terminate HTTPS connections.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-endtoend.html","title":"Configuring End-to-End Encryption"}],"answers":[{"id":"2c26367f893c5f15ae242c1965051017","text":"Terminate HTTPS connections on your EC2 instances","correct":true},{"id":"ed90effd6942a7cffc042b6cb610b798","text":"Perform SSL termination on the load balancer","correct":false},{"id":"cb9dd4ff2be4893bd209d207f1898600","text":"Perform SSL termination using Lambda","correct":false},{"id":"9f4e11571be1c566b8e30fdd753f0802","text":"Configure the instances in your environment to listen on the secure port","correct":true},{"id":"c2bc73f05693654154a5a002cf77d3e1","text":"Configure a secure listener on your load balancer","correct":true}]},{"id":"25cc389e-1ceb-4b17-b717-9458a1b8e133","domain":"security","question":"One of your junior developers has never had AWS Access before and needs access to an Elastic Load Balancer in your custom VPC. This is the first and only time she will need access. Which of the following choices is the most secure way to grant this access?","explanation":"It's always best practice to grant users access via IAM roles and groups. In this case, we would *not* assign the junior Dev to an existing group, as most Dev groups will have *more* access than is required for this Dev to perform the single task she has been asked to accomplish. Remember - always grant the *fewest* privileges possible.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege","title":"Least Privilege"}],"answers":[{"id":"2a067a28bf5990259e24ae36fff49ad3","text":"Let her log in with Admin credentials and change the Admin password when she is finished.","correct":false},{"id":"89c57f09d4cfd1c434b46868a4f3e488","text":"Add that developer to a Group with the requisite access (although that group may have *more* permissions than are needed for the Dev to do her job).","correct":false},{"id":"965dceca3b86aea95ef5de037128780c","text":"None of these.","correct":false},{"id":"f204900100e4e11c7582843062170feb","text":"Create a new IAM user with *only* the required credentials and delete that IAM user after the developer has finished her work.","correct":true}]},{"id":"832e5d50-044a-49a0-8e39-b67b7325b242","domain":"refactoring","question":"You have software on an EC2 instance that needs to access both the private and public IP address of that instance. What's the best way for the software to get that information?","explanation":"To view all categories of instance metadata from within a running instance, use the following URI: http://169.254.169.254/latest/meta-data/","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html","title":"Instance Metadata and User Data"}],"answers":[{"id":"239df81185d957b59d31a19adbc9322c","text":"Have the software use cURL or GET to access the instance metadata.","correct":true},{"id":"62683b06759958dafa2f12da5889a9b3","text":"Reference the instance metadata for the private IP and the instance user data for the public IP.","correct":false},{"id":"2ee1ba64e47acbacf84d2d788ee960f6","text":"Have the software use cURL or GET to access the instance user data.","correct":false},{"id":"d7f090ce9cc095c4e147cb891beeb897","text":"Call the EC2 API.","correct":false}]},{"id":"aadb1fdb-d919-4989-ab28-6bd8ffcd7c7a","domain":"security","question":"You are developing a healthy-eating application which tracks nutrition and water intake on a daily basis. Your users mainly access the application using a mobile device like a cell phone or tablet. You are planning to run a promotion to attract new users by providing a free trial period and you would like to make it easy for guest users to trial your application. Which of the following can you use to configure access for guest users?","explanation":"With a Cognito identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as federation through third-party IdPs.","links":[{"url":"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-scenarios.html","title":"Common Cognito Scenarios"}],"answers":[{"id":"1c3430e7715f141606c52e6ff2c15c05","text":"Identity Federation with AWS","correct":false},{"id":"12ffa121095c5fb3ad9fa1f1ed509e46","text":"Identity Federation with SAML","correct":false},{"id":"8102630f0a77efc6d438d44b1badd712","text":"IAM User Pools","correct":false},{"id":"22d4851cfa5c3b7d3fa506d938aeb081","text":"Cognito Identity Pools","correct":true}]},{"id":"76e6d618-2134-4a73-94dc-dc1c3af9eac3","domain":"development","question":"Which of the following are supported ways to upload and deploy your Lambda code?","explanation":"You can paste code or upload a zip file directly to the console, upload your code to S3 or use a CloudFormation template. Elastic Beanstalk is not a supported method of deploying Lambda.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html","title":"Lambda Deployment Packages"},{"url":"https://docs.aws.amazon.com/toolkit-for-eclipse/v1/user-guide/lambda-tutorial.html#lambda-tutorial-upload-code","title":"Uploading Lambda Code"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html","title":"CloudFormation and Lambda"}],"answers":[{"id":"5c0aaa802c93c303836e0babb6d7e99c","text":"Zip your code into a zip file, upload it to an S3 bucket and have Lambda download it from S3","correct":true},{"id":"9a700b9b08c4e3228582eae88ca49df0","text":"Write a CloudFormation template that will deploy your environment including your code.","correct":true},{"id":"cf448e03c3d4d06aae12f839f0b9c6ec","text":"Zip your code into a zip file, upload it to Elastic Beanstalk, then deploy your environment using Elastic Beanstalk","correct":false},{"id":"a1d81e8594c3bbc120a7270f2475c2b1","text":"Zip your code into a zip file and upload it via the Lambda console","correct":true},{"id":"185133cc98a2cbb85e317fd8493e7a26","text":"Copy and paste your code in to the integrated development environment (IDE) inside Lambda","correct":true}]},{"id":"cbd79fed-da8e-4342-9ce7-e646673c2690","domain":"deployment","question":"A developer is creating a CloudFormation template to deploy an application stack. The administrator password for the application needs to be configured at CloudFormation runtime. What CloudFormation section should be used for this requirement?","explanation":"CloudFormation template parameters section is used to pass input values to the template.  The template parameters can be set to custom values when the template is used to create a new stack. A parameter for the administrator password can be used to meet the question requirement. Metadata section is used to provide information about the template. Mappings section is used to define key and matching value mapping pairs. Resources section is used to specify resources that constitute the stack and need to be provisioned by the template.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"Template Anatomy"},{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html","title":"Parameters"}],"answers":[{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false},{"id":"ce21470ab49d1d1976bc3dc72438c183","text":"Metadata","correct":false},{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":true},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":false}]},{"id":"56a3a6cc-4c72-4b6c-a06c-4c310d107296","domain":"development","question":"An application successfully updates an existing object in S3. When checking the file contents, the developer does not see the updated file contents. What is the cause of this issue?","explanation":"Amazon S3 offers eventual consistency for overwrite PUTS and DELETES in all Regions. Amazon S3 provides high availability and high durability by replicating bucket objects across multiple availability zones and servers. This means that any updates to objects must replicate across all servers storing the data. This can take some time. Therefore, any updates to existing objects (using POST or DELETE), will take some time to be propagated across all of S3, and hence are eventually consistent.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html","title":"Introduction to Amazon S3"}],"answers":[{"id":"47ad6c66cb5b231f66170a797fbb7782","text":"Overwrite PUTS in S3 have eventual consistency.","correct":true},{"id":"4756256f32016be9d23caa16a75bb9c5","text":"HTTP 200 response code was not received.","correct":false},{"id":"a693ecd0f4b5da724ee692d2059fc4c3","text":"S3 bucket policy permissions were not correct.","correct":false},{"id":"57d934f14c5f166d5f0604507d795cc0","text":"S3 Bucket Versioning was not enabled.","correct":false}]},{"id":"bc3ffcc3-4225-4da3-8819-17ae2ee5c3dc","domain":"security","question":"A VPC has four subnets: 1) Subnet1 has a route table entry with destination: 0.0.0.0/0 and target: VPC Internet Gateway ID; 2) Subnet2 has a route table entry with destination 0.0.0.0/0 and target: NAT Gateway ID; 3) Subnet3 has a EC2 instance that serves as a bastion host 4) Subnet4 has an NSG Inbound Rule with Source: 0.0.0.0/0; Protocol: TCP; and Port Range: 1433. What would be the recommended subnet for hosting an RDS database instance?","explanation":"Security best practice would state that RDS Database instances should be deployed to a private subnet. A private subnet would only have private IP's with no direct access to the public internet. Outbound connectivity would be provided via a NAT gateway. Thus, Subnet2 is a suitable choice for deploying RDS instances as it is characterized as a private subnet. Subnet1 has direct connectivity to the public internet via the Internet gateway. Thus, it is characterized as a public subnet and would not be recommended location for deploying databases. Bastion hosts allow direct inbound connections from public internet. This means that Subnet3 would not be a good choice to host databases. Lastly, Subnet4 contains NSG rule that allows inbound connectivity from the public internet on the database port (1433). This makes it a poor candidate to host databases.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html","title":"VPC with Public and Private Subnets (NAT)"}],"answers":[{"id":"5cf0f214c90cf601b2f5f3bb0c6ec507","text":"Subnet3","correct":false},{"id":"6415a7d7642468e623d92a6df2333f86","text":"Subnet1","correct":false},{"id":"58726f362b0b49454806dac56cdef893","text":"Subnet4","correct":false},{"id":"5fb3d14ca24c9ce9e6eecb5a650bf8c9","text":"Subnet2","correct":true}]},{"id":"aef3c172-a47c-4705-8241-936c06d9bb7c","domain":"development","question":"You have configured your CI/CD process using CodePipeline, however you want to introduce a manual sign-off and approval process which needs to be completed before a new version of your application is deployed to Production. How can you achieve this?","explanation":"With CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.","links":[{"url":"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html","title":"Manual Approvals in CodePipeline"}],"answers":[{"id":"e53c0ca92d915920a376534f6d7a4e99","text":"Configure MFA for CodeDeploy deployments","correct":false},{"id":"5c0749c1409b05011fac6531c17f2cf8","text":"Configure two pipelines, one to handle code build and test, and one to handle automated deployment. Use SNS and Lambda to trigger the Deployment Pipeline following notification of successful completion of the Build and Test Pipeline","correct":false},{"id":"dc965161695cb07b80f56150a5a689f1","text":"Use CodePipeline to handle build, compile, test and packaging activities, then manually start a CodeDeploy job to run an automated deployment of successfully tested code","correct":false},{"id":"fbfc1e99b3679cb29f80f55cf63c4a8d","text":"Use the CodePipeline Manual Approvals feature","correct":true}]},{"id":"81a4f6d3-34ae-4753-8498-823bede20afe","domain":"deployment","question":"Your team is considering deploying an application on AWS Elastic Beanstalk. Your manager needs to know what infrastructure requirements are needed for the team, specifically in regards to maintenance, patching, and managing security. How would you explain what AWS is responsible for and what the team is responsible for to your manager?","explanation":"AWS and its customers share responsibility for achieving a high level of software component security and compliance. This shared model reduces customers' operational burden. AWS Elastic Beanstalk helps you perform your side of the shared responsibility model by providing a managed update feature. This feature automatically applies patches and minor updates for an Elastic Beanstalk supported platform version. Elastic Beanstalk publishes its platform support policy and retirement schedule for the coming 12 months. You (the customer) are responsible for the security of your application, your data, and any components that your application requires and that you downloaded. Be sure to review the Shared Responsibility Model in the URL provided.","links":[{"url":"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/platforms-shared-responsibility.html","title":"Shared Responsibility Model for Elastic Beanstalk Platform Maintenance"}],"answers":[{"id":"7086eb1de01b03376ee097977ba7cd94","text":"You are responsible for runtime, application server, and web server components if you opt into Elastic Beanstalk managed updates.","correct":false},{"id":"f1e49a967c2ae8dc1525d208a8170996","text":"AWS is responsible for the security of your application, your data, and any components that your application requires and that you downloaded.","correct":false},{"id":"9fafdcf1a8a7c82b9e94d6080c00a245","text":"AWS is responsible for patches, minor, and major updates of operating system on its supported platform versions.","correct":true},{"id":"8fc7a262705a2a78a08dbc0ea667db64","text":"You are responsible for publishing Elastic Beanstalk's platform support policy and retirement schedule.","correct":false},{"id":"9d22ad24853ac3a47fe7c0e7eb2585e3","text":"You are responsible for the security of your application, your data, and any components that your application requires and that you downloaded.","correct":true}]},{"id":"6cdfb12d-88c9-4182-8761-892327d0d6e1","domain":"security","question":"You are working on a mobile phone app for an online retailer which stores its customer data in DynamoDB. You would like to enable new users to sign-up using Facebook or Google credentials. What is the recommended approach?","explanation":"Using Cognito is the recommended approach to federating with Web ID Providers for mobile applications","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html","title":"Amazon Cognito for Mobile Apps"}],"answers":[{"id":"f27fa20337c218b3cbc5c69c91585adf","text":"After the user has authenticated with Facebook, allow them to download encrypted AWS credentials to their device so that the mobile app can access DynamoDB","correct":false},{"id":"7afe23d8d3d95622b0bc6973be88c599","text":"Write your own custom code which allows the user to log in via a Web Identity Provider and receive an authentication token, then calls the AssumeRoleWithWebIdentity API and exchanges the authentication tokens for temporary access to DynamoDB","correct":false},{"id":"f6b8be7d8470d0f29d80578d719a209e","text":"Once the user has logged in to the Web Identity Provider, use Cognito to exchange the authentication tokens for temporary access to DynamoDB","correct":true},{"id":"8a7c438e9d3be3a418abd985f62a5eba","text":"Embed encrypted AWS credentials into the application code, so that the application can access DynamoDB on the user's behalf","correct":false}]},{"id":"55ffe5c3-b16f-42d6-9b3e-5f4d77890f49","domain":"mon-trb","question":"You are troubleshooting a major incident which has resulted in data loss in your application. Your manager asks if you can provide a time-ordered sequence of any modifications which happened to the items in your DynamoDB table over the past 24 hours so that you can work out what happened. Which service could you use to most effectively provide this?","explanation":"DynamoDB Streams captures a time-ordered sequence of item-level modifications in a DynamoDB table and durably stores the information for up to 24 hours.","links":[{"url":"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/","title":"DynamoDB Streams Use Cases"}],"answers":[{"id":"1f60690ba7c488a02416a7bf195f900b","text":"DynamoDB Streams","correct":true},{"id":"48d30a1b2a41bc37892197dc4df30262","text":"Kinesis Streams","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false}]},{"id":"8b887631-86bd-436d-adee-4e2ba3b02111","domain":"security","question":"You have an application running on multiple EC2 instances, however every time an instance fails, your users complain that they lose their session. What can you do to prevent this from happening?","explanation":"There are various ways to manage user sessions including storing those sessions locally to the node responding to the HTTP request or designating a layer in your architecture which can store those sessions in a scalable and robust manner. Common approaches used include utilizing Sticky sessions or using a Distributed Cache for your session management. In order to address scalability and to provide a shared data storage for sessions that can be accessed from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution for this is to leverage an In-Memory Key/Value store such as ElastiCache.","links":[{"url":"https://aws.amazon.com/caching/session-management/","title":"Session management in AWS"}],"answers":[{"id":"b225818943ba4680b8e7dc9d9c376359","text":"Store session state in RDS","correct":false},{"id":"ef4fd36fa55c3c499f3fffa82a0c95e8","text":"Store session state in on the Elastic Load Balancer","correct":false},{"id":"b193b1caff1bda86125cc326ca1058ac","text":"Store session state on a dedicated EC2 instance","correct":false},{"id":"76fc6bddee6b0f8d088ea5cbe4e57160","text":"Store session state in S3","correct":false},{"id":"89230492f141a4f85234c624287bb96a","text":"Store session state in ElastiCache","correct":true}]},{"id":"42ff013e-95f1-4ead-958d-26a843b0b207","domain":"mon-trb","question":"Your security team have brought in an external auditor to review the security standards across your AWS account. They have identified that your development team have elevated privileges across a number of services, which according to company policy, they should not have access to. You have been asked to help work out which of the IAM policies are granting too much access to the team. Which of the following can you use to find out which policies are granting too many privileges?","explanation":"With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies attached to IAM users, groups, or roles in your AWS account. You can test which actions are allowed or denied by the selected policies for specific resources.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html","title":"Testing IAM Policies with the IAM Policy Simulator"}],"answers":[{"id":"fa535ffb25e1fd20341652f9be21e06e","text":"Config","correct":false},{"id":"0863526e514d88cd2d00b39f8dda0920","text":"X-Ray","correct":false},{"id":"df8a1f2103a58209a3008c02a93162b5","text":"Cognito","correct":false},{"id":"b6632fa69795d5fabc908fe75210b177","text":"IAM Policy Simulator","correct":true}]},{"id":"d1b0d0fe-4931-441c-83d5-716d63424a58","domain":"mon-trb","question":"You are working on an application which shares video content to subscribed users. This morning you have received a number of complaints that users are unable to access your content and they are seeing an HTTP 504 Status Code. Which of the following could be a possible explanation?","explanation":"An HTTP 504 status code is a Gateway Timeout which indicates that when CloudFront forwarded a request to the origin, because the requested object was not in the edge cache, one of the following happened: The origin returned an HTTP 504 status code to CloudFront; or, the origin didnâ€™t respond before the request expired. This is a server side issue, i.e. a problem or misconfiguration in your AWS infrastructure. Remember that any 5XX error indicates a server-side error, and a 4XX error indicates a client-side error.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/http-504-gateway-timeout.html","title":"HTTP 504 Gateway Timeout"}],"answers":[{"id":"84f05e093a7f246d10fef9ea043d217a","text":"The users have a network connectivity problem","correct":false},{"id":"76b78bdea733c0e70e13c80add3c975d","text":"There is a client side error in the user's infrastructure","correct":false},{"id":"b9434940d2516177efc8f8eef8b3de37","text":"There is a server side error within your AWS infrastructure","correct":true},{"id":"c8cffd0bf5d95cfc0541c7ad2d54d740","text":"The users could be attempting to access your site using an unsupported browser","correct":false}]},{"id":"a03dd6d9-131e-4ca1-8a32-ee232f5b1323","domain":"deployment","question":"Under what circumstances would you use an SQS Delay Queue?","explanation":"Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html","title":"SQS Delay Queues"}],"answers":[{"id":"325f46d52e7f4979ed4ac9b2f986a307","text":"To postpone the delivery of new messages to a queue for a number of seconds","correct":true},{"id":"a767183a27b8a253d76902f6d6b5faca","text":"To delay a message for a number of seconds while it is being polled from the queue","correct":false},{"id":"51338086c0084222e0b52b27cb080907","text":"To delay a message for a number of seconds until it has been processed","correct":false},{"id":"ec05848fbc20fc135aba127ca16850f2","text":"To hide a message for a number of seconds after it has been consumed from the queue","correct":false}]},{"id":"1a6ef8eb-4675-4004-ab47-3f8682a6e038","domain":"security","question":"Your EC2 instance needs to access files located in an S3 bucket, what is the best way to enable access?","explanation":"Using an IAM role associated with the EC2 instance is the recommended way, storing credentials locally is not recommended.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html","title":"IAM Roles"}],"answers":[{"id":"246491a279b8f11f71b3c083962c0e75","text":"Create a new IAM role and grant read access to S3. Store the role's credentials locally on the EC2 instance and configure your application to supply the credentials with each API request","correct":false},{"id":"7e00e264a60465681fb267faf13307e9","text":"Configure a bucket policy which grants read access based on the EC2 instance name","correct":false},{"id":"6642e43e9a2808c204a04b7c07a1f5ac","text":"Create a new IAM user and grant read access to S3. Store the user's credentials locally on the EC2 instance and configure your application to supply the credentials with each API request","correct":false},{"id":"f47981b92302bc40c2c9dc5f0ec40a23","text":"Create an IAM role with read access to S3 and assign the role to the EC2 instance","correct":true}]},{"id":"86524d3b-b3e8-46ca-97a2-e12d4edfabed","domain":"development","question":"Which of the following best describes Amazon ECS?","explanation":"ECS stands for Elastic Container Service: It manages running containers on your EC2 instances. It does not act as a scheduler and it is neither serverless nor software that you manage.","links":[{"url":"http://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html","title":"About Amazon ECS"}],"answers":[{"id":"8180f2f15ee5ff1c554855a0352e23bf","text":"The Elastic Container Scheduler is software that you can run and manage to orchestrate many running Docker containers.","correct":false},{"id":"d3958d5a87a697631979b920c68a9ae2","text":"The Elastic Container Service is a service that manages running Docker containers on a group of your EC2 instances.","correct":true},{"id":"f0fbcb4f668b638f39ce41a6972d9a94","text":"The Elastic Container Service is software that you can run and manage to orchestrate many running Docker containers.","correct":false},{"id":"a966ed9a7fc5f750acbbef6754f3ad57","text":"The Elastic Container Scheduler is a serverless system to manage running many Docker containers in a flexible and cost-effective way.","correct":false}]},{"id":"52809d95-a072-4314-8781-a45d93534ad5","domain":"development","question":"You are importing an existing API to API Gateway. Which format is supported for API definition files?","explanation":"The Import API feature supports OpenAPI v2.0 and OpenAPI v3.0 definition files. Swagger is a common tool set that is originally defined the OpenAPI v2.0 specification.  AWS use the name interchangeably with OpenAPI v2.0 .  RAML is supported in a different tool (API Gateway Importer).","links":[{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-import-api.html","title":"Importing APIs"},{"url":"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-export-api.html","title":"Export an API from API Gateway"},{"url":"https://en.wikipedia.org/wiki/OpenAPI_Specification#History","title":"Swagger & OpenAPI"}],"answers":[{"id":"336ff1e9aa6177ea7a71984fa8c241b9","text":"Swagger","correct":true},{"id":"c31c335ef37283c451b18ba0dd317de1","text":"Angular","correct":false},{"id":"9a0df1d09b45d123c4f21aa4caf2c205","text":"OpenAPI v3.0","correct":true},{"id":"a15aa8cb8ce8298ee404e93ab4afdf0b","text":"OpenAPI v2.0","correct":true},{"id":"51546bb736b0f9021b167202e967a6cb","text":"RAML","correct":false}]},{"id":"d5adb3f2-4ddd-4a5e-be37-47d74da0f6d6","domain":"development","question":"You are working on a Serverless application written in Node.js. You updated the Node.js code and uploaded a new zip file containing your code to Lambda. Your application references the function using the alias \"Prod\", however it not seem to be using the new code. Which of the following is likely to fix this?","explanation":"The problem is that the application is referencing the function using an alias pointing to a previous version of the code. When you use versioning in AWS Lambda, you can publish one or more versions of your function. So that you can use different variations of your Lambda function in your development workflow such as development, beta, and production. Lambda also supports creating aliases for each of your Lambda function versions. Conceptually, an AWS Lambda alias is a pointer to a specific Lambda function version. You can update aliases to point to different versions of functions.","links":[{"url":"https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html","title":"Lambda Function Versioning and Aliases"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/how-to-manage-versioning.html","title":"Lambda Versioning"}],"answers":[{"id":"6a4ac8c4aa2076312300dc09fa5c0f21","text":"You need to call the function using $LATEST","correct":false},{"id":"1ab8bb5a6ed0944df7316a0ae848727f","text":"You need to update the alias to reference the new version of your function","correct":true},{"id":"33064ca2d30b93ce4d46769270b3e34e","text":"You need to update your application to use an unqualified ARN","correct":false},{"id":"68796527ce97010c4a40777a6462b3fc","text":"You need to call version 2 of the function","correct":false}]},{"id":"418aa386-2a62-4e41-8724-c4d4adf6a318","domain":"refactoring","question":"You require a data storage solution for an application running on EC2. The data running on the application is not well-structured to fit into a defined schema. Even so, the schema would change very often as data is dependent on users. What choice of database solution would best support your application?","explanation":"Transactional data, such as e-commerce purchase transactions and financial transactions, are typically stored in relational database management systems (RDBMS) or SQL database systems. The choice of database solution depends on the use case and application characteristics. A NoSQL database is suitable when the data is not well-structured to fit into a defined schema, or when the schema changes very often. An RDBMS solution, on the other hand, is suitable when transactions happen across multiple table rows and the queries require complex joins. Amazon DynamoDB would be a better use case in this scenario versus Amazon RDS (which is a SQL-based, structured, relational database solution). AWS IoT can work with other AWS services like Lambda, Kinesis, S3, and DynamoDB to build applications that gather, process, analyze, and act on IoT data but is not a database solution. Amazon Kinesis is not a database solution, but rather a streaming data solution.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html","title":"From SQL to NoSQL"}],"answers":[{"id":"6211995f3e47e0e7423efeb101148a29","text":"Use Amazon Kinesis services to be able to collect, store, and process your users' data continuously.","correct":false},{"id":"58e198b9da277384f8be4026cb2431e1","text":"Use an RDBMS solution such as Amazon RDS to implement a SQL-based relational database solution for your application.","correct":false},{"id":"b1f7c088e6e85841d4830922aa622f9e","text":"Use a NoSQL database such as Amazon DynamoDB that can be used as an OLTP store for your application.","correct":true},{"id":"763409014035e2e7dad1fd45adc43794","text":"Use AWS IoT connected devices to interact easily and securely with AWS Lambda, Amazon Kinesis, and Amazon S3.","correct":false}]},{"id":"17aedf6b-012a-448d-805b-784c7f87ba15","domain":"deployment","question":"What is the CloudFormation helper script cfn-init used for?","explanation":"CloudFormation helper scripts are Python scripts that can be used as part of a CloudFormation template to automate common tasks during stack creation. cfn-init helper script can be used to install packages, create files, and start/stop services. cfn-get-metadata can be used to fetch a metadata block from AWS CloudFormation and print it to standard out.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-init.html","title":"cfn-init"}],"answers":[{"id":"2831dc5c18284a29977a09e8fc481cbb","text":"Fetch required credentials before provisioning AWS resources.","correct":false},{"id":"ab8e55f1fd87a343e434da332b588142","text":"Initialize CloudFormation IAM Service Role.","correct":false},{"id":"6fe3f8fbab579b6b679da8d9b773b7c2","text":"Fetch a metadata block from AWS CloudFormation template.","correct":false},{"id":"2a5d8dc3f4b9ee91839db7f876a01872","text":"Install packages and start/stop services on EC2 instance.","correct":true}]},{"id":"95a7c517-4b4e-4996-b4d4-8ae11f359065","domain":"development","question":"An organization is considering performing canary deployments with their application. Which of the following statements best describes a canary deployment?","explanation":"A canary deploy allows us to gain confidence in an application update by initially just releasing it to a subsection of users. Once we are satisfied that the update is working as expected, the update is then rolled out to the remaining users.\n\nThe concept of a canary deployment is covered in the AWS Well-Architected Framework, and is a feature of API Gateway.\n\nIt can also be performed manually using Route53 Weighted Records, or via an Application Load Balancer with a Forward Action and Weighted Target Groups.","links":[{"url":"https://wa.aws.amazon.com/wat.concept.canary-deployment.en.html","title":"Canary deployment - Well-Architected Framework"}],"answers":[{"id":"d593ae8cd7b7dc0ff31e7a286abc4a60","text":"A new version of the application is deployed alongside the existing version. Once the new version is ready to handle traffic, all traffic is redirected to it.","correct":false},{"id":"18c6683d5f9bd9718e5549310c8647e2","text":"A new version of the application is deployed alongside the existing version. A proportion of applicationâ€™s traffic is directed to the new application. If, after a given number of minutes, metrics demonstrate that the new version is performing correctly, the remainder of the traffic is moved to the new version.","correct":true},{"id":"03ff12365c5a8fc5fe5dfdbd2f1f00b6","text":"Each instance of the original application is taken out of service one at a time and replaced with the new version of the application. During the deployment, traffic is sent to a mix of the original and new versions.","correct":false},{"id":"0beefb480486ea595083b6d39062c27b","text":"All instances of the original application are stopped, after which new instances of the application are started up in their place. During the transition, there is a short period of downtime.","correct":false}]},{"id":"f4f9e915-de73-4bcb-b2ad-dc9f35db47b8","domain":"development","question":"A document management application stores a catalogue of documents, each uniquely identified by its Document Number. Each document is also described by additional attributes: Document Title, Publication Date, Publisher Name, Country of Origin, and Length.  Functional requirements specify that the application should be able to produce a listing of all documents for each country of origin.  What would be the optimal DynamoDB data model for this application?","explanation":"Document Number is unique for each item thus making it a good choice for the table partition key.  Using a random prefix for the GSI partition key and County of Origin as the sort key enables us to have high cardinality for the partition key (thus avoiding any hot partitions) while still allowing for fast querying based on the Country of Origin. Publication Date is a poor choice for the partition key as it is a low cardinality attribute. It results in a hot partition for all items published on that date. Country of Origin is a low cardinality attribute and makes a poor choice for the partition key of the GSI.  This would result in a hot partition for the GSI.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html","title":"Designing Partition Keys to Distribute Your Workload Evenly"},{"url":"https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/","title":"Choosing the Right DynamoDB Partition Key"}],"answers":[{"id":"a164aeb879c69b453a5da649dcf8644e","text":"Table Partition Key=Publication Date; Table Sort Key=Document Number; GSI Partition Key=Publisher Name; GSI Sort Key=Country of Origin;","correct":false},{"id":"6885e1c4312042be0a8e712ed8803a35","text":"Table Partition Key=Document Number; Table Sort Key=Document Type; GSI Partition Key=Country of Origin; GSI Sort Key=Publisher Name;","correct":false},{"id":"c564e5a2356a715aed2326166cf46f04","text":"Table Partition Key=Document Number; Table Sort Key=Document Title; GSI Partition Key=Random Prefix; GSI Sort Key=Country of Origin;","correct":true},{"id":"5758693e8b3c98551e39e76dc9a454f3","text":"Table Partition Key=Document Number; Table Sort Key=Document Type; GSI Partition Key=Publication Date; GSI Sort Key=Country of Origin;","correct":false}]},{"id":"4102ce86-6e8c-47b2-86d4-86e3b8b08558","domain":"mon-trb","question":"Your application runs in an Auto Scaling group to scale based on user demand. The Auto Scaling group runs behind an Elastic Load Balancer (ELB). When you check the ELB logs, you notice that a number of instances are failing the health check during periods of high demand. New instances are launching but they periodically fail health checks and subsequent instances are being launched which is increasing costs. What would you do to troubleshoot this issue?","explanation":"Amazon EC2 Auto Scaling waits until the health check grace period ends before checking the health status of the instance. Amazon EC2 status checks and Elastic Load Balancing health checks can complete before the health check grace period expires. However, Amazon EC2 Auto Scaling does not act on them until the health check grace period expires. To provide ample warm-up time for your instances, ensure that the health check grace period covers the expected startup time for your application. In this scenario, the health checks are most likely occurring before the EC2 instance and its applications have fully loaded, so increasing the health check grace period would likely resolve the issue. The cooldown period helps to ensure that your Auto Scaling group doesn't launch or terminate additional instances before the previous scaling activity takes effect, and are not used for health checks. Creating a new ELB or Auto Scaling group would have no impact but the problem would persist. AWS Config is a governance and management tool and is incapable of itself executing automated actions.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html","title":"Health Checks for Auto Scaling Instances"}],"answers":[{"id":"828102b2cd5f2f5f2873afea3714c7eb","text":"Increase the health check grace period.","correct":true},{"id":"1fec64ae8a7c39cb26d12b3f50438dde","text":"Increase the cooldown period of the Auto Scaling group.","correct":false},{"id":"47d484cdd97dff7b054f3cc746f8bbc6","text":"Create a new Auto Scaling group behind a new ELB. The current ELB is malfunctioning.","correct":false},{"id":"fe8f15e29da594ce0b07d192b5e8f4f3","text":"Use AWS Config to monitor instances with failed health checks to terminate them.","correct":false}]},{"id":"2689a73b-04ed-4719-9cdf-49c4ffe3eb17","domain":"deployment","question":"A developer is deploying a new application to ECS. The application requires permissions to send messages to an SQS queue. \n\nWhich role should the developer apply the policy to so that the application can access the SQS queue?","explanation":"The policy must be attached to the ECS Task's execution role to allow the application running in the container access SQS.","links":[{"url":"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html","title":"Amazon ECS Task Execution IAM Role"}],"answers":[{"id":"2413a4993d7f9395b5ddf84bc7b51b62","text":"The execution role attached to the ECS Container.","correct":false},{"id":"2d64ba22b9f8e2998aed499099374359","text":"The execution role attached to the ECS Service.","correct":false},{"id":"29aefbd6a89941ca26d2ef879d25237c","text":"The execution role attached to the ECS Task.","correct":true},{"id":"d89a15f255b7e5a2b6cfe8aa7d628173","text":"The execution role attached to the ECS Cluster.","correct":false}]}]}}}}
