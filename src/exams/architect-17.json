{"data":{"createNewExamAttempt":{"attempt":{"id":"a6daf406-9af7-4250-a5a7-4d67a271784b"},"exam":{"id":"f4eac6c1-0ebb-41c4-a085-6efbe609e704","title":"AWS Certified Solutions Architect – Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"b8be7547-0f55-4f09-addd-11907477196e","domain":"CostOptimized","question":"You have a one-year contract with a client to create and maintain its cloud environment with an AWS account, with the awareness that each AWS service and resource used has a 12-month free usage term. When that 12-month term expires, which of the following happens?","explanation":"After the 12-month free usage term expires for an AWS service, you will start paying the standard, pay-as-you-go service rates. There are no reduced rates, extensions from AWS Support, or automatic renewals of the 12-month term.","links":[{"url":"https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc","title":"AWS Free Tier"}],"answers":[{"id":"674c02a2df56483f7902847e2dbb8cd8","text":"The free usage term will be automatically renewed for another 12 months.","correct":false},{"id":"a0cdfc4210cbbb0f723b10fafa2c6120","text":"You will pay a fee that is reduced from the full pricing.","correct":false},{"id":"233245ebe184276bbec368b4fd80f067","text":"You will pay standard, pay-as-you-go service rates.","correct":true},{"id":"cb548d5d972e20059df43a3c680b32b5","text":"AWS Support will extend the free usage term to another 12 months upon request.","correct":false}]},{"id":"3e648b36-b215-4265-922c-8484055ff8bd","domain":"Performant","question":"You have been tasked with implementing a globally accessible storage solution that will scale from a few terabytes (now) to an unknown, but significantly greater, volume of data in three years time. Which AWS service would best meet your current and projected storage needs?","explanation":"Amazon S3 is highly scalable, secure storage for 'flat' files. S3 will scale to any projected volume of data. In this case, it's your best bet.","links":[{"url":"https://aws.amazon.com/what-is-cloud-object-storage/#benefits","title":"Benefits of Object Storage"}],"answers":[{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":false},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true},{"id":"1574cf43006500eb74cc583eef4a8b87","text":"EC2 with EBS","correct":false}]},{"id":"62c2730a-bc91-4f5d-a5e7-bf9c6465a185","domain":"Performant","question":"You are creating an application that will leverage EC2 for its web servers. The application data will be stored on the root device volume attached to the EC2 instance. Data on this volume must persist independently of the life of this particular instance. What EC2 volume should you choose?","explanation":"By using Amazon EBS, data on the root device can persist independently from the lifetime of the instance if you configure the appropriate settings.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html#EBSFeatures","title":"EBS Volumes - Benefits"}],"answers":[{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":false},{"id":"248c1b0bcb74394beb4a08030c4a6847","text":"EBS","correct":true},{"id":"202c7de7bba32a20b5b70439d58ec886","text":"Networked Instance Store","correct":false},{"id":"3d10947020abbde16f397afaf8e67e20","text":"Local Instance Store","correct":false}]},{"id":"c9683e3d-8753-425f-b254-6b05a82fe770","domain":"ResilientDesign","question":"Your company has a web application which runs on multiple on-premise Linux virtual machines, which needs to be migrated to AWS. The application relies on shared configuration files, currently hosted on an on-premise File Server, which are read each time the application handles a request. Any updates to the configuration files must be applied immediately. What would the most suitable solution be to store the configuration files?","explanation":"Amazon EFS is recommended here, since we are sharing a file between multiple Linux instances. Amazon S3 could be possible, but considering the requirements for immediate responses to configuration updates, the read-after-write consistency model of EFS is superior to the eventual consistency model of S3. Migrating the existing file server is possible, but excessive given the other options available, and would incur more costs than you would need. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources, but does not have Configuration Management components as described in the scenario","links":[{"url":"https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html","title":"EFS: How It Works"}],"answers":[{"id":"b77f0c550141c38ee398bb165b90ce6f","text":"Amazon S3 - Host the configuration file in an S3 bucket which can be read by the instances","correct":false},{"id":"2e611648f7e940d657fd27711a55645b","text":"Amazon EC2 - Migrate the File Server to EC2 as well, keeping it as it is on-premise","correct":false},{"id":"847f6bf6f9e24beea40dda8e7085bbaf","text":"AWS Config - Host the configuration files in AWS Config which can then be read by the instance","correct":false},{"id":"f4933c135fea3fe19b79cc67ae011b12","text":"Amazon Elastic File System - Create an EFS Filesystem and mount to your instances","correct":true}]},{"id":"b7f28443-c2d5-4605-be5a-960171a70ab0","domain":"Performant","question":"You are auditing your RDS estate and you discover an RDS production database that is not encrypted at rest. This violates company policy and you need to rectify this immediately. What should you do to encrypt the database as quickly and as easy as possible.","explanation":"At the present time, encrypting an existing DB Instance is not supported. To use Amazon RDS encryption for an existing database, create a new DB Instance with encryption enabled and migrate your data into it.  Alternately you can encrypt a copy of a Snapshot and restore the encrypted copy.  However you cannot encrypt as you are restoring from a snapshot.  A key point is that an outage will be required either way.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html","title":"Encrypting RDS Resources"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations","title":"Encrypting RDS Resources - Limitations"}],"answers":[{"id":"23f20d1a056e924aa8bcce37914200c4","text":"Use the RDS Import/Export Wizard to migrate the unencrypted RDS instance across to a new encrypted database.","correct":false},{"id":"4d7c7e6fcbf43a4e3cb390d6b8f4e775","text":"Use AWS Database Migration Service","correct":false},{"id":"ee1313fab08c86cd71ec82d1ef4ccf05","text":"Create a new DB Instance with encryption enabled and then manually migrate your data into it.","correct":true},{"id":"e3b45db054ee4c24e48b8fa4288bc219","text":"Take a snapshot of your unencrypted DB Instance and then restore it making sure you select to encrypt the new instance.","correct":false}]},{"id":"56bf1d42-e120-46cf-8324-6b624b3c0beb","domain":"Performant","question":"Which of the following services can stream configuration changes and notifications recorded by AWS Config and use email as the endpoint?","explanation":"AWS Config records configuration changes in resources, and it streams those changes and notifications to an SNS topic. Users subscribed to the topic receive email notifications of the changes. Although SES is an email service, Config does not use it to send email notifications. CloudWatch is for monitoring resources, not for recording their configuration changes. SQS is a message queuing service and is not used with Config.","links":[{"url":"https://docs.aws.amazon.com/config/latest/developerguide/notifications-for-AWS-Config.html","title":"Notifications that AWS Config Sends to an Amazon SNS Topic"}],"answers":[{"id":"f7be29c9a4de2fe0c3ced5ca83552403","text":"Amazon Simple Notification Service (SNS)","correct":true},{"id":"4b88d56bd65ef92611ac63064c38369f","text":"Amazon CloudWatch","correct":false},{"id":"64fdb9de34f1179f1b0a667717e6fba3","text":"Amazon Simple Queue Service (SQS)","correct":false},{"id":"2e03b43594eb74bc8c1a23deeb4774ef","text":"Amazon Simple Email Service (SES)","correct":false}]},{"id":"f3f58927-ada1-471c-b68b-25a088a2f90e","domain":"ResilientDesign","question":"You have an application that uses S3 to store objects. Company policy dictates that certain objects (such as JPGs and PDFs) must be replicated to another region for redundancy. However, some objects (such as Word files) can stay in a single region. Company policy also dictates that you should use as few buckets as possible. How should you architect this solution?","explanation":"You can use just one bucket and enable CRR on just a subset of uploaded objects (such as JPGs and PDFs) by using specifying prefixes.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html","title":"Cross-Region Replication"}],"answers":[{"id":"12f0eec09e1467773e991b7cc2467d94","text":"Two buckets: 1 with CRR enabled on it (for the JPGs and PDFs) and the other without.","correct":false},{"id":"3542502efe7b729b6e2bf03840fe2b3e","text":"Two buckets: one regular bucket and one with CRR enabled on it (for the Word files).","correct":false},{"id":"b9515339b56a00b22e147bd7ee3cdcaa","text":"Three buckets: one with CRR enabled on it for the JPGs, one with CRR enabled on it for the PDFs, and 1 bucket for the Word files without CRR enabled.","correct":false},{"id":"3dcfc0449d992a379f44de060f180c8b","text":"One bucket. Then enable CRR and use specifying prefixes to identify the subset of objects (such as JPGs and PDFs) that must be replicated.","correct":true}]},{"id":"ccdbbdf9-e9b6-4255-bed7-4e5f65b8c940","domain":"SecureSolutions","question":"You are deploying an application on to EC2 instances. The application must make AWS API calls. What is the most secure method to pass credentials to the application?","explanation":"You can use roles to delegate access to users, applications, or services that don't normally have access to your AWS resources.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html","title":"IAM Roles"}],"answers":[{"id":"59bdef2986f5b1db72e3113469a3f6d6","text":"Store the API credentials as an object in S3.","correct":false},{"id":"431d737549954a94c084b6c8532b8670","text":"Embed the API credentials in the application.","correct":false},{"id":"4fe6cac2e87dc307c93a2b029e6ae5dc","text":"Pass API credentials to the instance using userdata.","correct":false},{"id":"98e2209690b12719fd4018b9176c77a9","text":"Assign an IAM role to the EC2 instances.","correct":true}]},{"id":"ebdeed2c-780f-4f28-ab81-0de353938dfb","domain":"ResilientDesign","question":"Your company is migrating a number of its applications to AWS. Services used will include Amazon EC2, Amazon S3, Amazon ELB Application Load Balancers, NAT Gateways, and Amazon RDS MySQL instances. You'll be using AWS's Bring Your Own IP Address (BYOIP) offering to keep all server IP addresses the same as they were on-premises. You'd also like to leverage Elastic IP Addresses for failover scenarios. Which approach will provide the most reliable IP addressing for your new AWS environment?","explanation":"You can bring your public IPv4 address ranges from your on-premises network to your AWS account with the BYOIP offering. You don't need to bring private IP address ranges. You can then create Elastic IP Addresses from your BYOIP pool and use them with AWS resources such as EC2 instances, NAT Gateways, and ELB Network Load Balancers. Elastic IP Addresses cannot be used with ELB Application Load Balancers or RDS instances.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-byoip.html","title":"Bring Your Own IP Addresses (BYOIP)"},{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-eips.html","title":"Elastic IP Addresses"}],"answers":[{"id":"d332dd2c597e20fddcdce2c0d2725e21","text":"Register your own public IP address ranges with BYOIP. Create Elastic IP Addresses from your BYOIP address pool for the EC2 instances and the NAT Gateways.","correct":true},{"id":"7406ea0371eb195c45ac0db9d8c279b7","text":"Register all of your IP address ranges with BYOIP. Create Elastic IP Addresses from your BYOIP address pool for the EC2 instances and the RDS MySQL instances.","correct":false},{"id":"87a3a8c3c23724feef9fdbda3a8e5d50","text":"Register your own public IP address ranges with BYOIP. Create Elastic IP Addresses from your BYOIP address pool for the EC2 instances, the NAT Gateways, and the RDS MySQL instances.","correct":false},{"id":"f719d7ed8ad37a2eac59c037e87ea416","text":"Register your own public IP address ranges with BYOIP. Create Elastic IP Addresses from your BYOIP address pool for the EC2 instances, the Application Load Balancers, and the NAT Gateways.","correct":false}]},{"id":"3b0354a2-8cf6-47da-9653-d39e9a2b6777","domain":"ResilientDesign","question":"The company you work for has been acquired and you have been tasked with the redirection of all its website traffic to the new company's website. The old one is hosted on S3 as a static website while the target is a self-hosted website. Which of the following options describes the best approach to achieve that as quickly as possible?","explanation":"Although other listed options are feasible, the quickest way to achieve the desired outcome is to set up a redirect at the S3 bucket level.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-page-redirect.html","title":"(Optional) Configuring a Webpage Redirect"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html","title":"Hosting a Static Website on Amazon S3"}],"answers":[{"id":"26160870cff1bf371f535d8988240c46","text":"In the Amazon S3 console, configure a redirect to the new domain in the 'Redirect requests: Target bucket or domain' box within the 'Static website hosting' section under the Properties tab of the relevant bucket.","correct":true},{"id":"a0baa3a3691b6288e36546a1b852accc","text":"Amazon S3 does not support website redirects. You will need to contact your domain registrar and ask them to update the target URL to point to the self-hosted website.","correct":false},{"id":"bf5117b1a38008012872c692deb1b8cc","text":"In the Amazon S3 console, set the website redirect location in the metadata of each object in the relevant public bucket. You can do so by specifying the new domain as the value of the 'Website-Redirect-Location' key within the 'Metadata' section under the Properties tab.","correct":false},{"id":"62281fa5145f01f72a905dbfbb919615","text":"Amazon S3 static website hosting supports only redirects to other AWS S3 buckets but not to external URLs. Therefore, you should set up a redirect to a new bucket with a single HTML file in it that uses client-side scripting (window.location.ref and a 'refresh' http-equiv meta tag) for the redirect to the new domain.","correct":false}]},{"id":"9cfdc945-a7e0-479c-9bcc-973f9ebfd7dc","domain":"CostOptimized","question":"You want to set up 2 CloudWatch alarms in addition to the 6 you already have to monitor your cloud environment. It has been 13 months since you created your AWS account, and you want to avoid being charged for creating the alarms. What should you do?","explanation":"Upon signing up for an AWS account, you will get a range of service usage that will never cost you anything. Such offers include 10 alarms with CloudWatch. That’s why creating the alarms is the correct answer. Contacting support is wrong because it’s not necessary to request a service increase limit. Avoiding creating new alarms is also wrong because there’s no term limits on the CloudWatch alarm offer; it’s always free. Creating more alarms in this case is still free of charge because the free offer is limited to 10 CloudWatch alarms.","links":[{"url":"https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsm.page-all-free-tier=1","title":"AWS Free Tier"}],"answers":[{"id":"a336fb6001d622e126c7d02da7ab218f","text":"Go ahead and create the alarms; CloudWatch alarms are always free of charge, regardless of number.","correct":false},{"id":"222fcbee1f68dae67b4406597659a622","text":"Contact AWS Support for a service increase limit.","correct":false},{"id":"3e9a7ae3a0c82f4aaf031d7400a3774f","text":"Do not create the alarms; you will be charged, since you get a maximum of 10 alarms with CloudWatch for the first 12 months after your account sign-up.","correct":false},{"id":"d8991b206aa9d731ab999eddedf26d6e","text":"Go ahead and create the alarms; you can have up to 10 CloudWatch alarms without being charged.","correct":true}]},{"id":"5d18ab25-729c-46ea-aa11-66386391c0cd","domain":"SecureSolutions","question":"One of your junior developers needs access to an Elastic Load Balancer in your custom VPC. This is the first and only time he will need access to AWS services. Which of the following choices is the most secure way to grant this access?","explanation":"It's always best practice to grant users access via IAM roles and groups. In this case, there's no sense in adding him to a group that may have more permissions than he requires to do his job. Add an individual user with the minimum permissions required.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html","title":"Creating an IAM User"}],"answers":[{"id":"80e0b9e929eb83fd3841f5b6395082f3","text":"Let them log in with Admin credentials and change the Admin password when he is finished.","correct":false},{"id":"1b924bbbc14b974d21d136baae3e2618","text":"None of these","correct":false},{"id":"625d4a006020f25fe043418f2e83fab2","text":"Add that developer to a Group with the requisite access.","correct":false},{"id":"5b7444663d6c0d306878b7c2959e8297","text":"Create a new IAM user with only the required credentials.","correct":true}]},{"id":"f1715a54-ef4a-4912-9093-e8e36698b0c9","domain":"CostOptimized","question":"Your company needs to run several monthly workloads that will each take several hours to complete. Although critical, these workloads can be stopped and restarted without adversely affecting the outcome of the job. Which pricing model would you use to deliver the most economical solution?","explanation":"Spot instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html","title":"About Spot Instances"}],"answers":[{"id":"de53d38fe38e0fce729f15c292a59891","text":"Free-Tier Instances","correct":false},{"id":"c658c72ec41cc513ad91a3f3e6d2c060","text":"On-demand Instances","correct":false},{"id":"29068f6044e3fedf44165e646a2d2bb1","text":"Reserved Instances","correct":false},{"id":"026db7f2265667575c05772f32778b8a","text":"Spot Instances","correct":true}]},{"id":"66262de0-506e-4e56-901b-e49a60aa0c6d","domain":"ResilientDesign","question":"You are testing an application that uses EC2 instances to poll an SQS queue. At this stage of testing, you have verified that the EC2 instances can retrieve messages from the queue, but your coworkers are complaining about not being able to manually retrieve any messages from the queue from their on-premises workstations. What is the most likely source of this problem?","explanation":"Short polling may fail to retrieve messages sometimes, but if no messages can be retrieved after multiple attempts, permissions are the more likely cause.","links":[{"url":"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-authentication-and-access-control.html","title":"Authentication and Access Control for Amazon SQS"}],"answers":[{"id":"38672a9de56ddbc170a51a046d074146","text":"Short polling is occasionally leaving messages behind.","correct":false},{"id":"574a8d560b8f2dd210703546008b8c64","text":"SQS queues accept traffic only from within AWS.","correct":false},{"id":"6536a818dab64143ae0178e357e5e841","text":"Your coworkers do not have permission to access the SQS queue.","correct":true},{"id":"d16cb376900ea9c06c68759d668824d1","text":"It's not possible to poll an SQS queue manually.","correct":false}]},{"id":"39236967-6c02-4608-9ddb-9ae5cb590e7f","domain":"CostOptimized","question":"You purchased a reserved instance for hosting your website with a term of one year as this has significant cost savings compared to on-demand instances. What happens to this instance after one year?","explanation":"The reserved instance type has significant cost benefit when compared to the on-demand instance type when purchased in advance with one or three years term. Reserved instances are not physical instances, but rather a billing discount applied to the use of on-demand instances in the account you own. Reserved instances do not renew automatically; when they expire, you can continue using the EC2 instance without interruption, but you are charged on-demand rates.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html","title":"Amazon Elastic Compute Cloud - Reserved Instance Type"}],"answers":[{"id":"c97c0ee2fa55e86eeebafc57a4831b3a","text":"The reserved instance will be terminated automatically after one year, with a termination warning notice.","correct":false},{"id":"c412194739de1b9b17bd8734ab35aaa5","text":"The reserved instance will be shutdown automatically after one year and a 2 weeks of notice will be provided by AWS to either renew or terminate.","correct":false},{"id":"4c1a861266ab02cfae787e36ebfe54b4","text":"The EC2 instance continues to run without interruption, but the instance is billed at the on-demand rate.","correct":true},{"id":"85f13ce18baa70ee478fe656e5b02ca6","text":"The reserved instance will renew automatically if the auto-renew option is set to true.","correct":false}]},{"id":"424b1344-ed2d-45df-8705-ccc54d235d1e","domain":"ResilientDesign","question":"You've been storing social media post information about your company's products for a little over two years. Most of the information is frequently used by the marketing department. The raw posts and some metadata are stored in an Amazon Aurora database. One day, the query application begins returning error messages. Marketing department users tell you they need the application available immediately for work on a campaign that's launching next week. Upon investigation you find that database storage has grown to the 64 TB limit for Aurora. What will be the most expeditious way to solve this issue?","explanation":"You can use AWS Database Migration Service (DMS) to migrate data from Aurora to S3 in CSV format, which can then be queried by Amazon Athena if needed. Creating a case with AWS Support won't resolve this issue since the Aurora storage limit is not an adjustable quota. Using the Aurora Global Database feature will not increase the maximum storage limit for a single database. While an EMR cluster may be a better solution for this use case, there is not time to perform such a migration in the time frame required by the marketing department","links":[{"url":"https://aws.amazon.com/rds/aurora/","title":"Amazon Aurora"},{"url":"https://aws.amazon.com/dms/","title":"Amazon Database Migration Service"},{"url":"https://aws.amazon.com/blogs/database/archiving-data-from-relational-databases-to-amazon-glacier-via-aws-dms/","title":"How to archive data from relational databases to Amazon Glacier using AWS DMS"}],"answers":[{"id":"98787ed0461b156ab50abac0b75e22c4","text":"Use AWS Database Migration Service to move the least accessed data to Amazon S3","correct":true},{"id":"e9351ebb109e6e0e2d29eaa03a931116","text":"Open a case with the AWS Support Center to increase the Aurora database's storage quota","correct":false},{"id":"deb5ee8cd76c99a18d1c712fa2fe45a5","text":"Migrate the database to an Amazon EMR cluster and use data mining tools for analyses going forward","correct":false},{"id":"40f74ae9c39738abdadc3558462e1b39","text":"Turn on the Aurora Global Database feature and distribute part of the data to another AWS Region","correct":false}]},{"id":"a5e7d141-7d5e-4a87-a7e1-552203993c3e","domain":"SecureSolutions","question":"Your data warehousing company has a number of different RDS instances. You have a medium size instance with automated backups switched on and a retention period of 1 week. One of your staff carelessly deletes this database. Which of the following apply.","explanation":"Under normal circumstances, all automatic backups of an RDS instance are deleted upon termination. However, it is possible to  can create a final DB Snapshot upon deletion. If you do, you can use this DB Snapshot to restore the deleted DB Instance at a later date. Amazon RDS retains this final user-created DB Snapshot along with all other manually created DB Snapshots after the DB Instance is deleted.You can now retain Amazon RDS automated backups (system snapshots and transaction logs) when you delete a database instance. This allows you to restore a deleted database instance to a specified point in time within the backup retention period even after it has been deleted, protecting you against accidental deletion of data.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_DeleteInstance.html","title":"Deleting a DB Instance"},{"url":"https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-rds-automated-backups-can-now-be-retained/","title":"Amazon RDS Automated Backups Can Now Be Retained After Database Deletion"}],"answers":[{"id":"ee57cc2e6614ff0630f78349f8c1188d","text":"If the option is not enabled to retain them, the automatic backups are deleted when the instance is deleted.","correct":true},{"id":"201a44f070a04c2e9099ed1fdca9a346","text":"A final snapshot MAY have been created when the instance was deleted, depending on whether the 'SkipFinalSnapshot' parameter was set to 'False.'","correct":true},{"id":"e380a1c948f0262ff32fbc83f5ca8020","text":"A final snapshot will be created upon deletion automatically.","correct":false},{"id":"a3f7b42bb25bb66def213ba0f0db182c","text":"The automated backups will be retained for 2 weeks and then deleted after the 2 weeks has expired.","correct":false}]},{"id":"4053d5c2-356f-4851-a9af-540b7131076d","domain":"SecureSolutions","question":"You have just started creating a new AWS environment for your organisation to use. Architecturally, this environment consists of 2 VPCs which are peered (VPC A and VPC B), plus some EC2 instances in each VPC. These instances are residing in the private subnet of each VPC and do not have an EIP or public IP attached. The EC2 instances can communicate with each other across the VPCs, however you realise that you have forgotten to deploy the required infrastructure to allow the instances to download updates from the internet. Which of the following would be the most cost effective method for enabling downloads from the internet?","explanation":"There are several elements to this question, so the easiest way to approach it is tackle them one by one. Straight away, two of the options can be eliminated - as the instances are on private IPs, any solution that doesn't include a NAT Gateway will not work. The choice between the two remaining options boils down to whether or not the NAT & Internet Gateway can be shared between two VPCs - to which the answer is no. VPC Peering does not support transitive peering (where traffic \"passes through\" the peered VPC on its way to its final destination). This leaves the correct answer as deploying 2 x IGWs, 2 x NGWs and having the instances use the NGW in the same VPC as them. Out of all the solutions listed it may be the most expensive, but it is the only one that will work.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html#nat-%20gateway-basics","title":"NAT Gateway Basics"}],"answers":[{"id":"dbd0ade9a841a5d0baa2108b3ff1fdfb","text":"Setup an Internet Gateway in VPC A, as well as a NAT Gateway in VPC A. Route the internet-bound traffic from instances in VPC A to the NAT Gateway in VPC A, and route the internet-bound traffic from instances in VPC B to the NAT Gateway in VPC A","correct":false},{"id":"71d4b3e59f026b6bfdf37a7cc5a8d8a2","text":"Setup an Internet Gateway in each VPC, then deploy a NAT Gateway in each VPC, routing traffic to the Internet Gateway contained in the same VPC. Internet-bound traffic from instances in each VPC to be routed to the NAT instance in the same VPC","correct":true},{"id":"cff5aae8f4630fbe7325c4dfed52b427","text":"Setup an Internet Gateway in VPC A, and route internet-bound traffic from the instances in both VPCs to the Internet Gateway in VPC A","correct":false},{"id":"b8fb1ee1585a8be8a92cfdc46ede67f5","text":"Setup an Internet Gateway in each VPC, and route internet-bound traffic from the instances to the Internet Gateway in the same VPC","correct":false}]},{"id":"4eb40e8a-8251-44ca-a55f-7a2d2e39a78c","domain":"CostOptimized","question":"A large company is running multiple Amazon EC2 and Amazon RDS services across several AWS Regions. You are an AWS consultant and the company approaches you to provide recommendations on how to reduce operational cost without any major changes. The company confirms that certain instances are required to be run only during business hours from 8AM to 6PM on weekdays and can be shutdown on weekends and non-business hours. Which of the following automated solutions best matches the requirements?","explanation":"AWS offers infrastructure on demand so that customers can control their resource capacity and pay only for what they consume. One simple method to reduce costs is to stop resources that are not in use, and then start those resources again when their capacity is needed. The AWS Instance Scheduler is a simple AWS-provided solution that enables customers to easily configure custom start and stop schedules for their Amazon EC2 and Amazon RDS instances. The solution is easy to deploy and can help reduce operational costs for both development and production environments. Customers who use this solution to run instances during regular business hours can save up to 70% compared to running those instances 24 hours a day. AWS Auto Scaling is not a correct solution as auto-scaling groups can contain Amazon EC2 instances from multiple Availability Zones within the same Region and cannot contain instances from multiple regions. As the company confirms that the instances are required to be run during Business hours, Spot Instance is not a good choice as spot instances may be terminated if the spot price is higher than the bid price. Also, moving AWS Instances to lesser configurations is neither an automated solution nor guarantees saving operational cost if run 24 hours.","links":[{"url":"https://docs.aws.amazon.com/solutions/latest/instance-scheduler/overview.html","title":"AWS Instance Scheduler"}],"answers":[{"id":"f657bd5509ffc79a6cb033747ff52f50","text":"Move AWS instances to lesser configuration Instance Type","correct":false},{"id":"c9eccc5399c22049703ce9be447d23d7","text":"AWS Instance Scheduler","correct":true},{"id":"f4bf61a51bd424cab4d9129fd4f2ef6a","text":"Move Instances to Spot Instances","correct":false},{"id":"2f766b7c3ac605171e839f447d7e239c","text":"AWS Auto Scaling","correct":false}]},{"id":"056f9437-f1cc-46b7-948d-b824e4165927","domain":"CostOptimized","question":"What is a spot block?","explanation":"Spot instances with a specified duration are called spot blocks and are designed not to be interrupted and will run continuously for the desired duration. This is ideal for jobs that take a defined time to complete, such as batch processing, encoding and rendering, modeling and analysis, and continuous integration.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html","title":"Spot Instance Request"}],"answers":[{"id":"668cf201cc25653157e381211eb4f454","text":"A limit on the number of spot instances per AWS availability zone within an account.","correct":false},{"id":"630182c415a3baf017c6f8feb47653ff","text":"A number of spot instances that are launched to meet the target capacity specified.","correct":false},{"id":"7845bf17391493566afba67b5887284a","text":"A limit on the number of spot instances per AWS region within an account.","correct":false},{"id":"dfc941b3f66764f082aa64b118e967e2","text":"Spot instances that run for the desired duration without interruption.","correct":true}]},{"id":"9e8fac4c-6095-468a-bc93-9a274047c7ad","domain":"SecureSolutions","question":"Your mobile app needs to have images uploaded to S3. You want to bypass the existing web server for the uploads to avoid increasing load on the server. How can this be accomplished?","explanation":"All objects and buckets by default are private. The pre-signed URLs are useful if you want your user/customer to be able to upload a specific object to your bucket, but you don't require them to have AWS security credentials or permissions.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html","title":"Uploading Objects Using Pre-signed URLs"}],"answers":[{"id":"1768d724dcf884a855c2889522437fa6","text":"Use Pre-Signed URLs to upload the images.","correct":true},{"id":"83e87dae98de34e44fb616d962301f0d","text":"Create a second S3 bucket and use Lambda to sync the files to the primary bucket.","correct":false},{"id":"5f088b72412d6935807479e2840f0b16","text":"Use ECS Containers to upload the images.","correct":false},{"id":"8bf8469577529a39055c2f1b9a301286","text":"Upload the images to SQS and then push them to the S3 bucket.","correct":false}]},{"id":"609792c0-0acd-4d90-bf9b-ffa07ae1169b","domain":"Performant","question":"You are working on a research project for a healthcare insurer and your first task is to ingest 6 months of trial data collected by about 30 participating physicians around the country. Each data set is about 15 GB in size and contains protected health information. You are proposing to use S3 Transfer Acceleration for the data upload to an S3 bucket but a colleague raises some concerns about that. Which of the following statements are valid?","explanation":"S3 TA supports all bucket level features including multipart uploads. AWS has expanded its HIPAA compliance program to include Amazon S3 Transfer Acceleration as a HIPAA eligible service. In general; if there are recurring transfer jobs, and there is more than 25Mbps of available bandwidth, and it will not take more than a week to transfer over the Internet, S3 Transfer Acceleration is an acceptable option.","links":[{"url":"https://aws.amazon.com/s3/faqs/#s3ta","title":"Amazon S3 Transfer Acceleration FAQ"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html","title":"Amazon S3 Transfer Acceleration"}],"answers":[{"id":"b3af6fb0e6c3b7dc2e9d45ba6bc1a248","text":"The name of your bucket used for Transfer Acceleration must be DNS-compliant and must not contain periods ('.').","correct":true},{"id":"29083190847888332f9f5aa597e4f6ac","text":"It will take a long time because S3 Transfer Acceleration does not support all bucket level features including multipart uploads.","correct":false},{"id":"1ed0b88f3fb04b507c5e45d1323ac07d","text":"Because S3 Transfer Acceleration is not a HIPAA eligible service, you can't use it to transfer protected health information between the physicians and your Amazon S3 bucket.","correct":false},{"id":"d157554e5855b150436f13134d8aad6f","text":"Most physicians have only about 40 to 50Mbps of available bandwidth. S3 Transfer Acceleration is therefore not a good option.","correct":false}]},{"id":"bdfff765-ad59-45a9-9e3c-605a3d2ad9d7","domain":"ResilientDesign","question":"You have been asked to set up an EFS storage solution for a project team.  Which of the following tasks do you need to complete ?","explanation":"It is necessary to set up the bi-directional network permissions, normally with Security Groups. You will connect the EFS Target to your EC2 instance with a 'mount' statement. You do not need to stipulate the size or format the volume. AWS provide a nominally unlimited file system ready for you to use.  As normal under the shared security model AWS will ensure that the EFS system is secure, but you are responsible for the access control security inside the EFS file space provided to you.","links":[{"url":"https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html","title":"EFS - How It Works"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/limits.html","title":"EFS limits"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/security-considerations.html","title":"EFS Security"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-create-security-groups.html","title":"EFS Security Groups"},{"url":"https://docs.aws.amazon.com/efs/latest/ug/wt1-getting-started.html","title":"Mounting and EFS target"}],"answers":[{"id":"9bc552892ca2c2c5be9e7c352fc0cdc8","text":"Configure a Security Group to allow data traffic on port 2049 to connect to the EC2 server.","correct":true},{"id":"7f41c759c946659e0ef49f87f6684503","text":"Set Linux file system permissions on the presented EFS volume using 'chmod' and 'chown'.","correct":true},{"id":"8388b16ddec8dec25d6caba4dbe7f8cb","text":"specify and provision disk capacity on the EFS system using 'fdisk' and 'mkfs -t xfs'.","correct":false},{"id":"417164507c199eb8b0fb1daa3bae285c","text":"Configure a Security Group to allow data traffic on port 2049 to connect to the EFS target","correct":true},{"id":"30f2e62c04d187f6e6f58e730e462680","text":"Configure a Security Group to allow admin traffic on port 22 to connect to the EFS system.","correct":false},{"id":"9a0eabcd19e62a99932848808a473c0f","text":"mount EFS vol to your EC2 instance using 'mount -t nfs -o xxxx '.","correct":true}]},{"id":"5228f828-624d-4fc6-998c-c06c2d0d685b","domain":"SecureSolutions","question":"You need to restore an object from S3-Glacier. Which of the following will help you do that?","explanation":"When discussing GLACIER it is important to distinguish between the storage-class 'Glacier' use by S3, and the 'S3-Glacier' service.  The 1st is managed via the 'S3' console & API, and the 2nd the 'S3-Glacier' console & API.  The Amazon 'S3' service maintains the mapping between your user-defined object name and Amazon Glacier system-defined identifier. These objects are not accessible via the 'S3-Glacier' service.  Objects that are stored using the 'S3-Glacier' service are only accessible through the Amazon 'S3' CLI or APIs. ","links":[{"url":"https://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html","title":"What Is Amazon S3 Glacier?"},{"url":"https://docs.aws.amazon.com/cli/latest/reference/glacier/initiate-job.html","title":"Restoring S3-Glacier objects with CLI (glacier)"},{"url":"https://docs.aws.amazon.com/amazonglacier/latest/dev/api-initiate-job-post.html","title":"Restoring S3-Glacier objects with API (POST)"}],"answers":[{"id":"b2f31642b1d69fc886a2b9d175e65fcd","text":"Using the S3 sub-command from the AWS CLI","correct":false},{"id":"4896195ad6b3c8d2e0061f7df703cc2c","text":"Using the S3 REST API","correct":false},{"id":"3ff157bf0479ce95fdeaf68388b55232","text":"Using the AWS s3-Glacier Console","correct":false},{"id":"95668976125050f1b25d5a3b893d912c","text":"Using the Glacier API","correct":true}]},{"id":"8a4cd8a8-183f-11ea-8d71-362b9e155667","domain":"CostOptimized","question":"Your company recently expressed interest in upgrading to an AWS Support plan that provides infrastructure event management and incident response for the launch of a business-critical application. Which of the following support plans will satisfy your company's requirements?","explanation":"Each AWS account comes with Basic Support, so Basic is not the answer. Either the Business or the Enterprise plan grants access to AWS Infrastructure Event Management, the program your company needs for assistance with launching the application. The Enterprise plan, however, provides up to a 15-minute response time if the business-critical application goes down; the Business plan does not offer this option. In addition, the Busines Plan does not include Infrastructure Event Management - you need to pay an additional fee for this. Therefore the Enterprise plan is the only option that provides the appropriate level of incident response and infrastructure event management required.","links":[{"url":"https://console.aws.amazon.com/support/plans/home?#/","title":"AWS Support Plans"},{"url":"https://aws.amazon.com/premiumsupport/programs/iem/","title":"AWS Infrastructure Event Management"}],"answers":[{"id":"672caf27f5363dc833bda5099775e891","text":"Developer","correct":false},{"id":"d6e6cb19e3b9c02f89d6cd54cfa7c613","text":"Business","correct":false},{"id":"a0f27638260642afb474516cc918407e","text":"Enterprise","correct":true},{"id":"972e73b7a882d0802a4e3a16946a2f94","text":"Basic","correct":false}]},{"id":"bc814aa4-03e8-4a64-aa0e-84f22d812a95","domain":"SecureSolutions","question":"Which of the following DNS record types does Route 53 not support?","explanation":"Route 53 is a scalable and highly available DNS service and it currently supports 13 different DNS record types including; AAAA, CNAME and SPF.  However, Route 53 does not support DNSSEC (other than during domain registration) and therefore any DNSSEC related records, such as DNSKEY, are also not supported.","links":[{"url":"https://aws.amazon.com/route53/faqs/","title":"Amazon Route 53 FAQs"}],"answers":[{"id":"adc4bfdb0829dae99e3699393e3fbaa4","text":"CNAME","correct":false},{"id":"548deb43a9afe4abcde34a605eb44700","text":"DNSKEY","correct":true},{"id":"b4efb35349e5d93905531be07dbacd6d","text":"SPF","correct":false},{"id":"098890dde069e9abad63f19a0d9e1f32","text":"AAAA","correct":false}]},{"id":"a2470754-2333-11ea-978f-2e728ce88125","domain":"SecureSolutions","question":"As a monitoring and security measure, you want to create an alarm to trigger when there is a failed attempt to log into the company’s AWS account. Which of the following services will enable you to do that?","explanation":"CloudWatch enables you to set an alarm for any event deemed notable. In this situation, you want to be alerted of any possible unauthorized attempts to access your company’s cloud infrastructure. CloudTrail is used to capture all API actions within your environment (audit), IAM is used to manage users and control what actions can be taken against resources, and Systems Manager is used to centrally manage your data and services.","links":[{"url":"https://aws.amazon.com/cloudwatch/","title":"Amazon CloudWatch"}],"answers":[{"id":"26ca9b29af36f84c577a85a80a58a381","text":"AWS CloudTrail","correct":false},{"id":"4b88d56bd65ef92611ac63064c38369f","text":"Amazon CloudWatch","correct":true},{"id":"a86087964fb00f6ae81475d2c8c3c40c","text":"AWS Identity and Access Management (IAM)","correct":false},{"id":"113b1ad9ce6cdc3a37ad8475bc9bb2b2","text":"AWS Systems Manager","correct":false}]},{"id":"eba390fe-7699-47c0-b51f-f38cbc948112","domain":"CostOptimized","question":"What is the 'first-byte' latency when retrieving data from Glacier?","explanation":"You should expect data retrieval latency of 3-5 hours when retrieving data from Glacier.","links":[{"url":"https://aws.amazon.com/glacier/faqs/#dataretrievalpolicies","title":"Glacier Data Retrieval Policies"}],"answers":[{"id":"99d888e6893ac480abd21ecb8a18e060","text":"3-5 hours","correct":true},{"id":"b86a4270946442f3b17bd51e3aa226ce","text":"> 5 hours","correct":false},{"id":"72ab9d0304d3e84c6aa2dd15eda282f2","text":"1 hour","correct":false},{"id":"6619a70f8ab8877e4131643fb8ded723","text":"2 hours","correct":false}]},{"id":"096baab8-06c7-4b07-8e86-ba304b41102f","domain":"CostOptimized","question":"After migrating an application architecture from on-premise to AWS, you will not be responsible for the ongoing maintenance of which two of the following services.","explanation":"DynamoDB and Amazon RDS are managed services. As such, AWS handles the ongoing maintenance.","links":[{"url":"https://aws.amazon.com/rds/details/","title":"About RDS"},{"url":"https://aws.amazon.com/dynamodb/details/","title":"About DynamoDB"}],"answers":[{"id":"81b22456f78954c460ce2f531b5e048f","text":"EC2","correct":false},{"id":"3d6cbd7db2a4fa389808ea6f4a5fc1bc","text":"Elastic Beanstalk","correct":false},{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":true},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true}]},{"id":"ed03b04d-9536-4612-9418-842b35993276","domain":"SecureSolutions","question":"The various components of a web application are supposed to reside in a VPC that is only accessible by a designated System Administrator. Which of the following configurations will enable the SA to communicate with the EC2 Linux web servers that house the application using the Command Line Interface?","explanation":"You will have to go to select the security group associated with the VPC that contains the application’s components, click 'Edit inbound rules', and add a rule with type of protocol set to 'SSH', source of the traffic set to 'Custom', and the IP address of the administrators computer entered in the field next to the Source dropdown menu. Although each of the other responses have the right type of rule (inbound), the other configurations do not specifically grant the SA access to the application. An inbound rule for TCP communications could have been the answer, since SSH communication uses the TCP protocol. However, it is not specifying the type of TCP protocol for the SA, which is SSH. RDP is a Microsoft technology for remote desktop connections and HTTPS is for secure access to websites.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html","title":"Authorizing Inbound Traffic for Your Linux Instances"}],"answers":[{"id":"4c1f2abb1661d2c12fdd21f3d81d9a96","text":"Add an inbound rule in the security group associated with the VPC for TCP communication with an IP address range that includes that of the SA.","correct":false},{"id":"b3a4bdb6ea7e8979965c11c9b05752a0","text":"Add an inbound rule in the security group associated with the VPC for HTTPS communication.","correct":false},{"id":"999d6b54e6a87abb29b94b7170c79e40","text":"Add an inbound rule in the security group associated with the VPC for RDP communication with an IP address range that includes that of the SA.","correct":false},{"id":"097576029cd34dee2612943b1856928a","text":"Add an inbound rule in the security group associated with the VPC for SSH communication with the IP address of the administrators computer.","correct":true}]},{"id":"9d65d585-d756-42e0-83bb-d8a7f4c83e82","domain":"ResilientDesign","question":"You need to take a snapshot of an EBS volume.  You are concerned about the volume and instance becoming unavailable until the snapshot is complete.  Which of these statements best describe the facts that will allow you to assess the duration of the outage?","explanation":"in General terms a snapshot has two parts; the snapshot catalogue, and the copy off of the data.  Sometimes called 'the snapshot'.  During the catalogue phase all changed files and blocks are catalogued, locked, and a change log is started.  This is a relatively fast process for most disk file systems and is the only part of the process during which the disk cannot be accessed.  the 2nd phase is the slow part during which the data is copied to the backup system.  The system is not locked during this phase. The actual duration when the system is unavailable is most closely related to how many files or blocks have changed since the last backup as this is the only portion of the data that is relevant to the incremental backup (snapshot).","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html","title":"Amazon EBS Snapshots"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html","title":"Creating an Amazon EBS Snapshot"},{"url":"https://help.acloud.guru/hc/en-us/search?utf8=%E2%9C%93&query=ebs+snapshot","title":"EBS Snapshot KBs"},{"url":"https://aws.amazon.com/blogs/aws/new-lifecycle-management-for-amazon-ebs-snapshots/","title":"New – Lifecycle Management for Amazon EBS Snapshots"}],"answers":[{"id":"6e2f1ca114ad32c09d2c423450ab61e5","text":"The duration of the outage is determined by the size of the server.","correct":false},{"id":"3ac42d808544b4ab0806225881c0e525","text":"The duration of the outage is determined by the number of files changed since the last backup.","correct":true},{"id":"0462d5d8fc0d1947c3ed97376ea2ceba","text":"The duration of the outage is the time it takes to copy all the files from the disk to the backup.","correct":false},{"id":"26583a3f6db686ba6c478f8ee3badf00","text":"The duration of the outage is determined by the age of the server.","correct":false},{"id":"071b64034823256c8ea3bb0048764787","text":"The duration of the outage is only related to the initial cataloguing phase.","correct":true},{"id":"c18929ab2ad2fc22f4c285962aaa39c8","text":"The duration of the outage is determined by the number of files on the disk.","correct":false},{"id":"aae5747a54a5f12a114ff2d92fd668c7","text":"The duration of the outage is determined by the number of files changed since the server was commissioned.","correct":false}]},{"id":"cd86c07d-478b-4dc0-8e79-4b61bbdfc377","domain":"CostOptimized","question":"An online tutoring company used AWS Organizations to create an organization to consolidate and manage its AWS accounts. The organization has three accounts called production, development, and testing. As per company policy, all accounts use identical Linux EC2 instances that are in the same region and configured with default tenancy. The production account has seven Reserved regional EC2 instances to support its production environment. Last month, the testing account used two On-Demand EC2 instances running 24/7, while the development account used three On-Demand EC2 instances running 24/7. Because of a decrease in usage last month, the production account used only five of its reserved EC2 instances. How is the organization billed for EC2 instance usage last month?","explanation":"Unused Reserved EC2 instances can offset the cost of equivalent On-Demand EC2 instances. The higher cost of two On-Demand instances is replaced by the lower cost of two unused Reserved instances. Additionally, because AWS Organizations is enabled consolidated billing is enabled as well.","links":[{"url":"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-reservation-models/consolidated-billing.html","title":"Reserved Instances and AWS Organization"}],"answers":[{"id":"34d273571ee761d7887b5dc69462e039","text":"The company is billed for five Reserved instances and, none of the On-Demand instances since this is covered by the excess of Reserved instances.","correct":false},{"id":"b0feba15d706ec58daef8b79049aa2b9","text":"The company is billed for seven Reserved instances and three On-Demand instances.","correct":true},{"id":"bc2784c0ecdc81e04ad8ad582c89f961","text":"The company is billed for seven Reserved instances and all five On-Demand instances because consolidated billing is not enabled.","correct":false},{"id":"c9b739a791ede1c8bea892ec7326479e","text":"The company is billed for seven Reserved instances and two On-Demand instances.","correct":false}]},{"id":"9ac6fef3-785c-4655-a261-4e04f106fd58","domain":"ResilientDesign","question":"Your application has a global user base, and in order to improve user experience you have deployed an instance in both the us-east-1 and eu-central-1 regions. Which routing policy would you use to ensure users get the best experience possible?","explanation":"Latency Based routing will use data on the latency between the users' location and the location of your services in AWS to return the address which has the lowest latency for the user that will result in an improved user experience. Although Geolocation or Geoproximity could be used to route users to instances closest to them, latency will change over time with changes to network connectivity and routing over the internet, and the closest location may not be the one with least latency. Weighted routing will not help in this scenario.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy"}],"answers":[{"id":"bd2b592da6667c27766101b6983ce1ad","text":"Geoproximity Routing","correct":false},{"id":"77e6d59cd5033b8199d2472da9ed05c9","text":"Weighted Routing","correct":false},{"id":"cd88dcd797ede7375ca4f73d841fb6ca","text":"Geolocation Routing","correct":false},{"id":"2698e08f63eed21d0ba0f3889c42f1cf","text":"Latency-based Routing","correct":true}]},{"id":"067a4664-2e5b-11ea-978f-2e728ce88125","domain":"Performant","question":"As a Solutions Architect employed at a niche clothing and accessories company, you are assigned the task of figuring out how to record requests made to S3 buckets that contain the images. The Director of Marketing Strategy needs that information to help her understand the company’s customer base and help her shape her marketing strategy in the coming months. Which of the following features should be enabled to record this information?","explanation":"Server access logging is what you should enable to record and get information on the requests made to the company’s S3 buckets. You choose object-level logging for recording object-level API activity using AWS CloudTrail, which comes with a cost. Enabling versioning would keep all versions of an object in the same bucket, and tags are for tracking project costs.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html","title":"Amazon S3 Server Access Logging"}],"answers":[{"id":"189f63f277cd73395561651753563065","text":"Tags","correct":false},{"id":"198a30920479d1d8e2509b9b59f3d700","text":"Versioning","correct":false},{"id":"30054ccc9b587d53668a89942396874c","text":"Server access logging","correct":true},{"id":"f87d839acefa86eff3c90b1b44be4aba","text":"Object-level logging","correct":false}]},{"id":"daa2b1cf-f863-4a75-9b5c-bcd93b27ea18","domain":"Performant","question":"The users of your company's sales lead tracking system are reporting slow response times. Upon investigation, you find that the application's Amazon RDS MySQL database has become memory constrained due to increased transaction volumes. Most of the application's recent activity involves writing new lead records and updating existing lead records. How will you best scale the database to satisfy the increase in business opportunities?","explanation":"You can scale the compute resources for an RDS database by migrating to a larger DB Instance class through the Management Console, the RDS API, or the AWS CLI. RDS currently doesn't support horizontal DB Instance scaling, but Amazon Aurora offers multi-master capabilities. Read Replicas will probably not solve the slow response time issue since most of the application transactions involve write activity. Sharding data across multiple RDS databases will increase operational maintenance.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html","title":"Choosing the DB Instance Class"}],"answers":[{"id":"22072b591b511c34f793b93c492a2a67","text":"Add another compute instance to scale the RDS MySQL cluster horizontally","correct":false},{"id":"d93c105d76b90210dd755f35daf0995b","text":"Create another RDS MySQL database and shard the data between the two databases","correct":false},{"id":"885fbaedd6d0804dc01bca26777afada","text":"Deploy RDS Read Replicas and redirect read transactions to the replicas to offload the primary RDS instance","correct":false},{"id":"9b0d0b1a68e122afa3452e2820fdf2e7","text":"Modify the RDS DB Instance class to vertically scale the primary instance's CPU and memory allocation","correct":true}]},{"id":"40c8ad54-f968-412c-a7e0-9732db1d93ae","domain":"Performant","question":"You have a small database workload with infrequent I/O. Which storage medium would the most cost-effective way to meet these requirements?","explanation":" The question is specific that you are evaluating for RDS. Cold Storage is not a valid option for RDS. of the three valid types for RDS, Magnetic is still the cheapest","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html","title":"RDS Storage Types"},{"url":"https://aws.amazon.com/rds/pricing/","title":"RDS pricing"},{"url":"http://calculator.s3.amazonaws.com/index.html#s=RDS","title":"AWS pricing calculator"}],"answers":[{"id":"e774a96af9adfaa2402148c9828271e7","text":"Amazon RDS Magnetic Storage","correct":true},{"id":"4d548ff7278077c929b63c67f768cc97","text":"Amazon RDS Provisioned IOPS (SSD) Storage","correct":false},{"id":"a257df0abf37db5da9b921f8222eab2a","text":"Amazon RDS General Purpose (SSD) Storage","correct":false},{"id":"0db6de47eed9671e44098c2117fe8916","text":"Amazon RDS Cold Storage","correct":false}]},{"id":"f83080c3-ee9d-4ca9-ada5-f94b6642d2f2","domain":"CostOptimized","question":"Your company is running an older version of Windows on employees' desktops/laptops which will be going off of mainstream support in the near future. The most current version of Windows will require a large capital investment to purchase more powerful hardware to run it. All desktops/laptops require access to the Internet as well as access to multiple business applications running on Amazon EC2 web servers in the AWS cloud. Your manager has tasked you with determining how to move the company's desktops/laptops to the most current version of Windows. Which architecture will provide the most cost effective solution?","explanation":"Amazon Workspaces provides the capability to serve virtual cloud-based desktop sessions to your desktop/laptop users (either Windows or Linux). It eliminates the need for powerful hardware, and it removes the burden of individual desktop/laptop software maintenance. AppStream is not needed to access the Internet, nor is it needed to serve the EC2 applications in this use case since a browser can be used from WorkSpaces to access the web servers. A NAT Gateway is preferred to an Internet Gateway since all traffic is initiated from the desktop/laptop, instead of from out on the Internet. WorkSpaces provides for creating an authentication directory, so creating one separately is not needed. WorkSpaces also creates an ENI for each session inherently.","links":[{"url":"https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html","title":"What is Amazon WorkSpaces?"},{"url":"https://aws.amazon.com/blogs/desktop-and-application-streaming/why-customers-are-moving-their-windows-desktops-to-the-cloud-with-aws/","title":"Why Customers Are Moving Their Windows Desktops to the Cloud with Amazon WorkSpaces"}],"answers":[{"id":"7c8f5d29af4240a4ec8c68eaa488e933","text":"Use Amazon WorkSpaces with the most current version of Windows to provide Desktop-as-a-Service capability. Use an Internet Gateway in the same VPC to provide access to the Internet. Provision an AWS Managed Microsoft AD instance and link it to your on-premises Active Directory for user authentication.","correct":false},{"id":"e33c2a9e2b05853b6c1f1bc548357068","text":"Implement Amazon WorkSpaces with the most current version of Windows to provide Desktop-as-a-Service capability. Use Amazon AppStream to provide access to the Internet and to serve the EC2 applications to the desktops/laptops.","correct":false},{"id":"7c9f90a44ccdb77a69453cc39d1fdba7","text":"Implement Amazon WorkSpaces with the most current version of Windows to provide Desktop-as-a-Service capability. Use an Internet Gateway in the same VPC to provide access to the Internet. Provision Elastic Network Interfaces in the same VPC to connect the desktops/laptops to the EC2 applications.","correct":false},{"id":"58e179301471d5f030ad1b404a5c527f","text":"Deploy Amazon WorkSpaces with the most current version of Windows to provide Desktop-as-a-Service capability. Use a NAT Gateway in the same VPC to provide access to the Internet.","correct":true}]},{"id":"15c0f4d7-ac13-40b0-98fb-2d8fd8b077ee","domain":"Performant","question":"You have been asked to advise on a scaling concern.  The client has an elegant solution that works well.  As the information base grows they use CloudFormation to spin up another stack made up of an S3 bucket and supporting compute instances.  The trigger for creating a new stack is when the PUT rate approaches 100 PUTs per second.  the problem is that as the business grows that number of buckets is growing into the hundreds and will soon be in the thousands.  You have been asked what can be done to reduce the number of buckets without changing the basic architecture.","explanation":"Until 2018 there was a hard limit on S3 puts of 100 PUTs per second.  To achieve this care needed to be taken with the structure of the name Key to ensure parallel processing.  As of July 2018 the limit was raised to 3500 and the need for the Key design was basically eliminated. Disk IOPS is not the issue with the problem. The account limit is not the issue with the problem.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/","title":"S3 Request rates - Whats new"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html","title":"S3 Request rates documentation"},{"url":"https://aws.amazon.com/s3/storage-classes/","title":"S3 Storage classes"}],"answers":[{"id":"906c60dd95d60014479c321de23c5a4c","text":"Refine the key hashing to randomise the name Key to achieve the potential of 300 PUTs per second.","correct":false},{"id":"c1ee6963e6cab066324cc7e832d9ea3d","text":"Change the trigger level to around 3000 as S3 can now accommodate much higher PUT and GET levels.","correct":true},{"id":"0196ee21bff2d4c35c191d0973220d3c","text":"Upgrade all buckets to S3 provisioned IOPS to achieve better performance.","correct":false},{"id":"b183896b9af1543c0b43e9540b244954","text":"Set up multiple accounts so that the per account hard limit on S3 buckets is avoided.","correct":false}]},{"id":"21f8c7eb-3702-4f91-8512-9cf16a864164","domain":"SecureSolutions","question":"A junior team member asks you about IAM best practices. Which of the following statements are valid recommendations?","explanation":"If you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. If you do have an access key for your AWS account root user, delete it. If you must keep it, rotate (change) the access key regularly. Roles don't have their own permanent set of credentials the way IAM users do. In the case of Amazon EC2, IAM dynamically provides temporary credentials to the EC2 instance, and these credentials are automatically rotated for you","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html","title":"IAM Best Practices"}],"answers":[{"id":"4e81fb683ffec5c6b14d5fbf8ca23943","text":"Use IAM roles instead of sharing security credentials between accounts to allow users from another AWS account to access resources in your AWS account.","correct":true},{"id":"ed1cc3d72c302de4311e139931f75f13","text":"A user in an IAM group inherits the permissions assigned to the group. Although you can define additional permissions for an individual IAM user, it can be less obvious what set of permissions is applicable to an individual user when you mix these two approaches.","correct":true},{"id":"e64a9fadc174196728442c9e207040c6","text":"It is best practice to use access keys whenever possible. If you don't already have an access key for your AWS account root user, you should create one and use it instead of your account email address and password to sign in to the AWS Management Console. Rotate the access key regularly.","correct":false},{"id":"b07e4dc1c6a4ca0e981d7980363f11f2","text":"Applications that run on an Amazon EC2 instance need credentials in order to access other AWS services. To provide credentials to the application in a secure way, use IAM roles. Roles have their own permanent set of credentials the way IAM users do.","correct":false}]},{"id":"389822e6-0bcc-403a-a13b-77aa1aa40468","domain":"SecureSolutions","question":"Which of these things is true about your AWS 'Root' account?","explanation":"The Root account is synonymous with the account. Losing control of your Root account can be a major problem. You should keep the MFA setup for all account in a safe place.  However since when you need the Root account you will not want any impediment to using it (like trying to figure out what phone the MFA was set up on). Keeping these in the safe is a good idea.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html","title":"The AWS Account Root User"},{"url":"https://docs.aws.amazon.com/general/latest/gr/root-vs-iam.html","title":"Root Account vs. IAM account"},{"url":"https://docs.aws.amazon.com/general/latest/gr/aws_tasks-that-require-root.html","title":"Tasks That Require Root account Credentials"}],"answers":[{"id":"b9c8112ab4cea328fe5235ca79532656","text":"If you forget you Root account password there is no way to reset it.  You must contact AWS support for an identity check and backend reset.","correct":false},{"id":"b70f50875018fd9ba767169eca253790","text":"The Root account cannot be denied access to resources by Policy.","correct":true},{"id":"e363630dfc1be67aea0ded87d60ad9fe","text":"There is no task that can only be done by the 'Root' account.","correct":false},{"id":"9ae1c1b1d35f3fc98c95b79d53b45d30","text":"You should keep a copy of the MFA URL or QR code when you set up the Root account MFA.","correct":true}]},{"id":"ee772b8e-1672-479d-b28d-8952d89b24f5","domain":"SecureSolutions","question":"By default, how many Elastic IP addresses are you limited to per region?","explanation":"By default, all accounts are limited to 5 Elastic IP addresses per region.","links":[{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Appendix_Limits.html#vpc-limits-eips","title":"Elastic IP Addresses - Limits"}],"answers":[{"id":"9bf31c7ff062936a96d3c8bd1f8f2ff3","text":"15","correct":false},{"id":"98f13708210194c475687be6106a3b84","text":"20","correct":false},{"id":"d3d9446802a44259755d38e6d163e820","text":"10","correct":false},{"id":"e4da3b7fbbce2345d7772b0674a318d5","text":"5","correct":true}]},{"id":"7240e60f-f337-4116-be42-8df912212cfd","domain":"Performant","question":"What is the minimum time interval granularity for the data that Amazon CloudWatch receives and aggregates?","explanation":"The minimum time interval for CloudWatch is 1 minute.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch.html","title":"CloudWatch - Detailed Monitoring"}],"answers":[{"id":"3d393dc956fea776f25db075df639cb1","text":"30 seconds","correct":false},{"id":"4adcc26418f545df76ab44c53fc55702","text":"5 minutes","correct":false},{"id":"f77eb9f1b917ba78f6eb2ce8ede0a0e4","text":"1 minute","correct":true},{"id":"70abb32fcb0c9e8194526d1eef15eb05","text":"10 minutes","correct":false}]},{"id":"f464468f-7851-4684-a8a1-ece519bc244c","domain":"Performant","question":"An online music catalog application will use Amazon DynamoDB as it's database. Catalog entries will consist of information about individual song titles. Users will query songs primarily by artist. They'll also have the ability to retrieve a list of songs by genre, and from a specific decade within each genre. How should the music catalog table be structured in order to provide the best performance for these queries?","explanation":"Since users will be given the ability to query songs primarily by artist, the DynamoDB table's Primary Key should be set up with artist as the Partition Key and song title as the Sort Key. To provide the additional capability to query by genre, and to retrieve a list of songs according to decade within genre, a Global Secondary Index with genre as the Partition Key and decade as the Sort Key will give the best query performance. An artist Partition Key in the Primary Key without a song title Sort Key will place song titles randomly on the partition, resulting in poorer query performance. Same thing with a Global Secondary Index on genre without a decade Sort Key. Local Secondary Indexes must have the same Partition Key as the Primary Key. If artist doesn't exist as the Partition Key in the Primary Key or a Secondary Index, the query on artist will be performed as a full scan rather than a direct read.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html","title":"DynamoDB Core Components"},{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html","title":"Improving Data Access with Secondary Indexes"}],"answers":[{"id":"eb1a3f607b1bbeb89019415956a59252","text":"Create a Primary Key with song title as the Partition Key. Create a Local Secondary Index with artist as the Partition Key. Create a Local Secondary Index with genre as the Partition Key and decade as the Sort Key.","correct":false},{"id":"5304348a393031193fcd240783476b14","text":"Create a Primary Key with song title as the Partition Key and artist as the Sort Key. Create a Global Secondary Index with genre as the Partition Key. Create a Local Secondary Index with decade as the Sort Key.","correct":false},{"id":"7ffad0b101f65b02826d61402fa11b74","text":"Create a Primary Key with artist as the Partition Key and song title as the Sort Key. Create a Global Secondary Index with genre as the Partition Key and decade as the Sort Key.","correct":true},{"id":"a3a841e8ef9cd63612e0a1dce7979275","text":"Create a Primary Key with artist as the Partition Key. Create a Global Secondary Index with genre as the Partition Key. Create a Local Secondary Index with decade as the Partition Key.","correct":false}]},{"id":"5a681980-2650-11ea-978f-2e728ce88125","domain":"ResilientDesign","question":"A client of yours wants a highly available and fault-tolerant website. Which of the following configurations will ensure that?","explanation":"You can use the EC2 service to launch the virtual servers necessary to host the website. Since Availability Zones are isolated locations, you can deploy at least two EC2 instances across two Availability Zones. That way, if there’s failure in one location, the other location will still be available.","links":[{"url":"https://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf","title":"Fault-Tolerant Components on AWS"}],"answers":[{"id":"04f9b1d8b355d54731f3ac9c616cc031","text":"Deploy two Relational Database Service (RDS) instances across two Availability Zones.","correct":false},{"id":"2cc00a7779a58be4cd2ab2db2ba36aab","text":"Choose a Linux Amazon Machine Image (AMI).","correct":false},{"id":"fb46b3532f6853856525d7d8c756e431","text":"Deploy two Elastic Compute Cloud (EC2) instances across two Availability Zones.","correct":true},{"id":"98b7cef716f5b9a7ece4f4ba17ed0a45","text":"Deploy two EC2 instances in one Availability Zone.","correct":false}]},{"id":"a0f81c00-4d69-4eda-ab8e-4a3c3ff8162b","domain":"ResilientDesign","question":"You have been tasked with creating a secondary copy of your production environment - this environment is located in US-EAST-1 and the copy will need to exist in US-EAST-2. Which of the below resources will need to be copied or created in US-EAST-2?","explanation":"IAM Roles and Route 53 records are global, and do not need to be created in the new region. VPCs and security groups are regional entities and therefore will need to be re-created.","links":[],"answers":[{"id":"5af83476a58c870df52d1d246574922c","text":"IAM Roles","correct":false},{"id":"452cd4369ec5aa9b91c10fb0b19f352b","text":"Route 53 records","correct":false},{"id":"d85c80578ad0849a611c1056b63e385c","text":"VPC","correct":true},{"id":"fe11e14b29ee1462bdb7764b3ba2f4bd","text":"Security Groups","correct":true}]},{"id":"f368c76f-83f3-4a1f-b29d-f844fe380353","domain":"ResilientDesign","question":"Which of the below services create entities that only exist in the region that they are created in by default?","explanation":"IAM, SNS, Route 53 and CloudFront are all global services, with in-built redundancy, these entities are available in all regions. VPC and DynamoDB store all their entities and data in a redundant fashion in the region they were created in, and EC2 stores the data and objects in the AZ it was created in. S3 is a little more tricky - although it can be seen as a \"Global\" service as data can be accessed from anywhere, that data only exists in the region that the bucket was created in by default","links":[],"answers":[{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":false},{"id":"0893147538c21c827dc499b4852934e3","text":"Route 53","correct":false},{"id":"41dff7155cc7aeb11c06434f6a450bb3","text":"IAM","correct":false},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":true},{"id":"d85c80578ad0849a611c1056b63e385c","text":"VPC","correct":true},{"id":"81b22456f78954c460ce2f531b5e048f","text":"EC2","correct":true},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true},{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false}]},{"id":"c07cbbaf-7a26-44d9-9e50-86908c1ac754","domain":"ResilientDesign","question":"A football scoreboard app uses an AWS Lambda backend to retrieve game information stored in an Amazon DynamoDB database. An EC2 instance reads multiple Amazon Kinesis streams of scores and stats and writes them to the database. Two app users sitting side-by-side at a restaurant refresh the scoreboard at the same time and get different stats for the same game. What should the app developers do to resolve this?","explanation":"DynamoDB is eventually consistent by default, and may not reflect the results of a recently completed write since data is automatically replicated across three facilities in an AWS region for durability. You can request strongly consistent reads that reflect all previous writes. Consolidating Kinesis streams probably won't help since Kineses producers are generally singular data sources (all stats of a specific type will come from one producer). Replacing Lambda with EC2 will have cost consequences, and in this case will probably result in an undesirable stateful architecture. Timestamp information will not resolve an eventual consistency issue.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html","title":"DynamoDB Read Consistency"}],"answers":[{"id":"a8da8441084c1a5b79e47be3f6f98a0d","text":"Have the Lambda function perform a strongly consistent read from the database","correct":true},{"id":"c606fec0599df4127dba2d5df85a283f","text":"Store score and stat updates timestamps in the database to ensure the most recent information is served by the Lambda backend ","correct":false},{"id":"95bc7fe6a012dc37b025c1cb8fe566e7","text":"Replace Lambda with an EC2 instance that synchronizes reads of the data with database updates","correct":false},{"id":"c151deab4b337fd8377eed0413d50903","text":"Consolidate the Kinesis streams into a single stream to avoid writing different results to the database","correct":false}]},{"id":"546c3d7f-0bb0-4de0-9442-43c0a8537511","domain":"ResilientDesign","question":"You have a web application being hosted on EC2 instances in a couple of regions. Users in certain regions have been reporting extreme slowness. How could this be architected better to improve the experience for users?","explanation":"Amazon CloudFront is a service that speeds up distribution of static and some dynamic web content.  Such as .html, .css, .js, and image files. CloudFront delivers your content through a worldwide network of facilities called edge locations. When a user requests content that you're serving through CloudFront, the user is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html","title":"What Is Amazon CloudFront?"}],"answers":[{"id":"3d41bec1360974dbd7ddf7015f834d96","text":"Add Route 53 health checks to improve the performance.","correct":false},{"id":"67b20a8653585f33002f903f06b7336d","text":"Add more EC2 instances to support the load.","correct":false},{"id":"180c5083827844575f12b2700cf184da","text":"Change the Instance Type to a higher type.","correct":false},{"id":"dbab33ab37becc0a05ea59a20c78e360","text":"Place the EC2 instance behind CloudFront.","correct":true}]},{"id":"5c841746-5aea-4a32-8a87-65b78fb0184b","domain":"ResilientDesign","question":"You want to use an AWS service that can monitor your EC2 instances and automatically recover it if it becomes impaired. Which of the following automation tools will enable you to do so?","explanation":"Using a CloudWatch alarm, you can monitor your EC2 instance and automatically recover it if it becomes impaired. Elastic Beanstalk is for deploying and managing web applications, Auto Scaling is for automatically scaling your resources up or down, and Lambda can be set up to automatically execute a function on a regular schedule. None of these tools, however, are used for auto EC2 recovery.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html","title":"Recover Your Instance"}],"answers":[{"id":"afd14eb3f151c84e6ef7ab63812b4fbc","text":"Auto Scaling","correct":false},{"id":"4b88d56bd65ef92611ac63064c38369f","text":"Amazon CloudWatch","correct":true},{"id":"bcf6eb183b7da148701bcc059a34675f","text":"AWS Elastic Beanstalk","correct":false},{"id":"dee7ff8f7b10b9d388c7c561e9413b0d","text":"AWS Lambda","correct":false}]},{"id":"1def4473-403d-432b-a5fe-4a3671e41fa8","domain":"Performant","question":"You have a huge dataset for an insurance company which is located in Amazon Redshift. The data is used by data scientists intermittently to calculate risks of particular events so that the company can charge the correct premiums. The data is also used by TV’s in the office which run fast SQL queries that calculate data such as the number of claims today and the total value of claims. This data is updated every hour, however the dashboards query the data every minute. Recently your data scientists have been complaining that their queries are taking longer and longer. What should you do to increase the performance of your redshift cluster?","explanation":"Elasticache is a purpose built cache to offload reads from database systems. This is an ideal use case for it. CloudFront is a web content cache not a database cache.  DAX is specifically designed for DynamoDB.","links":[{"url":"https://aws.amazon.com/elasticache/features/","title":"ElastiCache features"},{"url":"https://aws.amazon.com/blogs/big-data/using-pgpool-and-amazon-elasticache-for-query-caching-with-amazon-redshift/","title":"ElastiCache for Query Caching with Amazon Redshift"},{"url":"https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-redshift-introduces-result-caching-for-sub-second-response-for-repeat-queries/","title":"Redshift Introduces Result Caching"}],"answers":[{"id":"bdbc824cad7340d1baab1e1909e5f976","text":"Change the storage from Magnetic to Provisioned IOPS so as to increase the underlying storage IO.","correct":false},{"id":"63a67f6a01dc9eec3668660975e89e79","text":"Use AWS CloudFront to cache the data at global edge locations and update the origin of the CloudFront distribution to point to the Redshift Cluster.","correct":false},{"id":"7088a4438456fc4c2545c57d8a2bc2f0","text":"Configure ElastiCache cluster to store the most frequently accessed data by the dashboards and update the dashboards to query elasticache rather than Redshift.","correct":true},{"id":"99d160edacf191e386a8aad4629ceeca","text":"Enable DynamoDB Accelerator (DAX) and update the dashboards to point to the DAX endpoint.","correct":false}]},{"id":"d5e8caaa-e2d5-4d00-b9bc-53efee2b366f","domain":"Performant","question":"You have a production work load on AWS consisting of a Web Tier, Application Tier and Database Tier. Your web application starts to slow down under heavy use and can even become unresponsive. You investigate the issue and discover the issues are with the PostgreSQL RDS. What two steps could you do to improve performance?","explanation":"This is a classic scale-up or scale-out question. upgrading the disk is a scale-up solution, and adding read-replicas is a scale-out solution. Multi-AZ is possible, but will not help with performance, only resiliency.  RDS autoscaling is available only with Aurora","links":[{"url":"https://aws.amazon.com/blogs/database/scaling-your-amazon-rds-instance-vertically-and-horizontally/","title":"Scaling Your Amazon RDS Instance Vertically and Horizontally"},{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html","title":"Aurora Auto Scaling"}],"answers":[{"id":"bd7d139b54a720f913080f4bec971220","text":"Provision 3 Read Replicas and direct all read traffic to these new instances.","correct":true},{"id":"eda178658eacb51c3dcfd2f1f9e2c4c5","text":"Add multi-AZ to the RDS cluster and direct all read traffic to the secondary instance.","correct":false},{"id":"4b8062204769a36aff6ebc2fb3ffb313","text":" Turn on RDS Autoscaling and scale when CPU Utilization reaches 90% for 5 minutes.","correct":false},{"id":"ad6b88231cab9ea7642a5c16ce20722e","text":"Upgrade the storage type from General Purpose SSD to Provisioned IOPS SSD.","correct":true}]},{"id":"f3e923a2-2e1a-11ea-978f-2e728ce88125","domain":"CostOptimized","question":"You want to track the amount of money you ideally want your company to spend for EC2 data transfers every month. Which of the following actions will accomplish that?","explanation":"AWS Cost Explorer is for providing information that you can use to track and manage costs, but it doesn’t enable the creation of budgets; that’s what AWS Budgets is for. If the question was strictly addressing cost, then creating a Cost budget with AWS Budgets would have been the correct answer. However, your concern is specifically with a usage type, which is EC2 data transfers. In this case, you would need to create a Usage budget with AWS Budgets and receive alerts when your defined threshold is met.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html","title":"Managing Your Costs with Budgets"}],"answers":[{"id":"8d35bf16b7d263f4ab864e392d023e54","text":"Enable AWS Cost Explorer","correct":false},{"id":"4407cc58412e1c4727ad336bd8b8453f","text":"Create a Reservation budget with AWS Budgets.","correct":false},{"id":"e7f799a3bc73b229cd75d773a9d7f547","text":"Create a Usage budget with AWS Budgets.","correct":true},{"id":"41e006394cc745a90a25e57065b658c2","text":"Create a Cost budget with AWS Budgets.","correct":false}]},{"id":"c3fd126e-3f97-443d-8aa6-30c582c8f4f0","domain":"Performant","question":"Amazon RDS supports which of the following databases?","explanation":"Amazon RDS currently supports MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server, and Amazon Aurora database engines.","links":[{"url":"https://aws.amazon.com/rds/","title":"RDS: Supported Engines"}],"answers":[{"id":"62a004b95946bb97541afa471dcca73a","text":"MySQL","correct":true},{"id":"4e6ac8e997ca8f5896cbc28cad3ede24","text":"Sybase","correct":false},{"id":"7f9733e208088b1ce6df3d4be1765396","text":"MariaDB","correct":true},{"id":"c890515f1055143925ae4fb85b86ec70","text":"DB2","correct":false}]},{"id":"54a97315-a053-4325-8561-ee1f495c4daf","domain":"SecureSolutions","question":"A DevOps team has started a new Sprint to create and deploy code on Amazon EC2 Linux instances. Each DevOps team member has an IAM user name in a single AWS account. One team member creates a test EC2 instance, and another member logs in successfully from her desktop using the SSH key. However, when she attempts to deploy code using the CLI, she receives an authentication error message. What is most likely the problem?","explanation":"Failing to include a valid Access Key ID and Secret Access Key in the 'aws configure' command from your CLI client will result in an authentication failure when CLI commands are run. An IAM policy issue will result in an authorization error message. A missing inbound Security Group rule will produce a connection timeout error. SSH keys are not needed for CLI requests.","links":[{"url":"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html","title":"Configuring the AWS CLI"}],"answers":[{"id":"bb707ca44acf191ea91f0d79b61d7fd3","text":"She has neglected to include the SSH key in her CLI request","correct":false},{"id":"8711823a056ce438338116fbd7ac2a44","text":"She has not included her Access Key and Secret Access Key in the CLI configuration","correct":true},{"id":"b861e17f8814c3d98bfa05753a8066c0","text":"She doesn't have an IAM policy associated with her user name authorizing her to write to an EC2 instance","correct":false},{"id":"a2ac4e17c4a14e6e069328f0a7316ba7","text":"The Security Group for the EC2 instance doesn't have an inbound rule allowing traffic from her desktop","correct":false}]},{"id":"cd934b42-8753-4bfe-b53e-7e3150a52532","domain":"CostOptimized","question":"A medical laboratory is deploying an online application for customers to retrieve both current and historical test result information. The data for each test will be stored in Parquet formatted files on a single server with replication for redundancy. The application will require fast response to present the information once a user selects a specific test. Which storage architecture will provide the best performance and cost efficiency?","explanation":"Amazon EBS General Purpose SSD volumes provide substantial performance for interactive application workloads. EBS Provisioned IOPS SSD volumes provide high performance as well, but at a higher cost. Amazon S3 is not a good architectural choice for transactional workloads, and Amazon EFS is not required since the application will run on a single server.","links":[{"url":"https://aws.amazon.com/ebs/features/","title":"Amazon EBS Features"}],"answers":[{"id":"d41d58ba72de81bfae50c0582edb34d0","text":"Amazon Elastic File System with NFS mounts","correct":false},{"id":"56339fcc83f54c52fb7a1b84d3841507","text":"Amazon Simple Storage Service with Transfer Acceleration","correct":false},{"id":"9c60fc99b53c9ed52adb77523afa1026","text":"Amazon Elastic Block Store with Provisioned IOPS SSD Drives","correct":false},{"id":"e32a986c4040754d7c67f2974e1bf240","text":"Amazon Elastic Block Store with General Purpose SSD drives","correct":true}]},{"id":"b5dbb10f-bfe9-4798-94bf-1058f0e38858","domain":"SecureSolutions","question":"One of your web servers runs in an EC2 Linux instance in the us-west-2 region. You need to SSH into the instance to make some configuration changes. Your AWS Management Console currently points to the us-east-1 region. You enter the connect string from your desktop, but the command hangs for a period of time and then returns an “Operation timed out” message. What is most likely the cause of the connection failure?","explanation":"When you attempt to SSH into an EC2 Linux instance without a Security Group inbound rule that allows access the requesting IP address, you will get an 'Operation timed out' message. If you include the wrong key pair file in your connect string, you will immediately receive a 'Permission denied' error message. You don't need to have your Management Console pointed to the region you are trying to SSH to. In fact you don't even need to be logged in to the Management Console. Route Table targets only apply to outbound traffic.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html","title":"Amazon EC2 Security Groups for Linux Instances"}],"answers":[{"id":"7d2ee468760d828148d74bdf16458a9d","text":"The Security Group for the EC2 instance is blocking your access","correct":true},{"id":"5a59d873dee868ebc4a43c10c1a954c5","text":"The Route Table for the subnet where the EC2 instance resides is missing an Internet Gateway target","correct":false},{"id":"7bd21fc3a95d9ab8bc81887f0eabe6e2","text":"You’ve specified the wrong key pair file in the connect string from your desktop","correct":false},{"id":"15f0fad365a5009122f37d5693f5ca56","text":"Your AWS Management Console needs to point to the region where the EC2 instance resides","correct":false}]},{"id":"ce84785e-7010-4243-99f8-3a1ab7d43e31","domain":"ResilientDesign","question":"You work for a cosmetic company which has their production website on AWS. The site itself is in a two-tier configuration with web servers in the front end and database servers at the back end. The site uses using Elastic Load Balancing and Auto Scaling. The databases maintain consistency by replicating changes to each other as and when they occur. This requires the databases to have extremely low latency. Your website needs to be highly redundant and must be designed so that if one availability zone goes offline and Auto Scaling cannot launch new instances in the remaining Availability Zones, the site will not go offline. How can the current architecture be enhanced to ensure this?","explanation":"Deploy your site in three different AZs within the same region. Configure the Auto Scaling minimum to handle 50 percent of the peak load per zone. So, if one AZ goes down, the two remaining AZs can each accommodate 50% of your traffic, giving you 100% coverage.","links":[{"url":"https://docs.aws.amazon.com/autoscaling/latest/userguide/as-register-lbs-with-asg.html","title":"How To Set Up a Scaled and Load-Balanced Application"}],"answers":[{"id":"1dd3aa163155f9eafe380ab0c55794fe","text":"Deploy your website in 2 different regions. Configure Route53 with Weighted Routing. Assign a weight of 25% to region 1 and a weight of 75% to region 2.","correct":false},{"id":"0dfa307c14f360c0a222400c5eecda8d","text":"Deploy your website in 2 different regions. Configure Route53 with a failover routing policy, and set up health checks on the primary site.","correct":false},{"id":"89990e4ab47ac0e1aa67ae395fa51530","text":"Deploy your site in three different AZs within the same region. Configure the Auto Scaling minimum to handle 50 percent of the peak load per zone.","correct":true},{"id":"311b1d3a4668bfd8843e4d5dac1e52d5","text":"Deploy your site in three different AZs within the same region. Configure the Auto Scaling minimum to handle 33 percent of the peak load per zone.","correct":false}]},{"id":"3252d84d-08d9-4bde-a8e7-d716502d1855","domain":"Performant","question":"You have a very heavily-trafficked WordPress blog that has approximately 95% read traffic and 5% write traffic. You notice that the blog is getting slower and slower. You discover that the bottleneck is in your RDS instance. Which of the following answers can improve your WordPress blog's performance?","explanation":"You should use a combination of Read Replicas and ElastiCache to help offload the traffic.","links":[{"url":"https://aws.amazon.com/elasticache/","title":"About ElastiCache"}],"answers":[{"id":"e94a05a7348f87c7b9c4f7036d632a9c","text":"Use ElastiCache to cache the most commonly read posts of your WordPress blog.","correct":true},{"id":"1c551a09129057627b3b75fb70e6f527","text":"Export the database to DynamoDB which has push button scalability.","correct":false},{"id":"1af32ee0c62b109e45b92828dbc33f2d","text":"Create a secondary Multi-AZ database and run the queries off the secondary Multi-AZ database.","correct":false},{"id":"fdc556bb3ab9b5da3b290c181aaefb3c","text":"Create a number of read replicas and update the connection string on your EC2 instances so that traffic is evenly shared amongst these new RDS instances.","correct":true}]},{"id":"db72323e-0c43-4542-9f6b-cd6136b5dcd8","domain":"ResilientDesign","question":"You have a production website for a car insurance company running on AWS. One of the important KPI’s for the company is the number of users on the website at any given time and you have been asked to track this. The website runs on a fleet of EC2 instances behind an application load balancer. What is the simplest way to track this metric.","explanation":"Be clear in yor own mind about the difference between CloudWatch & CloudTrail. An ELB is a service, you cannot install anything onto an ELB.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-cloudwatch-metrics.html","title":"CloudWatch Metrics for your ALB"}],"answers":[{"id":"dd97365a376f41ec276427520159fbd4","text":"Install the CloudWatch agent on the Application Load Balancer. After the agent is installed, look for the “ActiveConnectionCount” in CloudWatch.","correct":false},{"id":"0ea85fc826488c41b0f86072ac7fd0ad","text":"Install the CloudTrail agent on the Application Load Balancer. In CloudTrail look for the “ActiveConnectionCount” metric.","correct":false},{"id":"98c71b3c230ff081ea967bf92fad4707","text":"In CloudWatch metrics look for the “ActiveConnectionCount”","correct":true},{"id":"1056f30813bb025d216b87a2fdd790d9","text":"Enable CloudTrail in the region that your ALB is in. Using CloudTrail metrics look for the “ActiveConnectionCount” metric.","correct":false}]},{"id":"28b03a03-1f36-4a9d-97e2-921457895c4c","domain":"SecureSolutions","question":"Which of the following statements is FALSE regarding the role of a bastion host?","explanation":"A Bastion Host is a hardened instance accessible from a less secure environment (Internet) and used as a secure platform to administer a more secure environment which will typically be in a private subnet.  The Bastion host normally sits in a public subnet, but with a route to the private subnet. (Also known as a Jump Box in some places.)","links":[{"url":"https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html","title":"Linux Bastion Hosts on AWS"}],"answers":[{"id":"d6a9fb6da4dc9422be75916f103ebafe","text":"A bastion host should be protected in a private subnet.","correct":true},{"id":"e9e698f0ed17e25fb3b6f7676d66c090","text":"A bastion host sits outside a private subnet and is used as a secure gateway to that internal network.","correct":false},{"id":"065f5801e2d2aa076a66624ef3fd35cd","text":"As a bastion host is exposed to the Internet, it should be hardened.","correct":false},{"id":"6fca98f904dc4ff43fa1650e5e739446","text":"A bastion host can be used to SSH into an EC2 instance.","correct":false}]},{"id":"f3d178d0-2157-11ea-978f-2e728ce88125","domain":"ResilientDesign","question":"Which of the following AWS Support elements provide the assessment of how ready your AWS environment is for your application prior to launch?","explanation":"If you need an assessment of your AWS environment to help identify and mitigate risks that can affect your application prior to launch, you need an AWS Support plan that includes Infrastructure Event Management. The other elements mentioned here are not event or launch focused; Technical Account Managers handle more technical issues. The Support Concierge is a team of enterprise account specialists dedicated to billing and account issues, and Trusted Advisor is all about helping you reduce cost, increasing performance, and improving security.","links":[{"url":"https://aws.amazon.com/premiumsupport/programs/iem/","title":"AWS Infrastructure Event Management"},{"url":"https://aws.amazon.com/premiumsupport/plans/enterprise/","title":"AWS Enterprise Support"}],"answers":[{"id":"3d582ac943b9ba9113651d26bdee7a79","text":"Technical Account Managers","correct":false},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"af8f25ab81f04f0feba2075a09b65389","text":"Infrastructure Event Management","correct":true},{"id":"33b19f092c13caec1202c108b57d2bc1","text":"Support Concierge","correct":false}]},{"id":"2e6bc929-3cec-4a89-badc-2a3c95e0e27d","domain":"Performant","question":"Your company has a legacy SAN that has 75 TBs of data. Your company has decided that they want to migrate this data to AWS S3 in the quickest way possible. You company has a single internet connection with a maximum pipe line of 50Mbps. Which service should you use for this migration?","explanation":"Given the size of the data and the small internet connection, Snowball would be the fastest option available.","links":[{"url":"https://aws.amazon.com/snowball/faqs/#when-to-use","title":"When to Use Snowball"}],"answers":[{"id":"ac1fc9354c56b77f7143d2b6a7d185ad","text":"Direct Connect","correct":false},{"id":"c728a49363c9a93a43a7e7f232b5a54a","text":"FTP","correct":false},{"id":"c0429b6a658dd488f7262d983c7e02bb","text":"Snowball","correct":true},{"id":"ac27af728db591b665d751dc0178ad25","text":"S3 Transfer Acceleration","correct":false}]},{"id":"08233176-e11b-410e-98eb-25ca6e2eebcb","domain":"ResilientDesign","question":"A large enterprise has a distributed application in its own data center and relies on message brokers to connect and co-ordinate different systems. Message Brokers serve as the backbone for their IT environment and ultimately their business services. The enterprise has started to move some of its applications to the cloud and is looking for a cloud message broker solution so that the on-premise applications can interact with cloud-based application components. Which of the following services best suit the customer requirements?","explanation":"Amazon MQ is recommended for messaging between on-premises and cloud application components. It also supports industry-standard APIs and protocols such as JMS, AMQP and MQTT. Amazon SQS is best utilized as a messaging solution between components entirely on AWS. Amazon Step Functions is a fully managed service which makes it easy to co-ordinate components of distributed applications using visual workflows. Amazon SNS is a managed publish/subscribe service which reliably delivers messages to all valid AWS endpoints.","links":[{"url":"https://aws.amazon.com/blogs/compute/running-activemq-in-a-hybrid-cloud-environment-with-amazon-mq/","title":"AWS Application Integration Services"}],"answers":[{"id":"55daa020bacdf7e4ae1a33c9f14c45b3","text":"Amazon SNS","correct":false},{"id":"860f0e709d06a1c1529c01a39cdfd798","text":"Amazon Step Functions","correct":false},{"id":"0505378c4b7a69664dc5c9a5e845fcfd","text":"Amazon SQS","correct":false},{"id":"7c5d8cfa463c4a8a74627674da6299f4","text":"Amazon MQ","correct":true}]},{"id":"24f26b84-497a-4a07-a40f-1f636c0aa7da","domain":"Performant","question":"You have designed an application that stores large videos in S3. These videos are usually larger than 100Mb in size. You need to maximize upload performance. Which of the following will achieve this end.","explanation":"Multipart Upload is recommended for files greater than 100 Mb, and is required for files 5 GB or larger. S3 Transfer Acceleration is especially useful in cases where your bucket resides in a Region other than the one in which the file transfer was originated.","links":[{"url":"https://aws.amazon.com/about-aws/whats-new/2016/04/transfer-files-into-amazon-s3-up-to-300-percent-faster/","title":"Transfer Acceleration"},{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html","title":"Multipart Upload Overview"}],"answers":[{"id":"137ef2be67b8313e80b25b2c3ea64ec0","text":"Implement a third party CDN solution.","correct":false},{"id":"34188758a0d842537d7d14f43c94c315","text":"Design the application to use multipart upload, so that the file is split in to multiple parts which are then uploaded simultaneously.","correct":true},{"id":"c1b6778485fc5a390b6a08f16f22afec","text":"Require the users to use Direct Connect in order to use to application so as to maximize the upload bandwidth.","correct":false},{"id":"27cbe99b103845434c5d99034be17b10","text":"Utilize S3 Transfer Acceleration.","correct":true}]},{"id":"2eb2dc18-2e15-11ea-978f-2e728ce88125","domain":"Performant","question":"Your project manager (PM) tasked you with launching a Windows server to create and host a website for a federal agency. The PM is especially interested in using an AWS service that makes it easy to combine all the components needed for the website. He also favors that this solution provide predicable monthly pricing; he doesn’t want any surprises in cost. What AWS service is the correct choice?","explanation":"For a low and predictable price every month, Lightsail delivers all that you would need to create, host, and maintain the website in a cloud environment. You will not only get the virtual machine, but also features such as a managed database, SSD-based storage, data transfer, DNS management, and a static IP. Although you can use EC2, you will still need to configure some of these aforementioned features in addition to launching an instance, and its costs will vary according to usage.","links":[{"url":"https://lightsail.aws.amazon.com/ls/docs/en_us/articles/amazon-lightsail-frequently-asked-questions-faq","title":"Frequently Asked Questions in Amazon Lightsail"}],"answers":[{"id":"bef6cb89241de238f082cb243307ad1b","text":"CloudFront","correct":false},{"id":"b21eea42e76007ac061cf37a5a41037d","text":"Lightsail","correct":true},{"id":"1dd30e8d60c2a200781dbcc9286f0919","text":"Elasticsearch Service","correct":false},{"id":"81b22456f78954c460ce2f531b5e048f","text":"EC2","correct":false}]}]}}}}
