{"data":{"createNewExamAttempt":{"attempt":{"id":"e2ee8d56-cdce-486d-b6ff-dbcc9ece443c"},"exam":{"id":"a119f4b4-1784-41b3-b86a-899401ef9bf0","title":"AWS Certified SysOps Administrator - Associate Exam","duration":7800,"totalQuestions":65,"questions":[{"id":"d584866c-4884-40ad-a263-94eaad98f894","domain":"mon-rep","question":"AWS Config is a managed service which is part of the AWS Management & Governance portfolio of services.  Which of the following options are functions of the AWS Config service?","explanation":"AWS Config is a service that provides access to resource configuration history, an inventory of resources and alerts on any configuration changes, however it doesn't log API calls as this is the function of CloudTrail.","links":[{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"}],"answers":[{"id":"b3a15d8a30c303c913c4b2976f61f5c7","text":"Provides an inventory of all AWS resources","correct":true},{"id":"511bc797ffd63fd0702e8632fb5156db","text":"Provides a log of all configuration related API calls","correct":false},{"id":"46057f48c99511ac9dfb7fe92df1aefa","text":"Provides notification of configuration item changes","correct":true},{"id":"77eef0aefcbae3f15cc5ce086d0bff63","text":"Provides access to resource configuration history","correct":true}]},{"id":"df47b876-0fc7-439f-87b6-01f53249e980","domain":"data-man","question":"A company has a popular web app that frequently reads and writes customer data in a DynamoDB table. Your team is developing a new application and it needs to capture the data about these updates to the table, and provide some near real-time usage metrics for the web app. How should the application get the required data?","explanation":"The DynamoDB stream contains the information about changes to items in a DynamoDB table. After the stream is enabled, the information is stored in a log for up to 24 hours. The application can get the data from streams endpoints such as \"streams.dynamodb.<region>.amazonaws.com\". The application cannot fetch the streaming data from an S3 bucket. And the CloudTrail service does not capture the data events of DynamoDB tables.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html","title":"Capturing table activity with DynamoDB Streams"}],"answers":[{"id":"a71c3c217d4196fc1179a0ca0207cb90","text":"Enable DynamoDB Streams for the DynamoDB table so that a time-ordered sequence of modifications in the table is captured. Get the streaming data from the DynamoDB Streams endpoints.","correct":true},{"id":"b39b6791ccfc53f69a9e0c05b74a7bf1","text":"Configure a CloudTrail that includes data events for DynamoDB tables. The application can read the streaming data from the trail and generate near real-time usage metrics.","correct":false},{"id":"e2e524ab2e2071f38fd5e1e1c74daf56","text":"Enable item level streams for the DynamoDB table and configure a S3 bucket to save the streaming data. The application can get the sequence of modifications from the S3 bucket.","correct":false},{"id":"fadc7afe691bf8186ca9574e2cbef16e","text":"Enable the object level logging for the DynamoDB table and save the activity events in a S3 bucket. The application can get the events from the S3 bucket and generate real-time metrics.","correct":false}]},{"id":"9efe0b3f-2e6a-4918-81b2-c1a827654892","domain":"networking","question":"Which of the below statements about subnets and CIDR blocks as part of a VPC in AWS is correct?","explanation":"The maximum size for a subnet is /16 so this is a correct answer. Default behaviour is that every time you create a subnet it is added to the main route table - allowing other subnets using the main route table to route to it. As for the incorrect statements - the minimum size of a subnet is /28 (16 addresses) therefore /30 is incorrect, subnets are local to a single AZ and cannot span multiple AZs (although VPCs in which they live can). Every VPC will need an IPv4 CIDR allocated - even if the intent is just to use IPv6","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html","title":"VPC Subnets"}],"answers":[{"id":"70426e4ee2fa35fde1533f02d72886a1","text":"A subnet can span multiple AZs","correct":false},{"id":"badb9d21322bff4ed8278d92ebc7174b","text":"The maximum size for a subnet is /16 with 65,536 addresses","correct":true},{"id":"32df7fa7ee27b28e38ab223822a82204","text":"The minimum size for a subnet is /30 with 4 addresses","correct":false},{"id":"22b96919d7cd98b93972df376d49d356","text":"By default, all subnets you create can route to each other","correct":true},{"id":"ed1e31d6b69887380df735258ef1bd1a","text":"A subnet that will only be used for IPv6 doesn't need an IPv4 CIDR associated ","correct":false}]},{"id":"10c87ec7-567f-4119-8817-e484af01182e","domain":"high-avail","question":"Your development team has written an application to pick up and process images taken from an SQS queue. The app is growing in popularity and your manager wants to do this as cheaply as possible. What is the best way to achieve cheap and timely processing?","explanation":"The solution must respond in a timely manner to increases in workload and running during ‘off-peak’ hours is not appropriate. Using a lambda to perform the scaling is not necessary since it is supported out-of-box by AWS’ Autoscaling service. Since the application is growing in demand there isn’t a defined capacity to pre-purchase via Reserved Instances.  Therefore the best solution is to use Autoscaling to scale in and out based on queue length.","links":[{"url":"https://docs.aws.amazon.com/en_pv/autoscaling/ec2/userguide/as-using-sqs-queue.html","title":"Scaling Based on Amazon SQS"}],"answers":[{"id":"905df3db934191d38ccbd1dbec2e459a","text":"Deploy the app to EC2 and use lambda to stop and resize the instance based on the SQS queue length","correct":false},{"id":"e00965d574d10c8a9d991c96eb786139","text":"Deploy the app into an Autoscaling Group and run the processing during off-peak hours","correct":false},{"id":"a9f53d8ff41f6f104afe7dfd446f8cf0","text":"Deploy the app into an Autoscaling Group and scale in and out based on an SQS queue length","correct":true},{"id":"d3dd710c9e556214f0ae216bd74a6982","text":"Deploy the app into an Autoscaling Group and use reserved instances (RI) to pre-purchase a year’s worth of processing capacity","correct":false}]},{"id":"08cb4706-7aac-48de-a4c8-c7e742aeb4d9","domain":"data-man","question":"A Multi-AZ RDS deployment will automatically fail-over as a result of which two of the following?","explanation":"Amazon RDS detects and automatically recovers from the most common failure scenarios for Multi-AZ deployments so that you can resume database operations as quickly as possible without administrative intervention. Amazon RDS automatically performs a failover in the event of any of the following: Loss of availability in primary Availability Zone, Loss of network connectivity to primary, Compute unit failure on primary, Storage failure on primary.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html#Concepts.MultiAZ.Failover","title":"Failover Process for Amazon RDS"}],"answers":[{"id":"8851c500044adfe0ba4a247acdb86eda","text":"Loss of network connectivity to the Primary","correct":true},{"id":"7ecc550bc4f7f4c4d01c334d359bb0d9","text":"Loss of availability in standby Availability Zone","correct":false},{"id":"7b75b57dbe879950151e7a40cf0aed41","text":"A region-wide loss of service","correct":false},{"id":"215b7e6d7f48e4225c06a3c50bf400d2","text":"Loss of availability in primary Availability Zone","correct":true}]},{"id":"739bcd50-ad59-4cbe-a911-17cff575b80d","domain":"security-comp","question":"You have been asked by your company's CISO to create and manage an S3 bucket with highly confidential company information. Only two business-critical individuals within the company should have read and write access to the bucket. All other personnel should not have access. How would you go about ensuring the security and confidentiality of the bucket in the most efficient manner?","explanation":"Bucket policies provide centralized access control to buckets and objects based on a variety of conditions, including Amazon S3 operations, requestors, resources, and aspects of the request (for example, IP address). The policies are expressed in the access policy language and enable centralized management of permissions. The permissions attached to a bucket apply to all of the objects in that bucket. AWS Config would only report on if buckets are in compliance to any rules set. An explicitly deny for all IAM users would work but it isn't the most efficient solution. Bucket access control lists apply to bucket objects, not the bucket itself.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html","title":"Bucket Policies"}],"answers":[{"id":"06899090182266f944ef7ff8a34750ed","text":"Create an AWS Config rule that denies access to the S3 bucket except for the business-critical users. Set an alarm whenever someone attempts to access the bucket.","correct":false},{"id":"48f34a3e8e8a95fcbe3abee586e0f775","text":"Create a bucket access control list to explicitly grant access to the two individuals.","correct":false},{"id":"fdadf86a8030fc5cbfd2cdb0b16b95f6","text":"Create a bucket policy explicitly granting access to the two principals.","correct":true},{"id":"ebf041183040a34a7220734ce5497478","text":"Create an IAM policy for all non-business-critical individuals that explicitly denies access to the bucket.","correct":false}]},{"id":"aba20312-a023-419f-b495-12e7dd2c6577","domain":"mon-rep","question":"In which of the following are your CloudTrail logs stored?","explanation":"Logs are stored in S3. You must specify a storage  bucket name to enable CloudTrail.","links":[{"url":"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.html","title":"Finding Your CloudTrail Log Files"}],"answers":[{"id":"0f41d6f36f8eaee87ea08d9f4b1159e2","text":"RDS","correct":false},{"id":"7b1fb630c85b556e31fa54e3d2b6201a","text":"An EBS Volume","correct":false},{"id":"e2ab7c65b21ed8cc1c3b642b5e36429e","text":"S3","correct":true},{"id":"6ebb7423072c5943f52c11274fd71b0b","text":"DynamoDB","correct":false}]},{"id":"b44c9f0b-805c-4de9-8ae7-71a6c2885b76","domain":"mon-rep","question":"Which of the following can you use to monitor API usage in AWS?","explanation":"CloudTrail logs all API calls within your account, CloudWatch monitors performance metrics, RunCommand is a Systems Manager feature which lets you run a command simultaneously on multiple instances, Trusted Advisor makes security, performance and cost optimization recommendations.","links":[{"url":"https://aws.amazon.com/cloudtrail/faqs/","title":"CloudTrail FAQs"}],"answers":[{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":true},{"id":"64d231d79e9f7640a4572f7ae75aa226","text":"RunCommand","correct":false},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false}]},{"id":"84adda98-8315-454d-b0c1-b6478c5c0d98","domain":"mon-rep","question":"You are performing an update to all of your application servers, however some of your applications are failing following the upgrade and you notice that this seems to only be affecting servers with a specific application profile. How can you easily identify which of your systems are likely to be affected?","explanation":"AWS Config is a service that enables you to assess, audit and evaluate the configurations of your AWS resources.","links":[{"url":"https://aws.amazon.com/config/faq/","title":"AWS Config FAQs"}],"answers":[{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false},{"id":"2d80a80d60fea86242f99512dbac7529","text":"AWS Config","correct":true},{"id":"9deb03cd21d41a691cdc24bfaab2820c","text":"Inspector","correct":false},{"id":"055f466b265e26667e0bb23ddffc7970","text":"Run Command","correct":false}]},{"id":"95c885d1-c528-48bf-b403-af3cf7ff29cf","domain":"dep-prov","question":"You need to automate the creation of related AWS resources. Which AWS standalone service is your best choice?","explanation":"CloudFormation automates the creation of related AWS resources, provisioning and updating them in an orderly and predictable fashion. ","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html","title":"About CloudFormation"}],"answers":[{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":true},{"id":"a907a7338c1fb3821fbe8ed113c64b33","text":"CloudHSM","correct":false},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false}]},{"id":"988e212c-6400-4c9d-8e9b-25ce5413256b","domain":"dep-prov","question":"If you use an IAM user to copy an instance-store-backed AMI, the user must have which of the following Amazon S3 permissions?","explanation":"If you use an IAM user to copy an instance-store-backed AMI, the user must have the following Amazon S3 permissions: s3:CreateBucket, S3:GetObject, S3:PutObject, s3:GetBucketAcl, s3:ListAllMyBuckets and s3:PutObjectAcl.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/creating-an-ami-instance-store.html","title":"Creating an Instance Store-Backed Linux AMI"}],"answers":[{"id":"d0e4bee632dc55607e492853b76d8194","text":"s3:CreateBucket","correct":true},{"id":"7002b8d843e69183ff3fcc03410db0ad","text":"s3:GetObjectAcl","correct":false},{"id":"53cfd5032381d05f65cbc6d64e0004f5","text":"s3:GetObject","correct":true},{"id":"3d48378493503b3c7c04da8400942853","text":"s3:DeleteBucket","correct":false},{"id":"28e5f48e9f12d4928897b706c15e527a","text":"s3:RestoreObject","correct":false},{"id":"8e0864f8129f5d2b658668b9103e83a5","text":"s3:PutObject","correct":true}]},{"id":"5d1249a6-960b-4540-8631-50875e04850d","domain":"mon-rep","question":"A company is using a site-to-site AWS VPN connection with static routing to allow connectivity between its corporate office and a VPC in AWS. The SysOps Administrator wants to get notified if the connection goes down. What’s the most effective way to accomplish this?","explanation":"AWS support won't do this for you. The other options would work, however, creating a CloudWatch alarm is the simplest option.","links":[{"url":"https://docs.aws.amazon.com/vpn/latest/s2svpn/monitoring-cloudwatch-vpn.html","title":"Monitoring VPN Tunnels Using Amazon CloudWatch"}],"answers":[{"id":"759ca4c838619fc5548932dc516de2cf","text":"Create a CloudWatch alarm to track the TunnelState metric and send an SNS notification if necessary.","correct":true},{"id":"4ca2750b50a6661dbeff47ae8524ff24","text":"Set up a cron job in an EC2 instance to confirm the TunnelState metric every minute and send an SNS notification if necessary.","correct":false},{"id":"a05514224184194909112ecdabb8b939","text":"Write a Lambda function to check the TunnelState metric every minute and send an SNS notification if necessary.","correct":false},{"id":"b9f0b72860ab9c76417cf3f4c3ec6c98","text":"Ask AWS support to monitor the connection and send an SNS notification if necessary.","correct":false}]},{"id":"3eaf9c7f-ec35-42cb-af59-f26b9b358432","domain":"automation","question":"Which of the following sections is required for a CloudFormation template to be valid?","explanation":"The Resources section is the only required section. It specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resources-section-structure.html","title":"CloudFormation - Resources"}],"answers":[{"id":"229eb04083e06f419f9ac494329f957d","text":"Conditions","correct":false},{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":false},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":true}]},{"id":"cc89c061-ab6b-4cb4-8809-12daecdbc5b4","domain":"networking","question":"A SaaS company has an existing web application set up in the AWS Singapore Region. The company created a new web application setup in the Tokyo Region. The company decided to host an active-active setup with the traffic evenly distributed between the Singapore region setup and the Tokyo region setup. The company currently uses Route 53 to manage the routing policies. How can the company accomplish this?","explanation":"Route 53 Weighted Routing Policy can distribute traffic evenly between two application environments. The Failover routing policy is only used for routing requirements involving a primary setup and a backup / failover setup. Creating and adding load balancers in the architecture will not solve the problem.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Route 53 - Routing Policy"}],"answers":[{"id":"71053c311ca592ed50fdc7ec6c5a98f5","text":"Create an Application Load Balancer in the Singapore Region. Register the two web application environments in the Application Load Balancer. Point the Route 53 record set to the Application Load Balancer.","correct":false},{"id":"c6c1e82d29d79462db966c3f61ab6b84","text":"Use the Route 53 Failover Routing Policy","correct":false},{"id":"ad3851f2c1fb167d0adcff683b66ac90","text":"Use the Route 53 Weighted Routing Policy","correct":true},{"id":"b050fdf9ffd82093c1a184237e0849ca","text":"Create a Network Load Balancer in the Singapore Region. Register the two web application environments in the Network Load Balancer. Point the Route 53 record set to the Network Load Balancer.","correct":false}]},{"id":"b5cf6800-4ce3-4d24-8eaa-a1279c1c6409","domain":"mon-rep","question":"An insurance company has a monolithic application hosted in an EC2 instance and a serverless application hosted in AWS Lambda. After a few months of running the application, the customers have raised multiple delays and performance issues from the applications. The Operations Engineer responsible has mentioned that the latency issues might have been caused by code-level performance issues and the Head of Operations has instructed the team to add code-level monitoring support. How can the team accomplish this?","explanation":"X-Ray can be used for adding code tracing support for both monolithic application code (e.g. a large Django monolithic project) and serverless (Lambda function) code. CloudTrail is used for auditing API call logs. CloudWatch is used for monitoring resource usage and metrics. X-Ray is a distributed tracing system.","links":[{"url":"https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html","title":"AWS X-Ray"}],"answers":[{"id":"ab066c8d1096ed9b7b49b4637589b201","text":"Use AWS X-Ray for the monolithic application code. Use AWS CloudTrail for the serverless application code.","correct":false},{"id":"7d3a7bc0301958a4e3ad624f70b462d4","text":"Use AWS CloudTrail for the monolithic application code. Use AWS X-Ray for the serverless application code.","correct":false},{"id":"2eb097422fa45d569c37d094428d9a9d","text":"Use AWS CloudWatch for the monolithic application code. Use AWS X-Ray for the serverless application code.","correct":false},{"id":"359f4dd3d7689aca37514e23a8781431","text":"Use AWS X-Ray for both the monolithic application code and the serverless application code.","correct":true}]},{"id":"a7e11457-61a0-4212-b057-2853f6bf0c73","domain":"dep-prov","question":"You run a digital marketing company and many of your clients are bloggers and small businesses. The majority of your customers either use Wordpress or Joomla and would like these sites to be deployed on AWS so they can then go in a manage them independently. You want to be able to deploy these instances as quickly as possible for new customers as well as having them patched for OS security patches and application patches. What is the quickest way to achieve this?","explanation":"An Amazon Machine Image (AMI) provides all the information required to launch an instance, for example an Operating System with your applications, security settings and patches applied. A custom AMI can be created from an existing EC2 instance which you have configured according to the specification you require.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html","title":"Amazon Machine Images (AMIs)"}],"answers":[{"id":"7f84a9c50026f2d984ee3517ddaee6ed","text":"Create a gold template AMI with Wordpress/Joomla and MySQL/Apache/PHP already installed. Use a bootstrap script to apply any necessary updates","correct":true},{"id":"75454744a1bfe749b0f7c048f5c8374b","text":" Use a bootstrap script to install MySQL/Apache/PHP and then to download, unzip and install Wordpress/Joomla in an automated pattern","correct":false},{"id":"c5382eebf8fd57d5e2b5ef1d687dafa8","text":"Create a lambda function which will run at scheduled times. Program this lambda function to provision EC2 instances with the required software on it automatically","correct":false},{"id":"d50d598e74c83ecddc82ee3e3fc4a350","text":"Hire an intern who will create the EC2 instances manually, install wordpress/joomla as well as Apache and MySQL and then apply all necessary security updates","correct":false}]},{"id":"88de3a5c-b9ce-46eb-928f-f0f909bfe5dd","domain":"networking","question":"You own the registered domain name cloudcanines.com and are trying configure Route53 to map the DNS name to the DNS name of your Elastic Load Balancer. Which Route53 record can you use to achieve this?","explanation":"cloudcanines.com is a Zone Apex and you can't create a CNAME record at the zone apex. Instead you need to use an alias record to map the Zone Apex to the Elastic Load Balancer","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html","title":"Route53 Record sets"}],"answers":[{"id":"0b98720dcb2cc6fd60358a45dfbc5b87","text":"MX","correct":false},{"id":"effdb9ce6c5d44df31b89d7069c8e0fb","text":"Alias","correct":true},{"id":"de46eab399f3ea0bbf1912c1d14a1544","text":"Zone Apex","correct":false},{"id":"adc4bfdb0829dae99e3699393e3fbaa4","text":"CNAME","correct":false}]},{"id":"cb548784-60f3-4b72-a71d-4e5418a111f8","domain":"security-comp","question":"You are a security administrator for your organization tasked with implementing security policies as code for your organization. You would like to evaluate configuration changes daily to AWS resources based on these security policies. One organizational requirement is to ensure that VPC flow logs are enabled in all the VPCs in your AWS account. What is the best solution to implement this policy?","explanation":"AWS provides a number of predefined, managed Config rules. You also can create custom Config rules based on criteria you define within an AWS Lambda function. AWS Config Rules enables you to implement security policies as code for your organization and evaluate configuration changes to AWS resources against these policies. You can use Config rules to audit your use of AWS resources for compliance with custom rules such as VPC flow logging. A CloudFormation template will only ensure new VPC deployments are compliant. OpsWorks stacks apply to application modeling and management and would not be appropriate for VPCs. CloudFormation templates and Service Catalog would encounter the same issue of only making new VPC deployment compliant even if these solutions were administratively feasible.","links":[{"url":"https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/","title":"How to Audit Your AWS Resources for Security Compliance by Using Custom AWS Config Rules"}],"answers":[{"id":"a8052e2244cc8ebbce92632ffd1fdbcd","text":"Deploy an OpsWorks stack to automate the configuration of your VPC environments. Configure the OpsWorks stack to enable VPC flow logs and to automatically enable VPC flow logs when manually disabled. Manage the OpsWorks stack through the Puppet console.","correct":false},{"id":"2b0b6ac9ccf99cfa2cd117ba409aeee6","text":"Set up a Service Catalog portfolio with a product specifically designed for VPC. Share this portfolio with the necessary network administrators in your organization. Require your network admins to deploy VPCs through the Service Catalog portfolio. Ensure compliance to enable VPC flow logs by embedding the requirement in the product.","correct":false},{"id":"3066b6a0d5c132515f021a1859f25549","text":"Develop a CloudFormation template that deploys an Amazon VPC. Include a requirement in the CloudFormation template that automatically enables VPC flow logs during the VPC deployment. Share this CloudFormation template with the appropriate IAM users in your organization. Edit the IAM policies to allow VPC creation only through the CloudFormation template.","correct":false},{"id":"5429ae9bbd00dd31879570ed15142b94","text":"Create a Lambda function to poll Config periodically to detect noncompliant resources. Create a custom rule that uses the Lambda function as the source. Run the Lambda function daily to assess for noncompliance with the custom rule. When noncompliant resources are detected, send a notification by publishing a message to SNS.","correct":true}]},{"id":"7x37ji7v-n162-1qmy-13gi-0uo4m7wzzhxv","domain":"data-man","question":"You need to upload a 7 TB object to S3. What's the best way to go about this?","explanation":"In S3, you can upload objects of up to 5 GB in size in a single operation. For objects greater than 5 GB you must use the multipart upload API. Using the multipart upload API you can upload objects up to 5 TB each.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectOperations.html","title":"Operations on S3 Objects"}],"answers":[{"id":"ad38fdc937c75d02a8ce9f99dd2038ff","text":"Use Transfer Acceleration to expedite the transfer of the large object.","correct":false},{"id":"1f04ed6b37b4cb31587dbcf7bceb1483","text":"This cannot be done: the maximum S3 object size is 5TB.","correct":true},{"id":"cc539f0a12c590bfaf4b0090f879ed46","text":"Use Snowball to deliver the large object to S3.","correct":false},{"id":"7945f6a9bf6e09f798e85934b6d3bad4","text":"This can be done in one updload.","correct":false}]},{"id":"d585a94b-6bfb-47bf-83b2-d92525fa925b","domain":"security-comp","question":"Your CFO would like to hire a consulting company called CostControl Corp to monitor your AWS account and help optimize costs. In order to track your daily spending, CostControl Corp needs to access your AWS resources. CostControl Corp also monitors many other AWS accounts for other customers. How would you grant CostControl Corp access to your account in a secure and administratively efficient way?","explanation":"Do not give CostControl Corp access to an IAM user and its long-term credentials in your AWS account. Creating an IAM Group would still require long-term credentials. Instead, use an IAM role and its temporary security credentials. An IAM role provides a mechanism to allow a third party to access your AWS resources without needing to share long-term credentials. You can use an IAM role to establish a trusted relationship between your AWS account and a third party. After this relationship is established, a member of the exteranl account can call the AWS STS AssumeRole API to obtain temporary security credentials. Cognito is a web service that delivers scoped temporary credentials to mobile devices and other untrusted environments.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html","title":"How to Use an External ID When Granting Access to Your AWS Resources to a Third Party"}],"answers":[{"id":"2df4700c329bc33dc8133ca8de8c37ad","text":"Create an IAM role. Allow CostControl Corp users to assume this role for access to AWS resources in your account.","correct":true},{"id":"ef9e894add84e3a1607217b2cfa0f98d","text":"Use Amazon Cognito User Pools to enable authentication with the external party. Cognito will create a unique identifier for each user in CostControl Corp to access temporary, limited-privilege AWS credentials.","correct":false},{"id":"511bbeb0af589b7478846f2bc4488bb7","text":"Identify a secure user within CostControl Corp. Create an IAM user in your AWS account with the necessary permissions and grant access to that securely identified user.","correct":false},{"id":"0e0f124a17ea65e797f45765d7fac5a1","text":"Create an IAM Group for CostControl Corp users. Give this group limited AWS permissions. Create CloudTrail log for this group to ensure their API calls in your AWS are within the realm of their scope of work.","correct":false}]},{"id":"4fa7de17-2f03-4b83-a519-e00b011ca644","domain":"security-comp","question":"You are a SysOps Administrator for your organzation. The organization has one AWS account that all users share. You are asked by the CISO to make access to AWS secure and manageable. What would be the best solution?","explanation":"AWS Single Sign-On makes it easy to centrally manage access to all of your AWS accounts. It supports Security Assertion Markup Language (SAML) 2.0 so you don't have to create IAM users for every individual in your organization. The other solutions would be an administrative and unnecessary burden, while encrypted login credentials using KMS is not used for AWS access.","links":[{"url":"https://docs.aws.amazon.com/en_pv/singlesignon/latest/userguide/iam-auth-access.html &https://docs.aws.amazon.com/cli/latest/reference/sts/assume-role-with-saml.html","title":"AWS Single Sign-On"}],"answers":[{"id":"678d5bab1e8e14c5f9a7055f83d2aa56","text":"Set up an AWS Organization to manage accounts and apply permissions boundaries. Set up IAM users and group them in the appropriate OU.","correct":false},{"id":"b18f21a1b9a12f45b180d3ba09d175a0","text":"Configure a SAML federation between AWS and your organization's Active Directory. Set up Active Directory groups with AWS IAM groups to manage user permissions.","correct":true},{"id":"0949ef9615ffc62f343f10d27d0bea2e","text":"Encrypt Active Directory logins using AWS KMS. Store encrypted logins in an RDS table. When users access AWS, set up permissions to decrypt the credentials to securely access AWS resources.","correct":false},{"id":"48371df9efaa3423dc8c9153f0af03c3","text":"Group Active Directory users and mirror their permissions with IAM policies. Create IAM users in AWS and group them into IAM groups that correspond to the Active Directory Group.","correct":false}]},{"id":"2bd13304-db2d-4120-9487-7e13d7008a63","domain":"dep-prov","question":"EC2 instances are launched from Amazon Machine Images (AMIs). A given public AMI:","explanation":"An AMI cannot be launched into another region. To launch an AMI into a region other that the one in which it was created, the AMI must be copied to that other region first.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html","title":"Copying an AMI"}],"answers":[{"id":"5a70861f841598d574b8ebd61e20cfc1","text":"Can only be used to launch EC2 instances in the same AWS Availability Zone (AZ) as the AMI is stored.","correct":false},{"id":"3b690d9883c0bbec853ffdc3be150208","text":"Can only be used to launch EC2 instances in the same AWS country as the AMI is stored.","correct":false},{"id":"7096f3da806d7b3014ab808eb74d213a","text":"Can only be used to launch EC2 instances in the same AWS region as the AMI is stored.","correct":true},{"id":"9ba6ee29e152b251b8fc17d5d2aca126","text":"Can be used to launch EC2 instances in any AWS region.","correct":false}]},{"id":"1982b211-0620-43ed-9a88-f5237a42eae2","domain":"automation","question":"You're using CloudFormation templates to build out staging environments. Which section of the template would you edit in order to allow the user to specify the SSH key-name at start time?","explanation":"The parameters property type in CloudFormation allows you to accept user input when starting the template, allowing you to reference the user input as variable throughout your cloud formation template.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html","title":"About CloudFormation Parameters"}],"answers":[{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false},{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":false},{"id":"3225a10b07f1580f10dee4abc3779e6c","text":"Parameters","correct":true},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false}]},{"id":"edaeeffd-65ec-4d45-8f2d-f37d50773681","domain":"dep-prov","question":"You create an Oracle database in AWS RDS for an application. As the database instance needs to integrate with an S3 bucket, you want to configure an IAM role for the database to read and write the bucket objects. You already have an IAM role for EC2 and the EC2 instance can use the role to transfer files from and to the bucket properly. However, when you try to add the same role to the database, the role cannot be found. What is the reason and how to fix it?","explanation":"The trust entity of the IAM role must include rds.amazonaws.com for a RDS database to assume it. The existing role only contains the trust entity of ec2.amazonaws.com so it cannot be used for RDS. The resource and action parts of the role should be good as EC2 can work with S3 properly. Policies of an IAM role do not have the “principal” part.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/oracle-s3-integration.html","title":"RDS integrates with S3"}],"answers":[{"id":"d0d2809faf6ff80ea9807995d483b3e0","text":"For the RDS database to assume the role, the IAM policy should allow the action of \"rds:*\".","correct":false},{"id":"d87fa6b85878f5af177443f4c2aa638f","text":"The trust entity of the role is the service of EC2 instead of RDS. You should create a new role for the RDS service to assume.","correct":true},{"id":"9b1ce3b64821b8efb42f867e68acfbae","text":"The IAM role needs to add a principal of RDS so that any RDS instance can assume the role to integrate with S3.","correct":false},{"id":"edb854c1df5fa32a6d19c322a736f525","text":"The permissions of the IAM role do not list the resource of the S3 bucket.","correct":false}]},{"id":"60a05f6f-23b7-4f0d-b2bf-0e29c276f8ce","domain":"automation","question":"You are working on an application that has multiple phases: development, staging, and production. Each phase runs on Amazon EC2 with an Amazon EBS volume. You use an Application Load Balancer to manage the application's traffic and CloudWatch metrics to collect metric data for each phase. You need to find an efficient way to manage the services, and modify the settings of each phase of your application. What is the most effective way of doing this?","explanation":"You can use a single page to view and manage your resources using AWS Resource Groups. Check your resources for each stage of your application by opening the resource group. View the consolidated information on your resource group page. To modify a specific resource, choose the resource's links on your resource group page to access the service console that has the settings that you need. Without Resource Groups, you would have to access multiple consoles but this is unncessary. OpsWorks is configuration management service that provides managed instances of Chef and Puppet. Service Catalog is to create and manage catalogs of IT services that are approved for use on AWS for compliance.","links":[{"url":"https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html","title":"What Is AWS Resource Groups?"}],"answers":[{"id":"8b78b4f2fe4a52d464dc6d4e66bc684d","text":"Open multiple consoles to check the status of your services and to apply the necessary modifications to each phase's settings.","correct":false},{"id":"2dbb34771a820b660e255fc85e30bb10","text":"Create portfolios of products related to each phase of the application. Group the products to easily manage and update the services within a portfolio using version control.","correct":false},{"id":"47d6536a1b47257e2ed3d370744d7291","text":"Use AWS OpsWorks to automate operational tasks across your AWS resources, view operational data for monitoring and troubleshooting, and take action on your groups of resources.","correct":false},{"id":"1ed632c93b70e6d21a83ffdc04373d13","text":"Create a custom console in AWS Resource Groups that organizes and consolidates information based on criteria specified in tags, or the resources for each phase of your application.","correct":true}]},{"id":"be30bba9-7133-43b6-b1b9-a21de73dbf17","domain":"dep-prov","question":"You have created a new Auto Scaling group and you discover that your instances are not launching in to it. Which of the following is not a reason that this might be happening?","explanation":"The instance type specified is not supported for Auto Scaling","links":[{"url":"https://docs.aws.amazon.com/autoscaling/latest/userguide/ts-as-instancelaunchfailure.html#ts-as-instancelaunchfailure-6","title":"Troubleshooting Auto-Scaling"}],"answers":[{"id":"6788d6762b58bb5d71fe6d713712ca09","text":"The requested configuration is currently not supported.","correct":false},{"id":"04c0d163b1711459d69938ad0495fba5","text":"The security group does not exist.","correct":false},{"id":"21ff5db3973f2b3f7cf4a218ae1bb59f","text":"The instance type specified is not supported for Auto Scaling.","correct":true},{"id":"2094f86fb69fd7383b4a5b26e6fa5f65","text":"The associated Key Pair does not exist.","correct":false}]},{"id":"c4f661ee-5ea1-4e69-9440-52bea0321a6a","domain":"mon-rep","question":"You have set CloudWatch billing alarms for your instances running in eu-west-2. However, when you try to access the billing information and alarms, no information is visible. Why might this be?","explanation":"Billing and Alarm data can be accessed only from the us-east-1 region.","links":[{"url":"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/free-tier-alarms.html","title":"Creating a Billing Alarm"}],"answers":[{"id":"b5ff6f2dfd759f7e70f27d7529e4462b","text":"You need to login as the account owner to see such information.","correct":false},{"id":"fb7a7d16c3f39960e7afde6babd422e1","text":"Billing and Alarm data can be accessed only from the us-east-1 region.","correct":true},{"id":"7f3a3688c3f0cddb24c07255b9d13767","text":"Billing and Alarm data can be accessed only from the us-west-1 region.","correct":false},{"id":"cd2c9fa4324b5861276dbbf7f4f593a8","text":"You need to login as the root user to see such information.","correct":false}]},{"id":"a8d6d4f3-6877-4e89-9e7c-c6f83b83e1de","domain":"automation","question":"A small business has a monolithic application utilizing several EC2 instances. The technology consultant has suggested to the engineering team of the business that the application is containerized to improve the deployment and scaling processes of the team. The engineering team processes both critical and non-critical workloads within the application. Given the size of the application and the number of transactions being processed by the system, the CFO has instructed the engineering team to ensure that the setup must be as cost-effective as possible without introducing the risk of data loss for critical workloads. How can the engineering team accomplish this?","explanation":"On-demand EC2 instances should be used for critical workloads and spot instances can be used for non-critical workloads. ECR (Elastic Container Registry) is not used to run container processes. Instead, it is used to store container images. For containerization requirements, ECS (Elastic Container Service) is used to manage container workloads. Step Functions is used for orchestrating different processes and can not be used by itself to manage the workloads.","links":[{"url":"https://aws.amazon.com/blogs/compute/running-high-scale-web-on-spot-instances/","title":"Running High-Scale Web Applications on Spot Instances"}],"answers":[{"id":"b8864524b96a467a8f1023b5a2c9f003","text":"Use Step Functions and on-demand EC2 instances for the critical workloads. Use Step Functions and spot EC2 instances for non-critical workloads.","correct":false},{"id":"47e42a6e665f0a913236e5580553e3ad","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use ECS and spot EC2 instances for non-critical workloads.","correct":true},{"id":"1d3682fdb0beec13b13a37502cfb3108","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use Step Functions and on-demand EC2 instances for non-critical workloads.","correct":false},{"id":"ddf2c2e29f2118061f6a160054593338","text":"Use ECS and on-demand EC2 instances for the critical workloads. Use ECR and spot EC2 instances for non-critical workloads.","correct":false}]},{"id":"99568617-996c-454f-98ce-c120f3ea2c38","domain":"automation","question":"By default, what status message will you see if your CloudFormation stack encounters an error during creation?","explanation":"You will see the ROLLBACK_IN_PROGRESS message if your CloudFormation stack encounters an error during creation.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-describing-stacks.html","title":"Describing Your Stacks"}],"answers":[{"id":"f3337297902478338063007adf324783","text":"ERROR_STACK_DELETE","correct":false},{"id":"d2884fa6c360974bc033ce3ab23c63f6","text":"STACK_ERROR","correct":false},{"id":"31f5c2cdf809b189d85d0dfcc0d7a7b4","text":"DELETE_IN_PROGRESS","correct":false},{"id":"0ad1f7c4221eb5f69b578b85f24935d0","text":"ROLLBACK_IN_PROGRESS","correct":true}]},{"id":"524af9a6-94c5-4c0b-a6d1-8c29011045df","domain":"automation","question":"Which AWS service allows you to consolidate billing across multiple AWS accounts, automate account creation and control access to AWS services?","explanation":"AWS Organizations offers policy-based management for multiple AWS accounts as well as consolidated billing. Personal Health Dashboard provides alerts when AWS is experiencing outages and other events that may impact you. Inspector is used for vulnerability scanning of applications running on EC2. IAM is used for policy based access control for users under a single AWS account","links":[{"url":"https://aws.amazon.com/organizations/","title":"AWS Organizations"}],"answers":[{"id":"41dff7155cc7aeb11c06434f6a450bb3","text":"IAM","correct":false},{"id":"9deb03cd21d41a691cdc24bfaab2820c","text":"Inspector","correct":false},{"id":"b1820ee1cf68e2e65f263ff7bb207626","text":"AWS Organizations","correct":true},{"id":"cf4db9f312542c8284a6ccdefcd98544","text":"Personal Health Dashboard","correct":false}]},{"id":"41260d7b-e3b1-4ba3-81e2-f8f441b798e6","domain":"security-comp","question":"When providing mobile apps with temporary security credentials for access to AWS services, which of the following methods is best?","explanation":"AWS strongly recommends that you do not embed or distribute long-term AWS credentials with apps that a user downloads to a device, even in an encrypted store. Instead, build your app so that it requests temporary AWS security credentials dynamically when needed using web identity federation. The supplied temporary credentials map to an AWS role that has only the permissions needed to perform the tasks required by the mobile app.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html","title":"About Web Identity Federation"}],"answers":[{"id":"9d5170dbe7d5ca2ce9011d9234fc376a","text":"Web Identity Federation with Cognito","correct":true},{"id":"d48ff95a8acfcaabeb2f6a3f070af924","text":"LDAP with IAM","correct":false},{"id":"d6b9a7c9c7e8cd3c41fd1ebeae2fa4ef","text":"IAM credentials alone","correct":false},{"id":"0bcfd08b38a1d0c297fd1f4dc597a81a","text":"Active Directory with IAM","correct":false}]},{"id":"d69480f4-f12c-471d-bf7f-f6b0994a441f","domain":"mon-rep","question":"You are supporting an online gaming platform which runs on a number of application servers behind an Application Load Balancer, your Security Architect asks you to start monitoring HTTP requests from users to your application? Which of the following can you use to achieve this?","explanation":"Request Tracing can be used to track HTTP requests from clients to targets. CloudWatch monitors performance metrics, Trusted Advisor makes recommendations based on best practices for security, performance and cost optimization. Cloud Trail monitors API calls within your account. ","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-request-tracing.html","title":"Request Tracing on Application Load Balancer"}],"answers":[{"id":"e86c7149423f18f89a2a67af3295477a","text":"Request Tracing","correct":true},{"id":"50ed91980adb1dac23689554eb719277","text":"CloudWatch metrics","correct":false},{"id":"f0d36f4f7323e9b431c82349dcc3ab89","text":"CloudTrail Logs","correct":false},{"id":"739749e0ec278613ef4f8e6861efc722","text":"Trusted Advisor","correct":false}]},{"id":"2f65c4d8-8479-4429-b280-10cadff2d2c6","domain":"data-man","question":"A digital marketing company stores the company logo inside an S3 bucket and references the uploaded company logo file in the email signatures when sending emails. During the last few months of each year, the company modifies the logo shared in the email signature to have a different color in order to reflect the season. The logo shared in the email signature is returned back to the original image at the start of each year. The COO has mandated that the setup must be low-cost and the cost of managing and maintaining the services used is as minimal as possible. How can the team accomplish this?","explanation":"S3 versioning can be used to have two different files with the same filename. Given that the requirement involves having a minimal setup, creating an API Gateway API endpoint is not the priority solution. Storing the alternate image in the Infrequent Access tier will not solve the requirements.","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html","title":"S3 Versioning"}],"answers":[{"id":"41b65c87103c4da8b22fc2c33a785d77","text":"Enable versioning in the S3 bucket. Upload a new version during the last few months of the year. Permanently delete the new version at the start of each year.","correct":true},{"id":"720237f90948c2f456dacd3f9728f6fa","text":"Store the original image in the Standard S3 tier. Store the alternate image in the Infrequent Access tier.","correct":false},{"id":"ff6b9a671b78c3322047a4c27c96a3bd","text":"Upload two images in the S3 bucket. Create an AppSync API endpoint that triggers a Lambda function that returns the appropriate image depending on the date of execution.","correct":false},{"id":"4648a283c7c8e8e94567849ab5b57c53","text":"Upload two images in the S3 bucket. Create an API Gateway API endpoint that triggers a Lambda function that returns the appropriate image depending on the date of execution.","correct":false}]},{"id":"5bc4279d-2c7a-43fb-a69b-e7055d2e30e5","domain":"high-avail","question":"You have been hired by a large online store to help optimize their web application. There are 3 webservers behind an elastic load balancer and each connects to the same RDS instance. This RDS instance started out as a small memory optimized instance. However, as the traffic increased, the company has moved to a larger instance type. The current instance is already the largest RDS instance currently available and it is beginning to run out of memory. You need to find a way to further scale the web application. What should you do?","explanation":"You should add a couple of read replicas and adjust the application so that read-only traffic is diverted to these instances. Write traffic will remain with the main DB server.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.Overview","title":"Overview: Read Replicas"}],"answers":[{"id":"3007d193799f9cf824fb25d828aba2c0","text":"Advise your customer that their application has grown beyond the capabilities of AWS and should be migrated back to an on-premise solution.","correct":false},{"id":"447ef3eaa17197e0fee43bf9bf881f14","text":"Increase the number of EC2 web instances so you can have even more connections to the RDS instance.","correct":false},{"id":"012be8822c0ab4df608d90fadaf5b557","text":"Add a couple of read replicas and adjust the application so that read-only traffic is diverted to these instances. Write traffic will remain with the main DB server.","correct":true},{"id":"5677b50f46d6dbd051a4229783209657","text":"Advise the company to swap their EC2 instances for larger ones and then contact Amazon to pre-warm the Elastic Load Balancer.","correct":false}]},{"id":"216ba36a-8f55-47f4-80c8-44ac20c3a739","domain":"security-comp","question":"A new project is beginning in your company. The project utilizes two S3 buckets. All of the project members already have IAM user accounts provisioned. New members might join the project later on and some might leave before it's finished. What's the easiest way to configure access to the S3 bucket?","explanation":"All of the proposed solutions are technically feasible. However, you should grant the permissions on the whole group rather than for every individual user. If additional users require the same access, you can give it to them by adding them to the group. When a user no longer needs access to a resource, you can remove them from the group.","links":[{"url":"https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf","title":"AWS Security Best Practices"}],"answers":[{"id":"606624dc111236ee567ddebb534beb0c","text":"Create two IAM policies, each allowing access to a bucket. Attach the policies to a group where you add all the IAM users.","correct":true},{"id":"cde3f9182db55502f071cdb552e75a7c","text":"Create two bucket policies, one for each bucket. Allow access for all the IAM users in the bucket policies.","correct":false},{"id":"9b0876d4f2d9fd237877d5748604315b","text":"Create two IAM policies, each allowing access to a bucket. Attach the policies to all the IAM users.","correct":false},{"id":"80920c27da15505dd506eeabd067fc3f","text":"Create an IAM role which can access the buckets. Create inline policies to allow each IAM user to assume the role.","correct":false}]},{"id":"113a7914-1249-4e12-a748-03392c0570e8","domain":"high-avail","question":"You are a Security Administrator for your company. Your CIO wants to ensure that company data is highly available in multiple AWS Regions. What would you suggest to your CIO as the most effective approach?","explanation":"Deploying a multi-AZ RDS instance would only make it fault tolerant between Availability Zones, and not AWS Regions. Creating a Lambda function and creating/deploying EBS snapshots into different AWS Regions would both be an administrative and operational burden. The easiest, most effective, way is to utilize S3 Cross Region Replication","links":[{"url":"https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html","title":"S3 Cross Region Replication"}],"answers":[{"id":"a4ca2ada50730e36dd0f3302f60ab0dc","text":"Create multiple snapshots of your company data on EBS volumes. Deploy those EBS volumes on EC2 instance in different AWS Regions.","correct":false},{"id":"fbde7989c5f0da5bb91bb5593f9c8e4e","text":"Create a Lambda function that downloads data from your S3 Bucket and executes a PUT operation to upload copied objects into a new bucket in a new Region.","correct":false},{"id":"f876473fb6289d5a1e3e6d449713b0e7","text":"Copy the company data to an RDS instance. Deploy a multi-AZ configuration for your RDS instance to make it highly available.","correct":false},{"id":"6e22da724261694b4ecf8fa105ef5174","text":"Enable Cross-Region Replication on your bucket to copy objects to a destination bucket in another AWS Region.","correct":true}]},{"id":"2a16f1cd-530d-4e30-ad08-d89afae34484","domain":"mon-rep","question":"You create a new DynamoDB table with the provisioned read and write capacity units set to 5. The auto scaling feature is enabled for both read and write. And the target utilization is set as 70%. After monitoring the table for some time, you notice that there are two CloudWatch alarms related to the table. The description of one alarm is \"ConsumedWriteCapacityUnits < 150 for 15 datapoints within 15 minutes\". Which action do you need to take to address the alarm?","explanation":"DynamoDB manages the throughput capacities automatically with the auto scaling feature. The alarms are used for the feature and no action is required. There is no need to disable the feature or modify the provisioned capacities.","links":[{"url":"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html","title":"Manage throughput capacity automatically with DynamoDB Auto Scaling"}],"answers":[{"id":"252cd5486e4ae9685450c3e9ad208b05","text":"No action is required as the alarms are used for auto scaling for the DynamoDB table.","correct":true},{"id":"3a5208c005673a46313dce67b9e2dd57","text":"You should disable the auto scaling feature for the table.","correct":false},{"id":"2131aeffdfb01948cc4943432a680327","text":"You need to adjust the provisioned read and write capacities to a higher value such as 10.","correct":false},{"id":"859412aa0d0666ab4c2b8d20707c8f41","text":"They are fake alarms. You can manually delete them from the console.","correct":false}]},{"id":"2f757d58-833a-40ab-9eb6-2a981fb6b79f","domain":"security-comp","question":"Under the AWS shared responsibility model which of the following would be your responsibility as a customer?","explanation":"The shared Responsibility Model states that you are responsible for configuring the security of your EC2 instances, VPCs and S3 buckets as well as any guest Operating Systems, application and database security settings. AWS is responsible for patching RDS instances and software platforms provided by Elastic Beanstalk","links":[{"url":"https://aws.amazon.com/compliance/shared-responsibility-model/","title":"Shared Responsibility Model"}],"answers":[{"id":"b71690da3d36c4e2b8fe168de0e70b84","text":"PHP patching for applications using Elastic Beanstalk","correct":false},{"id":"96785b9a477bb691ff03d686c919e1cf","text":"Antivirus for EC2 instances","correct":true},{"id":"ceb259f9a51f0549f076e847bf207781","text":"Application level patching for RDS","correct":false},{"id":"e4e2b01049a70752d94080ef404f35cf","text":"OS level patching for RDS instances","correct":false},{"id":"fa7989aaf9918b210db642de0de21650","text":"OS level patching for EC2 instances","correct":true}]},{"id":"9ad0ca0f-bcc1-4a4c-866c-62f4fae5b51c","domain":"networking","question":"You have a customer who wants to connect her on premise data center to AWS. The customer plans to transfer a large amount of data starting in a few months to AWS. The requirement is to have a consistent network connection. It is arbitrary whether the connection itself is public or private. What AWS solution would you recommend to the customer?","explanation":"Using AWS Direct Connect, data that would have previously been transported over the Internet can now be delivered through a private network connection between AWS and your data center or corporate network. In many circumstances, private network connections can reduce costs, increase bandwidth, and provide a more consistent network experience than Internet-based connections. The requirement is to have a consistent network connection, and the transfer can happen in a few months. If you need an immediate connection, then use a VPN as it can be set up within a few hours or less. However a VPN does not provide a consistent network experience as it depends on an Internet connection. Uploading data with S3 is an option but would require administrative overhead, as will uploading EBS snapshots to S3.","links":[{"url":"https://aws.amazon.com/directconnect/faqs/","title":"AWS Direct Connect FAQs"}],"answers":[{"id":"0d2cde735fe999ffbbb7eda1d5814a11","text":"Save data to EBS volumes. Take snapshots of those volumes and upload to S3.","correct":false},{"id":"96b0822a1636db65a54a992ab74c239f","text":"Set up an AWS Direct Connect connection between the customer corporate data center and AWS. Transfer data to AWS via Direct Connect.","correct":true},{"id":"eaa0636a71f37c92d3dd6ec32277a3c1","text":"Set up a private VPN connection between the customer corporate data center and AWS. Attach a customer gateway to the customer data center, and a virtual private gateway in Amazon VPC. Transfer data via the VPN connection.","correct":false},{"id":"86841d550579b63dca9f9de04558994c","text":"Upload data using multi-part upload to an S3 bucket. Schedule the uploads on weekends to minimize business operation disruption.","correct":false}]},{"id":"de2610da-0b20-4258-b215-146e96c134d3","domain":"automation","question":"Which section of a CloudFormation template allows you to set up differing instance types based on environment type (e.g. 'Production' or 'QA')?","explanation":"","links":[{"url":"http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html","title":"CloudFormation Template Anatomy"}],"answers":[{"id":"ddcf50c29294d4414f3f7c1bbc892cb5","text":"Resources","correct":false},{"id":"bf3324c66080c0b764136797d841a2bc","text":"Outputs","correct":false},{"id":"229eb04083e06f419f9ac494329f957d","text":"Conditions","correct":true},{"id":"5f71daa4813d3bca5d795bc163a67eba","text":"Mappings","correct":false}]},{"id":"8dade21b-c268-4ccd-af0a-18c01cd2c638","domain":"automation","question":"You check the last bill of your AWS account and find that the storage of EBS snapshots charges a lot. A large number of EBS snapshots are very old and can be deleted. You want to keep 10 snapshots for an EBS volume and old snapshots are deleted automatically. The strategy should also help you to create a snapshot every 24 hours. Which is the best way of implementing this strategy?","explanation":"The Amazon EBS Snapshot Lifecycle can automate the creation, retention, and deletion of EBS snapshots. You only need to configure a lifecycle policy in the Lifecycle Manager. You do not need to configure a Lambda Function or a Cron job in an EC2 instance to implement the same policy.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html","title":"Automating the Amazon EBS Snapshot Lifecycle"}],"answers":[{"id":"630f293528bb790c603b2f77b266a56d","text":"Use a Cron job that runs in an T2.micro EC2 instance. The job creates a new snapshot and deletes the old one every day.","correct":false},{"id":"ac159fa50438475f44d25f96b6a2bea4","text":"Configure a Lambda Function that runs every 24 hours to create a snapshot and delete the old snapshot.","correct":false},{"id":"6e23ea86e4cf4e8929aa5b846a7e5ad4","text":"Create a Snapshot Lifecycle Policy to automatically create new snapshots and delete old snapshots.","correct":true},{"id":"e711537d471732823f103b6f63ab96c2","text":"Configure a Cloudwatch Event rule to execute every 24 hours. The target is a Lambda Function that creates a new snapshot and deletes the old one.","correct":false}]},{"id":"2b08815b-ca6c-44ed-969b-8cc0dfc22faf","domain":"data-man","question":"A large real estate broker wants to provide a mobile device dashboard of home sales trends by geographic area for its agents. Dashboard metrics will be updated three times each day based on recent activity. The backend application will run on Linux servers in the company data center. Minimizing operational costs is a priority, so they've decided to store the application's data on AWS. Which storage solution will be the most cost effective?","explanation":"A Storage Gateway Volume Gateway in cached mode uses S3 as it's primary storage and caches frequently accessed data locally. A Volume Gateway in cached mode will be more cost effective than one in stored mode because only 20% of the storage capacity needs to be purchased for the on-premises Storage Gateway server in cached mode, whereas the full storage capacity needs to be purchased for a stored mode gateway. Amazon EFS Standard storage is currently $0.30 per GB per month, whereas Storage Gateway Volume Gateway pricing is currently $0.023 per GB per month. EBS volumes can only be accessed by EC2 instances, not remotely as iSCSI devices.","links":[{"url":"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html","title":"How AWS Storage Gateway Works (Architecture)"}],"answers":[{"id":"eca78458c820df850fbb09ee72a3cabe","text":"Implement an AWS Storage Gateway Volume Gateway in cached mode. Mount the volumes as iSCSI devices on the servers","correct":true},{"id":"6179b86bded76a4fa17bb52009d596e7","text":"Use an AWS Storage Gateway Volume Gateway in stored mode. Mount the volumes as iSCSI devices on the servers","correct":false},{"id":"f79405543a29779912fb22c1d8fd858f","text":"Deploy an Amazon Elastic File System file system and mount it via NFS","correct":false},{"id":"1510097be2cd1ce079dfc261617a8176","text":"Store the data on Amazon Elastic Block Store volumes. Mount the volumes as iSCSI devices through a VPC endpoint","correct":false}]},{"id":"5d0c33cd-3b30-46ae-937a-7ffe3350e8d0","domain":"automation","question":"Which of the following services is used to develop and deliver infrastructure as code?","explanation":"CloudFormation is used for Infrastructure-as-Code.","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html","title":"About CloudFormation"}],"answers":[{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":false},{"id":"58e3bfbabf904de43a6a22aca509b0d8","text":"CloudFormation","correct":true},{"id":"a907a7338c1fb3821fbe8ed113c64b33","text":"CloudHSM","correct":false},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false}]},{"id":"6397b5f5-49ec-41f3-82da-1cac6fd6128e","domain":"dep-prov","question":"You are a SysOps engineer with a large self-help company.  Your website has just launched for your Yoga programme \"Yoga: Noob to Guru\" which your company is very excited about.  The application is deployed to web servers in an autoscaling group which are behind an AWS Application Load Balancer.  Your team is proud of the work they have done in just a short amount of time.  Unfortunately today, on the day of launch, your CEO tried the website and received an error: 504 Gateway Timeout.  What is the likely cause of this error and how should you proceed??","explanation":"The 5xx error codes indicate a problem on the server-side of a web request.  Gateway Timeout could indicate an issue between the load balancer which is serving the request to the CEO, and the web servers which the load balancer is trying to forward requests to.","links":[{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html","title":"Troubleshooting Your Application Load Balancer"}],"answers":[{"id":"4e94dcfe6494fd181c00b19e94d6701a","text":"The CEO entered the incorrect URL and the page was not found, he should try the correct URL","correct":false},{"id":"27529e162c1316a6e7403e428c6ee8a5","text":"Your website's SSL certificate is expired and needs to be replaced","correct":false},{"id":"1243ac6ffa178c8f5deab05f9499b796","text":"Your CEO has encountered an HTCPCP error, he should make a cup of coffee and try later","correct":false},{"id":"495fdf22664eed25be9c8e2d3631d9fd","text":"There is a problem between your Load Balancer and the web servers, your engineering team need to check the health of the web servers","correct":true}]},{"id":"d0b24152-771d-489e-8853-7221454dbcdb","domain":"networking","question":"You are running a Python Flask application on a private subnet in your Amazon VPC. The web application has been running into some issues, and you need to re-code the application to include more robust Python libraries. When you try to install Python libraries you get a network error but you are able to install these packages on your public-facing servers. How would you allow your private servers to download and install these libraries in the most effective manner?","explanation":"To enable instances in a private subnet to connect to the internet, you can create a NAT gateway or launch a NAT instance in a public subnet. Then add a route for the private subnet that routes IPv4 internet traffic (0.0.0.0/0) to the NAT device. To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. Copying files in an EBS volume for installation is administratively not a good idea if Internet connectivity is going to be persistent. A Bastion Host is not the required service to connect to the Internet.","links":[{"url":"https://docs.aws.amazon.com/en_pv/vpc/latest/userguide/route-table-options.html","title":"Routing Options"}],"answers":[{"id":"cf4856324ef6b157b83f98f140d49f7b","text":"Copy the Python libraries on your public servers unto an EBS volume. Detach the EBS volume and attach them to the private servers. Install the packages from the EBS volume.","correct":false},{"id":"24db8e12a44c9ec6662d67dca5687d5b","text":"Launch a NAT instance in a public subnet. Add a route for the private subnet that routes IPv4 0.0.0.0/0 traffic to the NAT device.","correct":true},{"id":"273a72b9d0bfd2145fb5f6c102cdae13","text":"Launch a NAT Gateway in a private subnet. Add a route for the web servers in the private subnet to 0.0.0.0/0 through the NAT device.","correct":false},{"id":"30d834ca94e17c6e2e40caf78391e8e9","text":"Launch a Bastion Host in a public server. Open port 80 on the Bastion Host to the IP range of your private web servers. Access downloadable libraries from your Bastion Host via SSH.","correct":false}]},{"id":"0e17eb91-9745-4365-8b54-5ebb2c6ffeb5","domain":"data-man","question":"A company wants to create a disaster recovery account involving creating snapshots of RDS, EC2 instances and EFS.  There are additional business and regulatory backup compliance requirements such as backups must be kept for three years but then must be destroyed.  Your manager wants to know how you could go about taking scheduled snapshots and deleting them once the retention period is expired with the lowest cost and operational overhead.","explanation":"AWS Backup is a centralised place to create backups of your EBS, RDS, and EFS resources.  There is no additional cost for setting up backup plans and retention policies, and this is a managed service so it's a perfect option to present to your manager.","links":[{"url":"https://aws.amazon.com/backup/","title":"AWS Backup"}],"answers":[{"id":"52b5e3e90161fce097eca53313efd955","text":"Use AWS Data Recovery Manager to create a storage vault and automated backup and retention rules.","correct":false},{"id":"d75279ac650753a76148e6ac6c1b380e","text":"Your team should write some Lambda functions which are triggered by CloudWatch Events on a cron expression.  Create another lambda to delete snapshots once they are expired.","correct":false},{"id":"fc24b51d19eedb53ab68e9e1569c9458","text":"Use AWS Backup to create a vault and a Backup Plan to take backups on a schedule and automatically delete them once expired.","correct":true},{"id":"8be1adb291f4f253ef46691652254d1f","text":"Browse the AWS Marketplace and purchase a backup tool which can run in your AWS account and perform the backup for you.","correct":false}]},{"id":"694ba690-219c-47cc-aca7-8e52a305016a","domain":"dep-prov","question":"Your 16TB gp2 EBS volume is experiencing I/O issues causing performance issues for the application. The Application Support team asks you if you can fix this without downtime, what do you recommend?","explanation":"At 16TB, the gp2 volume is already allocated the maximum 16,000 IOPS, in order to increase the I/O bandwidth, you will need to change to a Provisioned IOPS (io1) type volume type. You can modify the volume type without detaching the volume.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html","title":"EBS Volume Types"},{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modify-volume.html","title":"Modifying EBS Volumes"}],"answers":[{"id":"8e79d84c35c355fc300b424999da7727","text":"Migrate the data to S3","correct":false},{"id":"7fcf791ac787fb7fb7622591d6bfea93","text":"Modify the volume type to io1","correct":true},{"id":"3453d4afff7ffbc49232d037fd2b6819","text":"Detach the volume and re-attach an io1 volume","correct":false},{"id":"77a3dce9a889c55f548615174dca7769","text":"Increase the size of the volume which will increase the IOPS capability","correct":false}]},{"id":"8f2d3d61-092d-429c-a109-69ab00fa4065","domain":"mon-rep","question":"AWS Cost Management encompasses a number of services to help you to organize, control and optimize your AWS costs and usage.  Which of the following Cost Management related tools gives you the ability to set alerts when costs or usage are exceeded?","explanation":"The correct answer is AWS Budgets.  AWS Cost Explorer lets you visualize, understand, and manage your AWS costs and usage over time. AWS Cost & Usage Report lists AWS usage for each service category used by an account and its IAM users and finally, Reserved Instance Reporting provides a number of RI-specific cost management solutions to help you better understand and manage RI Utilization and Coverage.","links":[{"url":"https://aws.amazon.com/aws-cost-management/aws-budgets/","title":"AWS Budgets"}],"answers":[{"id":"824fd559c917b4ae56f36787b886eb81","text":"AWS Cost & Usage Report","correct":false},{"id":"c7f176d72688fd87853e31b84159d541","text":"AWS Cost Explorer","correct":false},{"id":"e32a801c8e0beab6abb9361e937365be","text":"AWS Budgets","correct":true},{"id":"eef79d956328d5e4ec426d448cc53c74","text":"Reserved Instance Reporting","correct":false}]},{"id":"17885db5-c61d-4edf-b0e3-e9d449d8e618","domain":"mon-rep","question":"Which of the following EC2 instance metrics are sent to Amazon CloudWatch by default? Select three.","explanation":"CPU utilization, disk I/O and network traffic are visible to the hypervisor running the instance and are sent to CloudWatch by default. For the others, you would need to install CloudWatch Agent on the instance.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html","title":"Available CloudWatch Metrics for Your Instances"}],"answers":[{"id":"b4e5bb2b6842990e919682b3d6d5726c","text":"Volume of incoming and outgoing network traffic","correct":true},{"id":"fb8326e1edbd06b1bf6ea0332e089055","text":"CPU utilization","correct":true},{"id":"c4903df1e41e0ba0b4636e753d8c7661","text":"Disk read and write operations","correct":true},{"id":"613b1188dd73dfdb768f39cfad3cc9a3","text":"Memory utilization","correct":false},{"id":"2105454033539f83d3b07265aac88d7a","text":"The amount of swap space currently in use","correct":false},{"id":"ec1e54ae04652319df5c011f228c07ac","text":"Free disk space","correct":false}]},{"id":"9de05324-9d1a-4252-a6ed-c8bc6e732afe","domain":"dep-prov","question":"You use a launch template to create an Auto Scaling group for an application. You need to ensure a number of EC2 instances are always online to meet minimum capacity requirements and also use lower priced instances when scaling up. Which of the following instance combinations would be the most cost effective solution to use in the Auto Scaling group?","explanation":"A combination of on-demand instances and spot instances should be launched to meet the target capacity that you specified in the spot fleet request. The on-demand instances ensure the minimum capacities are met and the spot instances provide extra capacity as the application scales up. The other options without the spot instances are not cost-efficient. And scheduled reserved instances do not run continuously and should not be selected.","links":[{"url":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html","title":"How spot fleet works"}],"answers":[{"id":"853ea21196f8eeabab96ee1faf8cb4bf","text":"On-demand instances + spot instances","correct":true},{"id":"eeea48d3cb7324682811fe139fd30093","text":"Scheduled reserved instances + spot instances","correct":false},{"id":"8f81bfbb0090b9ecba50b69be972ec7b","text":"Dedicated instances + on-demand instances","correct":false},{"id":"da8d4ca080444c6f36c71ed68a3c9a8f","text":"Reserved instances + on-demand instances","correct":false}]},{"id":"95b5fb6b-4742-4c0a-8563-4dc9dc116700","domain":"mon-rep","question":"You have been asked to monitor custom metrics generated by your own applications and services. There is a need for these metrics to be collated and returned to the product owner each week so they can plan for the following sprint's requirements. What services would you use to be able to provide this automated report?","explanation":"You can set up monitoring of custom metrics in CloudWatch, from this you can use CloudWatch also to trigger a Lambda Function at a scheduled time, which will collect use AWS SDK to pull the metrics into a report that will be sent via email using the SNS service","links":[{"url":"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html","title":"Publishing Custom Metrics"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html","title":"Using AWS Lambda with Amazon CloudWatch Events"},{"url":"https://docs.aws.amazon.com/lambda/latest/dg/with-sns-example.html","title":"Tutorial: Using AWS Lambda with Amazon Simple Notification Service"}],"answers":[{"id":"04a7da3c5b04cad85da1eebb92315b8b","text":"Lambda","correct":true},{"id":"54afc4697cf03d8e3ec9a05b16380622","text":"SNS","correct":true},{"id":"311bdda432aba736b8dcb987523c0c92","text":"CloudWatch","correct":true},{"id":"92fbbd5478621cf8f70624389759b44c","text":"CloudTrail","correct":false},{"id":"f79368a88745f4dba5e0fc92aa545c61","text":"EKS","correct":false}]},{"id":"44323aa9-db88-4c8e-8594-5af3edab91b8","domain":"networking","question":"You're consulting for a consumer electronics company that markets its products globally. A new customer-facing application will be deployed in seven AWS Regions worldwide. Business logic will be handled by microservices deployed on EC2 instances in each region. The data layer will be hosted on Amazon Aurora in a single AWS Region. Which architecture will provide the highest performing solution for end users?","explanation":"A Route 53 latency routing policy will send requests to the destination with the lowest latency, generally resulting in the best performance. A Route 53 geolocation routing policy will probably not provide better performance than a latency routing policy. Even though targets may be physically closer, they may involve more network hops. Geolocation policies are generally used to serve localized content. Application Load Balancers work well for microservice architectures since targets can be registered as a specific port on an EC2 instance. CloudFront path patterns are for routing different file types, not for distinguishing origins in different regions.","links":[{"url":"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html","title":"Choosing a Routing Policy"},{"url":"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html","title":"What Is an Application Load Balancer?"}],"answers":[{"id":"9a5a8c536cd4c0209770d8e9b9897abd","text":"Create an Amazon Route 53 record set for the application with a latency routing policy. Deploy an ELB Application Load Balancer in front of the EC2 instances in each region","correct":true},{"id":"562b2939dfd8d3940e82593f75d73725","text":"Implement an Amazon Route 53 record set for the application with a geolocation routing policy. Use an ELB Network Load Balancer in front of the EC2 instances in each region","correct":false},{"id":"915fca3a9a0bf0eaf51de3fe684ef9e0","text":"Configure an Amazon Route 53 record set for the application with a geolocation routing policy. Implement an ELB Application Load Balancer in front of the EC2 instances in each region","correct":false},{"id":"8f8597399845c09e57ce08b6791b3ab0","text":"Deploy the EC2 instances behind an ELB Network Load Balancer in each region and set each one up as an Amazon CloudFront origin. Create path patterns to route all requests to the load balancer in the desired region","correct":false}]},{"id":"64ee23de-9703-4090-983d-d36552dac361","domain":"networking","question":"You are a SysOps Administrator running security checks throughout your AWS environment. One of your tasks is to clean up the environment and remove any idle resources that are no longer in use. You identify a VPC that was configured and used by a team that no longer works at the company and you are looking to delete the VPC. The VPC has a few running instances, a route table, a NAT Gateway, and an Internet Gateway. When you try to delete the VPC you get an error. How would you troubleshoot this situation?","explanation":"In AWS, you will get the following error if you attempt to delete the VPC with a network interface in-use: 'The VPC contains one or more in-use network interfaces, and cannot be deleted until those network interfaces have been deleted. View in-use network interfaces in the VPC.' Moreover, you cannot delete a subnet that has instances in it. The best answer would be to terminate the instances before deleting the VPC. There is no need to take snapshots before deleting a VPC (unless for backup purpose), and detaching the Internet Gateway is unnecessary as well. Assuming the role of the VPC creator is unnecessary if you already have the proper IAM permissions to do so.","links":[{"url":"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html","title":"VPCs and Subnets"}],"answers":[{"id":"64ab90b44ef9b90a49072f93bb50a615","text":"Assume the role of the creator of the VPC. The credentials for the VPCs creator is required to delete the VPC.","correct":false},{"id":"dc869ce95aa68ff14f2265e56e10d33d","text":"Stop and terminate the running instances. Delete all the resources in the VPC before deleting the VPC itself.","correct":true},{"id":"bdba6bd2f5a22439d78f3ae5512c9213","text":"Detach the Internet Gateway from the VPC. Delete the Internet Gateway to restrict public traffic into the VPC. Delete the VPC.","correct":false},{"id":"4fd6f1d4acf6323fa52621653e2a6150","text":"Take a snapshot of the EBS volumes before deleting the VPC. Upload the snapshots into S3 and proceed deleting the VPC.","correct":false}]},{"id":"7432af70-890b-411b-abbb-6b0b403aa3f5","domain":"mon-rep","question":"You're an ops manager and you want one of your AWS accounts to receive the price breaks associated with Consolidated Billing for Organizations. What needs to happen for you to be included in the Organization and receive the discounted pricing?","explanation":"You can invite existing AWS accounts to join your organization. When you start this process, AWS Organizations sends an invitation to the account owner, who then decides whether to accept or decline the invitation. You can use the AWS Organizations console to initiate and manage invitations that you send to other accounts. You can send an invitation to another account only from the master account of your organization.","links":[{"url":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html","title":"Inviting an AWS Account to Join Your Organization"}],"answers":[{"id":"8a358ee9ae18997a0c91f0541fad6f77","text":"You must request to be added to consolidated billing.","correct":false},{"id":"5f46574bd37f7ef74f5a95c1debbad29","text":"The Master account must ask AWS to associate your account with theirs.","correct":false},{"id":"3b0f522229b67ca4ea9eb21840d96e41","text":"The Master account must send you an invitation to join your account with theirs and you must accept.","correct":true},{"id":"8cacf7f83f0c9a58c2e264de726c764f","text":"You must ask AWS to associate your account with the Master account.","correct":false}]},{"id":"8473f19c-c3f7-4f2b-93a5-55ed10d99f83","domain":"mon-rep","question":"You run a hybrid environment with some servers in AWS and other Servers on Premise. Your boss has been impressed with your CloudWatch dashboard which shows the performance of all your EC2 instances around the world, however it does not show any metrics for your on premise servers. What could you do to rectify this?","explanation":"AWS Systems Manager Agent (SSM Agent) is Amazon software that runs on your Amazon EC2 instances and your hybrid instances that are configured for Systems Manager (hybrid instances). You can manually install SSM Agent on servers or virtual machines in your on-premises environment","links":[{"url":"https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html","title":"SSM Agent"}],"answers":[{"id":"2b914a41bdba2bd7ceb93d6306696cd4","text":"Use AWS Storage Gateway and configure Gateway Monitoring Mode. Publish the monitoring statistics to S3 and use AWS Datapipeline to import this data in to CloudWatch","correct":false},{"id":"02a9311d5b1ac89de02acaf3c528c90a","text":"Install a third party monitoring agent and export performance data to the third party platform. Use AWS datapipeline to create a pipeline of this data from the third parties platform, in to CloudWatch","correct":false},{"id":"5aa17ac5ff5e522d0cbd19818e8b854d","text":"It is not yet possible to monitor on premise servers using AWS CloudWatch","correct":false},{"id":"38763eaec7d5f8126a56abeb236b9d27","text":"Install and run the SSM agent on your on premise servers. Once finished, install and run the CloudWatch agent on the on premise servers. Create the required role in IAM and verify that the agent is publishing data to CloudWatch. Add the widget for the on premise tools","correct":true}]},{"id":"ba52dee8-0d6c-4faf-9121-0e64c18bbf1a","domain":"high-avail","question":"Your website is evenly distributed across 10 EC2 instances in 5 AWS regions. How could you configure your site to maintain high-availability with minimum downtime if one of the 5 regions was to lose network connectivity for an extended period of time?","explanation":"If you are designing to check for loss of contact with the instances you need to use \"Evaluate Target Health\" to confirm connectivity.  The Latency policy will eventually detect the unavailability; however it is not a real-time test.","links":[{"url":"http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html","title":"How Health Checks Work in Complex Amazon Route 53 Configurations"}],"answers":[{"id":"90875d19db265ba7b3931ecbd5a5d813","text":"Establish VPN Connections between the instances in each region. Rely on BGP to failover in the case of a region-wide connectivity outage.","correct":false},{"id":"dc1f9d897e82bca789bb2983fa7ce22d","text":"Create a Route 53 Latency-based Routing Record Set that resolves to an Elastic Load Balancer in each region. Set an appropriate health check on each ELB.","correct":false},{"id":"b5494c49d052d62119e11eab7d8499c7","text":"Create an Elastic Load Balancer to place in front of each EC2 instance. Set an appropriate health check on each ELB.","correct":false},{"id":"0e2faa25f8eb6371b513d8d442513b10","text":"Create a Route 53 Latency-based Routing Record Set that resolves to Elastic Load Balancers in each region and has the Evaluate Target Health flag set to \"True\".","correct":true}]},{"id":"f94237f9-9b2f-48f3-9a0a-cd4a8350cead","domain":"networking","question":"Your department has made a decision to migrate a number of applications to AWS to reduce operational costs. These applications will need connectivity from your corporate network. Multiple AWS accounts and VPCs within accounts are needed. You currently contract with a single carrier for WAN services, and this carrier is an AWS Direct Connect Partner. Your manager has tasked you with creating a highly reliable networking solution between AWS and the corporate network. Which architecture will provide a highly reliable solution with the best cost efficiency?","explanation":"A single Direct Connect with VPN backup provides highly reliable connectivity between a corporate network and AWS at a lower cost than deploying two Direct Connects. Direct Connect Gateway is not required to achieve cross-account connectivity. Public Virtual Interfaces will not provide connectivity to VPCs. Private Virtual Interfaces are used for that.","links":[{"url":"https://aws.amazon.com/directconnect/","title":"AWS Direct Connect"},{"url":"https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/","title":"Multiple Data Center HA Network Connectivity"}],"answers":[{"id":"e406e073341c09e80aff0244163394af","text":"Implement two AWS Direct Connects through your current carrier to a single Direct Connect location to be provisioned on redundant Amazon routers. Create Private Virtual Interfaces across the different accounts and VPCs.","correct":false},{"id":"49ba7d5b47cab8f1b382c99edd2f513a","text":"Deploy a single AWS Direct Connect through your current carrier. Monitor the Direct Connect connection with Amazon CloudWatch and invoke AWS Lambda to failover traffic to an IPSec VPN tunnel if there are any issues.","correct":true},{"id":"d99e60f7ff5145b088e827b533122d02","text":"Employ a single AWS Direct Connect through your current carrier. Implement a second AWS Direct Connect through another Direct Connect Partner to another Direct Connect location in the same AWS region. Use Direct Connect Gateway to bridge across the multiple accounts.","correct":false},{"id":"d4025e698a2432c8861799040385c885","text":"Employ a single AWS Direct Connect through your current carrier. Implement a second AWS Direct Connect through another Direct Connect Partner to the same Direct Connect location. Create Public Virtual Interfaces across the different accounts and VPCs.","correct":false}]},{"id":"56ce4f08-8bea-4b3b-b67d-e5f36af188bd","domain":"security-comp","question":"When using Web Identity Federation and Cognito to allow a user to access an AWS service (such as an S3 bucket), which of the following is the correct order of steps?","explanation":"A user authenticates with Facebook first. They are then given an ID token by Facebook. An API call, AssumeRoleWithWebIdentity, is then used in conjunction with the ID token. A user is then granted temporary security credentials.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_manual.html","title":"Using Web Identity Federation APIs"}],"answers":[{"id":"811872e18604a3d8118cc1c9381bf702","text":"Users cannot use Facebook credentials to access the AWS platform.","correct":false},{"id":"b66b1e5f0bf94eca60bce37aa0702f6e","text":"A user logs in to the AWS platform using their Facebook credentials. AWS authenticates with Facebook to check the credentials. Temporary Security Access is granted to AWS.","correct":false},{"id":"e95a8dc9c210c0c1e6e36f8215ab4978","text":"A user makes the AssumeRoleWithWebIdentity API Call. The user is then redirected to Facebook to authenticate. Once authenticated, the user is given an ID token. The user is then granted temporary access to the AWS platform.","correct":false},{"id":"13bdd19ca71c9978a4642bd95fcb92c5","text":"A user authenticates with Facebook first. They are then given an ID token by Facebook. An API call, AssumeRoleWithWebIdentity, is then used in conjunction with the ID token. A user is then granted temporary security credentials.","correct":true}]},{"id":"95096463-bf44-4a55-a938-134e17096ace","domain":"security-comp","question":"Instance 'A' and instance 'B' are running in two different subnets 'A' and 'B' of a VPC. Instance 'A' is not able to ping instance 'B'. Which of the following is a possible reason for this failure?","explanation":"The NACL on subnet B does not allow outbound ICMP traffic; and the security group attached to instance B does not allow inbound ICMP traffic.","links":[{"url":"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html#ACLs","title":"Network ACL Basics"}],"answers":[{"id":"788e0ab5d1b2c372b5d80c26c17c6a71","text":"The NACL on subnet B does not allow outbound ICMP traffic; and the security group attached to instance B does not allow inbound ICMP traffic.","correct":true},{"id":"87877573324ed4201e723046eb1d8409","text":"The security group attached to instance 'B' does not allow inbound ICMP traffic; the policy linked to the IAM role on instance 'A' is not configured correctly.","correct":false},{"id":"48173a79d09d626e62e08c080f3dafcb","text":"The route table of subnet 'A' has no target route to subnet 'B'; and the security group attached to instance B does not allow inbound ICMP traffic.","correct":false},{"id":"b683f2644731441134b6c0db51d38f6e","text":"The policy linked to the IAM role on instance 'A' is not configured correctly; and the NACL on subnet 'B' does not allow outbound ICMP traffic.","correct":false}]},{"id":"4e911348-5056-4ece-a91d-ce96b619578f","domain":"automation","question":"The DevOps team of an insurance company has been instructed to use CloudFormation to manage the different environments of the company. Due to the size of the templates prepared exceeding the limit, the CloudFormation service rejected the processing of the template. How can the DevOps team resolve this issue?","explanation":"Due to the size of the templates exceeding the limit, dividing the CloudFormation template into smaller subparts is the solution. With this in mind, CloudFormation nested stacks will yield to the same template behavior but will involve different files","links":[{"url":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html","title":"Using CloudFormation Nested Stacks"}],"answers":[{"id":"d4838aa62aeb5717c4100906a75dea2d","text":"Use CloudFormation nested stacks","correct":true},{"id":"f163f3aff96f7db10bfa4b93ed9a0a67","text":"Use CloudFormation wait handlers","correct":false},{"id":"e29c7f5b3439e3e8355a1e6bede51fa1","text":"Minify the CloudFormation template","correct":false},{"id":"1d75dba4b52318bdc075c815a7690e4c","text":"Use CloudFormation custom resources","correct":false}]},{"id":"723a6cb2-65a0-454f-b391-8647401fa54d","domain":"networking","question":"A photo sharing application is growing in popularity.  The application uses S3 to store photographs. Your boss has asked you how you can improve the upload and download times for your end users?","explanation":"S3 upload and download times can be improved by enabling Transfer Acceleration. This leverages Points of Presents (PoPs) in the CloudFront network to provide connection points closer to users, thereby improving transfer speeds.  Disabling Object Versioning or enabling Intelligent Tiering would not affect upload or transfer speeds.  Lastly, BitTorrent support is for publicly-available files and is designed to increase availability of files and reduce S3 costs but does not improve upload and download times from the S3 service itself.","links":[{"url":"https://docs.aws.amazon.com/en_pv/AmazonS3/latest/dev/transfer-acceleration.html","title":"Amazon S3 Transfer Acceleration"}],"answers":[{"id":"792c81205244e2382feec0be4a8a8716","text":"Enable S3 BitTorrent support","correct":false},{"id":"3992a5af710f81ade0226fff00fedf2d","text":"Enable S3 Intelligent Tiering","correct":false},{"id":"26b2f3dfb7078bf49a2aafd952d45a13","text":"Disable S3 Object Versioning","correct":false},{"id":"5c6679012efaeb73c2123036ba676303","text":"Enable S3 Transfer Acceleration on your buckets","correct":true}]},{"id":"2ed81e01-78af-4ca1-8cc9-2ecfeeff8984","domain":"data-man","question":"Which of the following is not a use case for read replicas?","explanation":"Providing greater redundancy via automatic failovers is not a use case for read replicas. They're not useful in this case.","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.Overview","title":"Overview: Read Replicas"}],"answers":[{"id":"82d85c0b4a19e372bf779bb4908cd64a","text":"Business reporting or data warehousing scenarios; you may want business reporting queries to run against a read replica, rather than your primary DB Instance.","correct":false},{"id":"865c6a9aca74af0e2c37ab4e50ca6013","text":"Providing greater redundancy via automatic failovers.","correct":true},{"id":"5224fa97bf0d7d77ee80e90a695f1e40","text":"Serving read traffic while the source DB instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your read replicas.","correct":false},{"id":"61cb6fc4fb9c00d23b126d6e5c0d8905","text":"Scaling beyond the compute or I/O capacity of a single DB Instance for read-heavy database workloads. This excess read traffic can be directed to one or more read replicas.","correct":false}]},{"id":"46293632-1540-45c7-91df-7b6815a27847","domain":"security-comp","question":"Which of the following names is not a valid IAM role name?","explanation":"Names of users, groups, roles, policies, instance profiles, and server certificates must be alphanumeric, including the following common characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). CompanyMarketing#Role is therefore not an acceptable role name.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-limits.html","title":"Limitations on IAM Entities and Objects"}],"answers":[{"id":"a1b18d40d2aedd805efd8bc59f1d6969","text":"Company,Marketing,Role","correct":false},{"id":"7ab3a687b4689245604f8981502bdf09","text":"Company--Marketing+Role","correct":false},{"id":"e414a871abd69eafb829412a3a873a36","text":"CompanyMarketing#Role","correct":true},{"id":"dc6e80f51a713e21ec697bead75c1667","text":"Company.Marketing_Role","correct":false},{"id":"d8c8f0893b0743d016015e108e9d2ab9","text":"MarketingRole@Company","correct":false}]},{"id":"a56dcc4d-97b3-43f7-94b7-5daeeebefe20","domain":"security-comp","question":"What type of policies can grant permissions to IAM users? Select two.","explanation":"IAM policies and S3 bucket policies can grant IAM users permissions. SCPs, VPC endpoint policies and permission boundaries can be used to limit those permissions with allow and deny rules but they won't grant the permissions themselves.","links":[{"url":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html","title":"Policy Evaluation Logic"}],"answers":[{"id":"4e1f839391e849a6065024e60ace4c38","text":"IAM policy","correct":true},{"id":"701ae44da9c1b1b7f53753b480acd1b8","text":"Permissions boundary","correct":false},{"id":"98850da8d6a8c95f98551976f45f5b51","text":"AWS Organizations service control policy (SCP)","correct":false},{"id":"e87c75c0e57d922b4c9578483c661f61","text":"S3 bucket policy","correct":true},{"id":"326da76e6202b801f1b6ea68e76c422c","text":"VPC endpoint policy","correct":false}]},{"id":"762031fb-db4e-4e09-bab4-09bf0fc50472","domain":"high-avail","question":"You run a very popular fashion blog and during a major event your wordpress site struggles immensely with the amount of traffic that you are receiving. The wordpress site sits across a fleet of EC2 instances in an autoscaling group which scales based on CPU utilization. You notice from your CloudWatch metrics that your webservers appear fine, however your back end Aurora database is running at 100% CPU Utilization. What can you do to alleviate the situation?","explanation":"Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability","links":[{"url":"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html","title":"Aurora Read Replicas"}],"answers":[{"id":"98a15d02b2c6ead5dff3036e99269acd","text":"Place the Aurora database in to a the same Autoscaling group as the EC2 instances and configure the launch configuration to deploy new Aurora instances whenever a node capacity reaches 80% for 5 minutes","correct":false},{"id":"10239931e046b099c14736b5b9c422d0","text":" Place the Aurora database in to a separate Autoscaling group and configure the launch configuration to deploy new Aurora instances whenever a node capacity reaches 80% for 5 minutes","correct":false},{"id":"e38c819297b357216e3ddc63cecd22ec","text":"Migrate from Aurora to MySQL RDS instance with multi-AZ turned on to better handle the load","correct":false},{"id":"f663aa878eb8286dbadf97aa0f95be56","text":"Add additional Aurora Read Replica Nodes and update your connection string on the webservers to point to the new nodes to spread the load","correct":true}]}]}}}}
